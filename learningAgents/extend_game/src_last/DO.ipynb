{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2905d491-8924-44ef-9521-e76a35dce019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# added SPE to the base strategies at first, then deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dea34fbb-ece9-4c29-9b1b-ef758f09668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from stable_baselines3 import SAC, PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "import time\n",
    "import sys\n",
    "from src.environments import ConPricingGame\n",
    "import src.globals as gl\n",
    "import src.classes as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ca76fb0-453a-409b-ad83-b6d4196f62b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Iter_row:\n",
    "    def __init__(self, adv, agent_return, adv_return, rewards, adv_rewards, actions, prices, adv_prices, demands, adv_demands):\n",
    "        self.adv = adv\n",
    "        self.agent_return = agent_return\n",
    "        self.adv_return = adv_return\n",
    "        self.rewards = rewards\n",
    "        self.adv_rewards = adv_rewards\n",
    "        self.actions = actions\n",
    "        self.prices = prices\n",
    "        self.adv_prices = adv_prices\n",
    "        self.demands = demands\n",
    "        self.adv_demands = adv_demands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd831764-040c-4580-a8a0-6fc9986b3b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_base_agent(db,alg, memory, cost, strategies, strategy_probs):\n",
    "    \"\"\" the startegies should be the same class of agents as we are training. if low cost then low-cost strategies should be given to find similar ones. The trained agents that are not even added will be considered \"\"\"\n",
    "    strats=strategies.copy()\n",
    "    probs=strategy_probs.copy()\n",
    "    for i in range(len(probs)-1):\n",
    "        for j in range(i+1,len(probs)):\n",
    "            if probs[i]< probs[j]:\n",
    "                strats[i],strats[j] = strats[j], strats[i]\n",
    "                probs[i],probs[j] = probs[j], probs[i]\n",
    "    for st in strats:\n",
    "        if st.type==cl.StrategyType.sb3_model and  memory == st.memory and (alg is st.model) :\n",
    "            return st.name\n",
    "    query=f'SELECT name FROM {cl.DataBase.AGENTS_TABLE} WHERE cost={cost} and memory={memory} and alg=\\\"{str(alg)}\\\" ORDER BY id DESC'\n",
    "    db.cursor.execute(query)\n",
    "    tmp= db.cursor.fetchone()\n",
    "    if tmp is not None:\n",
    "        return tmp[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "286f8903-6ec8-4c8e-8868-a7da4d67bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(db,base_agent, env_class, costs, adv_mixed_strategy, target_payoff, num_procs, alg, lr, memory):\n",
    "    \"\"\"\n",
    "    trains an agent against adversaries. if the expected payoff of new agent is greater than expected payoff of NE, \\\n",
    "        returns acceptable=true and the new strategy and payoff to be added to the the strategies and matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    acceptable = False\n",
    "\n",
    "    pricing_game = env_class(\n",
    "        tuple_costs=costs, adversary_mixed_strategy=adv_mixed_strategy, memory=memory)\n",
    "\n",
    "    seed = int(time.time())\n",
    "\n",
    "    model_name = f\"{job_name}-{str(seed)}\"\n",
    "    models_dir = f\"{gl.MODELS_DIR}/{model_name}\"\n",
    "\n",
    "    log_dir = f\"{gl.LOG_DIR}/{model_name}\"\n",
    "\n",
    "    # if not os.path.exists(models_dir):\n",
    "    #     os.makedirs(models_dir)\n",
    "\n",
    "    # if not os.path.exists(log_dir):\n",
    "    #     os.makedirs(log_dir)\n",
    "\n",
    "    # number_episodes = gl.NUM_EPISODES + gl.EPISODE_ADV_INCREASE * \\\n",
    "    #     (adv_mixed_strategy.support_size-1)\n",
    "    # train_env = make_vec_env(env_class, n_envs=num_procs, seed=seed, vec_env_cls=SubprocVecEnv, env_kwargs=dict(\n",
    "    #     tuple_costs=costs, adversary_mixed_strategy=adv_mixed_strategy, memory=memory))\n",
    "\n",
    "    train_env = env_class(tuple_costs=costs, adversary_mixed_strategy=adv_mixed_strategy, memory=memory)\n",
    "    seed=0\n",
    "    if base_agent is None:\n",
    "        number_episodes = gl.N_EPISODES_BASE * (1 + gl.EPISODE_INCREASE_PORTION * (adv_mixed_strategy.support_size-1))\n",
    "        if alg is SAC:\n",
    "            model = alg('MlpPolicy', train_env, learning_rate=lr,\n",
    "                        verbose=0, tensorboard_log=log_dir, gamma=gl.GAMMA, target_entropy=0)\n",
    "        else:\n",
    "            model = alg('MlpPolicy', train_env, learning_rate=lr,\n",
    "                        verbose=0, tensorboard_log=log_dir, gamma=gl.GAMMA)\n",
    "    else:\n",
    "        number_episodes = gl.N_EPISODES_LOAD * (1 + gl.EPISODE_INCREASE_PORTION * (adv_mixed_strategy.support_size-1))\n",
    "        base_agent_dir = f\"{gl.MODELS_DIR}/{base_agent}\"\n",
    "        if alg is SAC:\n",
    "            model = alg.load(base_agent_dir, train_env, learning_rate=lr,\n",
    "                             verbose=0, tensorboard_log=log_dir, gamma=gl.GAMMA, target_entropy=0)\n",
    "        else:\n",
    "            model = alg.load(base_agent_dir, train_env, learning_rate=lr,\n",
    "                             verbose=0, tensorboard_log=log_dir, gamma=gl.GAMMA)\n",
    "\n",
    "    start = time.time()\n",
    "    # for i in range(gl.NUM_MODEL_SAVE):\n",
    "    # tmp = (number_episodes/gl.NUM_MODEL_SAVE)\n",
    "    # model.learn(total_timesteps=tmp, reset_num_timesteps=False,\n",
    "    #             tb_log_name=model_name)\n",
    "    # model.save(os.path.join(models_dir, str(tmp*(i+1))))\n",
    "    model.learn(total_timesteps=number_episodes, tb_log_name=model_name)\n",
    "    model.save(models_dir)\n",
    "    running_time = time.time() - start\n",
    "\n",
    "    agent_payoffs = np.zeros(len(adv_mixed_strategy.strategies))\n",
    "    adv_payoffs = np.zeros(len(adv_mixed_strategy.strategies))\n",
    "    expected_payoff = 0\n",
    "\n",
    "    model_strategy = cl.Strategy(strategy_type=cl.StrategyType.sb3_model,\n",
    "                                 model_or_func=alg, name=model_name, action_step=pricing_game.action_step,memory=memory)\n",
    "    iter_rows = []\n",
    "    for strategy_index in range(len(adv_mixed_strategy.strategies)):\n",
    "        if adv_mixed_strategy.strategy_probs[strategy_index] > 0:\n",
    "            payoffs = []\n",
    "            for _ in range(gl.NUM_STOCHASTIC_ITER):\n",
    "                # returns = algorithm.play_trained_agent(adversary=(\n",
    "                #     (adv_mixed_strategy._strategies[strategy_index]).to_mixed_strategy()), iterNum=gl.num_stochastic_iter)\n",
    "                payoffs.append(model_strategy.play_against(\n",
    "                    env=pricing_game, adversary=adv_mixed_strategy.strategies[strategy_index]))\n",
    "                \n",
    "                #adv, agent_return, adv_return, rewards, adv_rewards, actions, prices, adv_prices, demands, adv_demands\n",
    "                iter_row = Iter_row(adv=pricing_game.adversary_strategy.name, agent_return=sum(pricing_game.profit[0]), adv_return=sum(pricing_game.profit[1]), rewards=str(\n",
    "                    pricing_game.profit[0]), adv_rewards=str(pricing_game.profit[1]), actions=str(pricing_game.actions),prices=str(pricing_game.prices[0]), adv_prices=str(pricing_game.prices[1]) ,demands=str(pricing_game.demand_potential[0]), adv_demands=str(pricing_game.demand_potential[1]))\n",
    "\n",
    "                iter_rows.append(iter_row)\n",
    "\n",
    "            mean_payoffs = np.array(payoffs).mean(axis=0)\n",
    "\n",
    "            agent_payoffs[strategy_index] = mean_payoffs[0]\n",
    "            adv_payoffs[strategy_index] = mean_payoffs[1]\n",
    "            expected_payoff += (agent_payoffs[strategy_index]) * \\\n",
    "                (adv_mixed_strategy.strategy_probs[strategy_index])\n",
    "\n",
    "    acceptable = (expected_payoff > target_payoff)\n",
    "    # agent_id=db.insert_new_agent(model_name,number_episodes,costs[0], str(adv_mixed_strategy), expected_payoff,target_payoff, lr,memory, acceptable, pricing_game.action_step, seed,num_procs,running_time)\n",
    "    agent_id = db.insert_new_agent(db.AgentRow(model_name, base_agent, number_episodes, costs[0], str(\n",
    "        adv_mixed_strategy), expected_payoff, target_payoff,  str(alg),lr, memory, acceptable, pricing_game.action_step, seed, num_procs, running_time))\n",
    "\n",
    "    if expected_payoff > target_payoff:\n",
    "        acceptable = True\n",
    "        for row in iter_rows:\n",
    "            db.insert_new_iteration(agent_id, row.adv, row.agent_return, row.adv_return, row.rewards,\n",
    "                                    row.adv_rewards, row.actions, row.prices, row.adv_prices, row.demands, row.adv_demands)\n",
    "        # compute the payoff against all adv strategies, to be added to the matrix\n",
    "        for strategy_index in range(len(adv_mixed_strategy.strategies)):\n",
    "            if adv_mixed_strategy.strategy_probs[strategy_index] == 0:\n",
    "                payoffs = []\n",
    "                for _ in range(gl.NUM_STOCHASTIC_ITER):\n",
    "                    payoffs.append(model_strategy.play_against(\n",
    "                        env=pricing_game, adversary=adv_mixed_strategy.strategies[strategy_index]))\n",
    "                mean_payoffs = np.array(payoffs).mean(axis=0)\n",
    "\n",
    "                agent_payoffs[strategy_index] = mean_payoffs[0]\n",
    "                adv_payoffs[strategy_index] = mean_payoffs[1]\n",
    "\n",
    "    return [acceptable, agent_payoffs, adv_payoffs, model_strategy, expected_payoff]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7392fe9-84d2-467d-b610-3936ca16878e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training low-cost player with base=rnd_start ,alg=<class 'stable_baselines3.sac.sac.SAC'>, lr=0.0003, memory=12\n",
      "training low-cost player with base=None ,alg=<class 'stable_baselines3.sac.sac.SAC'>, lr=0.0003, memory=18\n",
      "training low-cost player with base=rnd_start ,alg=<class 'stable_baselines3.sac.sac.SAC'>, lr=0.0002, memory=12\n",
      "training low-cost player with base=rnd_Feb5-1707144365 ,alg=<class 'stable_baselines3.sac.sac.SAC'>, lr=0.0002, memory=18\n",
      "training low-cost player with base=None ,alg=<class 'stable_baselines3.ppo.ppo.PPO'>, lr=0.0003, memory=12\n",
      "training low-cost player with base=None ,alg=<class 'stable_baselines3.ppo.ppo.PPO'>, lr=0.0003, memory=18\n",
      "training low-cost player with base=rnd_Feb5-1707205022 ,alg=<class 'stable_baselines3.ppo.ppo.PPO'>, lr=0.0002, memory=12\n",
      "training low-cost player with base=rnd_Feb5-1707206605 ,alg=<class 'stable_baselines3.ppo.ppo.PPO'>, lr=0.0002, memory=18\n",
      "training high-cost player with base=rnd_start ,alg=<class 'stable_baselines3.sac.sac.SAC'>, lr=0.0003, memory=12\n",
      "training high-cost player with base=None ,alg=<class 'stable_baselines3.sac.sac.SAC'>, lr=0.0003, memory=18\n",
      "training high-cost player with base=rnd_start ,alg=<class 'stable_baselines3.sac.sac.SAC'>, lr=0.0002, memory=12\n",
      "training high-cost player with base=rnd_Feb5-1707221671 ,alg=<class 'stable_baselines3.sac.sac.SAC'>, lr=0.0002, memory=18\n",
      "training high-cost player with base=None ,alg=<class 'stable_baselines3.ppo.ppo.PPO'>, lr=0.0003, memory=12\n",
      "training high-cost player with base=None ,alg=<class 'stable_baselines3.ppo.ppo.PPO'>, lr=0.0003, memory=18\n",
      "training high-cost player with base=rnd_Feb5-1707278881 ,alg=<class 'stable_baselines3.ppo.ppo.PPO'>, lr=0.0002, memory=12\n",
      "training high-cost player with base=rnd_Feb5-1707280464 ,alg=<class 'stable_baselines3.ppo.ppo.PPO'>, lr=0.0002, memory=18\n",
      "training low-cost player with base=rnd_Feb5-1707131275 ,alg=<class 'stable_baselines3.sac.sac.SAC'>, lr=0.0003, memory=12\n",
      "training low-cost player with base=rnd_Feb5-1707144365 ,alg=<class 'stable_baselines3.sac.sac.SAC'>, lr=0.0003, memory=18\n",
      "training low-cost player with base=rnd_Feb5-1707131275 ,alg=<class 'stable_baselines3.sac.sac.SAC'>, lr=0.0002, memory=12\n",
      "training low-cost player with base=rnd_Feb5-1707144365 ,alg=<class 'stable_baselines3.sac.sac.SAC'>, lr=0.0002, memory=18\n",
      "training low-cost player with base=rnd_Feb5-1707208214 ,alg=<class 'stable_baselines3.ppo.ppo.PPO'>, lr=0.0003, memory=12\n",
      "training low-cost player with base=rnd_Feb5-1707208848 ,alg=<class 'stable_baselines3.ppo.ppo.PPO'>, lr=0.0003, memory=18\n",
      "training low-cost player with base=rnd_Feb5-1707340780 ,alg=<class 'stable_baselines3.ppo.ppo.PPO'>, lr=0.0002, memory=12\n",
      "training low-cost player with base=rnd_Feb5-1707341412 ,alg=<class 'stable_baselines3.ppo.ppo.PPO'>, lr=0.0002, memory=18\n",
      "training high-cost player with base=rnd_Feb5-1707209491 ,alg=<class 'stable_baselines3.sac.sac.SAC'>, lr=0.0003, memory=12\n",
      "training high-cost player with base=rnd_Feb5-1707221671 ,alg=<class 'stable_baselines3.sac.sac.SAC'>, lr=0.0003, memory=18\n",
      "training high-cost player with base=rnd_Feb5-1707209491 ,alg=<class 'stable_baselines3.sac.sac.SAC'>, lr=0.0002, memory=12\n",
      "training high-cost player with base=rnd_Feb5-1707221671 ,alg=<class 'stable_baselines3.sac.sac.SAC'>, lr=0.0002, memory=18\n",
      "training high-cost player with base=rnd_Feb5-1707278881 ,alg=<class 'stable_baselines3.ppo.ppo.PPO'>, lr=0.0003, memory=12\n",
      "training high-cost player with base=rnd_Feb5-1707280464 ,alg=<class 'stable_baselines3.ppo.ppo.PPO'>, lr=0.0003, memory=18\n",
      "training high-cost player with base=rnd_Feb5-1707278881 ,alg=<class 'stable_baselines3.ppo.ppo.PPO'>, lr=0.0002, memory=12\n",
      "training high-cost player with base=rnd_Feb5-1707280464 ,alg=<class 'stable_baselines3.ppo.ppo.PPO'>, lr=0.0002, memory=18\n",
      "training low-cost player with base=rnd_Feb5-1707131275 ,alg=<class 'stable_baselines3.sac.sac.SAC'>, lr=0.0003, memory=12\n"
     ]
    }
   ],
   "source": [
    "env_class = ConPricingGame\n",
    "gl.initialize()\n",
    "\n",
    "num_rounds = 50\n",
    "\n",
    "job_name = \"rnd_Feb5\"\n",
    "db_name = job_name+\".db\"\n",
    "db = cl.DataBase(db_name)\n",
    "low_strts, high_strts=db.get_list_of_added_strategies()\n",
    "cl.set_job_name(job_name)\n",
    "# num_procs = gl.NUM_PROCESS if (len(sys.argv) < 2) else int(sys.argv[1])\n",
    "num_procs = 7\n",
    "\n",
    "\n",
    "\n",
    "# changing params\n",
    "lrs = [0.0003, 0.00016]\n",
    "memories = [12,18]\n",
    "# memories_agents=[[None]*len(memories)]*2\n",
    "algs = [SAC,PPO]\n",
    "\n",
    "equilibria = []\n",
    "\n",
    "cl.create_directories()\n",
    "\n",
    "# strt1 = cl.Strategy(\n",
    "#     cl.StrategyType.static, model_or_func=cl.myopic, name=\"myopic\")\n",
    "# strt2 = cl.Strategy(\n",
    "#     cl.StrategyType.static, model_or_func=cl.const, name=\"const\", first_price=132)\n",
    "# strt3 = cl.Strategy(\n",
    "#     cl.StrategyType.static, model_or_func=cl.guess, name=\"guess\", first_price=132)\n",
    "# strt4 = cl.Strategy(\n",
    "#     cl.StrategyType.static, model_or_func=cl.spe, name=\"spe\")\n",
    "\n",
    "train_env = env_class(tuple_costs=None, adversary_mixed_strategy=None, memory=12)\n",
    "model_name=\"rnd_start\"\n",
    "log_dir = f\"{gl.LOG_DIR}/{model_name}\"\n",
    "model = SAC('MlpPolicy', train_env,\n",
    "                        verbose=0, tensorboard_log=log_dir, gamma=gl.GAMMA, target_entropy=0)\n",
    "# model.learn(total_timesteps=1, tb_log_name=model_name)\n",
    "model.save(f\"{gl.MODELS_DIR}/{model_name}\")\n",
    "\n",
    "strt_rnd= cl.Strategy(strategy_type=cl.StrategyType.sb3_model,\n",
    "                                 model_or_func=SAC, name=model_name, action_step=None,memory=12)\n",
    "\n",
    "bimatrix_game = cl.BimatrixGame(\n",
    "    low_cost_strategies=[strt_rnd]+low_strts, high_cost_strategies=[strt_rnd]+high_strts, env_class=env_class)\n",
    "\n",
    "bimatrix_game.reset_matrix()\n",
    "bimatrix_game.fill_matrix()\n",
    "\n",
    "\n",
    "\n",
    "cl.prt(\"\\n\" + time.ctime(time.time())+\"\\n\"+(\"-\"*50)+\"\\n\")\n",
    "\n",
    "dictionaries = bimatrix_game.compute_equilibria()\n",
    "\n",
    "# low_cost_probabilities, high_cost_probabilities, low_cost_payoff, high_cost_payoff = bimatrix_game.compute_equilibria()\n",
    "for round in range(num_rounds):\n",
    "    cl.prt(f\"Round {round} of {num_rounds}\")\n",
    "    \n",
    "    added_low=0\n",
    "    added_high=0\n",
    "    # for equilibrium in dictionaries:\n",
    "    for equi_i in range(len(dictionaries)):\n",
    "        new_equi_low = 0\n",
    "        new_equi_high = 0\n",
    "        equi = dictionaries[equi_i]\n",
    "        # low_prob_str = \", \".join(\n",
    "        #     map(\"{0:.2f}\".format, equi[\"low_cost_probs\"]))\n",
    "        # high_prob_str = \", \".join(\n",
    "        #     map(\"{0:.2f}\".format, equi[\"high_cost_probs\"]))\n",
    "        cl.prt(\n",
    "            f'equi: {str(equi[\"low_cost_support\"])}, {str(equi[\"high_cost_support\"])}\\n payoffs= {equi[\"low_cost_payoff\"]:.2f}, {equi[\"high_cost_payoff\"]:.2f}')\n",
    "    \n",
    "        # train a low-cost agent\n",
    "        high_mixed_strat = cl.MixedStrategy(\n",
    "            strategies_lst=bimatrix_game.high_strategies, probablities_lst=((equi[\"high_cost_probs\"]+([0]*added_high)) if added_high> 0 else equi[\"high_cost_probs\"]))\n",
    "    \n",
    "        base_agent=None\n",
    "        for alg in algs:\n",
    "            for lr in lrs:\n",
    "                for mem_i,memory in enumerate(memories):\n",
    "                    # base_strt=(high_mixed_strat.strategies[np.argmax(\n",
    "                    #     np.array(high_mixed_strat.strategy_probs))])\n",
    "                    # if base_strt.type == cl.StrategyType.sb3_model:\n",
    "                    #     base_agent = base_strt.name\n",
    "                    base_agent= find_base_agent(db=db,memory=memory, alg=alg, cost=gl.LOW_COST, strategies= bimatrix_game.low_strategies,strategy_probs=((equi[\"low_cost_probs\"]+([0]*added_low)) if added_low > 0 else equi[\"low_cost_probs\"])) \n",
    "                    print(f'training low-cost player with base={base_agent} ,alg={str(alg)}, lr={lr:.4f}, memory={memory}')\n",
    "    \n",
    "                    [acceptable, agent_payoffs, adv_payoffs, agent_strategy, expected_payoff] = training(db=db,base_agent=base_agent, env_class=env_class, costs=[\n",
    "                                                                        gl.LOW_COST, gl.HIGH_COST], adv_mixed_strategy=high_mixed_strat, target_payoff=equi[\"low_cost_payoff\"], num_procs=num_procs, alg=alg, lr=lr, memory=memory)\n",
    "                    if acceptable:\n",
    "                        new_equi_low += 1\n",
    "                        added_low+=1\n",
    "                        \n",
    "                        # update[int(i/2)] = True\n",
    "                        bimatrix_game.low_strategies.append(agent_strategy)\n",
    "                        bimatrix_game.add_low_cost_row(agent_payoffs, adv_payoffs)\n",
    "    \n",
    "                        # cl.prt(f\"low cost player {agent_strategy.name} added, trained with \", [\n",
    "                        #     equi[\"low_cost_probabilities\"], equi[\"high_cost_probabilities\"], equi[\"low_cost_payoff\"], equi[\"high_cost_payoff\"]])\n",
    "                        cl.prt(\n",
    "                            f'low-cost player {agent_strategy.name} , payoff= {expected_payoff:.2f} added, base={base_agent} ,alg={str(alg)}, lr={lr:.4f}, memory={memory}')\n",
    "    \n",
    "        # train a high-cost agent\n",
    "        low_mixed_strat = cl.MixedStrategy(\n",
    "            strategies_lst=bimatrix_game.low_strategies, probablities_lst=((equi[\"low_cost_probs\"]+([0]*added_low)) if added_low > 0 else equi[\"low_cost_probs\"]))\n",
    "        \n",
    "        base_agent=None\n",
    "        for alg in algs:\n",
    "            for lr in lrs:\n",
    "                for memory in memories:\n",
    "                    \n",
    "                    # base_strt=(low_mixed_strat.strategies[np.argmax(\n",
    "                    #     np.array(low_mixed_strat.strategy_probs))])\n",
    "                    # if base_strt.type == cl.StrategyType.sb3_model:\n",
    "                    #     base_agent = base_strt.name\n",
    "                    base_agent= find_base_agent(db=db,cost=gl.HIGH_COST,memory=memory, strategies= bimatrix_game.high_strategies,strategy_probs=((equi[\"high_cost_probs\"]+([0]*added_high)) if added_high> 0 else equi[\"high_cost_probs\"]),alg=alg)\n",
    "                    print(f'training high-cost player with base={base_agent} ,alg={str(alg)}, lr={lr:.4f}, memory={memory}')\n",
    "                    [acceptable, agent_payoffs, adv_payoffs, agent_strategy, expected_payoff] = training(db=db,base_agent=base_agent, env_class=env_class, costs=[\n",
    "                        gl.HIGH_COST, gl.LOW_COST], adv_mixed_strategy=low_mixed_strat, target_payoff=equi[\"high_cost_payoff\"], num_procs=num_procs, alg=alg, memory=memory, lr=lr)\n",
    "                    if acceptable:\n",
    "                        new_equi_high += 1\n",
    "                        added_high+=1\n",
    "                        bimatrix_game.high_strategies.append(agent_strategy)\n",
    "                        bimatrix_game.add_high_cost_col(adv_payoffs, agent_payoffs)\n",
    "    \n",
    "                        cl.prt(\n",
    "                            f'high-cost player {agent_strategy.name} , payoff= {expected_payoff:.2f} added, base={base_agent}, alg={str(alg)}, lr={lr:.4f}, memory={memory}')\n",
    "    \n",
    "        if new_equi_low>0 or new_equi_high>0:\n",
    "            equilibria.append(\n",
    "                [equi[\"low_cost_probs\"], equi[\"high_cost_probs\"], equi[\"low_cost_payoff\"], equi[\"high_cost_payoff\"]])\n",
    "            \n",
    "            \n",
    "    if added_low==0 and added_high==0:\n",
    "        gl.N_EPISODES_BASE *= 1.1\n",
    "        gl.N_EPISODES_LOAD *= 1.1\n",
    "    else:\n",
    "        dictionaries = bimatrix_game.compute_equilibria()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef372678-7225-48bc-8c31-2fad59bcc7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env=ConPricingGame(tuple_costs=[57,71], adversary_mixed_strategy= (cl.Strategy(\n",
    "#     cl.StrategyType.static, model_or_func=cl.myopic, name=\"myopic\")).to_mixed_strategy(), memory=3)\n",
    "\n",
    "# policy = (PPO.load(\"models/\"+\"NOV24-1700860722\", env=env)).predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98de76c-95c6-4052-b30b-4b63e49f1604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaenv2",
   "language": "python",
   "name": "condaenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
