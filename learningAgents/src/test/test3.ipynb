{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learningAgents import ReinforceAlgorithm\n",
    "from environmentModel import Model, AdversaryModes\n",
    "from NeuralNetwork import NeuralNetwork\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adversaryProbs=torch.zeros(len(AdversaryModes))\n",
    "adversaryProbs[0]=1\n",
    "adversaryProbs[1]=0\n",
    "adversaryProbs[8]=0\n",
    "game = Model(totalDemand = 400, \n",
    "               tupleCosts = (57, 71),\n",
    "              totalStages = 3, adversaryProbs=adversaryProbs, advHistoryNum=0)\n",
    "adversaryProbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralNet=NeuralNetwork(num_input=3+game.advHistoryNum, lr=0.0009,num_actions=4)\n",
    "algorithm = ReinforceAlgorithm(game, neuralNet, numberIterations=1, numberEpisodes=1_000_000, discountFactor =0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "0   actions:  tensor([2, 3, 0])\n",
      "loss=  tensor(14.3700, grad_fn=<DivBackward0>)   , return=  16182.078125\n",
      "discReturns/1000= tensor([14.5668, 10.5095,  5.6438])\n",
      "actionProbs tensor([[0.3411, 0.3447, 0.3265, 0.3323],\n",
      "        [0.3336, 0.3344, 0.3327, 0.3331],\n",
      "        [0.3253, 0.3209, 0.3408, 0.3346]], grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "1000   actions:  tensor([3, 0, 2])\n",
      "loss=  tensor(15.2851, grad_fn=<DivBackward0>)   , return=  16125.5\n",
      "discReturns/1000= tensor([14.5241, 10.4676,  5.5462])\n",
      "actionProbs tensor([[0.2986, 0.3239, 0.4163, 0.2936],\n",
      "        [0.3276, 0.3396, 0.3302, 0.3310],\n",
      "        [0.3739, 0.3365, 0.2535, 0.3755]], grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "2000   actions:  tensor([0, 2, 0])\n",
      "loss=  tensor(14.2920, grad_fn=<DivBackward0>)   , return=  16042.703125\n",
      "discReturns/1000= tensor([14.4485, 10.3736,  5.5689])\n",
      "actionProbs tensor([[0.3501, 0.5101, 0.3013, 0.3593],\n",
      "        [0.3330, 0.3122, 0.3273, 0.3369],\n",
      "        [0.3169, 0.1777, 0.3714, 0.3038]], grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "3000   actions:  tensor([2, 2, 1])\n",
      "loss=  tensor(14.0527, grad_fn=<DivBackward0>)   , return=  16148.578125\n",
      "discReturns/1000= tensor([14.5401, 10.4798,  5.6053])\n",
      "actionProbs tensor([[0.3331, 0.3801, 0.3867, 0.3945],\n",
      "        [0.3326, 0.3338, 0.3312, 0.3359],\n",
      "        [0.3343, 0.2861, 0.2821, 0.2695]], grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "4000   actions:  tensor([1, 1, 2])\n",
      "loss=  tensor(13.8139, grad_fn=<DivBackward0>)   , return=  16058.75\n",
      "discReturns/1000= tensor([14.4648, 10.3929,  5.5462])\n",
      "actionProbs tensor([[0.3823, 0.3647, 0.3374, 0.3419],\n",
      "        [0.3318, 0.3347, 0.3321, 0.3386],\n",
      "        [0.2860, 0.3006, 0.3304, 0.3195]], grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "5000   actions:  tensor([2, 3, 2])\n",
      "loss=  tensor(14.1625, grad_fn=<DivBackward0>)   , return=  16178.078125\n",
      "discReturns/1000= tensor([14.5635, 10.5059,  5.6398])\n",
      "actionProbs tensor([[0.3565, 0.3334, 0.3533, 0.3651],\n",
      "        [0.3361, 0.3335, 0.3344, 0.3365],\n",
      "        [0.3074, 0.3331, 0.3123, 0.2985]], grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "6000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(14.9211, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[0.2825, 0.2942, 0.3373, 0.3813],\n",
      "        [0.3325, 0.3331, 0.3331, 0.3309],\n",
      "        [0.3850, 0.3727, 0.3296, 0.2878]], grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "7000   actions:  tensor([1, 1, 3])\n",
      "loss=  tensor(13.2686, grad_fn=<DivBackward0>)   , return=  16053.75\n",
      "discReturns/1000= tensor([14.4608, 10.3884,  5.5413])\n",
      "actionProbs tensor([[0.4034, 0.3388, 0.3222, 0.2392],\n",
      "        [0.3422, 0.3375, 0.3311, 0.3078],\n",
      "        [0.2543, 0.3237, 0.3467, 0.4530]], grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "8000   actions:  tensor([1, 0, 0])\n",
      "loss=  tensor(14.1636, grad_fn=<DivBackward0>)   , return=  16026.5625\n",
      "discReturns/1000= tensor([14.4389, 10.3640,  5.5131])\n",
      "actionProbs tensor([[0.3481, 0.3311, 0.3639, 0.3795],\n",
      "        [0.3882, 0.3500, 0.3242, 0.3022],\n",
      "        [0.2637, 0.3189, 0.3118, 0.3183]], grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "9000   actions:  tensor([2, 1, 0])\n",
      "loss=  tensor(12.4751, grad_fn=<DivBackward0>)   , return=  16115.203125\n",
      "discReturns/1000= tensor([14.5133, 10.4501,  5.5689])\n",
      "actionProbs tensor([[0.0242, 0.1251, 0.2959, 0.7412],\n",
      "        [0.2700, 0.3417, 0.3357, 0.1711],\n",
      "        [0.7058, 0.5332, 0.3683, 0.0877]], grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "10000   actions:  tensor([0, 1, 1])\n",
      "loss=  tensor(13.1282, grad_fn=<DivBackward0>)   , return=  16007.453125\n",
      "discReturns/1000= tensor([14.4202, 10.3421,  5.5306])\n",
      "actionProbs tensor([[0.3997, 0.3434, 0.3187, 0.2670],\n",
      "        [0.3856, 0.3535, 0.3327, 0.2959],\n",
      "        [0.2147, 0.3031, 0.3486, 0.4371]], grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "11000   actions:  tensor([0, 3, 1])\n",
      "loss=  tensor(24.0207, grad_fn=<DivBackward0>)   , return=  16074.078125\n",
      "discReturns/1000= tensor([14.4734, 10.4013,  5.6053])\n",
      "actionProbs tensor([[0.0305, 0.1932, 0.4481, 0.5365],\n",
      "        [0.1028, 0.2804, 0.3412, 0.3283],\n",
      "        [0.8667, 0.5264, 0.2107, 0.1352]], grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "12000   actions:  tensor([3, 1, 2])\n",
      "loss=  tensor(9.3850, grad_fn=<DivBackward0>)   , return=  16161.8125\n",
      "discReturns/1000= tensor([14.5534, 10.5002,  5.5836])\n",
      "actionProbs tensor([[0.0074, 0.0636, 0.5593, 0.4518],\n",
      "        [0.9549, 0.7904, 0.1120, 0.1805],\n",
      "        [0.0377, 0.1461, 0.3287, 0.3677]], grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "13000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.7048, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.1943e-03, 7.1138e-07, 2.0163e-07, 1.5027e-07],\n",
      "        [6.3728e-02, 8.1566e-04, 2.5514e-04, 2.3243e-04],\n",
      "        [9.3408e-01, 9.9918e-01, 9.9974e-01, 9.9977e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "14000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5935, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.4860e-04, 1.4307e-08, 1.3893e-09, 1.0059e-09],\n",
      "        [1.6544e-02, 1.1095e-04, 2.0016e-05, 1.8005e-05],\n",
      "        [9.8331e-01, 9.9989e-01, 9.9998e-01, 9.9998e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "15000   actions:  tensor([0, 0, 1])\n",
      "loss=  tensor(2.5380, grad_fn=<DivBackward0>)   , return=  15971.328125\n",
      "discReturns/1000= tensor([14.3910, 10.3097,  5.4935])\n",
      "actionProbs tensor([[1.2894e-04, 6.7207e-10, 3.9580e-11, 2.9530e-11],\n",
      "        [1.5163e-02, 2.2266e-05, 3.0564e-06, 2.8011e-06],\n",
      "        [9.8471e-01, 9.9998e-01, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "16000   actions:  tensor([0, 0, 1])\n",
      "loss=  tensor(2.4510, grad_fn=<DivBackward0>)   , return=  15971.328125\n",
      "discReturns/1000= tensor([14.3910, 10.3097,  5.4935])\n",
      "actionProbs tensor([[2.0240e-02, 1.4748e-08, 6.5000e-11, 6.7803e-11],\n",
      "        [1.7072e-01, 8.7669e-05, 3.1515e-06, 3.2808e-06],\n",
      "        [8.0904e-01, 9.9991e-01, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "17000   actions:  tensor([0, 0, 1])\n",
      "loss=  tensor(2.5313, grad_fn=<DivBackward0>)   , return=  15971.328125\n",
      "discReturns/1000= tensor([14.3910, 10.3097,  5.4935])\n",
      "actionProbs tensor([[1.2494e-04, 1.2137e-12, 7.0943e-13, 5.7800e-13],\n",
      "        [1.6364e-02, 8.9921e-07, 3.7899e-07, 3.5636e-07],\n",
      "        [9.8351e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "18000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5972, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.5080e-05, 2.4054e-09, 3.2711e-13, 2.5893e-13],\n",
      "        [8.9534e-03, 1.1584e-04, 1.3333e-06, 1.1441e-06],\n",
      "        [9.9103e-01, 9.9988e-01, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "19000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5408, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.6082e-05, 1.4775e-10, 2.1370e-15, 1.8104e-15],\n",
      "        [8.8521e-03, 2.5656e-05, 9.1213e-08, 8.0155e-08],\n",
      "        [9.9113e-01, 9.9997e-01, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "20000   actions:  tensor([0, 0, 1])\n",
      "loss=  tensor(2.5565, grad_fn=<DivBackward0>)   , return=  15971.328125\n",
      "discReturns/1000= tensor([14.3910, 10.3097,  5.4935])\n",
      "actionProbs tensor([[1.0538e-05, 6.5032e-10, 5.9195e-15, 4.4514e-15],\n",
      "        [6.6051e-03, 3.9549e-05, 8.8732e-08, 7.7599e-08],\n",
      "        [9.9338e-01, 9.9996e-01, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "21000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5565, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.5801e-05, 2.4881e-09, 1.1989e-14, 9.3250e-15],\n",
      "        [1.1208e-02, 8.6259e-05, 1.5281e-07, 1.3481e-07],\n",
      "        [9.8877e-01, 9.9991e-01, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "22000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5456, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.8291e-05, 7.9380e-10, 2.7196e-15, 2.6802e-14],\n",
      "        [1.3511e-02, 5.6811e-05, 8.4469e-08, 2.5516e-07],\n",
      "        [9.8646e-01, 9.9994e-01, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "23000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5636, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.9507e-06, 1.7117e-10, 4.6402e-16, 1.0445e-14],\n",
      "        [5.2006e-03, 2.5885e-05, 3.3190e-08, 1.5132e-07],\n",
      "        [9.9480e-01, 9.9997e-01, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "24000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5750, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.2444e-06, 2.7993e-10, 6.1005e-16, 2.5271e-14],\n",
      "        [4.1440e-03, 3.5661e-05, 4.3352e-08, 2.6298e-07],\n",
      "        [9.9585e-01, 9.9996e-01, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "25000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5351, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.0452e-05, 2.0314e-10, 2.7550e-17, 3.4566e-15],\n",
      "        [1.7656e-02, 3.0928e-05, 9.0952e-09, 9.5844e-08],\n",
      "        [9.8230e-01, 9.9997e-01, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "26000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5457, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.7855e-06, 9.7789e-12, 3.2736e-15, 2.2839e-14],\n",
      "        [2.6971e-03, 2.2512e-06, 1.8368e-08, 5.2754e-08],\n",
      "        [9.9730e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "27000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5407, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.1926e-07, 1.8083e-13, 1.5933e-16, 7.7042e-16],\n",
      "        [6.7652e-04, 1.3856e-07, 1.3235e-09, 3.3401e-09],\n",
      "        [9.9932e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "28000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5397, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[7.6279e-08, 1.3311e-14, 2.0644e-17, 8.3193e-17],\n",
      "        [2.6647e-04, 2.1307e-08, 2.1912e-10, 5.2190e-10],\n",
      "        [9.9973e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "29000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5393, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.3618e-08, 1.6099e-15, 3.8478e-18, 1.3724e-17],\n",
      "        [1.2309e-04, 4.5325e-09, 4.8645e-11, 1.1217e-10],\n",
      "        [9.9988e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "30000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5392, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[8.3089e-09, 2.4618e-16, 8.4663e-19, 2.7542e-18],\n",
      "        [6.1409e-05, 1.1261e-09, 1.2422e-11, 2.8076e-11],\n",
      "        [9.9994e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "31000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5391, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.1308e-09, 4.2768e-17, 2.0071e-19, 6.0834e-19],\n",
      "        [3.1980e-05, 3.0495e-10, 3.4293e-12, 7.6539e-12],\n",
      "        [9.9997e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "32000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.2303e-09, 7.9708e-18, 4.9246e-20, 1.4097e-19],\n",
      "        [1.7129e-05, 8.7148e-11, 9.9394e-13, 2.2006e-12],\n",
      "        [9.9998e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "33000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.8835e-10, 1.4842e-18, 1.2195e-20, 3.2969e-20],\n",
      "        [9.3048e-06, 2.5782e-11, 2.9746e-13, 6.5498e-13],\n",
      "        [9.9999e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "34000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.9739e-10, 2.8820e-19, 3.0783e-21, 7.9096e-21],\n",
      "        [5.1212e-06, 7.8146e-12, 9.1010e-14, 1.9961e-13],\n",
      "        [9.9999e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "35000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[8.3473e-11, 6.0871e-20, 8.7299e-22, 2.1171e-21],\n",
      "        [2.8029e-06, 2.3866e-12, 2.8031e-14, 6.1306e-14],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "36000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.4695e-11, 1.2025e-20, 2.1115e-22, 4.9539e-22],\n",
      "        [1.5608e-06, 7.4220e-13, 8.7660e-15, 1.9160e-14],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "37000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.7698e-11, 3.3386e-21, 8.8350e-23, 1.8958e-22],\n",
      "        [8.7751e-07, 2.3387e-13, 2.7832e-15, 6.0672e-15],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "38000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[8.3404e-12, 8.6857e-22, 3.2943e-23, 6.4975e-23],\n",
      "        [4.9854e-07, 7.5243e-14, 9.0402e-16, 1.9572e-15],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "39000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[8.6184e-12, 1.2343e-21, 1.4397e-22, 2.1807e-22],\n",
      "        [2.8209e-07, 2.7088e-14, 3.2649e-16, 7.0847e-16],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "40000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.8167e-12, 2.3500e-22, 2.6903e-23, 4.1757e-23],\n",
      "        [1.6084e-07, 1.2097e-14, 1.4681e-16, 3.1868e-16],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "41000   actions:  tensor([0, 0, 1])\n",
      "loss=  tensor(2.5385, grad_fn=<DivBackward0>)   , return=  15971.328125\n",
      "discReturns/1000= tensor([14.3910, 10.3097,  5.4935])\n",
      "actionProbs tensor([[1.4076e-09, 4.2209e-20, 3.5140e-22, 8.6742e-22],\n",
      "        [1.5936e-07, 8.6000e-15, 1.5144e-16, 4.1089e-16],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "42000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5507, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.6732e-04, 9.5150e-15, 3.2005e-17, 6.8905e-17],\n",
      "        [8.2168e-03, 7.8889e-10, 6.5889e-12, 1.2909e-11],\n",
      "        [9.9152e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "43000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5347, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.1320e-06, 2.4679e-17, 2.1126e-19, 3.3816e-19],\n",
      "        [2.6040e-04, 6.5036e-12, 6.2308e-14, 1.0644e-13],\n",
      "        [9.9974e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "44000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5370, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.4575e-06, 3.0842e-17, 2.7249e-19, 4.3387e-19],\n",
      "        [2.8748e-04, 7.6502e-12, 7.5444e-14, 1.2844e-13],\n",
      "        [9.9971e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "45000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5347, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.7969e-06, 3.9357e-17, 3.6612e-19, 5.7763e-19],\n",
      "        [3.1745e-04, 9.4446e-12, 9.8416e-14, 1.6605e-13],\n",
      "        [9.9968e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "46000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5395, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.9520e-06, 4.4919e-17, 4.4184e-19, 6.8836e-19],\n",
      "        [3.3050e-04, 1.0522e-11, 1.1505e-13, 1.9206e-13],\n",
      "        [9.9967e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "47000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5370, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.0383e-06, 5.1157e-17, 5.5099e-19, 8.3898e-19],\n",
      "        [3.3848e-04, 1.1737e-11, 1.3963e-13, 2.2775e-13],\n",
      "        [9.9966e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "48000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5371, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[8.9885e-07, 1.2128e-17, 1.7896e-19, 2.4433e-19],\n",
      "        [1.4818e-04, 4.0200e-12, 5.2903e-14, 8.0603e-14],\n",
      "        [9.9985e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "49000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5394, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.1240e-06, 4.1339e-17, 8.8502e-19, 1.1335e-18],\n",
      "        [2.7463e-04, 1.1492e-11, 2.1477e-13, 3.0935e-13],\n",
      "        [9.9972e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "50000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5347, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.0371e-06, 4.2458e-17, 1.1825e-18, 1.4094e-18],\n",
      "        [2.6518e-04, 1.1630e-11, 2.5623e-13, 3.5106e-13],\n",
      "        [9.9973e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "51000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5191, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.8355e-03, 5.0034e-16, 1.1666e-18, 1.8665e-18],\n",
      "        [3.7543e-02, 8.2853e-10, 1.2247e-11, 1.9446e-11],\n",
      "        [9.6062e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "52000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5295, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.8180e-04, 7.2007e-17, 2.0745e-19, 2.5587e-19],\n",
      "        [1.6184e-02, 1.5875e-10, 2.3594e-12, 3.2731e-12],\n",
      "        [9.8333e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "53000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5300, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.4585e-04, 2.5924e-17, 7.9633e-20, 9.4305e-20],\n",
      "        [1.0448e-02, 6.7373e-11, 9.7323e-13, 1.3194e-12],\n",
      "        [9.8931e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "54000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5330, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.8955e-04, 2.0647e-17, 6.8307e-20, 7.8137e-20],\n",
      "        [8.8863e-03, 5.5394e-11, 8.2553e-13, 1.0970e-12],\n",
      "        [9.9092e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "55000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5342, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.0863e-04, 1.3087e-17, 5.2045e-20, 5.5105e-20],\n",
      "        [6.4215e-03, 4.0429e-11, 6.6138e-13, 8.3933e-13],\n",
      "        [9.9347e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "56000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5315, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.3666e-04, 4.2585e-17, 3.5450e-19, 2.8690e-19],\n",
      "        [1.2056e-02, 1.5395e-10, 4.8860e-12, 5.4115e-12],\n",
      "        [9.8771e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "57000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5314, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.4311e-04, 6.3380e-17, 5.6089e-19, 4.4252e-19],\n",
      "        [1.2359e-02, 2.0554e-10, 6.8584e-12, 7.4822e-12],\n",
      "        [9.8740e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "58000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5288, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.4021e-04, 7.4797e-17, 6.1756e-19, 4.9190e-19],\n",
      "        [1.3049e-02, 2.8745e-10, 9.0855e-12, 9.8594e-12],\n",
      "        [9.8671e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "59000   actions:  tensor([0, 0, 1])\n",
      "loss=  tensor(2.0119, grad_fn=<DivBackward0>)   , return=  15971.328125\n",
      "discReturns/1000= tensor([14.3910, 10.3097,  5.4935])\n",
      "actionProbs tensor([[9.7257e-01, 2.8119e-18, 9.6907e-14, 2.0380e-17],\n",
      "        [2.7221e-02, 2.2712e-09, 5.3586e-07, 6.6865e-09],\n",
      "        [2.0959e-04, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "60000   actions:  tensor([0, 0, 1])\n",
      "loss=  tensor(2.0119, grad_fn=<DivBackward0>)   , return=  15971.328125\n",
      "discReturns/1000= tensor([14.3910, 10.3097,  5.4935])\n",
      "actionProbs tensor([[9.7797e-01, 4.8010e-19, 4.4624e-14, 5.7223e-18],\n",
      "        [2.1904e-02, 3.7777e-10, 1.6514e-07, 1.4573e-09],\n",
      "        [1.2446e-04, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "61000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.0089, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[9.8122e-01, 1.7438e-19, 2.0926e-14, 2.1107e-18],\n",
      "        [1.8702e-02, 1.1688e-10, 6.4833e-08, 4.5457e-10],\n",
      "        [8.1473e-05, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "62000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.0089, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[9.8434e-01, 6.4931e-20, 1.0242e-14, 8.1008e-19],\n",
      "        [1.5611e-02, 4.0401e-11, 2.8383e-08, 1.6031e-10],\n",
      "        [5.1351e-05, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "63000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.0088, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[9.8727e-01, 2.4883e-20, 5.2772e-15, 3.2842e-19],\n",
      "        [1.2695e-02, 1.7229e-11, 1.4993e-08, 7.0661e-11],\n",
      "        [3.1587e-05, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "64000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5396, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.2698e-07, 1.1843e-17, 7.5200e-14, 6.6233e-15],\n",
      "        [2.7211e-04, 6.1369e-11, 1.1710e-08, 2.2705e-09],\n",
      "        [9.9973e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "65000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5391, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.0041e-08, 1.0975e-19, 5.3514e-16, 4.6351e-17],\n",
      "        [5.4866e-05, 2.7603e-12, 5.0268e-10, 9.6825e-11],\n",
      "        [9.9995e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "66000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[9.4575e-10, 8.3933e-23, 3.2252e-19, 2.5685e-20],\n",
      "        [2.1456e-06, 9.3031e-15, 1.5729e-12, 2.8050e-13],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "67000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.7096e-10, 2.7267e-23, 1.9684e-20, 1.5955e-21],\n",
      "        [5.4067e-07, 2.0903e-15, 9.0301e-14, 1.6161e-14],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "68000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.4127e-10, 2.1993e-23, 7.3411e-21, 6.2481e-22],\n",
      "        [2.9197e-07, 7.6030e-16, 1.4778e-14, 2.7326e-15],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "69000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.3753e-10, 1.5451e-23, 3.9870e-21, 3.4757e-22],\n",
      "        [1.5875e-07, 4.9522e-16, 7.7530e-15, 1.4650e-15],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "70000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.6619e-11, 6.0161e-24, 1.2789e-21, 1.1296e-22],\n",
      "        [6.3712e-08, 2.3749e-16, 3.3676e-15, 6.4431e-16],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "71000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.6714e-11, 8.4956e-24, 1.9482e-21, 1.7239e-22],\n",
      "        [5.8626e-08, 2.4323e-16, 3.4712e-15, 6.6434e-16],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "72000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.6660e-11, 9.9376e-24, 2.2346e-21, 1.9785e-22],\n",
      "        [5.8249e-08, 2.4088e-16, 3.4142e-15, 6.5358e-16],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "73000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[8.8695e-11, 1.3604e-23, 3.0038e-21, 2.6613e-22],\n",
      "        [5.8033e-08, 2.4679e-16, 3.5068e-15, 6.7172e-16],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "74000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.4690e-10, 2.3297e-23, 4.9595e-21, 4.3901e-22],\n",
      "        [5.7700e-08, 2.5128e-16, 3.5590e-15, 6.8234e-16],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "75000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.0253e-10, 5.0918e-23, 1.2434e-20, 1.0583e-21],\n",
      "        [5.7913e-08, 2.6890e-16, 3.8413e-15, 7.3659e-16],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "76000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.6722e-10, 1.0569e-22, 2.9025e-20, 2.3995e-21],\n",
      "        [5.8839e-08, 2.9938e-16, 4.2965e-15, 8.2364e-16],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "77000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[9.3414e-10, 3.6227e-22, 1.1247e-19, 8.9823e-21],\n",
      "        [5.5982e-08, 3.4013e-16, 4.8414e-15, 9.2798e-16],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "78000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.8431e-09, 1.1870e-21, 2.9112e-19, 2.4474e-20],\n",
      "        [5.3006e-08, 3.4858e-16, 4.7103e-15, 9.2659e-16],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "79000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.9290e-09, 4.6894e-21, 8.1369e-19, 7.7490e-20],\n",
      "        [5.4457e-08, 3.5464e-16, 4.4496e-15, 9.3179e-16],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "80000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.6900e-09, 2.7067e-20, 2.8606e-18, 3.1693e-19],\n",
      "        [4.9789e-08, 4.2031e-16, 4.8899e-15, 1.0624e-15],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "81000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[7.6490e-09, 2.9137e-20, 2.8760e-18, 3.3301e-19],\n",
      "        [5.1331e-08, 3.5602e-16, 3.7994e-15, 8.6239e-16],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "82000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[7.1274e-09, 4.8394e-20, 3.8664e-18, 4.8595e-19],\n",
      "        [4.8591e-08, 5.1280e-16, 4.3413e-15, 1.0748e-15],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "83000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[7.3645e-09, 4.7192e-20, 3.0698e-18, 4.0658e-19],\n",
      "        [4.3816e-08, 6.6829e-16, 3.5214e-15, 1.0134e-15],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "84000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[7.9998e-09, 1.0830e-19, 3.9019e-18, 5.2114e-19],\n",
      "        [4.0573e-08, 5.3614e-16, 2.4320e-15, 7.2312e-16],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "85000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.0218e-10, 4.3768e-23, 8.8902e-22, 1.5935e-22],\n",
      "        [1.8348e-08, 4.3821e-17, 1.9317e-16, 7.1918e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "86000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.8536e-11, 3.4322e-24, 6.4018e-23, 1.1812e-23],\n",
      "        [1.3832e-08, 2.7261e-17, 1.2697e-16, 4.7988e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "87000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.3131e-11, 2.6812e-24, 5.0087e-23, 9.2606e-24],\n",
      "        [1.3735e-08, 2.6678e-17, 1.2474e-16, 4.7172e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "88000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.0738e-11, 2.3772e-24, 4.4357e-23, 8.2125e-24],\n",
      "        [1.3691e-08, 2.6400e-17, 1.2365e-16, 4.6776e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "89000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.6490e-11, 1.8651e-24, 3.4554e-23, 6.4188e-24],\n",
      "        [1.3603e-08, 2.5810e-17, 1.2132e-16, 4.5921e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "90000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[5.7429e-11, 5.8619e-24, 1.1453e-22, 2.0808e-23],\n",
      "        [1.5250e-08, 3.0425e-17, 1.4189e-16, 5.3328e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "91000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.2133e-11, 2.2351e-24, 4.2119e-23, 7.7587e-24],\n",
      "        [1.4115e-08, 2.6375e-17, 1.2471e-16, 4.7094e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "92000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.8447e-12, 3.3629e-26, 4.6730e-25, 9.5360e-26],\n",
      "        [1.1891e-08, 1.6650e-17, 8.4162e-17, 3.1954e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "93000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.2863e-12, 1.7339e-26, 2.3495e-25, 4.8236e-26],\n",
      "        [1.1669e-08, 1.5065e-17, 7.6909e-17, 2.9215e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "94000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[6.5627e-16, 1.5132e-30, 6.1297e-30, 1.6511e-30],\n",
      "        [4.8974e-09, 5.2784e-18, 2.8634e-17, 1.1017e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "95000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[4.2601e-16, 8.6587e-31, 3.3285e-30, 9.0483e-31],\n",
      "        [4.6606e-09, 4.9468e-18, 2.6761e-17, 1.0315e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "96000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[7.0167e-16, 1.3875e-30, 5.5059e-30, 1.4868e-30],\n",
      "        [5.0424e-09, 5.2101e-18, 2.8532e-17, 1.0953e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "97000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[8.0010e-17, 1.0136e-31, 4.0564e-31, 1.1089e-31],\n",
      "        [3.0856e-09, 3.0812e-18, 1.6048e-17, 6.2980e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "98000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.0195e-16, 1.0599e-31, 4.1565e-31, 1.1358e-31],\n",
      "        [3.3036e-09, 3.1126e-18, 1.6494e-17, 6.4431e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "99000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.7582e-17, 2.7968e-32, 1.1112e-31, 3.0347e-32],\n",
      "        [2.3332e-09, 2.1215e-18, 1.0812e-17, 4.2994e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "100000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.5040e-17, 2.0073e-32, 7.9130e-32, 2.1548e-32],\n",
      "        [2.2588e-09, 1.9224e-18, 9.8447e-18, 3.9131e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "101000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.3478e-17, 1.6645e-32, 6.5621e-32, 1.7829e-32],\n",
      "        [2.2824e-09, 1.8043e-18, 9.1530e-18, 3.6385e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "102000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.8950e-17, 8.3065e-33, 3.1846e-32, 8.7932e-33],\n",
      "        [1.8152e-09, 1.4947e-18, 7.4671e-18, 3.0031e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "103000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.5429e-17, 5.9841e-33, 2.2727e-32, 6.3043e-33],\n",
      "        [1.6987e-09, 1.3612e-18, 6.7323e-18, 2.7188e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "104000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.0905e-17, 7.8191e-33, 3.0274e-32, 8.3126e-33],\n",
      "        [1.9530e-09, 1.4523e-18, 7.2250e-18, 2.8990e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "105000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.9801e-17, 7.0244e-33, 2.7175e-32, 7.4732e-33],\n",
      "        [1.9217e-09, 1.4087e-18, 6.9976e-18, 2.8115e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "106000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.2171e-17, 7.6177e-33, 2.9729e-32, 8.1461e-33],\n",
      "        [2.0268e-09, 1.4354e-18, 7.1518e-18, 2.8670e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "107000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.1661e-17, 4.1438e-33, 1.5129e-32, 4.2897e-33],\n",
      "        [1.6027e-09, 1.4031e-18, 6.6789e-18, 2.7464e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "108000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.7098e-17, 5.1068e-33, 1.9727e-32, 5.4509e-33],\n",
      "        [1.8484e-09, 1.2919e-18, 6.3878e-18, 2.5771e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "109000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.8047e-17, 5.1424e-33, 2.0009e-32, 5.5208e-33],\n",
      "        [1.8975e-09, 1.2879e-18, 6.3920e-18, 2.5764e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "110000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.1009e-17, 5.7941e-33, 2.2820e-32, 6.2671e-33],\n",
      "        [2.0321e-09, 1.3242e-18, 6.6073e-18, 2.6550e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "111000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.8348e-17, 4.9049e-33, 1.9104e-32, 5.2842e-33],\n",
      "        [1.9404e-09, 1.3023e-18, 6.4473e-18, 2.6048e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "112000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.6838e-17, 4.5847e-33, 1.7623e-32, 4.9116e-33],\n",
      "        [1.8896e-09, 1.3362e-18, 6.5499e-18, 2.6614e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "113000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.3304e-17, 3.8334e-33, 1.4173e-32, 4.0236e-33],\n",
      "        [1.7651e-09, 1.4231e-18, 6.7588e-18, 2.7863e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "114000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.8651e-17, 4.5554e-33, 1.7689e-32, 4.9097e-33],\n",
      "        [2.0129e-09, 1.3260e-18, 6.5001e-18, 2.6361e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "115000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.8311e-17, 4.3911e-33, 1.6995e-32, 4.7341e-33],\n",
      "        [2.0140e-09, 1.3451e-18, 6.5703e-18, 2.6722e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "116000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.6872e-17, 4.1089e-33, 1.5704e-32, 4.4068e-33],\n",
      "        [1.9643e-09, 1.3787e-18, 6.6706e-18, 2.7281e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "117000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.5727e-17, 3.8111e-33, 1.4389e-32, 4.0670e-33],\n",
      "        [1.9361e-09, 1.4137e-18, 6.7652e-18, 2.7826e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "118000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.6097e-17, 3.6799e-33, 1.3883e-32, 3.9250e-33],\n",
      "        [1.9920e-09, 1.4205e-18, 6.7555e-18, 2.7818e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "119000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.2811e-17, 3.0941e-33, 1.1252e-32, 3.2401e-33],\n",
      "        [1.8597e-09, 1.5108e-18, 6.9797e-18, 2.9152e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "120000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.5639e-17, 3.4061e-33, 1.2757e-32, 3.6281e-33],\n",
      "        [2.0115e-09, 1.4555e-18, 6.8544e-18, 2.8371e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "121000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.5150e-17, 3.2314e-33, 1.2021e-32, 3.4336e-33],\n",
      "        [2.0123e-09, 1.4799e-18, 6.9163e-18, 2.8731e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "122000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.5259e-17, 3.1830e-33, 1.1850e-32, 3.3886e-33],\n",
      "        [2.0278e-09, 1.4874e-18, 6.9484e-18, 2.8892e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "123000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.7833e-17, 3.4196e-33, 1.3028e-32, 3.6907e-33],\n",
      "        [2.1587e-09, 1.4476e-18, 6.8638e-18, 2.8347e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "124000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.3489e-17, 2.8417e-33, 1.0380e-32, 3.0046e-33],\n",
      "        [1.9595e-09, 1.5517e-18, 7.1403e-18, 2.9969e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "125000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.3551e-17, 2.7103e-33, 9.8627e-33, 2.8597e-33],\n",
      "        [2.0037e-09, 1.5661e-18, 7.1460e-18, 3.0058e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "126000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.5407e-17, 2.8598e-33, 1.0606e-32, 3.0529e-33],\n",
      "        [2.1122e-09, 1.5336e-18, 7.0819e-18, 2.9633e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "127000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.2658e-17, 2.5383e-33, 9.1068e-33, 2.6653e-33],\n",
      "        [1.9824e-09, 1.6437e-18, 7.4071e-18, 3.1386e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "128000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.5479e-17, 2.7506e-33, 1.0203e-32, 2.9463e-33],\n",
      "        [2.1394e-09, 1.5556e-18, 7.1691e-18, 3.0077e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "129000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.9910e-17, 3.1098e-33, 1.1977e-32, 3.4048e-33],\n",
      "        [2.3633e-09, 1.4868e-18, 7.0215e-18, 2.9124e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "130000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.3163e-17, 2.5591e-33, 9.0955e-33, 2.6846e-33],\n",
      "        [2.0856e-09, 1.8168e-18, 8.0777e-18, 3.4486e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "131000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.1370e-17, 3.0403e-33, 1.1805e-32, 3.3535e-33],\n",
      "        [2.4771e-09, 1.4939e-18, 7.0587e-18, 2.9287e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "132000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.3837e-17, 3.1657e-33, 1.2493e-32, 3.5290e-33],\n",
      "        [2.5964e-09, 1.4736e-18, 7.0317e-18, 2.9059e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "133000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.0266e-17, 2.7676e-33, 1.0638e-32, 3.0463e-33],\n",
      "        [2.4759e-09, 1.5383e-18, 7.1860e-18, 3.0011e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "134000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.6169e-17, 2.5542e-33, 9.3189e-33, 2.7330e-33],\n",
      "        [2.3383e-09, 1.8005e-18, 8.0746e-18, 3.4360e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "135000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.5919e-17, 2.5639e-33, 9.2557e-33, 2.7297e-33],\n",
      "        [2.3596e-09, 1.9065e-18, 8.4633e-18, 3.6186e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "136000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.1051e-17, 2.6123e-33, 1.0077e-32, 2.8944e-33],\n",
      "        [2.5729e-09, 1.5658e-18, 7.2917e-18, 3.0548e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "137000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.6139e-17, 2.5701e-33, 9.1236e-33, 2.7139e-33],\n",
      "        [2.4718e-09, 2.1160e-18, 9.1980e-18, 3.9647e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "138000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.8577e-17, 2.5620e-33, 9.3491e-33, 2.7518e-33],\n",
      "        [2.6219e-09, 1.9729e-18, 8.7161e-18, 3.7287e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "139000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.8676e-17, 2.5621e-33, 9.2430e-33, 2.7341e-33],\n",
      "        [2.6978e-09, 2.1006e-18, 9.1483e-18, 3.9328e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "140000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.2759e-17, 2.5665e-33, 9.7289e-33, 2.8277e-33],\n",
      "        [2.8661e-09, 1.8312e-18, 8.2705e-18, 3.5069e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "141000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.1985e-17, 2.5753e-33, 9.5950e-33, 2.8132e-33],\n",
      "        [2.8885e-09, 1.9939e-18, 8.8626e-18, 3.7855e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "142000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.8553e-17, 2.5842e-33, 9.0883e-33, 2.7323e-33],\n",
      "        [2.8321e-09, 2.4955e-18, 1.0570e-17, 4.6080e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "143000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.9687e-17, 2.5810e-33, 9.1124e-33, 2.7380e-33],\n",
      "        [2.9519e-09, 2.5336e-18, 1.0706e-17, 4.6692e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "144000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.0454e-17, 2.5878e-33, 9.1565e-33, 2.7556e-33],\n",
      "        [3.0350e-09, 2.6024e-18, 1.0983e-17, 4.7976e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "145000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.8664e-17, 2.5925e-33, 8.8503e-33, 2.7084e-33],\n",
      "        [3.0336e-09, 3.0374e-18, 1.2422e-17, 5.5021e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "146000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.1178e-17, 2.6005e-33, 9.1435e-33, 2.7712e-33],\n",
      "        [3.1700e-09, 2.8337e-18, 1.1831e-17, 5.2014e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "147000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.2448e-17, 2.6237e-33, 1.0400e-32, 3.0143e-33],\n",
      "        [3.5499e-09, 1.9984e-18, 9.1328e-18, 3.8771e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "148000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.0758e-17, 2.5930e-33, 8.8767e-33, 2.7276e-33],\n",
      "        [3.3021e-09, 3.2794e-18, 1.3295e-17, 5.9170e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "149000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.9952e-17, 2.5926e-33, 8.7108e-33, 2.7024e-33],\n",
      "        [3.3257e-09, 3.5908e-18, 1.4306e-17, 6.4180e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "150000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.2528e-17, 2.5926e-33, 8.8963e-33, 2.7400e-33],\n",
      "        [3.5247e-09, 3.4578e-18, 1.3921e-17, 6.2152e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "151000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.5091e-17, 2.5901e-33, 9.0324e-33, 2.7679e-33],\n",
      "        [3.7327e-09, 3.3951e-18, 1.3752e-17, 6.1216e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "152000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.9351e-17, 2.5654e-33, 8.2284e-33, 2.6112e-33],\n",
      "        [3.5726e-09, 4.5646e-18, 1.7261e-17, 7.9040e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "153000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.2052e-17, 2.5718e-33, 8.4637e-33, 2.6717e-33],\n",
      "        [3.7997e-09, 4.4754e-18, 1.7178e-17, 7.8381e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "154000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.4396e-17, 2.5722e-33, 8.6057e-33, 2.7036e-33],\n",
      "        [4.0011e-09, 4.3935e-18, 1.6986e-17, 7.7278e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "155000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.9451e-17, 2.5996e-33, 9.1243e-33, 2.8191e-33],\n",
      "        [4.2457e-09, 3.8907e-18, 1.5574e-17, 6.9944e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "156000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.5797e-17, 2.5371e-33, 8.3666e-33, 2.6500e-33],\n",
      "        [4.3366e-09, 4.9150e-18, 1.8523e-17, 8.5009e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "157000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.3186e-17, 2.5256e-33, 7.9848e-33, 2.5767e-33],\n",
      "        [4.3092e-09, 5.7860e-18, 2.1005e-17, 9.7902e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "158000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.9847e-17, 2.4869e-33, 7.3570e-33, 2.4376e-33],\n",
      "        [4.2999e-09, 7.2961e-18, 2.4921e-17, 1.1879e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "159000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.3578e-17, 2.5330e-33, 8.6243e-33, 2.7154e-33],\n",
      "        [5.1077e-09, 5.0568e-18, 1.9130e-17, 8.7661e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "160000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.7760e-17, 2.4256e-33, 6.6924e-33, 2.2844e-33],\n",
      "        [4.4927e-09, 9.6660e-18, 3.0741e-17, 1.5042e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "161000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.4257e-17, 2.4573e-33, 7.3908e-33, 2.4542e-33],\n",
      "        [4.9814e-09, 7.9801e-18, 2.7042e-17, 1.2952e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "162000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.8821e-17, 2.4602e-33, 7.6790e-33, 2.5202e-33],\n",
      "        [5.3851e-09, 7.4807e-18, 2.5905e-17, 1.2305e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "163000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.2014e-17, 2.4051e-33, 6.8514e-33, 2.3329e-33],\n",
      "        [5.1248e-09, 1.0063e-17, 3.2274e-17, 1.5798e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "164000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.7387e-17, 2.4306e-33, 7.3322e-33, 2.4508e-33],\n",
      "        [5.5468e-09, 8.9138e-18, 2.9761e-17, 1.4365e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "165000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.7957e-17, 2.3793e-33, 7.0270e-33, 2.3673e-33],\n",
      "        [5.8874e-09, 9.7943e-18, 3.1738e-17, 1.5447e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "166000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.3933e-17, 2.2976e-33, 6.2955e-33, 2.1835e-33],\n",
      "        [5.9781e-09, 1.2666e-17, 3.8160e-17, 1.9050e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "167000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.6971e-17, 2.3072e-33, 6.4992e-33, 2.2422e-33],\n",
      "        [6.3248e-09, 1.2465e-17, 3.8162e-17, 1.8985e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "168000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.3308e-17, 2.2599e-33, 6.0422e-33, 2.1328e-33],\n",
      "        [6.2099e-09, 1.5032e-17, 4.3950e-17, 2.2281e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "169000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.9389e-17, 2.2943e-33, 6.5263e-33, 2.2646e-33],\n",
      "        [6.7852e-09, 1.3607e-17, 4.1505e-17, 2.0773e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "170000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.8107e-17, 2.1137e-33, 5.0559e-33, 1.8727e-33],\n",
      "        [6.2884e-09, 2.2673e-17, 5.9512e-17, 3.1442e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "171000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.9012e-17, 2.0766e-33, 4.9351e-33, 1.8367e-33],\n",
      "        [6.6765e-09, 2.4275e-17, 6.2640e-17, 3.3281e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "172000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.1735e-17, 2.0909e-33, 5.1385e-33, 1.8989e-33],\n",
      "        [7.0881e-09, 2.3619e-17, 6.2199e-17, 3.2886e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "173000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.5877e-17, 2.2031e-33, 6.2669e-33, 2.2094e-33],\n",
      "        [8.3331e-09, 1.7316e-17, 5.0818e-17, 2.5900e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "174000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.0148e-17, 2.0162e-33, 4.7376e-33, 1.7957e-33],\n",
      "        [7.3738e-09, 2.9896e-17, 7.4819e-17, 4.0454e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "175000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.3733e-17, 1.8766e-33, 3.8647e-33, 1.5417e-33],\n",
      "        [6.8902e-09, 4.4195e-17, 9.8729e-17, 5.5681e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "176000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[8.5883e-18, 1.6948e-33, 2.9373e-33, 1.2449e-33],\n",
      "        [6.4133e-09, 6.9869e-17, 1.3425e-16, 7.9651e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "177000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.4353e-17, 1.7684e-33, 3.5422e-33, 1.4333e-33],\n",
      "        [7.6864e-09, 5.2182e-17, 1.1118e-16, 6.3661e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "178000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[9.8149e-18, 1.5796e-33, 2.7384e-33, 1.1670e-33],\n",
      "        [7.4235e-09, 7.8103e-17, 1.4600e-16, 8.7349e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "179000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.3919e-17, 1.6396e-33, 3.1438e-33, 1.3002e-33],\n",
      "        [8.3871e-09, 6.5156e-17, 1.3085e-16, 7.6510e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "180000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[7.8745e-18, 1.4895e-33, 2.3948e-33, 1.0591e-33],\n",
      "        [7.2644e-09, 1.0521e-16, 1.8319e-16, 1.1305e-16],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "181000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[8.1580e-18, 1.4521e-33, 2.3187e-33, 1.0311e-33],\n",
      "        [7.6564e-09, 1.1257e-16, 1.9262e-16, 1.1961e-16],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "182000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[5.0084e-18, 1.2922e-33, 1.7468e-33, 8.2467e-34],\n",
      "        [7.0179e-09, 1.7553e-16, 2.6043e-16, 1.6988e-16],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "183000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[9.4347e-18, 1.3960e-33, 2.2740e-33, 1.0122e-33],\n",
      "        [8.6402e-09, 1.2112e-16, 2.0601e-16, 1.2839e-16],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "184000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[8.6202e-18, 1.3348e-33, 2.0832e-33, 9.4518e-34],\n",
      "        [8.7771e-09, 1.4237e-16, 2.3138e-16, 1.4668e-16],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "185000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.9313e-20, 3.5155e-38, 1.0407e-37, 5.5990e-38],\n",
      "        [7.0221e-10, 9.1095e-19, 2.1832e-18, 1.4709e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "186000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.9006e-20, 3.4938e-38, 1.0322e-37, 5.5585e-38],\n",
      "        [6.9973e-10, 9.1111e-19, 2.1791e-18, 1.4691e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "187000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.9869e-20, 3.5398e-38, 1.0515e-37, 5.6495e-38],\n",
      "        [7.0695e-10, 9.0975e-19, 2.1873e-18, 1.4722e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "188000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.8219e-20, 3.4359e-38, 1.0096e-37, 5.4506e-38],\n",
      "        [6.9369e-10, 9.1264e-19, 2.1698e-18, 1.4659e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "189000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.4013e-20, 3.2183e-38, 9.1366e-38, 5.0050e-38],\n",
      "        [6.6097e-10, 9.4354e-19, 2.1648e-18, 1.4797e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "190000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.7437e-20, 3.3929e-38, 9.8899e-38, 5.3594e-38],\n",
      "        [6.9011e-10, 9.2534e-19, 2.1801e-18, 1.4775e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "191000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.6251e-20, 3.3797e-38, 9.7125e-38, 5.2939e-38],\n",
      "        [6.8534e-10, 9.5684e-19, 2.2205e-18, 1.5122e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "192000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.0325e-20, 3.4623e-38, 1.0329e-37, 5.5485e-38],\n",
      "        [7.1046e-10, 8.9432e-19, 2.1555e-18, 1.4504e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "193000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.5643e-20, 3.4007e-38, 9.6573e-38, 5.2907e-38],\n",
      "        [6.8717e-10, 9.9506e-19, 2.2782e-18, 1.5585e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "194000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.8339e-20, 3.4314e-38, 1.0016e-37, 5.4297e-38],\n",
      "        [7.0335e-10, 9.4133e-19, 2.2156e-18, 1.5025e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "195000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.6973e-20, 3.4215e-38, 9.8264e-38, 5.3624e-38],\n",
      "        [6.9818e-10, 9.7900e-19, 2.2649e-18, 1.5446e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "196000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.9600e-20, 3.5101e-38, 1.1278e-37, 5.8923e-38],\n",
      "        [7.5462e-10, 7.6341e-19, 1.9821e-18, 1.3031e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "197000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.9437e-20, 3.4423e-38, 1.0115e-37, 5.4736e-38],\n",
      "        [7.1404e-10, 9.3681e-19, 2.2164e-18, 1.5010e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "198000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.8559e-20, 3.4363e-38, 9.9924e-38, 5.4310e-38],\n",
      "        [7.1147e-10, 9.6201e-19, 2.2503e-18, 1.5297e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "199000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.9717e-20, 3.4455e-38, 1.0121e-37, 5.4808e-38],\n",
      "        [7.1905e-10, 9.4389e-19, 2.2298e-18, 1.5111e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "200000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.7774e-20, 3.4321e-38, 9.8661e-38, 5.3895e-38],\n",
      "        [7.1127e-10, 9.9402e-19, 2.2949e-18, 1.5669e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "201000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.2308e-20, 3.4644e-38, 1.0400e-37, 5.5876e-38],\n",
      "        [7.3506e-10, 9.0605e-19, 2.1859e-18, 1.4718e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "202000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.6594e-20, 3.4242e-38, 9.6807e-38, 5.3262e-38],\n",
      "        [7.0945e-10, 1.0400e-18, 2.3567e-18, 1.6191e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "203000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.9682e-20, 3.4480e-38, 1.0054e-37, 5.4671e-38],\n",
      "        [7.2728e-10, 9.7473e-19, 2.2787e-18, 1.5502e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "204000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.7470e-20, 3.4323e-38, 9.7663e-38, 5.3630e-38],\n",
      "        [7.1781e-10, 1.0325e-18, 2.3524e-18, 1.6138e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "205000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.9903e-20, 3.4510e-38, 1.0049e-37, 5.4707e-38],\n",
      "        [7.3272e-10, 9.8573e-19, 2.2978e-18, 1.5650e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "206000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.9257e-20, 3.4471e-38, 9.9560e-38, 5.4391e-38],\n",
      "        [7.3144e-10, 1.0072e-18, 2.3268e-18, 1.5896e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "207000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.8126e-20, 3.4391e-38, 9.8039e-38, 5.3847e-38],\n",
      "        [7.2731e-10, 1.0400e-18, 2.3691e-18, 1.6259e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "208000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.3890e-20, 3.4801e-38, 1.0472e-37, 5.6330e-38],\n",
      "        [7.5679e-10, 9.2560e-19, 2.2279e-18, 1.5024e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "209000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.7721e-20, 3.5074e-38, 1.0868e-37, 5.7821e-38],\n",
      "        [7.7554e-10, 8.7034e-19, 2.1575e-18, 1.4417e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "210000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.7687e-20, 3.5079e-38, 1.0850e-37, 5.7782e-38],\n",
      "        [7.7745e-10, 8.7704e-19, 2.1687e-18, 1.4504e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "211000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.0732e-20, 3.4606e-38, 1.0060e-37, 5.4901e-38],\n",
      "        [7.4869e-10, 1.0112e-18, 2.3439e-18, 1.6004e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "212000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.5289e-20, 3.4923e-38, 1.0562e-37, 5.6774e-38],\n",
      "        [7.7160e-10, 9.2928e-19, 2.2423e-18, 1.5117e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "213000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.2762e-20, 3.4760e-38, 1.0266e-37, 5.5713e-38],\n",
      "        [7.6238e-10, 9.8340e-19, 2.3140e-18, 1.5728e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "214000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.4867e-20, 3.4909e-38, 1.0487e-37, 5.6552e-38],\n",
      "        [7.7382e-10, 9.5004e-19, 2.2738e-18, 1.5372e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "215000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.9613e-20, 3.5246e-38, 1.0968e-37, 5.8366e-38],\n",
      "        [7.9638e-10, 8.8194e-19, 2.1869e-18, 1.4623e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "216000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.0194e-20, 3.4592e-38, 9.9217e-38, 5.4512e-38],\n",
      "        [7.5601e-10, 1.0605e-18, 2.4168e-18, 1.6601e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "217000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.9715e-20, 3.5267e-38, 1.0948e-37, 5.8349e-38],\n",
      "        [8.0086e-10, 8.9267e-19, 2.2055e-18, 1.4768e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "218000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.4760e-20, 3.4924e-38, 1.0406e-37, 5.6370e-38],\n",
      "        [7.8261e-10, 9.8314e-19, 2.3266e-18, 1.5794e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "219000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.1248e-20, 3.4685e-38, 9.9950e-38, 5.4870e-38],\n",
      "        [7.6816e-10, 1.0625e-18, 2.4278e-18, 1.6669e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "220000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.3401e-20, 3.4842e-38, 1.0224e-37, 5.5750e-38],\n",
      "        [7.8079e-10, 1.0250e-18, 2.3843e-18, 1.6279e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "221000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.6444e-20, 3.5062e-38, 1.0540e-37, 5.6951e-38],\n",
      "        [7.9658e-10, 9.7475e-19, 2.3231e-18, 1.5741e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "222000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.8517e-20, 3.5215e-38, 1.0737e-37, 5.7721e-38],\n",
      "        [8.0802e-10, 9.4841e-19, 2.2918e-18, 1.5463e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "223000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.4831e-20, 3.4953e-38, 1.0332e-37, 5.6232e-38],\n",
      "        [7.9411e-10, 1.0202e-18, 2.3864e-18, 1.6270e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "224000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.6441e-20, 3.5080e-38, 1.0487e-37, 5.6847e-38],\n",
      "        [8.0389e-10, 9.9928e-19, 2.3627e-18, 1.6056e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "225000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.8903e-20, 3.5263e-38, 1.0727e-37, 5.7774e-38],\n",
      "        [8.1668e-10, 9.6462e-19, 2.3205e-18, 1.5686e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "226000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.7078e-20, 3.5139e-38, 1.0519e-37, 5.7029e-38],\n",
      "        [8.1165e-10, 1.0046e-18, 2.3750e-18, 1.6145e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "227000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.4367e-20, 3.4944e-38, 1.0214e-37, 5.5907e-38],\n",
      "        [8.0137e-10, 1.0628e-18, 2.4503e-18, 1.6792e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "228000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.8651e-20, 3.5258e-38, 1.0648e-37, 5.7569e-38],\n",
      "        [8.2289e-10, 9.9227e-19, 2.3641e-18, 1.6036e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "229000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.4793e-20, 3.4986e-38, 1.0220e-37, 5.5998e-38],\n",
      "        [8.0902e-10, 1.0745e-18, 2.4715e-18, 1.6955e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "230000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.8031e-20, 3.5231e-38, 1.0543e-37, 5.7250e-38],\n",
      "        [8.2619e-10, 1.0227e-18, 2.4095e-18, 1.6407e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "231000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.2282e-20, 3.4822e-38, 9.9102e-38, 5.4891e-38],\n",
      "        [8.0167e-10, 1.1474e-18, 2.5656e-18, 1.7765e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "232000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[4.0882e-20, 3.5446e-38, 1.0791e-37, 5.8252e-38],\n",
      "        [8.4378e-10, 9.9332e-19, 2.3781e-18, 1.6116e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "233000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.6389e-20, 3.5123e-38, 1.0315e-37, 5.6484e-38],\n",
      "        [8.2685e-10, 1.0805e-18, 2.4911e-18, 1.7086e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "234000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.0979e-20, 3.4726e-38, 9.6998e-38, 5.4183e-38],\n",
      "        [8.0319e-10, 1.2122e-18, 2.6522e-18, 1.8501e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "235000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.7792e-20, 3.5238e-38, 1.0414e-37, 5.6937e-38],\n",
      "        [8.3970e-10, 1.0769e-18, 2.4940e-18, 1.7088e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "236000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[2.9909e-20, 3.4648e-38, 9.5369e-38, 5.3619e-38],\n",
      "        [8.0286e-10, 1.2634e-18, 2.7184e-18, 1.9070e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "237000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.2054e-20, 3.4823e-38, 9.7649e-38, 5.4527e-38],\n",
      "        [8.1707e-10, 1.2190e-18, 2.6709e-18, 1.8631e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "238000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.6260e-20, 3.5138e-38, 1.0197e-37, 5.6210e-38],\n",
      "        [8.4107e-10, 1.1366e-18, 2.5773e-18, 1.7786e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "239000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.8069e-20, 3.5950e-38, 1.1304e-37, 6.0436e-38],\n",
      "        [8.9362e-10, 9.5640e-19, 2.3526e-18, 1.5826e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "240000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.9430e-20, 3.5382e-38, 1.0478e-37, 5.7353e-38],\n",
      "        [8.6134e-10, 1.0970e-18, 2.5352e-18, 1.7396e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "241000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.9392e-20, 3.5384e-38, 1.0456e-37, 5.7300e-38],\n",
      "        [8.6382e-10, 1.1069e-18, 2.5506e-18, 1.7520e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "242000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.9635e-20, 3.5407e-38, 1.0455e-37, 5.7339e-38],\n",
      "        [8.6851e-10, 1.1151e-18, 2.5649e-18, 1.7632e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "243000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[4.4522e-20, 3.5749e-38, 1.0897e-37, 5.9060e-38],\n",
      "        [8.9242e-10, 1.0445e-18, 2.4800e-18, 1.6882e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "244000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[4.3303e-20, 3.5674e-38, 1.0763e-37, 5.8587e-38],\n",
      "        [8.9051e-10, 1.0736e-18, 2.5202e-18, 1.7220e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "245000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.0922e-20, 3.5510e-38, 1.0518e-37, 5.7684e-38],\n",
      "        [8.8302e-10, 1.1228e-18, 2.5844e-18, 1.7771e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "246000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[4.3756e-20, 3.5714e-38, 1.0770e-37, 5.8676e-38],\n",
      "        [8.9758e-10, 1.0835e-18, 2.5383e-18, 1.7359e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "247000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.6724e-20, 3.5202e-38, 1.0067e-37, 5.6001e-38],\n",
      "        [8.6864e-10, 1.2225e-18, 2.7107e-18, 1.8865e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "248000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.9910e-20, 3.5451e-38, 1.0364e-37, 5.7191e-38],\n",
      "        [8.8677e-10, 1.1705e-18, 2.6523e-18, 1.8336e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "249000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.7779e-20, 3.5286e-38, 1.0131e-37, 5.6317e-38],\n",
      "        [8.7986e-10, 1.2238e-18, 2.7196e-18, 1.8920e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "250000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.3633e-20, 3.4967e-38, 9.6836e-38, 5.4605e-38],\n",
      "        [8.6135e-10, 1.3287e-18, 2.8438e-18, 2.0024e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "251000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.0679e-20, 3.5513e-38, 1.0377e-37, 5.7338e-38],\n",
      "        [8.9909e-10, 1.1880e-18, 2.6837e-18, 1.8579e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "252000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.2297e-20, 3.5635e-38, 1.0509e-37, 5.7885e-38],\n",
      "        [9.0955e-10, 1.1700e-18, 2.6657e-18, 1.8409e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "253000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.8943e-20, 3.5388e-38, 1.0169e-37, 5.6595e-38],\n",
      "        [8.9686e-10, 1.2435e-18, 2.7568e-18, 1.9205e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "254000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[4.6198e-20, 3.5910e-38, 1.0820e-37, 5.9157e-38],\n",
      "        [9.3260e-10, 1.1269e-18, 2.6200e-18, 1.7984e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "255000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[4.6912e-20, 3.5963e-38, 1.0865e-37, 5.9362e-38],\n",
      "        [9.3845e-10, 1.1254e-18, 2.6215e-18, 1.7987e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "256000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[4.4112e-20, 3.5776e-38, 1.0587e-37, 5.8338e-38],\n",
      "        [9.3061e-10, 1.1844e-18, 2.6983e-18, 1.8647e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "257000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[4.2611e-20, 3.5671e-38, 1.0428e-37, 5.7755e-38],\n",
      "        [9.2709e-10, 1.2220e-18, 2.7472e-18, 1.9068e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "258000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[5.3179e-20, 3.6364e-38, 1.1310e-37, 6.1196e-38],\n",
      "        [9.7502e-10, 1.0746e-18, 2.5705e-18, 1.7500e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "259000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[4.9383e-20, 3.6137e-38, 1.0974e-37, 5.9954e-38],\n",
      "        [9.6368e-10, 1.1377e-18, 2.6536e-18, 1.8213e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "260000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.3230e-20, 3.5717e-38, 1.0405e-37, 5.7791e-38],\n",
      "        [9.4151e-10, 1.2529e-18, 2.7976e-18, 1.9471e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "261000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[4.7960e-20, 3.6049e-38, 1.0797e-37, 5.9362e-38],\n",
      "        [9.6604e-10, 1.1864e-18, 2.7225e-18, 1.8790e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "262000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[4.7236e-20, 3.6005e-38, 1.0709e-37, 5.9063e-38],\n",
      "        [9.6699e-10, 1.2117e-18, 2.7577e-18, 1.9086e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "263000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[4.7359e-20, 3.6016e-38, 1.0696e-37, 5.9053e-38],\n",
      "        [9.7106e-10, 1.2219e-18, 2.7741e-18, 1.9218e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "264000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[5.7675e-20, 3.6661e-38, 1.1508e-37, 6.2223e-38],\n",
      "        [1.0155e-09, 1.0894e-18, 2.6146e-18, 1.7803e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "265000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.3157e-20, 3.5721e-38, 1.0275e-37, 5.7482e-38],\n",
      "        [9.5993e-10, 1.3241e-18, 2.9038e-18, 2.0345e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "266000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.0024e-20, 3.5361e-38, 9.9251e-38, 5.6085e-38],\n",
      "        [9.4924e-10, 1.4005e-18, 2.9926e-18, 2.1145e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "267000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.2654e-20, 3.6376e-38, 1.1035e-37, 6.0544e-38],\n",
      "        [1.0086e-09, 1.1934e-18, 2.7576e-18, 1.9013e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "268000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.9182e-20, 3.6145e-38, 1.0723e-37, 5.9363e-38],\n",
      "        [9.9810e-10, 1.2590e-18, 2.8408e-18, 1.9737e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "269000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[4.6202e-20, 3.5937e-38, 1.0448e-37, 5.8314e-38],\n",
      "        [9.8823e-10, 1.3210e-18, 2.9176e-18, 2.0411e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "270000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.5595e-20, 3.5872e-38, 1.0364e-37, 5.8016e-38],\n",
      "        [9.8927e-10, 1.3453e-18, 2.9496e-18, 2.0687e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "271000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.2960e-20, 3.5533e-38, 1.0071e-37, 5.6818e-38],\n",
      "        [9.8046e-10, 1.4038e-18, 3.0158e-18, 2.1291e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "272000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.5739e-20, 3.6565e-38, 1.1154e-37, 6.1198e-38],\n",
      "        [1.0392e-09, 1.2096e-18, 2.7976e-18, 1.9300e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "273000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.7497e-20, 3.5977e-38, 1.0452e-37, 5.8457e-38],\n",
      "        [1.0083e-09, 1.3481e-18, 2.9645e-18, 2.0783e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "274000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[4.6146e-20, 3.5788e-38, 1.0290e-37, 5.7808e-38],\n",
      "        [1.0056e-09, 1.3817e-18, 3.0042e-18, 2.1141e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "275000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.9001e-20, 3.6757e-38, 1.1329e-37, 6.1989e-38],\n",
      "        [1.0625e-09, 1.2003e-18, 2.7980e-18, 1.9265e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "276000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[4.6903e-20, 3.5787e-38, 1.0289e-37, 5.7862e-38],\n",
      "        [1.0171e-09, 1.3957e-18, 3.0283e-18, 2.1333e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "277000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.5049e-20, 3.5544e-38, 1.0081e-37, 5.7013e-38],\n",
      "        [1.0119e-09, 1.4400e-18, 3.0787e-18, 2.1793e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "278000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.7341e-20, 3.5753e-38, 1.0259e-37, 5.7790e-38],\n",
      "        [1.0271e-09, 1.4152e-18, 3.0581e-18, 2.1580e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "279000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.9036e-20, 3.5902e-38, 1.0384e-37, 5.8342e-38],\n",
      "        [1.0384e-09, 1.3994e-18, 3.0456e-18, 2.1446e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "280000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.8933e-20, 3.6756e-38, 1.1174e-37, 6.1630e-38],\n",
      "        [1.0855e-09, 1.2752e-18, 2.9147e-18, 2.0215e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "281000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.0894e-20, 3.5985e-38, 1.0448e-37, 5.8700e-38],\n",
      "        [1.0587e-09, 1.4093e-18, 3.0700e-18, 2.1625e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "282000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.3299e-20, 3.6184e-38, 1.0618e-37, 5.9445e-38],\n",
      "        [1.0739e-09, 1.3881e-18, 3.0529e-18, 2.1445e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "283000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.8323e-20, 3.5608e-38, 1.0130e-37, 5.7427e-38],\n",
      "        [1.0568e-09, 1.4842e-18, 3.1596e-18, 2.2430e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "284000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[7.1801e-20, 3.7485e-38, 1.1940e-37, 6.4834e-38],\n",
      "        [1.1529e-09, 1.1857e-18, 2.8280e-18, 1.9370e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "285000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.2299e-20, 3.5943e-38, 1.0409e-37, 5.8668e-38],\n",
      "        [1.0835e-09, 1.4482e-18, 3.1323e-18, 2.2134e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "286000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.3639e-20, 3.6036e-38, 1.0487e-37, 5.9024e-38],\n",
      "        [1.0938e-09, 1.4419e-18, 3.1308e-18, 2.2101e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "287000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[5.3833e-20, 3.6010e-38, 1.0462e-37, 5.8954e-38],\n",
      "        [1.0997e-09, 1.4552e-18, 3.1506e-18, 2.2267e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "288000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[5.4855e-20, 3.6065e-38, 1.0506e-37, 5.9178e-38],\n",
      "        [1.1091e-09, 1.4561e-18, 3.1573e-18, 2.2309e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "289000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.0626e-20, 3.6576e-38, 1.0939e-37, 6.1027e-38],\n",
      "        [1.1377e-09, 1.3907e-18, 3.0934e-18, 2.1685e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "290000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[4.3739e-20, 3.4745e-38, 9.4338e-38, 5.4658e-38],\n",
      "        [1.0661e-09, 1.6891e-18, 3.4032e-18, 2.4623e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "291000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.5359e-20, 3.5971e-38, 1.0427e-37, 5.8938e-38],\n",
      "        [1.1254e-09, 1.4941e-18, 3.2129e-18, 2.2776e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "292000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.2595e-20, 3.5631e-38, 1.0145e-37, 5.7780e-38],\n",
      "        [1.1192e-09, 1.5580e-18, 3.2855e-18, 2.3442e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "293000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.1492e-20, 3.5470e-38, 1.0010e-37, 5.7241e-38],\n",
      "        [1.1191e-09, 1.5941e-18, 3.3279e-18, 2.3826e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "294000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[6.8099e-20, 3.6990e-38, 1.1289e-37, 6.2669e-38],\n",
      "        [1.1929e-09, 1.3769e-18, 3.1073e-18, 2.1710e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "295000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[4.7107e-20, 3.4868e-38, 9.5342e-38, 5.5256e-38],\n",
      "        [1.1090e-09, 1.7169e-18, 3.4624e-18, 2.5076e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "296000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[7.6752e-20, 3.7587e-38, 1.1811e-37, 6.4911e-38],\n",
      "        [1.2350e-09, 1.3169e-18, 3.0533e-18, 2.1162e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "297000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.9762e-20, 3.6066e-38, 1.0505e-37, 5.9489e-38],\n",
      "        [1.1774e-09, 1.5362e-18, 3.2943e-18, 2.3410e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "298000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.8819e-20, 3.5907e-38, 1.0378e-37, 5.8985e-38],\n",
      "        [1.1796e-09, 1.5700e-18, 3.3357e-18, 2.3781e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "299000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[4.6408e-20, 3.4540e-38, 9.2970e-38, 5.4342e-38],\n",
      "        [1.1263e-09, 1.8121e-18, 3.5791e-18, 2.6119e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "300000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[5.8253e-20, 3.5742e-38, 1.0244e-37, 5.8483e-38],\n",
      "        [1.1880e-09, 1.6144e-18, 3.3932e-18, 2.4286e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "301000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[6.5225e-20, 3.6348e-38, 1.0742e-37, 6.0633e-38],\n",
      "        [1.2215e-09, 1.5288e-18, 3.3108e-18, 2.3477e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "302000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.0196e-20, 3.5851e-38, 1.0332e-37, 5.8909e-38],\n",
      "        [1.2044e-09, 1.6113e-18, 3.3994e-18, 2.4310e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "303000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[6.2415e-20, 3.5997e-38, 1.0453e-37, 5.9463e-38],\n",
      "        [1.2197e-09, 1.5980e-18, 3.3926e-18, 2.4220e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "304000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.0759e-20, 3.5783e-38, 1.0282e-37, 5.8763e-38],\n",
      "        [1.2182e-09, 1.6402e-18, 3.4413e-18, 2.4665e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "305000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[4.7332e-20, 3.4310e-38, 9.1431e-38, 5.3829e-38],\n",
      "        [1.1605e-09, 1.9085e-18, 3.7061e-18, 2.7230e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "306000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.9258e-20, 3.7228e-38, 1.1483e-37, 6.3949e-38],\n",
      "        [1.3033e-09, 1.4511e-18, 3.2609e-18, 2.2882e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "307000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[8.8797e-20, 3.7866e-38, 1.2043e-37, 6.6320e-38],\n",
      "        [1.3410e-09, 1.3757e-18, 3.1847e-18, 2.2144e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "308000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.5762e-20, 3.5984e-38, 1.0456e-37, 5.9654e-38],\n",
      "        [1.2634e-09, 1.6459e-18, 3.4743e-18, 2.4881e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "309000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[5.8232e-20, 3.5205e-38, 9.8455e-38, 5.7042e-38],\n",
      "        [1.2369e-09, 1.7823e-18, 3.6149e-18, 2.6224e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "310000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.7071e-20, 3.5031e-38, 9.7143e-38, 5.6496e-38],\n",
      "        [1.2367e-09, 1.8211e-18, 3.6582e-18, 2.6626e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "311000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.5262e-20, 3.6596e-38, 1.0961e-37, 6.1931e-38],\n",
      "        [1.3202e-09, 1.5838e-18, 3.4327e-18, 2.4398e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "312000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.8087e-20, 3.5926e-38, 1.0420e-37, 5.9645e-38],\n",
      "        [1.2983e-09, 1.6946e-18, 3.5513e-18, 2.5519e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "313000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.5848e-20, 3.5657e-38, 1.0210e-37, 5.8774e-38],\n",
      "        [1.2956e-09, 1.7480e-18, 3.6112e-18, 2.6074e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "314000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.2425e-20, 3.5259e-38, 9.9078e-38, 5.7490e-38],\n",
      "        [1.2877e-09, 1.8243e-18, 3.6928e-18, 2.6844e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "315000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.3139e-20, 3.4258e-38, 9.1603e-38, 5.4211e-38],\n",
      "        [1.2495e-09, 2.0175e-18, 3.8796e-18, 2.8673e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "316000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.1244e-20, 3.4999e-38, 9.7162e-38, 5.6724e-38],\n",
      "        [1.2963e-09, 1.8935e-18, 3.7750e-18, 2.7591e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "317000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.2532e-20, 3.5059e-38, 9.7649e-38, 5.6970e-38],\n",
      "        [1.3081e-09, 1.8929e-18, 3.7816e-18, 2.7628e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "318000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.7510e-20, 3.5440e-38, 1.0057e-37, 5.8301e-38],\n",
      "        [1.3380e-09, 1.8418e-18, 3.7429e-18, 2.7207e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "319000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[8.0515e-20, 3.6452e-38, 1.0853e-37, 6.1812e-38],\n",
      "        [1.3980e-09, 1.6948e-18, 3.6088e-18, 2.5855e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "320000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.8598e-20, 3.5385e-38, 1.0023e-37, 5.8229e-38],\n",
      "        [1.3570e-09, 1.8739e-18, 3.7902e-18, 2.7609e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "321000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.8717e-20, 3.5310e-38, 9.9733e-38, 5.8047e-38],\n",
      "        [1.3650e-09, 1.8975e-18, 3.8211e-18, 2.7881e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "322000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[5.8814e-20, 3.4295e-38, 9.2289e-38, 5.4761e-38],\n",
      "        [1.3262e-09, 2.0922e-18, 4.0076e-18, 2.9719e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "323000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[6.4115e-20, 3.4725e-38, 9.5463e-38, 5.6229e-38],\n",
      "        [1.3597e-09, 2.0261e-18, 3.9585e-18, 2.9181e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "324000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[6.5332e-20, 3.4754e-38, 9.5744e-38, 5.6390e-38],\n",
      "        [1.3728e-09, 2.0324e-18, 3.9732e-18, 2.9293e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "325000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.5792e-20, 3.4725e-38, 9.5571e-38, 5.6348e-38],\n",
      "        [1.3818e-09, 2.0496e-18, 3.9972e-18, 2.9501e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "326000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[8.8448e-20, 3.6492e-38, 1.0911e-37, 6.2366e-38],\n",
      "        [1.4828e-09, 1.7690e-18, 3.7428e-18, 2.6928e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "327000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[8.0174e-20, 3.5783e-38, 1.0360e-37, 5.9999e-38],\n",
      "        [1.4596e-09, 1.8932e-18, 3.8720e-18, 2.8169e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "328000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[6.9049e-20, 3.4770e-38, 9.6071e-38, 5.6686e-38],\n",
      "        [1.4198e-09, 2.0794e-18, 4.0524e-18, 2.9944e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "329000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.4806e-20, 3.5193e-38, 9.9248e-38, 5.8142e-38],\n",
      "        [1.4519e-09, 2.0143e-18, 4.0015e-18, 2.9397e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "330000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[8.8401e-20, 3.6184e-38, 1.0682e-37, 6.1530e-38],\n",
      "        [1.5138e-09, 1.8627e-18, 3.8682e-18, 2.8029e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "331000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[8.6817e-20, 3.5963e-38, 1.0515e-37, 6.0854e-38],\n",
      "        [1.5185e-09, 1.9142e-18, 3.9295e-18, 2.8589e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "332000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[8.8570e-20, 3.6017e-38, 1.0561e-37, 6.1094e-38],\n",
      "        [1.5330e-09, 1.9175e-18, 3.9415e-18, 2.8673e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "333000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[9.7321e-20, 3.6542e-38, 1.0973e-37, 6.2957e-38],\n",
      "        [1.5749e-09, 1.8504e-18, 3.8884e-18, 2.8100e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "334000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[7.2723e-20, 3.4568e-38, 9.5015e-38, 5.6434e-38],\n",
      "        [1.4844e-09, 2.1935e-18, 4.2128e-18, 3.1334e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "335000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[8.7423e-20, 3.5659e-38, 1.0307e-37, 6.0100e-38],\n",
      "        [1.5552e-09, 2.0150e-18, 4.0629e-18, 2.9768e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "336000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[8.0894e-20, 3.5064e-38, 9.8758e-38, 5.8202e-38],\n",
      "        [1.5367e-09, 2.1280e-18, 4.1748e-18, 3.0863e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "337000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[7.9427e-20, 3.4822e-38, 9.7114e-38, 5.7506e-38],\n",
      "        [1.5417e-09, 2.1880e-18, 4.2416e-18, 3.1488e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "338000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[7.9207e-20, 3.4709e-38, 9.6378e-38, 5.7213e-38],\n",
      "        [1.5497e-09, 2.2227e-18, 4.2829e-18, 3.1867e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "339000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.0352e-20, 3.3868e-38, 9.0509e-38, 5.4569e-38],\n",
      "        [1.5185e-09, 2.4043e-18, 4.4510e-18, 3.3553e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "340000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[7.4346e-20, 3.4134e-38, 9.2399e-38, 5.5476e-38],\n",
      "        [1.5455e-09, 2.3654e-18, 4.4288e-18, 3.3276e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "341000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[7.4934e-20, 3.4083e-38, 9.2089e-38, 5.5382e-38],\n",
      "        [1.5587e-09, 2.3952e-18, 4.4678e-18, 3.3623e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "342000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[8.9296e-20, 3.5078e-38, 9.9271e-38, 5.8720e-38],\n",
      "        [1.6343e-09, 2.2257e-18, 4.3375e-18, 3.2208e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "343000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[8.1680e-20, 3.4406e-38, 9.4585e-38, 5.6611e-38],\n",
      "        [1.6096e-09, 2.3611e-18, 4.4639e-18, 3.3475e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "344000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[7.7479e-20, 3.3955e-38, 9.1529e-38, 5.5248e-38],\n",
      "        [1.6015e-09, 2.4693e-18, 4.5692e-18, 3.4513e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "345000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[8.9132e-20, 3.4744e-38, 9.7114e-38, 5.7878e-38],\n",
      "        [1.6656e-09, 2.3372e-18, 4.4734e-18, 3.3446e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "346000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.0608e-19, 3.5834e-38, 1.0506e-37, 6.1532e-38],\n",
      "        [1.7417e-09, 2.1610e-18, 4.3304e-18, 3.1921e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "347000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[7.3905e-20, 3.3331e-38, 8.7618e-38, 5.3538e-38],\n",
      "        [1.6137e-09, 2.6487e-18, 4.7540e-18, 3.6298e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "348000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[8.5908e-20, 3.4215e-38, 9.3668e-38, 5.6402e-38],\n",
      "        [1.6790e-09, 2.4811e-18, 4.6292e-18, 3.4927e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "349000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[9.8032e-20, 3.4999e-38, 9.9231e-38, 5.9011e-38],\n",
      "        [1.7401e-09, 2.3491e-18, 4.5302e-18, 3.3837e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "350000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[7.1479e-20, 3.2824e-38, 8.4514e-38, 5.2187e-38],\n",
      "        [1.6304e-09, 2.8176e-18, 4.9280e-18, 3.7977e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "351000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.1621e-19, 3.5965e-38, 1.0634e-37, 6.2352e-38],\n",
      "        [1.8326e-09, 2.2203e-18, 4.4466e-18, 3.2843e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "352000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.1472e-19, 3.5732e-38, 1.0478e-37, 6.1699e-38],\n",
      "        [1.8405e-09, 2.2754e-18, 4.5100e-18, 3.3436e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "353000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[8.4310e-20, 3.3526e-38, 8.9497e-38, 5.4664e-38],\n",
      "        [1.7260e-09, 2.7114e-18, 4.8871e-18, 3.7350e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "354000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[8.9761e-20, 3.3835e-38, 9.1695e-38, 5.5729e-38],\n",
      "        [1.7601e-09, 2.6582e-18, 4.8558e-18, 3.6967e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "355000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[9.2804e-20, 3.3937e-38, 9.2509e-38, 5.6151e-38],\n",
      "        [1.7850e-09, 2.6542e-18, 4.8672e-18, 3.7021e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "356000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.1512e-19, 3.5273e-38, 1.0188e-37, 6.0574e-38],\n",
      "        [1.8917e-09, 2.4256e-18, 4.6983e-18, 3.5143e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "357000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.2009e-19, 3.5447e-38, 1.0320e-37, 6.1237e-38],\n",
      "        [1.9243e-09, 2.4175e-18, 4.7087e-18, 3.5174e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "358000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[9.4332e-20, 3.3650e-38, 9.0929e-38, 5.5555e-38],\n",
      "        [1.8315e-09, 2.7793e-18, 5.0190e-18, 3.8414e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "359000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[8.0942e-20, 3.2508e-38, 8.3642e-38, 5.2117e-38],\n",
      "        [1.7801e-09, 3.0585e-18, 5.2506e-18, 4.0852e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "360000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[8.8789e-20, 3.2988e-38, 8.6813e-38, 5.3689e-38],\n",
      "        [1.8318e-09, 2.9699e-18, 5.2019e-18, 4.0236e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "361000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[8.0896e-20, 3.2264e-38, 8.2329e-38, 5.1562e-38],\n",
      "        [1.8031e-09, 3.1599e-18, 5.3589e-18, 4.1891e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "362000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[9.4771e-20, 3.3125e-38, 8.7996e-38, 5.4358e-38],\n",
      "        [1.8890e-09, 2.9862e-18, 5.2537e-18, 4.0622e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "363000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[8.0801e-20, 3.1930e-38, 8.0594e-38, 5.0816e-38],\n",
      "        [1.8333e-09, 3.2971e-18, 5.5029e-18, 4.3283e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "364000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.0651e-19, 3.3606e-38, 9.1478e-38, 5.6122e-38],\n",
      "        [1.9721e-09, 2.9265e-18, 5.2481e-18, 4.0374e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "365000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.7101e-20, 3.0434e-38, 7.2097e-38, 4.6673e-38],\n",
      "        [1.7815e-09, 3.7549e-18, 5.8596e-18, 4.7126e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "366000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.1328e-19, 3.3775e-38, 9.2875e-38, 5.6876e-38],\n",
      "        [2.0267e-09, 2.9289e-18, 5.2836e-18, 4.0606e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "367000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.1322e-19, 3.3654e-38, 9.2171e-38, 5.6585e-38],\n",
      "        [2.0392e-09, 2.9760e-18, 5.3368e-18, 4.1108e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "368000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[7.0024e-20, 3.0315e-38, 7.1898e-38, 4.6641e-38],\n",
      "        [1.8291e-09, 3.8290e-18, 5.9530e-18, 4.7989e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "369000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[8.3922e-20, 3.1349e-38, 7.8030e-38, 4.9766e-38],\n",
      "        [1.9249e-09, 3.5691e-18, 5.8020e-18, 4.6138e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "370000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[9.2392e-20, 3.1878e-38, 8.1320e-38, 5.1432e-38],\n",
      "        [1.9803e-09, 3.4503e-18, 5.7357e-18, 4.5307e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "371000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[9.1486e-20, 3.1686e-38, 8.0270e-38, 5.0954e-38],\n",
      "        [1.9887e-09, 3.5250e-18, 5.8076e-18, 4.6025e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "372000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[8.6412e-20, 3.1165e-38, 7.7288e-38, 4.9515e-38],\n",
      "        [1.9766e-09, 3.6941e-18, 5.9462e-18, 4.7496e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "373000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[9.3381e-20, 3.1538e-38, 7.9626e-38, 5.0730e-38],\n",
      "        [2.0288e-09, 3.6240e-18, 5.9220e-18, 4.7101e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "374000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[8.5043e-20, 3.0770e-38, 7.5251e-38, 4.8582e-38],\n",
      "        [1.9985e-09, 3.8642e-18, 6.1060e-18, 4.9109e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "375000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.2113e-19, 3.3075e-38, 8.9385e-38, 5.5620e-38],\n",
      "        [2.1870e-09, 3.2928e-18, 5.7279e-18, 4.4706e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "376000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[8.6477e-20, 3.0624e-38, 7.4665e-38, 4.8355e-38],\n",
      "        [2.0324e-09, 3.9565e-18, 6.2074e-18, 5.0079e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "377000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.1921e-19, 3.2700e-38, 8.7218e-38, 5.4661e-38],\n",
      "        [2.2090e-09, 3.4369e-18, 5.8753e-18, 4.6152e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "378000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.3754e-19, 3.3579e-38, 9.2941e-38, 5.7516e-38],\n",
      "        [2.3045e-09, 3.2697e-18, 5.7806e-18, 4.4955e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "379000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[9.1073e-20, 3.0488e-38, 7.4373e-38, 4.8320e-38],\n",
      "        [2.1026e-09, 4.0751e-18, 6.3542e-18, 5.1443e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "380000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.0303e-19, 3.1180e-38, 7.8547e-38, 5.0486e-38],\n",
      "        [2.1829e-09, 3.9091e-18, 6.2713e-18, 5.0341e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "381000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[9.9697e-20, 3.0792e-38, 7.6457e-38, 4.9471e-38],\n",
      "        [2.1825e-09, 4.0490e-18, 6.3884e-18, 5.1579e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "382000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.2136e-19, 3.2022e-38, 8.3904e-38, 5.3256e-38],\n",
      "        [2.3026e-09, 3.7373e-18, 6.2022e-18, 4.9304e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "383000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[8.1950e-20, 2.8835e-38, 6.5703e-38, 4.3835e-38],\n",
      "        [2.0984e-09, 4.6650e-18, 6.8189e-18, 5.6462e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "384000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[8.8818e-20, 2.9233e-38, 6.8117e-38, 4.5170e-38],\n",
      "        [2.1618e-09, 4.5752e-18, 6.7961e-18, 5.6019e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "385000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[8.4646e-20, 2.8645e-38, 6.5053e-38, 4.3553e-38],\n",
      "        [2.1486e-09, 4.7919e-18, 6.9533e-18, 5.7776e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "386000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[8.9916e-20, 2.8931e-38, 6.6773e-38, 4.4518e-38],\n",
      "        [2.1986e-09, 4.7327e-18, 6.9455e-18, 5.7535e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "387000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.0900e-19, 3.0368e-38, 7.5033e-38, 4.8963e-38],\n",
      "        [2.3289e-09, 4.3408e-18, 6.7289e-18, 5.4804e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "388000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[9.2784e-20, 2.8822e-38, 6.6464e-38, 4.4414e-38],\n",
      "        [2.2448e-09, 4.8277e-18, 7.0529e-18, 5.8567e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "389000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.1292e-19, 3.0264e-38, 7.4760e-38, 4.8896e-38],\n",
      "        [2.3827e-09, 4.4313e-18, 6.8397e-18, 5.5842e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "390000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[8.6563e-20, 2.7871e-38, 6.1737e-38, 4.1893e-38],\n",
      "        [2.2315e-09, 5.2050e-18, 7.3251e-18, 6.1619e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "391000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[8.6848e-20, 2.7658e-38, 6.0879e-38, 4.1445e-38],\n",
      "        [2.2496e-09, 5.3058e-18, 7.4108e-18, 6.2524e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "392000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[9.5932e-20, 2.8289e-38, 6.4307e-38, 4.3360e-38],\n",
      "        [2.3244e-09, 5.1197e-18, 7.3313e-18, 6.1393e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "393000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[7.4651e-20, 2.6153e-38, 5.3619e-38, 3.7438e-38],\n",
      "        [2.1861e-09, 5.9667e-18, 7.8270e-18, 6.7430e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "394000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[8.5584e-20, 2.6976e-38, 5.7858e-38, 3.9851e-38],\n",
      "        [2.2842e-09, 5.6624e-18, 7.6940e-18, 6.5583e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "395000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.1683e-19, 2.9245e-38, 7.0068e-38, 4.6609e-38],\n",
      "        [2.5075e-09, 4.9293e-18, 7.3207e-18, 6.0707e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "396000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[9.2599e-20, 2.7168e-38, 5.9184e-38, 4.0654e-38],\n",
      "        [2.3665e-09, 5.6617e-18, 7.7580e-18, 6.6029e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "397000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.3223e-19, 2.9871e-38, 7.4023e-38, 4.8811e-38],\n",
      "        [2.6284e-09, 4.8048e-18, 7.3062e-18, 6.0202e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "398000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[9.9470e-20, 2.7307e-38, 6.0218e-38, 4.1301e-38],\n",
      "        [2.4486e-09, 5.6964e-18, 7.8477e-18, 6.6761e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "399000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.4603e-19, 3.0292e-38, 7.6869e-38, 5.0417e-38],\n",
      "        [2.7390e-09, 4.7561e-18, 7.3420e-18, 6.0278e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "400000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.0153e-19, 2.7013e-38, 5.9134e-38, 4.0753e-38],\n",
      "        [2.4985e-09, 5.8822e-18, 8.0171e-18, 6.8512e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "401000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[9.0615e-20, 2.5905e-38, 5.3870e-38, 3.7803e-38],\n",
      "        [2.4395e-09, 6.3782e-18, 8.3117e-18, 7.2093e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "402000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.0276e-19, 2.6649e-38, 5.7695e-38, 4.0010e-38],\n",
      "        [2.5444e-09, 6.1119e-18, 8.2137e-18, 7.0597e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "403000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[9.6345e-20, 2.5987e-38, 5.4352e-38, 3.8176e-38],\n",
      "        [2.5200e-09, 6.4883e-18, 8.4521e-18, 7.3421e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "404000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[8.3978e-20, 2.6607e-38, 5.1535e-38, 3.7150e-38],\n",
      "        [2.4183e-09, 7.2070e-18, 8.8352e-18, 7.8281e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "405000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.0882e-19, 2.6412e-38, 5.7077e-38, 3.9764e-38],\n",
      "        [2.6442e-09, 6.3502e-18, 8.4572e-18, 7.3042e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "406000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.2583e-19, 2.7367e-38, 6.2106e-38, 4.2630e-38],\n",
      "        [2.7708e-09, 6.0101e-18, 8.3152e-18, 7.0990e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "407000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.0270e-19, 2.6501e-38, 5.4593e-38, 3.8731e-38],\n",
      "        [2.6293e-09, 6.9090e-18, 8.8218e-18, 7.7313e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "408000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.3192e-19, 2.7141e-38, 6.1472e-38, 4.2364e-38],\n",
      "        [2.8620e-09, 6.2135e-18, 8.5273e-18, 7.3105e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "409000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.0196e-19, 2.6898e-38, 5.3760e-38, 3.8630e-38],\n",
      "        [2.6553e-09, 7.3518e-18, 9.1267e-18, 8.0817e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "410000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[9.7543e-20, 2.7219e-38, 5.2628e-38, 3.8275e-38],\n",
      "        [2.6302e-09, 7.7441e-18, 9.3520e-18, 8.3595e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "411000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.0320e-19, 2.7227e-38, 5.3501e-38, 3.8757e-38],\n",
      "        [2.6942e-09, 7.6773e-18, 9.3671e-18, 8.3510e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "412000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[7.1583e-20, 2.8604e-38, 4.6484e-38, 3.5857e-38],\n",
      "        [2.3973e-09, 9.8116e-18, 1.0324e-17, 9.6523e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "413000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.0040e-19, 2.7723e-38, 5.2364e-38, 3.8534e-38],\n",
      "        [2.7016e-09, 8.2649e-18, 9.7354e-18, 8.7891e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "414000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.1532e-19, 2.7480e-38, 5.4901e-38, 3.9726e-38],\n",
      "        [2.8446e-09, 7.8041e-18, 9.5764e-18, 8.5400e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "415000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.0113e-19, 2.8071e-38, 5.1977e-38, 3.8600e-38],\n",
      "        [2.7373e-09, 8.6643e-18, 1.0007e-17, 9.1030e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "416000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[9.5741e-20, 2.8440e-38, 5.0612e-38, 3.8130e-38],\n",
      "        [2.7035e-09, 9.2098e-18, 1.0292e-17, 9.4679e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "417000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.3090e-19, 2.7713e-38, 5.6783e-38, 4.0927e-38],\n",
      "        [3.0182e-09, 7.8525e-18, 9.7560e-18, 8.6844e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "418000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.4901e-19, 2.7548e-38, 5.9410e-38, 4.2178e-38],\n",
      "        [3.1699e-09, 7.4735e-18, 9.6342e-18, 8.4825e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "419000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.2654e-19, 2.8155e-38, 5.5343e-38, 4.0545e-38],\n",
      "        [3.0185e-09, 8.4581e-18, 1.0130e-17, 9.1355e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "420000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.2744e-19, 2.8313e-38, 5.5165e-38, 4.0590e-38],\n",
      "        [3.0427e-09, 8.6520e-18, 1.0268e-17, 9.2935e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "421000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[9.9341e-20, 2.9273e-38, 4.9782e-38, 3.8340e-38],\n",
      "        [2.8175e-09, 1.0349e-17, 1.1034e-17, 1.0339e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "422000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.0189e-19, 2.9359e-38, 4.9991e-38, 3.8535e-38],\n",
      "        [2.8558e-09, 1.0443e-17, 1.1122e-17, 1.0432e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "423000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.0751e-19, 2.9366e-38, 5.0803e-38, 3.9008e-38],\n",
      "        [2.9214e-09, 1.0350e-17, 1.1136e-17, 1.0418e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "424000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.0323e-19, 2.9687e-38, 4.9645e-38, 3.8606e-38],\n",
      "        [2.9003e-09, 1.0932e-17, 1.1428e-17, 1.0798e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "425000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[8.7811e-20, 3.0315e-38, 4.6344e-38, 3.7150e-38],\n",
      "        [2.7605e-09, 1.2311e-17, 1.1987e-17, 1.1592e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "426000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.3250e-19, 2.9378e-38, 5.4151e-38, 4.0923e-38],\n",
      "        [3.1899e-09, 9.9715e-18, 1.1177e-17, 1.0346e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "427000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.0716e-19, 3.0168e-38, 4.9486e-38, 3.8897e-38],\n",
      "        [2.9877e-09, 1.1618e-17, 1.1878e-17, 1.1328e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "428000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[8.8183e-20, 3.0924e-38, 4.5561e-38, 3.7134e-38],\n",
      "        [2.8139e-09, 1.3383e-17, 1.2570e-17, 1.2323e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "429000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.2214e-19, 3.0245e-38, 5.1454e-38, 4.0101e-38],\n",
      "        [3.1617e-09, 1.1437e-17, 1.1959e-17, 1.1343e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "430000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.1144e-19, 3.0628e-38, 4.9344e-38, 3.9186e-38],\n",
      "        [3.0792e-09, 1.2323e-17, 1.2335e-17, 1.1869e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "431000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[8.1900e-20, 3.1597e-38, 4.3284e-38, 3.6282e-38],\n",
      "        [2.7907e-09, 1.5123e-17, 1.3334e-17, 1.3366e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "432000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[8.2642e-20, 3.1732e-38, 4.3130e-38, 3.6299e-38],\n",
      "        [2.8150e-09, 1.5448e-17, 1.3505e-17, 1.3585e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "433000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[9.5930e-20, 3.1556e-38, 4.5401e-38, 3.7595e-38],\n",
      "        [2.9812e-09, 1.4662e-17, 1.3347e-17, 1.3269e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "434000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.3448e-19, 3.0812e-38, 5.1647e-38, 4.0777e-38],\n",
      "        [3.3597e-09, 1.2372e-17, 1.2631e-17, 1.2123e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "435000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[9.3553e-20, 3.1875e-38, 4.4395e-38, 3.7257e-38],\n",
      "        [2.9831e-09, 1.5542e-17, 1.3743e-17, 1.3806e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "436000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.3086e-19, 4.1341e-38, 6.2034e-38, 5.0956e-38],\n",
      "        [3.5506e-09, 1.8225e-17, 1.6879e-17, 1.6745e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "437000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.9005e-20, 3.2484e-38, 3.8352e-38, 3.4055e-38],\n",
      "        [2.6791e-09, 1.8361e-17, 1.4313e-17, 1.4979e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "438000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.1615e-19, 3.8388e-38, 5.4409e-38, 4.5665e-38],\n",
      "        [3.3753e-09, 1.8281e-17, 1.6187e-17, 1.6307e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "439000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.2872e-19, 3.9167e-38, 5.7262e-38, 4.7668e-38],\n",
      "        [3.5357e-09, 1.8274e-17, 1.6505e-17, 1.6539e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "440000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.4538e-19, 4.0239e-38, 6.1099e-38, 5.0343e-38],\n",
      "        [3.7349e-09, 1.8270e-17, 1.6912e-17, 1.6835e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "441000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[9.9276e-20, 3.4439e-38, 4.5128e-38, 3.9045e-38],\n",
      "        [3.1592e-09, 1.8336e-17, 1.5248e-17, 1.5700e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "442000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.0633e-19, 3.4757e-38, 4.6424e-38, 3.9985e-38],\n",
      "        [3.2592e-09, 1.8330e-17, 1.5421e-17, 1.5833e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "443000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.1884e-19, 3.5567e-38, 4.9159e-38, 4.1952e-38],\n",
      "        [3.4265e-09, 1.8316e-17, 1.5754e-17, 1.6079e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "444000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[8.4227e-20, 3.0873e-38, 3.7350e-38, 3.3316e-38],\n",
      "        [2.9464e-09, 1.8378e-17, 1.4345e-17, 1.5095e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "445000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.3592e-19, 3.6115e-38, 5.1777e-38, 4.3818e-38],\n",
      "        [3.6429e-09, 1.8304e-17, 1.6089e-17, 1.6337e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "446000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[8.4691e-20, 2.9992e-38, 3.5900e-38, 3.2236e-38],\n",
      "        [2.9590e-09, 1.8400e-17, 1.4195e-17, 1.5015e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "447000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[9.6503e-20, 3.0966e-38, 3.8672e-38, 3.4305e-38],\n",
      "        [3.1370e-09, 1.8371e-17, 1.4580e-17, 1.5299e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "448000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[1.6556e-19, 3.6916e-38, 5.5943e-38, 4.6725e-38],\n",
      "        [3.9835e-09, 1.8230e-17, 1.6572e-17, 1.6692e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "449000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.1580e-19, 3.1981e-38, 4.2150e-38, 3.6877e-38],\n",
      "        [3.4061e-09, 1.8350e-17, 1.5067e-17, 1.5671e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "450000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[9.7634e-20, 2.9613e-38, 3.6428e-38, 3.2629e-38],\n",
      "        [3.1620e-09, 1.8389e-17, 1.4347e-17, 1.5168e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "451000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.1558e-20, 2.4811e-38, 2.5597e-38, 2.4291e-38],\n",
      "        [2.5812e-09, 1.8524e-17, 1.2721e-17, 1.3998e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "452000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[9.1574e-20, 2.7996e-38, 3.3201e-38, 3.0190e-38],\n",
      "        [3.0794e-09, 1.8414e-17, 1.3936e-17, 1.4896e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "453000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.6942e-20, 2.4846e-38, 2.6165e-38, 2.4737e-38],\n",
      "        [2.6831e-09, 1.8510e-17, 1.2855e-17, 1.4115e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "454000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[7.6544e-20, 2.5748e-38, 2.8351e-38, 2.6456e-38],\n",
      "        [2.8485e-09, 1.8477e-17, 1.3227e-17, 1.4397e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "455000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[8.8156e-20, 2.6651e-38, 3.0717e-38, 2.8291e-38],\n",
      "        [3.0344e-09, 1.8449e-17, 1.3616e-17, 1.4693e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "456000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.0605e-19, 2.7959e-38, 3.4264e-38, 3.0999e-38],\n",
      "        [3.2953e-09, 1.8401e-17, 1.4156e-17, 1.5095e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "457000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.1674e-20, 2.2964e-38, 2.2883e-38, 2.2148e-38],\n",
      "        [2.5962e-09, 1.8663e-17, 1.2359e-17, 1.3809e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "458000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[7.1965e-20, 2.3760e-38, 2.4935e-38, 2.3778e-38],\n",
      "        [2.7825e-09, 1.8581e-17, 1.2747e-17, 1.4099e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "459000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.8154e-20, 2.2954e-38, 2.3446e-38, 2.2589e-38],\n",
      "        [2.7188e-09, 1.8617e-17, 1.2500e-17, 1.3929e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "460000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.3956e-20, 2.3259e-38, 2.4348e-38, 2.3311e-38],\n",
      "        [2.8210e-09, 1.8591e-17, 1.2680e-17, 1.4072e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "461000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[9.6049e-20, 2.5104e-38, 2.8755e-38, 2.6790e-38],\n",
      "        [3.1688e-09, 1.8520e-17, 1.3441e-17, 1.4646e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "462000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.4858e-20, 2.1779e-38, 2.1460e-38, 2.1000e-38],\n",
      "        [2.6661e-09, 1.8729e-17, 1.2186e-17, 1.3737e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "463000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[6.4641e-20, 2.1458e-38, 2.0991e-38, 2.0617e-38],\n",
      "        [2.6643e-09, 1.8748e-17, 1.2111e-17, 1.3692e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "464000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.5715e-20, 2.2277e-38, 2.3008e-38, 2.2246e-38],\n",
      "        [2.8597e-09, 1.8656e-17, 1.2511e-17, 1.3994e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "465000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[5.2306e-20, 1.9630e-38, 1.7481e-38, 1.7744e-38],\n",
      "        [2.4294e-09, 1.9008e-17, 1.1424e-17, 1.3224e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "466000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.1338e-20, 2.1230e-38, 2.1160e-38, 2.0755e-38],\n",
      "        [2.7906e-09, 1.8746e-17, 1.2202e-17, 1.3793e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "467000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[7.2251e-20, 2.1085e-38, 2.1008e-38, 2.0635e-38],\n",
      "        [2.8084e-09, 1.8765e-17, 1.2191e-17, 1.3797e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "468000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.4275e-20, 2.0108e-38, 1.9060e-38, 1.9043e-38],\n",
      "        [2.6685e-09, 1.8885e-17, 1.1814e-17, 1.3533e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "469000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.2569e-20, 1.7448e-38, 1.3995e-38, 1.4776e-38],\n",
      "        [2.2248e-09, 1.9299e-17, 1.0664e-17, 1.2705e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "470000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.5054e-20, 1.9621e-38, 1.8459e-38, 1.8543e-38],\n",
      "        [2.6870e-09, 1.8905e-17, 1.1718e-17, 1.3481e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "471000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[9.0021e-20, 2.1430e-38, 2.2637e-38, 2.1958e-38],\n",
      "        [3.1047e-09, 1.8699e-17, 1.2571e-17, 1.4120e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "472000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.6715e-20, 1.8316e-38, 1.6141e-38, 1.6605e-38],\n",
      "        [2.5323e-09, 1.9079e-17, 1.1232e-17, 1.3148e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "473000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.6400e-20, 1.8054e-38, 1.5774e-38, 1.6297e-38],\n",
      "        [2.5283e-09, 1.9124e-17, 1.1167e-17, 1.3114e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "474000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.0362e-20, 1.8224e-38, 1.6265e-38, 1.6711e-38],\n",
      "        [2.6075e-09, 1.9083e-17, 1.1298e-17, 1.3219e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "475000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[8.3762e-20, 1.9911e-38, 2.0059e-38, 1.9857e-38],\n",
      "        [3.0163e-09, 1.8787e-17, 1.2128e-17, 1.3827e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "476000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.3746e-20, 1.8104e-38, 1.6338e-38, 1.6767e-38],\n",
      "        [2.6750e-09, 1.9060e-17, 1.1342e-17, 1.3264e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "477000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[9.7673e-20, 2.0442e-38, 2.1580e-38, 2.1100e-38],\n",
      "        [3.2322e-09, 1.8724e-17, 1.2456e-17, 1.4089e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "478000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.7732e-20, 1.7191e-38, 1.4771e-38, 1.5437e-38],\n",
      "        [2.5637e-09, 1.9215e-17, 1.0996e-17, 1.3030e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "479000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.0942e-20, 1.6411e-38, 1.3361e-38, 1.4224e-38],\n",
      "        [2.4272e-09, 1.9398e-17, 1.0655e-17, 1.2795e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "480000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.4715e-20, 1.6525e-38, 1.3743e-38, 1.4555e-38],\n",
      "        [2.5074e-09, 1.9345e-17, 1.0771e-17, 1.2890e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "481000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[5.3111e-20, 1.6225e-38, 1.3275e-38, 1.4149e-38],\n",
      "        [2.4761e-09, 1.9409e-17, 1.0662e-17, 1.2820e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "482000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.6041e-20, 1.6266e-38, 1.3511e-38, 1.4350e-38],\n",
      "        [2.5378e-09, 1.9366e-17, 1.0738e-17, 1.2882e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "483000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.9229e-20, 1.7110e-38, 1.5350e-38, 1.5927e-38],\n",
      "        [2.7886e-09, 1.9148e-17, 1.1213e-17, 1.3235e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "484000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.8334e-20, 1.5260e-38, 1.1851e-38, 1.2891e-38],\n",
      "        [2.3797e-09, 1.9582e-17, 1.0307e-17, 1.2579e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "485000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.7499e-20, 1.5878e-38, 1.3124e-38, 1.4011e-38],\n",
      "        [2.5719e-09, 1.9412e-17, 1.0675e-17, 1.2861e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "486000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[4.6157e-20, 1.5132e-38, 1.1457e-38, 1.2601e-38],\n",
      "        [2.3497e-09, 2.0173e-17, 1.0435e-17, 1.2825e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "487000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.3284e-20, 1.7169e-38, 1.5469e-38, 1.6120e-38],\n",
      "        [2.8876e-09, 1.9723e-17, 1.1564e-17, 1.3685e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "488000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.5848e-20, 1.6440e-38, 1.4141e-38, 1.4973e-38],\n",
      "        [2.7561e-09, 1.9863e-17, 1.1242e-17, 1.3456e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "489000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.0230e-20, 1.6780e-38, 1.4737e-38, 1.5526e-38],\n",
      "        [2.8458e-09, 2.0044e-17, 1.1520e-17, 1.3735e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "490000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[6.1454e-20, 1.5920e-38, 1.3199e-38, 1.4183e-38],\n",
      "        [2.6844e-09, 2.0216e-17, 1.1121e-17, 1.3449e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "491000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[9.7182e-20, 1.8133e-38, 1.7856e-38, 1.8193e-38],\n",
      "        [3.2899e-09, 1.9782e-17, 1.2310e-17, 1.4347e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "492000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[6.9765e-20, 1.6203e-38, 1.3973e-38, 1.4864e-38],\n",
      "        [2.8434e-09, 2.0142e-17, 1.1366e-17, 1.3654e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "493000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.7051e-20, 1.5984e-38, 1.3523e-38, 1.4494e-38],\n",
      "        [2.8030e-09, 2.0394e-17, 1.1352e-17, 1.3705e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "494000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.6668e-20, 1.6742e-38, 1.4858e-38, 1.5711e-38],\n",
      "        [2.9876e-09, 2.0589e-17, 1.1875e-17, 1.4195e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "495000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.4379e-20, 1.4108e-38, 1.0087e-38, 1.1440e-38],\n",
      "        [2.3490e-09, 2.1303e-17, 1.0491e-17, 1.3175e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "496000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.5369e-20, 1.5724e-38, 1.3004e-38, 1.4101e-38],\n",
      "        [2.7947e-09, 2.0987e-17, 1.1482e-17, 1.3968e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "497000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.8584e-20, 1.4408e-38, 1.0580e-38, 1.1931e-38],\n",
      "        [2.4572e-09, 2.1556e-17, 1.0816e-17, 1.3524e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "498000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[8.0408e-20, 1.6540e-38, 1.4666e-38, 1.5603e-38],\n",
      "        [3.0749e-09, 2.1036e-17, 1.2081e-17, 1.4504e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "499000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.4137e-20, 1.4490e-38, 1.0971e-38, 1.2287e-38],\n",
      "        [2.5818e-09, 2.1471e-17, 1.0976e-17, 1.3663e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "500000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.8285e-20, 1.4670e-38, 1.1377e-38, 1.2660e-38],\n",
      "        [2.6695e-09, 2.1419e-17, 1.1126e-17, 1.3790e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "501000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.3469e-20, 1.4136e-38, 1.0550e-38, 1.1899e-38],\n",
      "        [2.5708e-09, 2.1536e-17, 1.0860e-17, 1.3592e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "502000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.8226e-20, 1.5176e-38, 1.2397e-38, 1.3606e-38],\n",
      "        [2.8738e-09, 2.1490e-17, 1.1572e-17, 1.4204e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "503000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[7.2682e-20, 1.5580e-38, 1.2993e-38, 1.4194e-38],\n",
      "        [2.9711e-09, 2.1835e-17, 1.1950e-17, 1.4612e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "504000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.3373e-20, 1.5594e-38, 1.3006e-38, 1.4225e-38],\n",
      "        [2.9909e-09, 2.2002e-17, 1.2040e-17, 1.4731e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "505000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[7.1594e-20, 1.5681e-38, 1.2922e-38, 1.4213e-38],\n",
      "        [2.9781e-09, 2.2562e-17, 1.2266e-17, 1.5054e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "506000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.3863e-20, 1.5627e-38, 1.2954e-38, 1.4243e-38],\n",
      "        [3.0220e-09, 2.2569e-17, 1.2297e-17, 1.5094e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "507000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.5260e-20, 1.3292e-38, 9.0677e-39, 1.0620e-38],\n",
      "        [2.4326e-09, 2.3122e-17, 1.0917e-17, 1.4012e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "508000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[7.6953e-20, 1.5606e-38, 1.3014e-38, 1.4327e-38],\n",
      "        [3.0912e-09, 2.2826e-17, 1.2471e-17, 1.5320e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "509000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[9.4250e-20, 1.6392e-38, 1.4677e-38, 1.5829e-38],\n",
      "        [3.3849e-09, 2.2674e-17, 1.3010e-17, 1.5761e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "510000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.0327e-20, 1.4056e-38, 1.0553e-38, 1.2045e-38],\n",
      "        [2.7781e-09, 2.3082e-17, 1.1642e-17, 1.4676e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "511000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[5.5463e-20, 1.3731e-38, 9.9471e-39, 1.1498e-38],\n",
      "        [2.6852e-09, 2.3454e-17, 1.1532e-17, 1.4663e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "512000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.8046e-20, 1.5393e-38, 1.2684e-38, 1.4101e-38],\n",
      "        [3.1431e-09, 2.3564e-17, 1.2741e-17, 1.5750e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "513000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[8.0059e-20, 1.5333e-38, 1.2691e-38, 1.4109e-38],\n",
      "        [3.1811e-09, 2.3574e-17, 1.2763e-17, 1.5782e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "514000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[7.6201e-20, 1.4952e-38, 1.2097e-38, 1.3559e-38],\n",
      "        [3.1137e-09, 2.3633e-17, 1.2569e-17, 1.5635e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "515000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.1515e-20, 1.3805e-38, 1.0233e-38, 1.1806e-38],\n",
      "        [2.8327e-09, 2.3854e-17, 1.1888e-17, 1.5093e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "516000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.6649e-20, 1.3557e-38, 9.6966e-39, 1.1336e-38],\n",
      "        [2.7458e-09, 2.4422e-17, 1.1873e-17, 1.5205e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "517000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.8502e-20, 1.4124e-38, 1.0805e-38, 1.2387e-38],\n",
      "        [2.9902e-09, 2.4211e-17, 1.2331e-17, 1.5581e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "518000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.8773e-20, 1.4030e-38, 1.0697e-38, 1.2295e-38],\n",
      "        [3.0013e-09, 2.4344e-17, 1.2357e-17, 1.5642e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "519000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.8375e-20, 1.3164e-38, 9.3735e-39, 1.1023e-38],\n",
      "        [2.7915e-09, 2.4519e-17, 1.1823e-17, 1.5213e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "520000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[8.4362e-20, 1.9613e-38, 1.4929e-38, 1.7209e-38],\n",
      "        [3.4419e-09, 3.2442e-17, 1.6331e-17, 2.0753e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "521000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.1357e-20, 1.5447e-38, 1.0729e-38, 1.2753e-38],\n",
      "        [2.9109e-09, 2.8055e-17, 1.3312e-17, 1.7242e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "522000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.1429e-19, 2.3343e-38, 1.9398e-38, 2.1836e-38],\n",
      "        [4.0389e-09, 3.6341e-17, 1.9393e-17, 2.4263e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "523000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.0110e-20, 1.6348e-38, 1.1839e-38, 1.3949e-38],\n",
      "        [3.1340e-09, 2.9353e-17, 1.4376e-17, 1.8483e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "524000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.1155e-20, 1.4360e-38, 9.9852e-39, 1.1941e-38],\n",
      "        [2.9087e-09, 2.7093e-17, 1.2907e-17, 1.6753e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "525000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.2171e-20, 1.4116e-38, 9.8627e-39, 1.1800e-38],\n",
      "        [2.9305e-09, 2.6844e-17, 1.2828e-17, 1.6652e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "526000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.4634e-20, 1.4013e-38, 9.8940e-39, 1.1819e-38],\n",
      "        [2.9823e-09, 2.6701e-17, 1.2830e-17, 1.6640e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "527000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.5331e-20, 1.3707e-38, 9.7019e-39, 1.1600e-38],\n",
      "        [2.9915e-09, 2.6320e-17, 1.2653e-17, 1.6420e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "528000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[8.6823e-20, 1.6371e-38, 1.2570e-38, 1.4670e-38],\n",
      "        [3.4707e-09, 2.9405e-17, 1.4902e-17, 1.9045e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "529000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[5.4874e-20, 1.1655e-38, 7.8382e-39, 9.5533e-39],\n",
      "        [2.7235e-09, 2.3857e-17, 1.1087e-17, 1.4562e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "530000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.4594e-20, 1.1527e-38, 7.7521e-39, 9.4641e-39],\n",
      "        [2.7217e-09, 2.3799e-17, 1.1082e-17, 1.4560e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "531000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.2859e-20, 1.0951e-38, 7.2901e-39, 8.9428e-39],\n",
      "        [2.6695e-09, 2.3049e-17, 1.0650e-17, 1.4036e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "532000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.1193e-19, 1.7770e-38, 1.4649e-38, 1.6820e-38],\n",
      "        [3.9441e-09, 3.1018e-17, 1.6432e-17, 2.0784e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "533000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.0822e-20, 1.1624e-38, 8.0731e-39, 9.8071e-39],\n",
      "        [2.8797e-09, 2.4093e-17, 1.1481e-17, 1.5018e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "534000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.4115e-20, 9.0448e-39, 5.7180e-39, 7.1705e-39],\n",
      "        [2.4273e-09, 2.0654e-17, 9.2471e-18, 1.2343e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "535000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.1320e-20, 9.9320e-39, 6.5673e-39, 8.1304e-39],\n",
      "        [2.6314e-09, 2.1949e-17, 1.0135e-17, 1.3410e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "536000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.6477e-20, 1.2620e-38, 9.3530e-39, 1.1185e-38],\n",
      "        [3.2340e-09, 2.5399e-17, 1.2612e-17, 1.6330e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "537000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[5.5613e-20, 9.8859e-39, 6.6808e-39, 8.2392e-39],\n",
      "        [2.7310e-09, 2.1851e-17, 1.0212e-17, 1.3481e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "538000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.4965e-20, 1.1021e-38, 7.8110e-39, 9.5039e-39],\n",
      "        [2.9770e-09, 2.3535e-17, 1.1390e-17, 1.4890e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "539000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.0194e-20, 1.0176e-38, 7.0486e-39, 8.6521e-39],\n",
      "        [2.8538e-09, 2.2398e-17, 1.0669e-17, 1.4026e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "540000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.8022e-20, 8.5363e-39, 5.5351e-39, 6.9484e-39],\n",
      "        [2.5303e-09, 2.0104e-17, 9.1649e-18, 1.2222e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "541000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.9835e-20, 9.5671e-39, 6.6058e-39, 8.1421e-39],\n",
      "        [2.8310e-09, 2.1534e-17, 1.0214e-17, 1.3465e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "542000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.6352e-20, 1.1161e-38, 8.2656e-39, 9.9768e-39],\n",
      "        [3.2182e-09, 2.3733e-17, 1.1787e-17, 1.5332e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "543000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[7.8566e-20, 1.1270e-38, 8.4215e-39, 1.0152e-38],\n",
      "        [3.2723e-09, 2.3976e-17, 1.1999e-17, 1.5585e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "544000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.0303e-20, 7.0605e-39, 4.3524e-39, 5.5833e-39],\n",
      "        [2.3043e-09, 1.7996e-17, 7.9459e-18, 1.0733e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "545000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.3314e-20, 7.1784e-39, 4.5140e-39, 5.7633e-39],\n",
      "        [2.3866e-09, 1.8165e-17, 8.1152e-18, 1.0930e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "546000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.7205e-20, 8.3930e-39, 5.7154e-39, 7.1247e-39],\n",
      "        [2.7537e-09, 1.9970e-17, 9.3861e-18, 1.2452e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "547000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.6468e-20, 8.4168e-39, 5.7253e-39, 7.1489e-39],\n",
      "        [2.7497e-09, 2.0159e-17, 9.5080e-18, 1.2611e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "548000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.8882e-20, 7.4286e-39, 4.8423e-39, 6.1386e-39],\n",
      "        [2.5437e-09, 1.8663e-17, 8.5529e-18, 1.1455e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "549000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.6716e-20, 8.1504e-39, 5.5517e-39, 6.9487e-39],\n",
      "        [2.7537e-09, 1.9821e-17, 9.3622e-18, 1.2430e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "550000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.0907e-20, 9.1767e-39, 6.6569e-39, 8.1819e-39],\n",
      "        [3.0864e-09, 2.1301e-17, 1.0465e-17, 1.3740e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "551000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.9989e-20, 8.9467e-39, 6.4705e-39, 7.9752e-39],\n",
      "        [3.0675e-09, 2.1039e-17, 1.0328e-17, 1.3577e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "552000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.9149e-20, 5.9619e-39, 3.6458e-39, 4.7444e-39],\n",
      "        [2.2606e-09, 1.6404e-17, 7.2184e-18, 9.8195e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "553000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.1471e-19, 1.1940e-38, 9.9234e-39, 1.1743e-38],\n",
      "        [3.9574e-09, 2.5133e-17, 1.3468e-17, 1.7266e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "554000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.4887e-20, 5.2174e-39, 3.0825e-39, 4.0689e-39],\n",
      "        [2.1315e-09, 1.5238e-17, 6.5762e-18, 9.0167e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "555000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.8405e-20, 5.4667e-39, 3.3210e-39, 4.3511e-39],\n",
      "        [2.2284e-09, 1.5574e-17, 6.8160e-18, 9.3095e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "556000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.5462e-20, 5.9676e-39, 3.8042e-39, 4.9152e-39],\n",
      "        [2.4304e-09, 1.6438e-17, 7.4202e-18, 1.0045e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "557000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.2865e-20, 7.1855e-39, 5.0252e-39, 6.3141e-39],\n",
      "        [2.8719e-09, 1.8401e-17, 8.8109e-18, 1.1720e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "558000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.5917e-20, 6.5661e-39, 4.4420e-39, 5.6476e-39],\n",
      "        [2.7027e-09, 1.7452e-17, 8.1863e-18, 1.0966e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "559000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.2192e-20, 6.2535e-39, 4.1519e-39, 5.3173e-39],\n",
      "        [2.6132e-09, 1.7015e-17, 7.9016e-18, 1.0626e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "560000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.1721e-20, 6.8306e-39, 4.7562e-39, 6.0067e-39],\n",
      "        [2.8471e-09, 1.7949e-17, 8.5894e-18, 1.1450e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "561000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[8.5814e-20, 8.4278e-39, 6.4472e-39, 7.9200e-39],\n",
      "        [3.3873e-09, 2.0518e-17, 1.0447e-17, 1.3682e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "562000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.6515e-20, 5.5296e-39, 3.5536e-39, 4.6182e-39],\n",
      "        [2.4613e-09, 1.5876e-17, 7.2320e-18, 9.8058e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "563000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.9335e-20, 4.8941e-39, 2.9976e-39, 3.9602e-39],\n",
      "        [2.2545e-09, 1.4751e-17, 6.5145e-18, 8.9238e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "564000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[6.0126e-20, 6.2901e-39, 4.3501e-39, 5.5370e-39],\n",
      "        [2.8073e-09, 1.7185e-17, 8.2034e-18, 1.0976e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "565000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.0682e-20, 5.5688e-39, 3.6680e-39, 4.7471e-39],\n",
      "        [2.5703e-09, 1.5994e-17, 7.4062e-18, 1.0010e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "566000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.7466e-20, 5.8841e-39, 4.0160e-39, 5.1466e-39],\n",
      "        [2.7382e-09, 1.6528e-17, 7.8241e-18, 1.0510e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "567000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[6.9286e-20, 6.5337e-39, 4.6984e-39, 5.9298e-39],\n",
      "        [3.0138e-09, 1.7622e-17, 8.6189e-18, 1.1470e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "568000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.3999e-20, 5.3822e-39, 3.6025e-39, 4.6570e-39],\n",
      "        [2.6401e-09, 1.5633e-17, 7.2943e-18, 9.8535e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "569000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.7363e-20, 6.8103e-39, 5.0539e-39, 6.3346e-39],\n",
      "        [3.1922e-09, 1.8151e-17, 9.0683e-18, 1.2009e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "570000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.3104e-20, 5.1923e-39, 3.4613e-39, 4.4921e-39],\n",
      "        [2.6185e-09, 1.5353e-17, 7.1547e-18, 9.6825e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "571000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.6589e-20, 4.7295e-39, 3.0378e-39, 3.9940e-39],\n",
      "        [2.4478e-09, 1.4538e-17, 6.6204e-18, 9.0297e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "572000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.1379e-20, 4.3170e-39, 2.6798e-39, 3.5659e-39],\n",
      "        [2.3006e-09, 1.3770e-17, 6.1359e-18, 8.4307e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "573000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.5470e-20, 4.4895e-39, 2.8619e-39, 3.7808e-39],\n",
      "        [2.4121e-09, 1.4094e-17, 6.3840e-18, 8.7320e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "574000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.2401e-20, 5.4285e-39, 3.7885e-39, 4.8699e-39],\n",
      "        [2.8442e-09, 1.5856e-17, 7.6171e-18, 1.0239e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "575000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.6676e-20, 6.1002e-39, 4.5094e-39, 5.7000e-39],\n",
      "        [3.1611e-09, 1.7029e-17, 8.4798e-18, 1.1282e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "576000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[5.2900e-20, 4.7056e-39, 3.1327e-39, 4.0974e-39],\n",
      "        [2.6054e-09, 1.4546e-17, 6.7780e-18, 9.2075e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "577000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.1670e-20, 4.6025e-39, 3.0461e-39, 3.9971e-39],\n",
      "        [2.5782e-09, 1.4405e-17, 6.6990e-18, 9.1128e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "578000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.1490e-20, 5.0420e-39, 3.5042e-39, 4.5335e-39],\n",
      "        [2.8176e-09, 1.5226e-17, 7.3005e-18, 9.8446e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "579000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.3179e-20, 4.4764e-39, 2.9830e-39, 3.9152e-39],\n",
      "        [2.6072e-09, 1.4146e-17, 6.5963e-18, 8.9756e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "580000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.7363e-20, 4.0340e-39, 2.5978e-39, 3.4522e-39],\n",
      "        [2.4484e-09, 1.3263e-17, 6.0423e-18, 8.2877e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "581000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.9277e-20, 5.1038e-39, 3.6621e-39, 4.7098e-39],\n",
      "        [2.9848e-09, 1.5348e-17, 7.4986e-18, 1.0075e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "582000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.6064e-20, 3.9342e-39, 2.5183e-39, 3.3607e-39],\n",
      "        [2.4220e-09, 1.3156e-17, 5.9901e-18, 8.2282e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "583000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.2272e-20, 4.2448e-39, 2.8200e-39, 3.7234e-39],\n",
      "        [2.5905e-09, 1.3822e-17, 6.4562e-18, 8.8045e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "584000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[8.7049e-20, 5.9052e-39, 4.5345e-39, 5.7302e-39],\n",
      "        [3.3870e-09, 1.7020e-17, 8.7414e-18, 1.1594e-17],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "585000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.6289e-20, 4.3138e-39, 2.9273e-39, 3.8494e-39],\n",
      "        [2.6931e-09, 1.4019e-17, 6.6475e-18, 9.0362e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "586000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.2592e-20, 4.0313e-39, 2.6809e-39, 3.5522e-39],\n",
      "        [2.5944e-09, 1.3441e-17, 6.2861e-18, 8.5862e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "587000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.3521e-20, 3.9753e-39, 2.6550e-39, 3.5178e-39],\n",
      "        [2.6126e-09, 1.3318e-17, 6.2397e-18, 8.5233e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "588000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.0524e-20, 4.6468e-39, 3.3515e-39, 4.3414e-39],\n",
      "        [3.0100e-09, 1.4653e-17, 7.2039e-18, 9.7054e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "589000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.6717e-20, 3.4761e-39, 2.2294e-39, 2.9994e-39],\n",
      "        [2.4245e-09, 1.2255e-17, 5.5827e-18, 7.7013e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "590000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.7014e-20, 2.9191e-39, 1.7495e-39, 2.4081e-39],\n",
      "        [2.1430e-09, 1.1008e-17, 4.7948e-18, 6.7111e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "591000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.0569e-20, 3.9541e-39, 2.7295e-39, 3.5985e-39],\n",
      "        [2.7719e-09, 1.3293e-17, 6.3502e-18, 8.6463e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "592000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.0098e-20, 3.8791e-39, 2.6719e-39, 3.5292e-39],\n",
      "        [2.7608e-09, 1.3161e-17, 6.2822e-18, 8.5614e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "593000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.8359e-20, 3.3083e-39, 2.1412e-39, 2.8869e-39],\n",
      "        [2.4615e-09, 1.1930e-17, 5.4676e-18, 7.5477e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "594000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.6613e-20, 2.7561e-39, 1.6473e-39, 2.2794e-39],\n",
      "        [2.1359e-09, 1.0719e-17, 4.6731e-18, 6.5528e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "595000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.2453e-20, 3.4324e-39, 2.2771e-39, 3.0534e-39],\n",
      "        [2.5734e-09, 1.2269e-17, 5.7270e-18, 7.8712e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "596000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.8406e-20, 3.6009e-39, 2.4606e-39, 3.2732e-39],\n",
      "        [2.7165e-09, 1.2629e-17, 6.0024e-18, 8.2102e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "597000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.9894e-20, 2.7548e-39, 1.6859e-39, 2.3235e-39],\n",
      "        [2.2268e-09, 1.0726e-17, 4.7441e-18, 6.6337e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "598000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.6495e-20, 3.4156e-39, 2.3120e-39, 3.0919e-39],\n",
      "        [2.6676e-09, 1.2253e-17, 5.7904e-18, 7.9431e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "599000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.2467e-20, 3.5697e-39, 2.4840e-39, 3.2974e-39],\n",
      "        [2.8057e-09, 1.2584e-17, 6.0475e-18, 8.2589e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "600000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.1890e-20, 3.8793e-39, 2.8108e-39, 3.6903e-39],\n",
      "        [3.0225e-09, 1.3291e-17, 6.5633e-18, 8.8981e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "601000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.1564e-20, 3.0654e-39, 2.0226e-39, 2.7375e-39],\n",
      "        [2.5416e-09, 1.1515e-17, 5.3586e-18, 7.4007e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "602000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.0001e-20, 3.3621e-39, 2.3165e-39, 3.0969e-39],\n",
      "        [2.7534e-09, 1.2225e-17, 5.8557e-18, 8.0222e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "603000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.5726e-20, 2.7753e-39, 1.7644e-39, 2.4153e-39],\n",
      "        [2.4035e-09, 1.0918e-17, 4.9654e-18, 6.8964e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "604000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.0078e-20, 2.5360e-39, 1.5403e-39, 2.1259e-39],\n",
      "        [2.2865e-09, 1.0474e-17, 4.6293e-18, 6.4432e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "605000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.8394e-20, 2.8141e-39, 1.8196e-39, 2.4832e-39],\n",
      "        [2.4756e-09, 1.1043e-17, 5.0829e-18, 7.0425e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "606000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.1849e-20, 2.8508e-39, 1.8817e-39, 2.5591e-39],\n",
      "        [2.5492e-09, 1.1088e-17, 5.1649e-18, 7.1457e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "607000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.8763e-20, 2.4050e-39, 1.4391e-39, 1.9915e-39],\n",
      "        [2.2860e-09, 1.0318e-17, 4.5307e-18, 6.2940e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "608000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.6277e-20, 2.6030e-39, 1.6494e-39, 2.2597e-39],\n",
      "        [2.4539e-09, 1.0675e-17, 4.8528e-18, 6.7198e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "609000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.1346e-20, 2.3849e-39, 1.4520e-39, 2.0039e-39],\n",
      "        [2.3529e-09, 1.0247e-17, 4.5416e-18, 6.3000e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "610000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.1277e-20, 2.3837e-39, 1.4501e-39, 2.0008e-39],\n",
      "        [2.3688e-09, 1.0328e-17, 4.5874e-18, 6.3527e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "611000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.2698e-20, 2.9915e-39, 2.0855e-39, 2.8081e-39],\n",
      "        [2.8017e-09, 1.1468e-17, 5.5341e-18, 7.6062e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "612000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.5087e-20, 2.4034e-39, 1.5000e-39, 2.0609e-39],\n",
      "        [2.4614e-09, 1.0338e-17, 4.6589e-18, 6.4391e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "613000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.5142e-20, 2.6780e-39, 1.7867e-39, 2.4257e-39],\n",
      "        [2.6752e-09, 1.0896e-17, 5.1225e-18, 7.0481e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "614000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.6664e-20, 2.3822e-39, 1.4998e-39, 2.0577e-39],\n",
      "        [2.5101e-09, 1.0320e-17, 4.6785e-18, 6.4554e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "615000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.5275e-20, 2.3215e-39, 1.4456e-39, 1.9870e-39],\n",
      "        [2.4929e-09, 1.0242e-17, 4.6171e-18, 6.3675e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "616000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.3412e-20, 2.5014e-39, 1.6434e-39, 2.2377e-39],\n",
      "        [2.6653e-09, 1.0581e-17, 4.9268e-18, 6.7728e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "617000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.5599e-20, 2.5000e-39, 1.6612e-39, 2.2576e-39],\n",
      "        [2.7133e-09, 1.0562e-17, 4.9488e-18, 6.7966e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "618000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.2999e-20, 2.6649e-39, 1.8434e-39, 2.4878e-39],\n",
      "        [2.8654e-09, 1.0928e-17, 5.2533e-18, 7.1937e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "619000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.5038e-20, 2.4146e-39, 1.5944e-39, 2.1694e-39],\n",
      "        [2.7226e-09, 1.0432e-17, 4.8725e-18, 6.6840e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "620000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.9845e-20, 2.2739e-39, 1.4517e-39, 1.9867e-39],\n",
      "        [2.6369e-09, 1.0227e-17, 4.6893e-18, 6.4340e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "621000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[5.6485e-20, 2.3878e-39, 1.5865e-39, 2.1562e-39],\n",
      "        [2.7755e-09, 1.0440e-17, 4.9027e-18, 6.7101e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "622000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[4.4591e-20, 2.0388e-39, 1.2488e-39, 1.7224e-39],\n",
      "        [2.5335e-09, 9.7053e-18, 4.3320e-18, 5.9534e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "623000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.1745e-20, 1.9415e-39, 1.1620e-39, 1.6091e-39],\n",
      "        [2.4796e-09, 9.5223e-18, 4.1943e-18, 5.7656e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "624000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.0154e-20, 2.1425e-39, 1.3626e-39, 1.8663e-39],\n",
      "        [2.6812e-09, 1.0011e-17, 4.5851e-18, 6.2738e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "625000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.2121e-20, 2.1495e-39, 1.3826e-39, 1.8901e-39],\n",
      "        [2.7266e-09, 1.0014e-17, 4.6156e-18, 6.3099e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "626000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.7818e-20, 2.2325e-39, 1.4842e-39, 2.0173e-39],\n",
      "        [2.8458e-09, 1.0173e-17, 4.7819e-18, 6.5234e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "627000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.4369e-20, 1.8988e-39, 1.1538e-39, 1.5936e-39],\n",
      "        [2.5778e-09, 9.4971e-18, 4.2304e-18, 5.7925e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "628000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.9008e-20, 2.2341e-39, 1.4938e-39, 2.0277e-39],\n",
      "        [2.9009e-09, 1.0284e-17, 4.8675e-18, 6.6218e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "629000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.1865e-20, 2.2921e-39, 1.5569e-39, 2.1070e-39],\n",
      "        [2.9742e-09, 1.0476e-17, 5.0178e-18, 6.8104e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "630000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.1999e-20, 2.0200e-39, 1.2913e-39, 1.7671e-39],\n",
      "        [2.7822e-09, 9.8735e-18, 4.5527e-18, 6.1979e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "631000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.9429e-20, 2.1213e-39, 1.4150e-39, 1.9220e-39],\n",
      "        [2.9407e-09, 1.0079e-17, 4.7659e-18, 6.4699e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "632000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.6551e-20, 2.4053e-39, 1.7418e-39, 2.3328e-39],\n",
      "        [3.2569e-09, 1.0666e-17, 5.2976e-18, 7.1615e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "633000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.4020e-20, 1.9276e-39, 1.2409e-39, 1.6967e-39],\n",
      "        [2.8505e-09, 9.6595e-18, 4.4686e-18, 6.0704e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "634000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.1218e-20, 1.8236e-39, 1.1507e-39, 1.5793e-39],\n",
      "        [2.7968e-09, 9.4039e-18, 4.2938e-18, 5.8374e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "635000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.0685e-20, 2.0339e-39, 1.3601e-39, 1.8464e-39],\n",
      "        [3.0229e-09, 1.0016e-17, 4.7612e-18, 6.4358e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "636000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[8.2713e-20, 2.3849e-39, 1.7644e-39, 2.3538e-39],\n",
      "        [3.4249e-09, 1.0768e-17, 5.4375e-18, 7.3119e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "637000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[9.6149e-20, 2.5994e-39, 2.0197e-39, 2.6719e-39],\n",
      "        [3.6571e-09, 1.1280e-17, 5.8779e-18, 7.8781e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "638000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.5956e-20, 1.6171e-39, 9.7952e-40, 1.3541e-39],\n",
      "        [2.7353e-09, 9.0359e-18, 4.0424e-18, 5.4833e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "639000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.0046e-19, 2.5559e-39, 2.0085e-39, 2.6526e-39],\n",
      "        [3.7460e-09, 1.1205e-17, 5.8775e-18, 7.8636e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "640000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.1901e-20, 1.6763e-39, 1.0550e-39, 1.4485e-39],\n",
      "        [2.8958e-09, 9.2270e-18, 4.2306e-18, 5.7158e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "641000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.1876e-20, 1.4330e-39, 8.3703e-40, 1.1651e-39],\n",
      "        [2.6654e-09, 8.5708e-18, 3.7501e-18, 5.0860e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "642000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.1563e-20, 1.5928e-39, 9.9652e-40, 1.3702e-39],\n",
      "        [2.9068e-09, 9.0142e-18, 4.1162e-18, 5.5562e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "643000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.4551e-20, 1.8365e-39, 1.2408e-39, 1.6817e-39],\n",
      "        [3.2168e-09, 9.7674e-18, 4.6997e-18, 6.2984e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "644000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.0659e-20, 1.7265e-39, 1.1399e-39, 1.5517e-39],\n",
      "        [3.1441e-09, 9.4791e-18, 4.4937e-18, 6.0281e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "645000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.7175e-20, 1.6255e-39, 1.0500e-39, 1.4353e-39],\n",
      "        [3.0764e-09, 9.2051e-18, 4.3020e-18, 5.7766e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "646000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.8386e-20, 1.7695e-39, 1.2125e-39, 1.6402e-39],\n",
      "        [3.3095e-09, 9.5583e-18, 4.6283e-18, 6.1919e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "647000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[8.0259e-20, 1.8968e-39, 1.3697e-39, 1.8356e-39],\n",
      "        [3.5345e-09, 9.8521e-18, 4.9222e-18, 6.5631e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "648000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.0527e-19, 2.2397e-39, 1.7692e-39, 2.3346e-39],\n",
      "        [3.9690e-09, 1.0752e-17, 5.6876e-18, 7.5413e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "649000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.1070e-19, 2.2577e-39, 1.8103e-39, 2.3832e-39],\n",
      "        [4.0574e-09, 1.0781e-17, 5.7496e-18, 7.6161e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "650000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.6409e-20, 1.7456e-39, 1.2351e-39, 1.6618e-39],\n",
      "        [3.5065e-09, 9.5363e-18, 4.7124e-18, 6.2754e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "651000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.5975e-20, 1.5873e-39, 1.0680e-39, 1.4501e-39],\n",
      "        [3.3260e-09, 9.1775e-18, 4.4051e-18, 5.8757e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "652000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.5431e-20, 1.5400e-39, 1.0314e-39, 1.4018e-39],\n",
      "        [3.3215e-09, 9.0363e-18, 4.3213e-18, 5.7636e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "653000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.5701e-20, 1.2263e-39, 7.2675e-40, 1.0101e-39],\n",
      "        [2.8963e-09, 8.1779e-18, 3.6335e-18, 4.8761e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "654000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.4503e-20, 1.4661e-39, 9.7442e-40, 1.3266e-39],\n",
      "        [3.3292e-09, 8.8626e-18, 4.2211e-18, 5.6237e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "655000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.0570e-20, 1.3879e-39, 9.0198e-40, 1.2331e-39],\n",
      "        [3.2635e-09, 8.6715e-18, 4.0776e-18, 5.4342e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "656000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[8.2894e-20, 1.6327e-39, 1.1783e-39, 1.5807e-39],\n",
      "        [3.7067e-09, 9.3302e-18, 4.6747e-18, 6.1882e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "657000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.1231e-20, 1.0563e-39, 6.0100e-40, 8.4227e-40],\n",
      "        [2.8181e-09, 7.6649e-18, 3.3255e-18, 4.4625e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "658000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.6916e-20, 1.1463e-39, 6.8166e-40, 9.4734e-40],\n",
      "        [2.9867e-09, 8.0179e-18, 3.5809e-18, 4.7849e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "659000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.2808e-20, 1.0566e-39, 6.0803e-40, 8.5035e-40],\n",
      "        [2.8849e-09, 7.7093e-18, 3.3713e-18, 4.5127e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "660000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[8.6668e-20, 1.6149e-39, 1.1810e-39, 1.5802e-39],\n",
      "        [3.8565e-09, 9.4665e-18, 4.8123e-18, 6.3353e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "661000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.4573e-20, 1.3152e-39, 8.6837e-40, 1.1842e-39],\n",
      "        [3.4356e-09, 8.5969e-18, 4.1006e-18, 5.4308e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "662000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.7630e-20, 1.3168e-39, 8.8159e-40, 1.1996e-39],\n",
      "        [3.5056e-09, 8.5862e-18, 4.1269e-18, 5.4597e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "663000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.0847e-20, 1.1078e-39, 6.7356e-40, 9.3271e-40],\n",
      "        [3.1535e-09, 8.0039e-18, 3.6387e-18, 4.8335e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "664000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.0875e-20, 1.1049e-39, 6.7205e-40, 9.3041e-40],\n",
      "        [3.1796e-09, 8.0664e-18, 3.6800e-18, 4.8794e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "665000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.5446e-20, 1.1268e-39, 7.0414e-40, 9.7036e-40],\n",
      "        [3.2978e-09, 8.1223e-18, 3.7631e-18, 4.9800e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "666000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.8680e-20, 1.1406e-39, 7.2554e-40, 9.9685e-40],\n",
      "        [3.3782e-09, 8.1559e-18, 3.8173e-18, 5.0451e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "667000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.0268e-20, 1.1289e-39, 7.2337e-40, 9.9283e-40],\n",
      "        [3.4204e-09, 8.1028e-18, 3.8061e-18, 5.0268e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "668000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.1583e-20, 9.9931e-40, 6.0635e-40, 8.4090e-40],\n",
      "        [3.2185e-09, 7.6435e-18, 3.4692e-18, 4.5965e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "669000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.0592e-20, 1.0757e-39, 6.8820e-40, 9.4539e-40],\n",
      "        [3.4387e-09, 7.8962e-18, 3.6990e-18, 4.8839e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "670000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[8.0671e-20, 1.2764e-39, 9.0040e-40, 1.2147e-39],\n",
      "        [3.8831e-09, 8.6095e-18, 4.2927e-18, 5.6250e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "671000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.5545e-20, 9.0299e-40, 5.2486e-40, 7.3363e-40],\n",
      "        [3.1074e-09, 7.3911e-18, 3.2813e-18, 4.3452e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "672000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.0029e-20, 1.0280e-39, 6.5442e-40, 8.9990e-40],\n",
      "        [3.4789e-09, 7.8269e-18, 3.6695e-18, 4.8305e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "673000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.2490e-20, 1.1723e-39, 7.7087e-40, 1.0258e-39],\n",
      "        [4.0236e-09, 8.7520e-18, 4.2098e-18, 5.3879e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "674000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.6293e-20, 7.3171e-40, 3.8833e-40, 5.4882e-40],\n",
      "        [2.8499e-09, 6.7211e-18, 2.8337e-18, 3.7625e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "675000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.4034e-20, 1.0233e-39, 6.4905e-40, 8.7568e-40],\n",
      "        [3.7683e-09, 8.0952e-18, 3.8050e-18, 4.9117e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "676000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.9225e-20, 9.7101e-40, 6.0269e-40, 8.1910e-40],\n",
      "        [3.6311e-09, 7.8907e-18, 3.6674e-18, 4.7529e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "677000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.0295e-20, 9.4170e-40, 5.8774e-40, 7.9916e-40],\n",
      "        [3.6526e-09, 7.7363e-18, 3.6018e-18, 4.6704e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "678000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.4593e-20, 8.8207e-40, 5.3538e-40, 7.3462e-40],\n",
      "        [3.4822e-09, 7.4849e-18, 3.4326e-18, 4.4737e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "679000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.4354e-20, 9.7059e-40, 6.2047e-40, 8.4002e-40],\n",
      "        [3.7998e-09, 7.9651e-18, 3.7846e-18, 4.8861e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "680000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.9411e-20, 9.9305e-40, 6.4970e-40, 8.7491e-40],\n",
      "        [3.9428e-09, 8.0668e-18, 3.8857e-18, 4.9999e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "681000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.0984e-20, 8.8513e-40, 5.5673e-40, 7.5935e-40],\n",
      "        [3.6790e-09, 7.5242e-18, 3.5293e-18, 4.5800e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "682000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.0137e-20, 7.7166e-40, 4.5598e-40, 6.3244e-40],\n",
      "        [3.3470e-09, 6.9890e-18, 3.1520e-18, 4.1305e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "683000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.9557e-20, 7.4263e-40, 4.3630e-40, 6.0637e-40],\n",
      "        [3.3281e-09, 6.8414e-18, 3.0693e-18, 4.0264e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "684000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.8157e-20, 9.8433e-40, 6.6998e-40, 8.9717e-40],\n",
      "        [4.1860e-09, 8.0743e-18, 3.9919e-18, 5.1137e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "685000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.9240e-20, 7.2926e-40, 4.2789e-40, 5.9543e-40],\n",
      "        [3.3426e-09, 6.8472e-18, 3.0792e-18, 4.0360e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "686000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.4619e-20, 9.1364e-40, 6.1400e-40, 8.2752e-40],\n",
      "        [4.0801e-09, 7.7347e-18, 3.7911e-18, 4.8757e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "687000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[7.6706e-20, 9.0460e-40, 6.1308e-40, 8.2541e-40],\n",
      "        [4.1294e-09, 7.6813e-18, 3.7807e-18, 4.8598e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "688000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.7715e-20, 7.3740e-40, 4.5477e-40, 6.2717e-40],\n",
      "        [3.5944e-09, 6.8572e-18, 3.1716e-18, 4.1361e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "689000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.9652e-20, 5.8768e-40, 3.1899e-40, 4.5270e-40],\n",
      "        [3.0328e-09, 6.1321e-18, 2.6157e-18, 3.4632e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "690000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.4676e-20, 7.6203e-40, 4.8814e-40, 6.6837e-40],\n",
      "        [3.8013e-09, 6.9993e-18, 3.3186e-18, 4.3072e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "691000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.8297e-20, 6.2868e-40, 3.6401e-40, 5.0990e-40],\n",
      "        [3.3207e-09, 6.3328e-18, 2.8095e-18, 3.6929e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "692000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.3686e-20, 7.2621e-40, 4.6198e-40, 6.3424e-40],\n",
      "        [3.7814e-09, 6.8373e-18, 3.2249e-18, 4.1896e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "693000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.2422e-20, 4.8195e-40, 2.4342e-40, 3.5177e-40],\n",
      "        [2.7811e-09, 5.5734e-18, 2.2709e-18, 3.0324e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "694000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.2226e-20, 6.9781e-40, 4.4032e-40, 6.0627e-40],\n",
      "        [3.7636e-09, 6.7511e-18, 3.1740e-18, 4.1248e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "695000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[7.3114e-20, 7.6193e-40, 5.0836e-40, 6.9176e-40],\n",
      "        [4.0703e-09, 7.1017e-18, 3.4677e-18, 4.4734e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "696000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.3720e-20, 5.4960e-40, 3.0713e-40, 4.3484e-40],\n",
      "        [3.2085e-09, 5.9957e-18, 2.6074e-18, 3.4384e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "697000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.9178e-20, 5.7723e-40, 3.3561e-40, 4.7138e-40],\n",
      "        [3.3967e-09, 6.1686e-18, 2.7538e-18, 3.6132e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "698000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.8120e-20, 6.8682e-40, 4.4623e-40, 6.1199e-40],\n",
      "        [3.9564e-09, 6.7603e-18, 3.2434e-18, 4.1978e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "699000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.7746e-20, 6.6187e-40, 4.2833e-40, 5.8834e-40],\n",
      "        [3.9445e-09, 6.6217e-18, 3.1646e-18, 4.0991e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "700000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.8264e-20, 5.3899e-40, 3.1073e-40, 4.3801e-40],\n",
      "        [3.3827e-09, 5.9844e-18, 2.6579e-18, 3.4904e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "701000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.2296e-20, 4.1683e-40, 2.0941e-40, 3.0446e-40],\n",
      "        [2.8086e-09, 5.2374e-18, 2.1257e-18, 2.8398e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "702000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.5533e-20, 5.5465e-40, 3.3447e-40, 4.6743e-40],\n",
      "        [3.6072e-09, 6.0562e-18, 2.7618e-18, 3.6093e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "703000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.6943e-20, 5.4820e-40, 3.3296e-40, 4.6488e-40],\n",
      "        [3.6483e-09, 6.0115e-18, 2.7505e-18, 3.5925e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "704000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[9.4947e-20, 7.2712e-40, 5.2609e-40, 7.0719e-40],\n",
      "        [4.6286e-09, 6.9636e-18, 3.5682e-18, 4.5616e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "705000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[6.7954e-20, 5.9434e-40, 3.8365e-40, 5.2915e-40],\n",
      "        [3.9857e-09, 6.3234e-18, 3.0168e-18, 3.9080e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "706000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.9320e-20, 4.3406e-40, 2.3284e-40, 3.3463e-40],\n",
      "        [3.1100e-09, 5.4206e-18, 2.3010e-18, 3.0466e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "707000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.4527e-20, 5.1074e-40, 3.0572e-40, 4.2918e-40],\n",
      "        [3.6164e-09, 5.8846e-18, 2.6776e-18, 3.4994e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "708000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.9156e-20, 5.2204e-40, 3.2089e-40, 4.4815e-40],\n",
      "        [3.7542e-09, 5.9428e-18, 2.7472e-18, 3.5799e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "709000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.8477e-20, 3.9953e-40, 2.1194e-40, 3.0589e-40],\n",
      "        [3.0766e-09, 5.1753e-18, 2.1733e-18, 2.8842e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "710000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.6313e-20, 4.9276e-40, 2.9768e-40, 4.1778e-40],\n",
      "        [3.6855e-09, 5.8006e-18, 2.6545e-18, 3.4651e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "711000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.7260e-20, 4.8715e-40, 2.9562e-40, 4.1470e-40],\n",
      "        [3.7130e-09, 5.7602e-18, 2.6411e-18, 3.4466e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "712000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.8880e-20, 3.7932e-40, 2.0153e-40, 2.9130e-40],\n",
      "        [3.1036e-09, 5.0587e-18, 2.1257e-18, 2.8205e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "713000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.8843e-20, 5.2434e-40, 3.3861e-40, 4.6912e-40],\n",
      "        [4.0595e-09, 6.0105e-18, 2.8708e-18, 3.7169e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "714000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.6321e-20, 4.5325e-40, 2.7291e-40, 3.8425e-40],\n",
      "        [3.6985e-09, 5.5686e-18, 2.5381e-18, 3.3157e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "715000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.4319e-20, 3.8586e-40, 2.1390e-40, 3.0686e-40],\n",
      "        [3.3097e-09, 5.1210e-18, 2.2103e-18, 2.9177e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "716000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.3497e-20, 3.8015e-40, 2.0956e-40, 3.0114e-40],\n",
      "        [3.2941e-09, 5.1105e-18, 2.2019e-18, 2.9068e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "717000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[5.0960e-20, 4.1248e-40, 2.4003e-40, 3.4105e-40],\n",
      "        [3.5571e-09, 5.3562e-18, 2.3937e-18, 3.1378e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "718000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.4065e-20, 4.1549e-40, 2.4635e-40, 3.4883e-40],\n",
      "        [3.6546e-09, 5.3685e-18, 2.4250e-18, 3.1726e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "719000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.4190e-20, 4.0439e-40, 2.3961e-40, 3.3957e-40],\n",
      "        [3.6571e-09, 5.2863e-18, 2.3834e-18, 3.1196e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "720000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.7857e-20, 4.5021e-40, 2.8776e-40, 4.0134e-40],\n",
      "        [4.0612e-09, 5.5876e-18, 2.6441e-18, 3.4294e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "721000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.8640e-20, 3.6449e-40, 2.0786e-40, 2.9745e-40],\n",
      "        [3.4874e-09, 5.0229e-18, 2.2085e-18, 2.9045e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "722000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.6760e-20, 3.0968e-40, 1.6077e-40, 2.3498e-40],\n",
      "        [3.0754e-09, 4.6464e-18, 1.9265e-18, 2.5615e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "723000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.9140e-20, 3.1748e-40, 1.6835e-40, 2.4503e-40],\n",
      "        [3.1720e-09, 4.7176e-18, 1.9841e-18, 2.6307e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "724000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.1117e-20, 3.2351e-40, 1.7449e-40, 2.5315e-40],\n",
      "        [3.2565e-09, 4.7871e-18, 2.0387e-18, 2.6962e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "725000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.8102e-20, 4.1825e-40, 2.6729e-40, 3.7389e-40],\n",
      "        [4.1104e-09, 5.4507e-18, 2.5854e-18, 3.3507e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "726000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.2725e-20, 3.1258e-40, 1.7027e-40, 2.4683e-40],\n",
      "        [3.3122e-09, 4.6891e-18, 2.0042e-18, 2.6493e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "727000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.5382e-20, 2.7549e-40, 1.4068e-40, 2.0696e-40],\n",
      "        [3.0348e-09, 4.3909e-18, 1.7984e-18, 2.3967e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "728000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.4808e-20, 3.1260e-40, 1.7299e-40, 2.5017e-40],\n",
      "        [3.3976e-09, 4.7093e-18, 2.0347e-18, 2.6840e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "729000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[6.3558e-20, 3.7245e-40, 2.3181e-40, 3.2690e-40],\n",
      "        [3.9925e-09, 5.1443e-18, 2.3945e-18, 3.1146e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "730000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.0063e-20, 3.1948e-40, 1.8320e-40, 2.6316e-40],\n",
      "        [3.5745e-09, 4.7512e-18, 2.0959e-18, 2.7542e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "731000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.5040e-20, 2.9382e-40, 1.6237e-40, 2.3528e-40],\n",
      "        [3.4029e-09, 4.5458e-18, 1.9556e-18, 2.5824e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "732000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.3337e-20, 2.9162e-40, 1.5951e-40, 2.3175e-40],\n",
      "        [3.3699e-09, 4.5862e-18, 1.9700e-18, 2.6007e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "733000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.0284e-20, 2.3497e-40, 1.1387e-40, 1.6997e-40],\n",
      "        [2.8592e-09, 4.1136e-18, 1.6349e-18, 2.1903e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "734000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.9436e-20, 3.3456e-40, 2.0330e-40, 2.8897e-40],\n",
      "        [3.9040e-09, 4.9207e-18, 2.2591e-18, 2.9454e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "735000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.2057e-20, 2.3119e-40, 1.1393e-40, 1.6966e-40],\n",
      "        [2.9335e-09, 4.0684e-18, 1.6304e-18, 2.1813e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "736000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[5.7058e-20, 3.1214e-40, 1.8664e-40, 2.6654e-40],\n",
      "        [3.8286e-09, 4.7370e-18, 2.1472e-18, 2.8067e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "737000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.3940e-20, 1.8989e-40, 8.4762e-41, 1.2913e-40],\n",
      "        [2.5748e-09, 3.7002e-18, 1.3940e-18, 1.8867e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "738000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.0141e-20, 2.4977e-40, 1.3257e-40, 1.9455e-40],\n",
      "        [3.2668e-09, 4.2485e-18, 1.7857e-18, 2.3675e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "739000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.4068e-20, 1.8569e-40, 8.3019e-41, 1.2656e-40],\n",
      "        [2.5830e-09, 3.6592e-18, 1.3781e-18, 1.8653e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "740000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.8394e-20, 2.4209e-40, 1.2679e-40, 1.8677e-40],\n",
      "        [3.2235e-09, 4.2261e-18, 1.7675e-18, 2.3447e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "741000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.4505e-20, 2.2248e-40, 1.1227e-40, 1.6685e-40],\n",
      "        [3.0670e-09, 4.0421e-18, 1.6485e-18, 2.1975e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "742000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.1834e-20, 2.4402e-40, 1.3135e-40, 1.9257e-40],\n",
      "        [3.3595e-09, 4.2482e-18, 1.8069e-18, 2.3894e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "743000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.4230e-20, 2.1356e-40, 1.0734e-40, 1.5984e-40],\n",
      "        [3.0609e-09, 3.9638e-18, 1.6116e-18, 2.1496e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "744000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.2163e-20, 2.0163e-40, 9.9131e-41, 1.4841e-40],\n",
      "        [2.9729e-09, 3.8443e-18, 1.5392e-18, 2.0592e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "745000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[5.5030e-20, 2.6526e-40, 1.5609e-40, 2.2481e-40],\n",
      "        [3.8098e-09, 4.4154e-18, 1.9816e-18, 2.5947e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "746000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.6067e-20, 2.0512e-40, 1.0456e-40, 1.5549e-40],\n",
      "        [3.1325e-09, 3.8676e-18, 1.5812e-18, 2.1074e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "747000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.5901e-20, 1.9944e-40, 1.0138e-40, 1.5096e-40],\n",
      "        [3.1247e-09, 3.8071e-18, 1.5516e-18, 2.0694e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "748000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.9698e-20, 2.1920e-40, 1.1326e-40, 1.6615e-40],\n",
      "        [3.3752e-09, 4.1072e-18, 1.7017e-18, 2.2399e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "749000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.1330e-20, 2.1213e-40, 1.1123e-40, 1.6279e-40],\n",
      "        [3.4383e-09, 4.0144e-18, 1.6764e-18, 2.2037e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "750000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.4698e-20, 1.8653e-40, 9.2756e-41, 1.3786e-40],\n",
      "        [3.1357e-09, 3.7601e-18, 1.5155e-18, 2.0128e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "751000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.9383e-20, 1.6652e-40, 7.8691e-41, 1.1868e-40],\n",
      "        [2.8741e-09, 3.5398e-18, 1.3796e-18, 1.8502e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "752000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.0767e-20, 1.9540e-40, 1.0211e-40, 1.5007e-40],\n",
      "        [3.3923e-09, 3.8550e-18, 1.6001e-18, 2.1102e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "753000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.4042e-20, 1.7233e-40, 8.5198e-41, 1.2724e-40],\n",
      "        [3.0859e-09, 3.6059e-18, 1.4428e-18, 1.9231e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "754000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.4963e-20, 1.7586e-40, 8.7871e-41, 1.3098e-40],\n",
      "        [3.1419e-09, 3.6686e-18, 1.4828e-18, 1.9722e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "755000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.3896e-20, 1.6954e-40, 8.3917e-41, 1.2552e-40],\n",
      "        [3.0870e-09, 3.5981e-18, 1.4441e-18, 1.9253e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "756000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.1252e-20, 1.8321e-40, 9.6328e-41, 1.4199e-40],\n",
      "        [3.4039e-09, 3.7503e-18, 1.5616e-18, 2.0626e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "757000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.6653e-20, 1.4663e-40, 6.7681e-41, 1.0339e-40],\n",
      "        [2.7408e-09, 3.3606e-18, 1.2946e-18, 1.7479e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "758000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.9216e-20, 1.5077e-40, 7.1560e-41, 1.0862e-40],\n",
      "        [2.8668e-09, 3.4112e-18, 1.3361e-18, 1.7966e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "759000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.8770e-20, 1.1593e-40, 4.8098e-41, 7.5770e-41],\n",
      "        [2.2832e-09, 2.9692e-18, 1.0669e-18, 1.4697e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "760000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.2869e-20, 1.5533e-40, 7.6480e-41, 1.1518e-40],\n",
      "        [3.0386e-09, 3.4727e-18, 1.3910e-18, 1.8609e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "761000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.7550e-20, 1.3764e-40, 6.4188e-41, 9.8189e-41],\n",
      "        [2.7702e-09, 3.2566e-18, 1.2587e-18, 1.7013e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "762000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.9211e-20, 1.6287e-40, 8.4636e-41, 1.2593e-40],\n",
      "        [3.3107e-09, 3.5622e-18, 1.4729e-18, 1.9556e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "763000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.3409e-20, 1.2190e-40, 5.4152e-41, 8.4099e-41],\n",
      "        [2.5450e-09, 3.0611e-18, 1.1465e-18, 1.5648e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "764000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.3818e-20, 1.6865e-40, 9.0712e-41, 1.3394e-40],\n",
      "        [3.5018e-09, 3.6392e-18, 1.5382e-18, 2.0317e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "765000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.7650e-20, 1.2821e-40, 5.9915e-41, 9.1976e-41],\n",
      "        [2.7610e-09, 3.1450e-18, 1.2140e-18, 1.6448e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "766000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.4360e-20, 1.4159e-40, 7.0700e-41, 1.0672e-40],\n",
      "        [3.0802e-09, 3.3161e-18, 1.3338e-18, 1.7877e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "767000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.6536e-20, 1.2360e-40, 5.7160e-41, 8.8146e-41],\n",
      "        [2.7088e-09, 3.1016e-18, 1.1915e-18, 1.6181e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "768000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.2131e-20, 1.5594e-40, 8.3049e-41, 1.2342e-40],\n",
      "        [3.4253e-09, 3.5105e-18, 1.4739e-18, 1.9542e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "769000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.3205e-20, 1.3409e-40, 6.6392e-41, 1.0072e-40],\n",
      "        [3.0268e-09, 3.2397e-18, 1.2971e-18, 1.7432e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "770000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.9071e-20, 1.4497e-40, 7.5524e-41, 1.1314e-40],\n",
      "        [3.2917e-09, 3.3869e-18, 1.4018e-18, 1.8678e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "771000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.0707e-20, 1.2432e-40, 6.0172e-41, 9.2037e-41],\n",
      "        [2.9041e-09, 3.1212e-18, 1.2312e-18, 1.6632e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "772000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.1469e-20, 1.2392e-40, 6.0431e-41, 9.2325e-41],\n",
      "        [2.9362e-09, 3.1159e-18, 1.2339e-18, 1.6658e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "773000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.2035e-20, 1.2570e-40, 6.1743e-41, 9.4206e-41],\n",
      "        [2.9718e-09, 3.1531e-18, 1.2572e-18, 1.6948e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "774000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.5355e-20, 1.2977e-40, 6.5691e-41, 9.9533e-41],\n",
      "        [3.1192e-09, 3.2075e-18, 1.3021e-18, 1.7476e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "775000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.4857e-20, 1.2774e-40, 6.4470e-41, 9.7861e-41],\n",
      "        [3.1003e-09, 3.1912e-18, 1.2945e-18, 1.7387e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "776000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.9647e-20, 1.3401e-40, 7.0356e-41, 1.0580e-40],\n",
      "        [3.3045e-09, 3.2740e-18, 1.3602e-18, 1.8161e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "777000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.2527e-20, 1.3841e-40, 7.4322e-41, 1.1118e-40],\n",
      "        [3.4298e-09, 3.3417e-18, 1.4101e-18, 1.8756e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "778000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.1034e-20, 1.1473e-40, 5.5958e-41, 8.5961e-41],\n",
      "        [2.9152e-09, 3.0243e-18, 1.1994e-18, 1.6233e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "779000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.9014e-20, 1.2729e-40, 6.6556e-41, 1.0045e-40],\n",
      "        [3.2709e-09, 3.1964e-18, 1.3237e-18, 1.7712e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "780000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.7028e-20, 1.0370e-40, 4.8563e-41, 7.5569e-41],\n",
      "        [2.7154e-09, 2.8762e-18, 1.1119e-18, 1.5170e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "781000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.8388e-20, 8.4198e-41, 3.5024e-41, 5.6104e-41],\n",
      "        [2.2500e-09, 2.5962e-18, 9.3184e-19, 1.2934e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "782000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.6177e-20, 1.1718e-40, 6.0019e-41, 9.1376e-41],\n",
      "        [3.1455e-09, 3.0768e-18, 1.2581e-18, 1.6921e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "783000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.6250e-20, 1.1515e-40, 5.9007e-41, 8.9903e-41],\n",
      "        [3.1433e-09, 3.0484e-18, 1.2456e-18, 1.6764e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "784000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.1889e-20, 1.0558e-40, 5.2036e-41, 8.0187e-41],\n",
      "        [2.9392e-09, 2.9110e-18, 1.1592e-18, 1.5719e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "785000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.0511e-20, 1.0105e-40, 4.9137e-41, 7.6064e-41],\n",
      "        [2.8684e-09, 2.8440e-18, 1.1217e-18, 1.5258e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "786000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.1082e-20, 1.0257e-40, 5.0277e-41, 7.7734e-41],\n",
      "        [2.9068e-09, 2.8834e-18, 1.1465e-18, 1.5569e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "787000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.5132e-20, 1.0877e-40, 5.5412e-41, 8.4874e-41],\n",
      "        [3.0977e-09, 2.9836e-18, 1.2170e-18, 1.6420e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "788000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.6242e-20, 1.0857e-40, 5.5842e-41, 8.5397e-41],\n",
      "        [3.1419e-09, 2.9809e-18, 1.2221e-18, 1.6473e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "789000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.7672e-20, 1.0829e-40, 5.6376e-41, 8.6040e-41],\n",
      "        [3.1976e-09, 2.9772e-18, 1.2283e-18, 1.6538e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "790000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.2551e-20, 8.1832e-41, 3.6201e-41, 5.7295e-41],\n",
      "        [2.5017e-09, 2.6017e-18, 9.6832e-19, 1.3309e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "791000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.8360e-20, 1.1913e-40, 6.6936e-41, 1.0028e-40],\n",
      "        [3.6197e-09, 3.1327e-18, 1.3539e-18, 1.8016e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "792000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.6070e-20, 8.6719e-41, 4.0380e-41, 6.3490e-41],\n",
      "        [2.6670e-09, 2.6727e-18, 1.0286e-18, 1.4098e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "793000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.5348e-20, 9.9425e-41, 5.0908e-41, 7.8331e-41],\n",
      "        [3.0906e-09, 2.8611e-18, 1.1677e-18, 1.5799e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "794000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.9656e-20, 9.0295e-41, 4.3901e-41, 6.8550e-41],\n",
      "        [2.8310e-09, 2.7282e-18, 1.0791e-18, 1.4733e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "795000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.2016e-20, 9.2368e-41, 4.5971e-41, 7.1400e-41],\n",
      "        [2.9389e-09, 2.7616e-18, 1.1075e-18, 1.5071e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "796000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.5402e-20, 8.1546e-41, 3.7441e-41, 5.8772e-41],\n",
      "        [2.6769e-09, 2.6388e-18, 1.0065e-18, 1.3729e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "797000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.4948e-20, 7.9428e-41, 3.6173e-41, 5.6789e-41],\n",
      "        [2.6656e-09, 2.6165e-18, 9.9229e-19, 1.3522e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "798000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.6320e-20, 7.9896e-41, 3.7035e-41, 5.8014e-41],\n",
      "        [2.7251e-09, 2.6181e-18, 1.0027e-18, 1.3654e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "799000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.2758e-20, 8.6572e-41, 4.3362e-41, 6.7414e-41],\n",
      "        [2.9604e-09, 2.6748e-18, 1.0726e-18, 1.4606e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "800000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.3598e-20, 8.7538e-41, 4.4242e-41, 6.8647e-41],\n",
      "        [3.0074e-09, 2.7026e-18, 1.0916e-18, 1.4836e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "801000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.5105e-20, 8.7483e-41, 4.4847e-41, 6.9444e-41],\n",
      "        [3.0657e-09, 2.6997e-18, 1.0987e-18, 1.4918e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "802000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.7610e-20, 8.9484e-41, 4.6915e-41, 7.2355e-41],\n",
      "        [3.1624e-09, 2.7261e-18, 1.1243e-18, 1.5235e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "803000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.1455e-20, 8.2472e-41, 4.0593e-41, 6.2995e-41],\n",
      "        [2.9655e-09, 2.6716e-18, 1.0621e-18, 1.4380e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "804000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.9868e-20, 7.9103e-41, 3.8152e-41, 5.9279e-41],\n",
      "        [2.9170e-09, 2.6379e-18, 1.0351e-18, 1.3997e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "805000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.9632e-20, 7.7475e-41, 3.7207e-41, 5.7797e-41],\n",
      "        [2.9152e-09, 2.6200e-18, 1.0245e-18, 1.3841e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "806000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.5776e-20, 8.4714e-41, 4.3566e-41, 6.7156e-41],\n",
      "        [3.1523e-09, 2.7161e-18, 1.1100e-18, 1.4967e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "807000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.8293e-20, 7.4941e-41, 3.5351e-41, 5.4892e-41],\n",
      "        [2.8967e-09, 2.6205e-18, 1.0158e-18, 1.3672e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "808000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.5211e-20, 7.0128e-41, 3.1693e-41, 4.9380e-41],\n",
      "        [2.7811e-09, 2.5695e-18, 9.6990e-19, 1.3040e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "809000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.5641e-20, 7.0243e-41, 3.1923e-41, 4.9673e-41],\n",
      "        [2.8094e-09, 2.5786e-18, 9.7720e-19, 1.3121e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "810000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.4864e-20, 6.8239e-41, 3.0632e-41, 4.7693e-41],\n",
      "        [2.7832e-09, 2.5555e-18, 9.6053e-19, 1.2886e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "811000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.6864e-20, 6.9472e-41, 3.2022e-41, 4.9710e-41],\n",
      "        [2.8699e-09, 2.5659e-18, 9.7918e-19, 1.3129e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "812000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.3615e-20, 6.6152e-41, 2.9127e-41, 4.5340e-41],\n",
      "        [2.7607e-09, 2.5609e-18, 9.5394e-19, 1.2750e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "813000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.5345e-20, 7.7808e-41, 3.9574e-41, 6.0746e-41],\n",
      "        [3.2142e-09, 2.6801e-18, 1.0880e-18, 1.4545e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "814000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.0408e-20, 8.1315e-41, 4.3365e-41, 6.6234e-41],\n",
      "        [3.3857e-09, 2.7123e-18, 1.1320e-18, 1.5126e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "815000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.5134e-20, 7.5554e-41, 3.8261e-41, 5.8663e-41],\n",
      "        [3.2300e-09, 2.6651e-18, 1.0788e-18, 1.4386e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "816000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.4342e-20, 6.2487e-41, 2.7657e-41, 4.2908e-41],\n",
      "        [2.8260e-09, 2.5190e-18, 9.3918e-19, 1.2496e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "817000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.3182e-20, 6.0004e-41, 2.6059e-41, 4.0472e-41],\n",
      "        [2.7819e-09, 2.4872e-18, 9.1594e-19, 1.2174e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "818000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.9746e-20, 6.6744e-41, 3.1697e-41, 4.8767e-41],\n",
      "        [3.0701e-09, 2.5792e-18, 1.0037e-18, 1.3323e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "819000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.0424e-20, 7.6546e-41, 4.0628e-41, 6.1797e-41],\n",
      "        [3.4625e-09, 2.7016e-18, 1.1260e-18, 1.4931e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "820000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.3543e-20, 7.0857e-41, 3.5161e-41, 5.3737e-41],\n",
      "        [3.2596e-09, 2.6735e-18, 1.0737e-18, 1.4189e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "821000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.3510e-20, 5.9226e-41, 2.5812e-41, 3.9908e-41],\n",
      "        [2.8616e-09, 2.5349e-18, 9.4065e-19, 1.2409e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "822000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.6343e-20, 6.0860e-41, 2.7582e-41, 4.2455e-41],\n",
      "        [2.9941e-09, 2.5515e-18, 9.6826e-19, 1.2762e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "823000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.7466e-20, 6.0774e-41, 2.7919e-41, 4.2890e-41],\n",
      "        [3.0486e-09, 2.5476e-18, 9.7371e-19, 1.2822e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "824000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.1497e-20, 6.3804e-41, 3.0770e-41, 4.7002e-41],\n",
      "        [3.2258e-09, 2.5935e-18, 1.0211e-18, 1.3426e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "825000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.9766e-20, 7.0763e-41, 3.7140e-41, 5.6223e-41],\n",
      "        [3.5390e-09, 2.6890e-18, 1.1156e-18, 1.4649e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "826000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.1008e-20, 6.2047e-41, 2.9709e-41, 4.5340e-41],\n",
      "        [3.2314e-09, 2.5845e-18, 1.0137e-18, 1.3293e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "827000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.7983e-20, 5.8033e-41, 2.6733e-41, 4.0928e-41],\n",
      "        [3.1160e-09, 2.5305e-18, 9.6887e-19, 1.2692e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "828000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.7754e-20, 5.6942e-41, 2.6126e-41, 3.9975e-41],\n",
      "        [3.1194e-09, 2.5212e-18, 9.6293e-19, 1.2594e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "829000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.0094e-20, 5.7935e-41, 2.7332e-41, 4.1684e-41],\n",
      "        [3.2228e-09, 2.5314e-18, 9.8222e-19, 1.2837e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "830000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.8612e-20, 5.7027e-41, 2.6434e-41, 4.0328e-41],\n",
      "        [3.1890e-09, 2.5477e-18, 9.8126e-19, 1.2790e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "831000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.3863e-20, 6.1646e-41, 3.0382e-41, 4.6023e-41],\n",
      "        [3.4215e-09, 2.6297e-18, 1.0533e-18, 1.3702e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "832000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.7630e-20, 5.4944e-41, 2.5114e-41, 3.8293e-41],\n",
      "        [3.1776e-09, 2.5380e-18, 9.7060e-19, 1.2609e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "833000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.0924e-20, 6.4425e-41, 3.3917e-41, 5.0986e-41],\n",
      "        [3.6919e-09, 2.6582e-18, 1.1055e-18, 1.4356e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "834000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.0752e-20, 6.3378e-41, 3.3281e-41, 5.0004e-41],\n",
      "        [3.6997e-09, 2.6496e-18, 1.1003e-18, 1.4270e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "835000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.9392e-20, 5.4899e-41, 2.5616e-41, 3.8858e-41],\n",
      "        [3.3066e-09, 2.5730e-18, 9.9921e-19, 1.2909e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "836000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.3165e-20, 5.6547e-41, 2.7517e-41, 4.1541e-41],\n",
      "        [3.4703e-09, 2.5922e-18, 1.0311e-18, 1.3308e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "837000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.6764e-20, 5.7738e-41, 2.9116e-41, 4.3771e-41],\n",
      "        [3.6176e-09, 2.6042e-18, 1.0569e-18, 1.3627e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "838000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.1615e-20, 5.2601e-41, 2.5069e-41, 3.7867e-41],\n",
      "        [3.4261e-09, 2.5287e-18, 9.9093e-19, 1.2761e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "839000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.6420e-20, 4.8715e-41, 2.1771e-41, 3.3043e-41],\n",
      "        [3.2310e-09, 2.5000e-18, 9.4584e-19, 1.2143e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "840000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.6093e-20, 4.7557e-41, 2.1132e-41, 3.2073e-41],\n",
      "        [3.2224e-09, 2.4795e-18, 9.3400e-19, 1.1981e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "841000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.0652e-20, 5.6914e-41, 2.9657e-41, 4.4295e-41],\n",
      "        [3.8176e-09, 2.6130e-18, 1.0818e-18, 1.3870e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "842000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.3529e-20, 5.1068e-41, 2.4775e-41, 3.7238e-41],\n",
      "        [3.5581e-09, 2.5266e-18, 1.0012e-18, 1.2821e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "843000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.8787e-20, 5.4387e-41, 2.7824e-41, 4.1540e-41],\n",
      "        [3.7907e-09, 2.5983e-18, 1.0653e-18, 1.3606e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "844000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.2917e-20, 4.9445e-41, 2.3798e-41, 3.5722e-41],\n",
      "        [3.5698e-09, 2.5215e-18, 9.9572e-19, 1.2705e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "845000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.8973e-20, 5.8335e-41, 3.2422e-41, 4.7959e-41],\n",
      "        [4.1569e-09, 2.6485e-18, 1.1390e-18, 1.4526e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "846000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.0863e-20, 4.6518e-41, 2.1834e-41, 3.2783e-41],\n",
      "        [3.5197e-09, 2.4911e-18, 9.6968e-19, 1.2327e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "847000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.2927e-20, 5.3779e-41, 2.8460e-41, 4.2180e-41],\n",
      "        [4.0152e-09, 2.6212e-18, 1.0981e-18, 1.3932e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "848000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.0366e-20, 4.4829e-41, 2.0884e-41, 3.1320e-41],\n",
      "        [3.5302e-09, 2.4771e-18, 9.6040e-19, 1.2170e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "849000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.7077e-20, 4.1575e-41, 1.8553e-41, 2.7926e-41],\n",
      "        [3.3881e-09, 2.4178e-18, 9.1261e-19, 1.1552e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "850000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.2456e-20, 4.4357e-41, 2.1113e-41, 3.1564e-41],\n",
      "        [3.6349e-09, 2.4628e-18, 9.6536e-19, 1.2212e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "851000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.6726e-20, 5.2521e-41, 2.8567e-41, 4.2081e-41],\n",
      "        [4.2127e-09, 2.6221e-18, 1.1167e-18, 1.4086e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "852000   actions:  tensor([0, 0, 3])\n",
      "loss=  tensor(2.5348, grad_fn=<DivBackward0>)   , return=  15963.328125\n",
      "discReturns/1000= tensor([14.3845, 10.3025,  5.4855])\n",
      "actionProbs tensor([[3.7039e-20, 4.5921e-41, 2.2906e-41, 3.4007e-41],\n",
      "        [3.8674e-09, 2.5128e-18, 1.0148e-18, 1.2785e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "853000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.0225e-20, 4.6754e-41, 2.4004e-41, 3.5517e-41],\n",
      "        [3.9983e-09, 2.5236e-18, 1.0359e-18, 1.3040e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "854000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.8870e-20, 3.9340e-41, 1.7881e-41, 2.6763e-41],\n",
      "        [3.5321e-09, 2.3911e-18, 9.1159e-19, 1.1464e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "855000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.3020e-20, 4.0971e-41, 1.9528e-41, 2.9073e-41],\n",
      "        [3.7250e-09, 2.4182e-18, 9.4748e-19, 1.1905e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "856000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.3929e-20, 4.1127e-41, 1.9792e-41, 2.9406e-41],\n",
      "        [3.7853e-09, 2.4340e-18, 9.6034e-19, 1.2043e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "857000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.1227e-20, 3.9469e-41, 1.8434e-41, 2.7439e-41],\n",
      "        [3.6922e-09, 2.4214e-18, 9.4034e-19, 1.1766e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "858000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.5648e-20, 3.5828e-41, 1.5584e-41, 2.3330e-41],\n",
      "        [3.4503e-09, 2.3677e-18, 8.8268e-19, 1.1018e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "859000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.6280e-20, 4.2370e-41, 2.0920e-41, 3.0892e-41],\n",
      "        [3.9660e-09, 2.5187e-18, 1.0162e-18, 1.2658e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "860000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.0415e-20, 4.3726e-41, 2.2432e-41, 3.2981e-41],\n",
      "        [4.1398e-09, 2.5405e-18, 1.0478e-18, 1.3040e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "861000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.3103e-20, 3.8906e-41, 1.8531e-41, 2.7435e-41],\n",
      "        [3.8438e-09, 2.4460e-18, 9.6397e-19, 1.1987e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "862000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.0091e-20, 3.6263e-41, 1.6660e-41, 2.4736e-41],\n",
      "        [3.7157e-09, 2.3897e-18, 9.2050e-19, 1.1434e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "863000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.0110e-20, 3.6331e-41, 1.6706e-41, 2.4767e-41],\n",
      "        [3.7494e-09, 2.4172e-18, 9.3471e-19, 1.1578e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "864000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.3870e-20, 3.7364e-41, 1.7903e-41, 2.6416e-41],\n",
      "        [3.9297e-09, 2.4336e-18, 9.6332e-19, 1.1920e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "865000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.0471e-20, 3.4714e-41, 1.5985e-41, 2.3664e-41],\n",
      "        [3.7827e-09, 2.3752e-18, 9.1701e-19, 1.1336e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "866000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.8051e-20, 3.2640e-41, 1.4565e-41, 2.1621e-41],\n",
      "        [3.6720e-09, 2.3265e-18, 8.8055e-19, 1.0876e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "867000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.9738e-20, 3.2938e-41, 1.4998e-41, 2.2206e-41],\n",
      "        [3.7609e-09, 2.3306e-18, 8.9202e-19, 1.1009e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "868000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.6310e-20, 3.1469e-41, 1.3727e-41, 2.0380e-41],\n",
      "        [3.6233e-09, 2.3282e-18, 8.7202e-19, 1.0731e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "869000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.7563e-20, 3.1435e-41, 1.3928e-41, 2.0636e-41],\n",
      "        [3.6951e-09, 2.3239e-18, 8.7759e-19, 1.0790e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "870000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.6935e-20, 3.4887e-41, 1.7159e-41, 2.5139e-41],\n",
      "        [4.1430e-09, 2.3955e-18, 9.6137e-19, 1.1805e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "871000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.1971e-20, 3.2017e-41, 1.4927e-41, 2.1975e-41],\n",
      "        [3.9275e-09, 2.3298e-18, 9.0479e-19, 1.1102e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "872000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.0580e-20, 3.5833e-41, 1.8236e-41, 2.6571e-41],\n",
      "        [4.3427e-09, 2.4411e-18, 1.0023e-18, 1.2261e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "873000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.4471e-20, 2.7506e-41, 1.1634e-41, 1.7256e-41],\n",
      "        [3.5892e-09, 2.2443e-18, 8.2395e-19, 1.0068e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "874000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.1618e-20, 3.4490e-41, 1.7670e-41, 2.5694e-41],\n",
      "        [4.4051e-09, 2.4055e-18, 9.8915e-19, 1.2074e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "875000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.2215e-20, 2.9932e-41, 1.3953e-41, 2.0472e-41],\n",
      "        [4.0022e-09, 2.2985e-18, 8.9235e-19, 1.0884e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "876000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.8604e-20, 3.6340e-41, 1.9699e-41, 2.8420e-41],\n",
      "        [4.7309e-09, 2.4721e-18, 1.0539e-18, 1.2812e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "877000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.8027e-20, 3.1612e-41, 1.5647e-41, 2.2767e-41],\n",
      "        [4.3163e-09, 2.3634e-18, 9.5322e-19, 1.1577e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "878000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.9291e-20, 2.2407e-41, 8.6642e-42, 1.2916e-41],\n",
      "        [3.3534e-09, 2.1336e-18, 7.4461e-19, 9.0268e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "879000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.8606e-20, 2.7241e-41, 1.2163e-41, 1.7841e-41],\n",
      "        [3.9198e-09, 2.2845e-18, 8.6901e-19, 1.0510e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "880000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.9753e-20, 2.7117e-41, 1.2267e-41, 1.7958e-41],\n",
      "        [3.9873e-09, 2.2782e-18, 8.7242e-19, 1.0541e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "881000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.6280e-20, 2.4995e-41, 1.0798e-41, 1.5873e-41],\n",
      "        [3.8100e-09, 2.2176e-18, 8.2525e-19, 9.9625e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "882000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.5768e-20, 2.4799e-41, 1.0647e-41, 1.5638e-41],\n",
      "        [3.8138e-09, 2.2357e-18, 8.3147e-19, 1.0010e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "883000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.8284e-20, 2.9268e-41, 1.4494e-41, 2.0972e-41],\n",
      "        [4.4461e-09, 2.3524e-18, 9.5168e-19, 1.1445e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "884000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.5722e-20, 3.1156e-41, 1.6446e-41, 2.3631e-41],\n",
      "        [4.7676e-09, 2.3961e-18, 1.0059e-18, 1.2086e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "885000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.4806e-20, 2.2829e-41, 9.6367e-42, 1.4157e-41],\n",
      "        [3.7775e-09, 2.1688e-18, 7.9602e-19, 9.5598e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "886000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.2445e-20, 2.9066e-41, 1.4913e-41, 2.1458e-41],\n",
      "        [4.6643e-09, 2.3503e-18, 9.6984e-19, 1.1623e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "887000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.1013e-20, 2.4988e-41, 1.1442e-41, 1.6638e-41],\n",
      "        [4.1635e-09, 2.2584e-18, 8.7281e-19, 1.0439e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "888000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.2506e-20, 2.0949e-41, 8.5325e-42, 1.2547e-41],\n",
      "        [3.6893e-09, 2.1327e-18, 7.6765e-19, 9.1743e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "889000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.0790e-20, 2.4513e-41, 1.0908e-41, 1.5501e-41],\n",
      "        [4.3503e-09, 2.3347e-18, 8.8780e-19, 1.0378e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "890000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.8867e-20, 2.2810e-41, 9.9492e-42, 1.4223e-41],\n",
      "        [4.1975e-09, 2.2484e-18, 8.4278e-19, 9.8933e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "891000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.3930e-20, 2.5071e-41, 1.1578e-41, 1.6356e-41],\n",
      "        [4.5870e-09, 2.3883e-18, 9.3247e-19, 1.0845e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "892000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[4.0595e-20, 2.6936e-41, 1.3192e-41, 1.8409e-41],\n",
      "        [5.0102e-09, 2.4793e-18, 1.0026e-18, 1.1569e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "893000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.3466e-20, 2.3739e-41, 1.0934e-41, 1.5483e-41],\n",
      "        [4.5362e-09, 2.3210e-18, 9.0229e-19, 1.0515e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "894000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.4099e-20, 2.3451e-41, 1.0874e-41, 1.5388e-41],\n",
      "        [4.5703e-09, 2.3061e-18, 8.9915e-19, 1.0475e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "895000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.9915e-20, 1.7708e-41, 6.8594e-42, 1.0108e-41],\n",
      "        [3.4929e-09, 2.0148e-18, 7.0097e-19, 8.3876e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "896000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.7867e-20, 1.6457e-41, 6.1349e-42, 9.1225e-42],\n",
      "        [3.3001e-09, 1.9445e-18, 6.5846e-19, 7.9302e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "897000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[4.2261e-20, 2.4610e-41, 1.2298e-41, 1.7163e-41],\n",
      "        [5.0871e-09, 2.3837e-18, 9.7368e-19, 1.1234e-18],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "898000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[3.0689e-20, 2.0299e-41, 9.1603e-42, 1.3092e-41],\n",
      "        [4.3211e-09, 2.1558e-18, 8.2549e-19, 9.6782e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "899000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.7704e-20, 1.8821e-41, 8.2088e-42, 1.1831e-41],\n",
      "        [4.0949e-09, 2.0748e-18, 7.7616e-19, 9.1532e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "900000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.2148e-20, 1.6549e-41, 6.6660e-42, 9.7825e-42],\n",
      "        [3.6500e-09, 1.9502e-18, 6.9095e-19, 8.2519e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "901000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.2378e-20, 1.9716e-41, 9.0888e-42, 1.2961e-41],\n",
      "        [4.4295e-09, 2.1335e-18, 8.2700e-19, 9.6799e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "902000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[3.0548e-20, 1.9188e-41, 8.7049e-42, 1.2465e-41],\n",
      "        [4.3196e-09, 2.1198e-18, 8.1566e-19, 9.5660e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "903000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.8589e-20, 1.4866e-41, 5.6514e-42, 8.4162e-42],\n",
      "        [3.3561e-09, 1.8762e-18, 6.4132e-19, 7.7273e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "904000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.4595e-20, 1.6426e-41, 6.8958e-42, 1.0060e-41],\n",
      "        [3.8534e-09, 1.9656e-18, 7.1588e-19, 8.5080e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "905000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.8239e-20, 1.4016e-41, 5.2927e-42, 7.9075e-42],\n",
      "        [3.3084e-09, 1.8218e-18, 6.1774e-19, 7.4634e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "906000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.0951e-20, 1.4680e-41, 5.8224e-42, 8.6124e-42],\n",
      "        [3.5417e-09, 1.8614e-18, 6.5112e-19, 7.8144e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "907000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.6621e-20, 1.6280e-41, 7.0401e-42, 1.0225e-41],\n",
      "        [4.0022e-09, 1.9639e-18, 7.2806e-19, 8.6262e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "908000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.6731e-20, 1.5941e-41, 6.9042e-42, 1.0033e-41],\n",
      "        [4.0016e-09, 1.9431e-18, 7.1984e-19, 8.5340e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "909000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.3182e-20, 1.4538e-41, 5.9863e-42, 8.8044e-42],\n",
      "        [3.7155e-09, 1.8585e-18, 6.6464e-19, 7.9462e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "910000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.8126e-20, 1.5751e-41, 6.9560e-42, 1.0082e-41],\n",
      "        [4.1002e-09, 1.9379e-18, 7.2633e-19, 8.5949e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "911000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.7802e-20, 1.2659e-41, 4.7602e-42, 7.1508e-42],\n",
      "        [3.2603e-09, 1.7525e-18, 5.9092e-19, 7.1627e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "912000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.5382e-20, 1.1589e-41, 4.1408e-42, 6.2932e-42],\n",
      "        [3.0231e-09, 1.6795e-18, 5.4654e-19, 6.6806e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "913000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.0887e-20, 1.3001e-41, 5.1722e-42, 7.6875e-42],\n",
      "        [3.5177e-09, 1.7725e-18, 6.1824e-19, 7.4430e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "914000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.0956e-20, 9.6970e-42, 3.0829e-42, 4.8121e-42],\n",
      "        [2.5643e-09, 1.5634e-18, 4.7306e-19, 5.8805e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "915000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.2820e-20, 1.3551e-41, 5.5842e-42, 8.2424e-42],\n",
      "        [3.7016e-09, 1.8300e-18, 6.5595e-19, 7.8507e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "916000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.6007e-20, 1.1308e-41, 4.1128e-42, 6.2428e-42],\n",
      "        [3.0912e-09, 1.6787e-18, 5.5328e-19, 6.7507e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "917000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.6293e-20, 1.3842e-41, 5.9934e-42, 8.7651e-42],\n",
      "        [3.9587e-09, 1.8459e-18, 6.8172e-19, 8.1106e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "918000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.3363e-20, 1.2839e-41, 5.3319e-42, 7.8767e-42],\n",
      "        [3.7227e-09, 1.7803e-18, 6.3859e-19, 7.6506e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "919000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.5708e-20, 1.0514e-41, 3.7989e-42, 5.7902e-42],\n",
      "        [3.0433e-09, 1.6187e-18, 5.2872e-19, 6.4720e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "920000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.1961e-20, 1.1980e-41, 4.8681e-42, 7.2363e-42],\n",
      "        [3.5945e-09, 1.7207e-18, 6.0660e-19, 7.3013e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "921000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.9507e-20, 1.1386e-41, 4.4477e-42, 6.6702e-42],\n",
      "        [3.3958e-09, 1.6883e-18, 5.8067e-19, 7.0289e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "922000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.7487e-20, 1.3230e-41, 5.8378e-42, 8.5311e-42],\n",
      "        [4.0411e-09, 1.8197e-18, 6.7914e-19, 8.0726e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "923000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.0734e-20, 1.1416e-41, 4.5626e-42, 6.8187e-42],\n",
      "        [3.5004e-09, 1.6962e-18, 5.9197e-19, 7.1465e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "924000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.6989e-20, 1.0252e-41, 3.8213e-42, 5.8014e-42],\n",
      "        [3.1615e-09, 1.6112e-18, 5.3623e-19, 6.5454e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "925000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.8931e-20, 1.0620e-41, 4.1156e-42, 6.2007e-42],\n",
      "        [3.3407e-09, 1.6440e-18, 5.6163e-19, 6.8173e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "926000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.7018e-20, 9.9198e-42, 3.7036e-42, 5.6290e-42],\n",
      "        [3.1607e-09, 1.5911e-18, 5.2959e-19, 6.4687e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "927000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.1058e-20, 1.0651e-41, 4.2838e-42, 6.4137e-42],\n",
      "        [3.5094e-09, 1.6444e-18, 5.7398e-19, 6.9394e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "928000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.3480e-20, 8.8268e-42, 3.0464e-42, 4.7154e-42],\n",
      "        [2.8174e-09, 1.5151e-18, 4.7920e-19, 5.9235e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "929000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.3244e-20, 1.0947e-41, 4.5682e-42, 6.7921e-42],\n",
      "        [3.6945e-09, 1.6760e-18, 5.9993e-19, 7.2152e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "930000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.6107e-20, 9.1561e-42, 3.3617e-42, 5.1428e-42],\n",
      "        [3.0685e-09, 1.5397e-18, 5.0578e-19, 6.2038e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "931000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.5841e-20, 8.9417e-42, 3.2636e-42, 5.0026e-42],\n",
      "        [3.0380e-09, 1.5217e-18, 4.9738e-19, 6.1094e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "932000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.7966e-20, 1.1226e-41, 4.9998e-42, 7.3442e-42],\n",
      "        [4.0315e-09, 1.6929e-18, 6.3001e-19, 7.5191e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "933000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.4115e-20, 8.1626e-42, 2.8629e-42, 4.4323e-42],\n",
      "        [2.8561e-09, 1.4558e-18, 4.6202e-19, 5.7174e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "934000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.4863e-20, 8.2144e-42, 2.9343e-42, 4.5276e-42],\n",
      "        [2.9265e-09, 1.4595e-18, 4.6820e-19, 5.7817e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "935000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.4072e-20, 7.9187e-42, 2.7802e-42, 4.3090e-42],\n",
      "        [2.8480e-09, 1.4361e-18, 4.5582e-19, 5.6436e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "936000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.4859e-20, 8.1359e-42, 2.9161e-42, 4.5010e-42],\n",
      "        [2.9367e-09, 1.4634e-18, 4.7193e-19, 5.8225e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "937000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.5655e-20, 1.0123e-41, 4.3917e-42, 6.5062e-42],\n",
      "        [3.8548e-09, 1.6204e-18, 5.9214e-19, 7.1051e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "938000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.0666e-20, 6.8201e-42, 2.1832e-42, 3.4570e-42],\n",
      "        [2.4794e-09, 1.3447e-18, 4.0144e-19, 5.0429e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "939000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.6736e-20, 8.1107e-42, 3.0324e-42, 4.6467e-42],\n",
      "        [3.1010e-09, 1.4581e-18, 4.8169e-19, 5.9169e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "940000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.7371e-20, 8.2452e-42, 3.1277e-42, 4.7812e-42],\n",
      "        [3.1674e-09, 1.4768e-18, 4.9336e-19, 6.0457e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "941000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.7032e-20, 9.8189e-42, 4.3454e-42, 6.4264e-42],\n",
      "        [3.9465e-09, 1.6024e-18, 5.9196e-19, 7.0939e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "942000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.1619e-20, 6.7318e-42, 2.2225e-42, 3.5046e-42],\n",
      "        [2.5820e-09, 1.3410e-18, 4.0779e-19, 5.1070e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "943000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.3302e-20, 7.0639e-42, 2.4509e-42, 3.8255e-42],\n",
      "        [2.7670e-09, 1.3747e-18, 4.3273e-19, 5.3778e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "944000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.3257e-20, 6.9154e-42, 2.3976e-42, 3.7443e-42],\n",
      "        [2.7570e-09, 1.3601e-18, 4.2721e-19, 5.3140e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "945000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.7038e-20, 7.5642e-42, 2.8586e-42, 4.3861e-42],\n",
      "        [3.1208e-09, 1.4180e-18, 4.7101e-19, 5.7872e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "946000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.1663e-20, 8.2256e-42, 3.3799e-42, 5.0951e-42],\n",
      "        [3.5131e-09, 1.4741e-18, 5.1666e-19, 6.2742e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "947000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.5660e-20, 7.0906e-42, 2.6064e-42, 4.0287e-42],\n",
      "        [2.9870e-09, 1.3777e-18, 4.4858e-19, 5.5402e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "948000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.4707e-20, 6.9140e-42, 2.4999e-42, 3.8788e-42],\n",
      "        [2.9058e-09, 1.3676e-18, 4.4197e-19, 5.4677e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "949000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.0237e-20, 7.7716e-42, 3.1389e-42, 4.7574e-42],\n",
      "        [3.4027e-09, 1.4440e-18, 5.0126e-19, 6.1045e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "950000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.8656e-20, 7.3918e-42, 2.9007e-42, 4.4281e-42],\n",
      "        [3.2623e-09, 1.4096e-18, 4.7976e-19, 5.8701e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "951000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.0139e-20, 7.5124e-42, 3.0282e-42, 4.5977e-42],\n",
      "        [3.3842e-09, 1.4197e-18, 4.9111e-19, 5.9889e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "952000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.1925e-20, 5.9275e-42, 1.9926e-42, 3.1501e-42],\n",
      "        [2.6000e-09, 1.2694e-18, 3.8927e-19, 4.8789e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "953000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.2281e-20, 5.9611e-42, 2.0277e-42, 3.1964e-42],\n",
      "        [2.6399e-09, 1.2755e-18, 3.9410e-19, 4.9323e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "954000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.2028e-20, 5.8168e-42, 1.9632e-42, 3.1039e-42],\n",
      "        [2.6086e-09, 1.2603e-18, 3.8711e-19, 4.8525e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "955000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.4420e-20, 6.1489e-42, 2.2098e-42, 3.4486e-42],\n",
      "        [2.8504e-09, 1.2928e-18, 4.1316e-19, 5.1352e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "956000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.5097e-20, 6.2862e-42, 2.2981e-42, 3.5747e-42],\n",
      "        [2.9237e-09, 1.3120e-18, 4.2480e-19, 5.2650e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "957000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.5092e-20, 6.1671e-42, 2.2547e-42, 3.5089e-42],\n",
      "        [2.9179e-09, 1.2995e-18, 4.2012e-19, 5.2107e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "958000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.6101e-20, 6.2904e-42, 2.3570e-42, 3.6504e-42],\n",
      "        [3.0159e-09, 1.3134e-18, 4.3184e-19, 5.3362e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "959000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.0862e-20, 5.2507e-42, 1.7194e-42, 2.7437e-42],\n",
      "        [2.4733e-09, 1.2059e-18, 3.6233e-19, 4.5707e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "960000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.3499e-20, 7.1256e-42, 3.0422e-42, 4.5850e-42],\n",
      "        [3.6313e-09, 1.3908e-18, 4.9718e-19, 6.0329e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "961000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.6948e-20, 6.0872e-42, 2.3220e-42, 3.5901e-42],\n",
      "        [3.0778e-09, 1.2910e-18, 4.2779e-19, 5.2827e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "962000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.1013e-20, 6.6800e-42, 2.7564e-42, 4.1899e-42],\n",
      "        [3.4397e-09, 1.3551e-18, 4.7436e-19, 5.7860e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "963000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.7480e-20, 6.0942e-42, 2.3598e-42, 3.6406e-42],\n",
      "        [3.1329e-09, 1.2975e-18, 4.3510e-19, 5.3601e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "964000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.5446e-20, 5.6753e-42, 2.1062e-42, 3.2818e-42],\n",
      "        [2.9395e-09, 1.2540e-18, 4.0831e-19, 5.0657e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "965000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.2345e-20, 5.1652e-42, 1.7811e-42, 2.8194e-42],\n",
      "        [2.6342e-09, 1.2042e-18, 3.7410e-19, 4.6909e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "966000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.4463e-20, 5.4188e-42, 1.9716e-42, 3.0885e-42],\n",
      "        [2.8462e-09, 1.2309e-18, 3.9580e-19, 4.9264e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "967000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.3354e-20, 5.1596e-42, 1.8273e-42, 2.8811e-42],\n",
      "        [2.7308e-09, 1.2023e-18, 3.7931e-19, 4.7430e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "968000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.6586e-20, 5.5744e-42, 2.1272e-42, 3.3015e-42],\n",
      "        [3.0392e-09, 1.2462e-18, 4.1253e-19, 5.1042e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "969000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.3808e-20, 5.1161e-42, 1.8343e-42, 2.8867e-42],\n",
      "        [2.7726e-09, 1.1984e-18, 3.8061e-19, 4.7544e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "970000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.0567e-20, 4.4968e-42, 1.4714e-42, 2.3640e-42],\n",
      "        [2.4215e-09, 1.1272e-18, 3.3662e-19, 4.2651e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "971000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.8943e-20, 5.7145e-42, 2.2883e-42, 3.5201e-42],\n",
      "        [3.2484e-09, 1.2675e-18, 4.3300e-19, 5.3237e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "972000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.7076e-20, 5.3810e-42, 2.0795e-42, 3.2258e-42],\n",
      "        [3.0793e-09, 1.2317e-18, 4.1048e-19, 5.0769e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "973000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.7982e-20, 5.4202e-42, 2.1328e-42, 3.2973e-42],\n",
      "        [3.1554e-09, 1.2352e-18, 4.1610e-19, 5.1354e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "974000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.5817e-20, 5.0587e-42, 1.9044e-42, 2.9750e-42],\n",
      "        [2.9551e-09, 1.1953e-18, 3.9063e-19, 4.8555e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "975000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.1251e-20, 4.3875e-42, 1.4714e-42, 2.3570e-42],\n",
      "        [2.4952e-09, 1.1206e-18, 3.3955e-19, 4.2937e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "976000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[2.0214e-20, 5.5309e-42, 2.2729e-42, 3.4850e-42],\n",
      "        [3.3455e-09, 1.2504e-18, 4.3362e-19, 5.3199e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "977000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.8115e-20, 5.2044e-42, 2.0585e-42, 3.1866e-42],\n",
      "        [3.1624e-09, 1.2147e-18, 4.1042e-19, 5.0664e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "978000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.6305e-20, 4.9130e-42, 1.8749e-42, 2.9259e-42],\n",
      "        [2.9962e-09, 1.1818e-18, 3.8953e-19, 4.8370e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "979000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.5988e-20, 4.7952e-42, 1.8189e-42, 2.8432e-42],\n",
      "        [2.9624e-09, 1.1679e-18, 3.8280e-19, 4.7605e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "980000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.2385e-20, 4.2992e-42, 1.4952e-42, 2.3836e-42],\n",
      "        [2.6097e-09, 1.1120e-18, 3.4443e-19, 4.3391e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "981000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.2842e-20, 4.2922e-42, 1.5120e-42, 2.4046e-42],\n",
      "        [2.6530e-09, 1.1105e-18, 3.4641e-19, 4.3587e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "982000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.5430e-20, 4.5780e-42, 1.7180e-42, 2.6975e-42],\n",
      "        [2.9084e-09, 1.1471e-18, 3.7318e-19, 4.6528e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "983000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.5868e-20, 4.5710e-42, 1.7320e-42, 2.7157e-42],\n",
      "        [2.9455e-09, 1.1458e-18, 3.7475e-19, 4.6678e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "984000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.4002e-20, 4.3426e-42, 1.5835e-42, 2.5041e-42],\n",
      "        [2.7745e-09, 1.1217e-18, 3.5857e-19, 4.4893e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "985000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.0386e-20, 3.7877e-42, 1.2486e-42, 2.0193e-42],\n",
      "        [2.3868e-09, 1.0518e-18, 3.1412e-19, 3.9948e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "986000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.4470e-20, 4.2712e-42, 1.5751e-42, 2.4873e-42],\n",
      "        [2.8120e-09, 1.1119e-18, 3.5732e-19, 4.4712e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "987000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.1343e-20, 3.8255e-42, 1.2990e-42, 2.0893e-42],\n",
      "        [2.4877e-09, 1.0557e-18, 3.2098e-19, 4.0677e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "988000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.1701e-20, 3.8087e-42, 1.3074e-42, 2.0991e-42],\n",
      "        [2.5224e-09, 1.0530e-18, 3.2200e-19, 4.0769e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "989000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[9.3613e-21, 3.4360e-42, 1.0930e-42, 1.7867e-42],\n",
      "        [2.2540e-09, 1.0030e-18, 2.9155e-19, 3.7346e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "990000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[2.1286e-20, 4.8107e-42, 2.0291e-42, 3.1193e-42],\n",
      "        [3.4052e-09, 1.1802e-18, 4.1374e-19, 5.0815e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "991000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.4473e-20, 4.0385e-42, 1.4924e-42, 2.3626e-42],\n",
      "        [2.8044e-09, 1.0868e-18, 3.4882e-19, 4.3715e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "992000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.3662e-20, 3.8802e-42, 1.4069e-42, 2.2365e-42],\n",
      "        [2.7206e-09, 1.0661e-18, 3.3738e-19, 4.2431e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "993000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.7481e-20, 4.2053e-42, 1.6577e-42, 2.5910e-42],\n",
      "        [3.0704e-09, 1.1061e-18, 3.6945e-19, 4.5922e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "994000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.3217e-20, 3.7653e-42, 1.3509e-42, 2.1552e-42],\n",
      "        [2.6761e-09, 1.0543e-18, 3.3140e-19, 4.1760e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "995000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.6142e-20, 4.0287e-42, 1.5484e-42, 2.4341e-42],\n",
      "        [2.9528e-09, 1.0876e-18, 3.5724e-19, 4.4585e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "996000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.6227e-20, 3.9825e-42, 1.5330e-42, 2.4102e-42],\n",
      "        [2.9566e-09, 1.0812e-18, 3.5516e-19, 4.4336e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "997000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.4828e-20, 3.8480e-42, 1.4419e-42, 2.2813e-42],\n",
      "        [2.8344e-09, 1.0675e-18, 3.4530e-19, 4.3254e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "998000   actions:  tensor([0, 0, 0])\n",
      "loss=  tensor(2.5390, grad_fn=<DivBackward0>)   , return=  15972.328125\n",
      "discReturns/1000= tensor([14.3918, 10.3106,  5.4945])\n",
      "actionProbs tensor([[1.5259e-20, 3.8494e-42, 1.4559e-42, 2.3009e-42],\n",
      "        [2.8719e-09, 1.0671e-18, 3.4713e-19, 4.3440e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "999000   actions:  tensor([0, 0, 2])\n",
      "loss=  tensor(2.5372, grad_fn=<DivBackward0>)   , return=  15968.328125\n",
      "discReturns/1000= tensor([14.3886, 10.3070,  5.4905])\n",
      "actionProbs tensor([[1.2672e-20, 3.5201e-42, 1.2500e-42, 2.0039e-42],\n",
      "        [2.6142e-09, 1.0230e-18, 3.1891e-19, 4.0304e-19],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "algorithm.solver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "advModeNames=\"\"\n",
    "for i in range(len(adversaryProbs)):\n",
    "    if adversaryProbs[i]!=0:\n",
    "        tmp=\"{:.1f}\".format(adversaryProbs[i])\n",
    "        advModeNames+=f\"{(AdversaryModes(i)).name}-{tmp}-\"\n",
    "    \n",
    "name=f\"ep {algorithm.numberEpisodes}, {advModeNames}, {game.advHistoryNum} hist, {neuralNet.lr} lr\"\n",
    "neuralNet.save(name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "profits = pd.DataFrame(game.profit).T\n",
    "prices = pd.DataFrame(game.prices).T\n",
    "demandPotential = pd.DataFrame(game.demandPotential).T\n",
    "learning = pd.DataFrame(algorithm.returns.mean(axis = 0),columns=['learning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = pd.DataFrame(algorithm.loss.mean(axis = 0),columns=['loss'])\n",
    "loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>203.50</td>\n",
       "      <td>196.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>205.25</td>\n",
       "      <td>194.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1\n",
       "0  200.00  200.00\n",
       "1  203.50  196.50\n",
       "2  205.25  194.75"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "demandPotential.plot()\n",
    "demandPotential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    15972.328125\n",
       "1    11926.328125\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profits.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5112.250000</td>\n",
       "      <td>4160.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5365.562500</td>\n",
       "      <td>3937.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5494.515625</td>\n",
       "      <td>3828.515625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0            1\n",
       "0  5112.250000  4160.250000\n",
       "1  5365.562500  3937.562500\n",
       "2  5494.515625  3828.515625"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "profits.plot()\n",
    "profits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128.500</td>\n",
       "      <td>135.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130.250</td>\n",
       "      <td>133.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131.125</td>\n",
       "      <td>132.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1\n",
       "0  128.500  135.500\n",
       "1  130.250  133.750\n",
       "2  131.125  132.875"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prices.plot()\n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pricelearning = pd.DataFrame(game.prices.mean(axis = 0))\n",
    "# pricelearning.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_1 = learning.to_numpy()\n",
    "learning_2 = [0]*len(learning)\n",
    "for i in range(len(learning_1)):\n",
    "    learning_2[i] = learning_1[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_learning = np.convolve(learning_2, np.ones(1000)/1000, mode = 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_learning)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2411abce730>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(learning['entry'],loss['entry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
