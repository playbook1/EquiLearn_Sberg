{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learningBase import ReinforceAlgorithm\n",
    "from environmentModelBase import Model, AdversaryModes\n",
    "from neuralNetworkSimple import NNBase\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperParams=[0.00001, 1, 0]\n",
    "codeParams=[1, 10000, 1, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy reset\n",
      "----------------------------------------\n",
      "iter  0  stage  24  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5663, 0.5663, 0.5663, 0.5663, 0.5663, 0.5663, 0.5663, 0.5663, 0.5663,\n",
      "        0.5663, 0.5663, 0.5663, 0.5663, 0.5663, 0.5663, 0.5663, 0.5663, 0.5663,\n",
      "        0.5663, 0.5663, 0.5663, 0.5663, 0.5663, 0.5663, 0.5663]) return=  139847.7232627179\n",
      "probs of actions:  tensor([0.8786, 0.8836, 0.8852, 0.9041, 0.9160, 0.8963, 0.8981, 0.8989, 0.9127,\n",
      "        0.9010, 0.9015, 0.9021, 0.0518, 0.8964, 0.8851, 0.9235, 0.9054, 0.0415,\n",
      "        0.9164, 0.9023, 0.9071, 0.9030, 0.0111, 0.9274, 0.9877],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5366, 0.5495, 0.5560, 0.5592, 0.5609, 0.5617, 0.5621, 0.5623,\n",
      "        0.5624, 0.5624, 0.5625, 0.5624, 0.5662, 0.5644, 0.5634, 0.5630, 0.5626,\n",
      "        0.5664, 0.5644, 0.5635, 0.5630, 0.5623, 0.5701, 0.5663])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  0  stage  23  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([ 0,  0,  4,  8, 14,  0,  0,  1,  0, 22, 20,  0,  5,  5,  0,  0, 16,  0,\n",
      "         0,  0,  1,  0,  0, 22,  0])\n",
      "loss=  tensor(0.0097, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.1279, 1.1279, 1.1279, 1.1279, 1.1279, 1.1279, 1.1279, 1.1279, 1.1279,\n",
      "        1.1279, 1.1279, 1.1279, 1.1279, 1.1279, 1.1279, 1.1279, 1.1279, 1.1279,\n",
      "        1.1279, 1.1279, 1.1279, 1.1279, 1.1279, 1.1279, 0.5635]) return=  145876.19064216543\n",
      "probs of actions:  tensor([0.4697, 0.5450, 0.0549, 0.0497, 0.0049, 0.4636, 0.4981, 0.0854, 0.5431,\n",
      "        0.0618, 0.0067, 0.5533, 0.0268, 0.0303, 0.4079, 0.6228, 0.0022, 0.5265,\n",
      "        0.5884, 0.4364, 0.0926, 0.5506, 0.5699, 0.7906, 0.9886],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5366, 0.5479, 0.5646, 0.5776, 0.6343, 0.5978, 0.5799, 0.5750,\n",
      "        0.5203, 0.6114, 0.6865, 0.6204, 0.6092, 0.6062, 0.5841, 0.5477, 0.6298,\n",
      "        0.5957, 0.5790, 0.5706, 0.5704, 0.5664, 0.5161, 0.6491])\n",
      "finalReturns:  tensor([0.0372, 0.0856])\n",
      "----------------------------------------\n",
      "iter  0  stage  22  ep  78431   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22,  0, 22, 22,  0])\n",
      "loss=  tensor(0.0009, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.8150, 1.8150, 1.8150, 1.8150, 1.8150, 1.8150, 1.8150, 1.8150, 1.8150,\n",
      "        1.8150, 1.8150, 1.8150, 1.8150, 1.8150, 1.8150, 1.8150, 1.8150, 1.8150,\n",
      "        1.8150, 1.8150, 1.8150, 1.8150, 1.8150, 1.1670, 0.5625]) return=  167444.53665689877\n",
      "probs of actions:  tensor([0.9807, 0.9663, 0.9871, 0.9885, 0.9861, 0.9875, 0.9866, 0.9924, 0.9865,\n",
      "        0.9892, 0.9886, 0.9758, 0.9819, 0.9908, 0.9891, 0.9866, 0.9716, 0.9830,\n",
      "        0.9787, 0.9912, 0.9762, 0.0128, 0.9990, 0.9996, 0.9954],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.7396, 0.5996, 0.6447, 0.7161])\n",
      "finalReturns:  tensor([0.1454, 0.1938, 0.1536])\n",
      "----------------------------------------\n",
      "iter  0  stage  21  ep  21111   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22,  0, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0010, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.3420, 2.3420, 2.3420, 2.3420, 2.3420, 2.3420, 2.3420, 2.3420, 2.3420,\n",
      "        2.3420, 2.3420, 2.3420, 2.3420, 2.3420, 2.3420, 2.3420, 2.3420, 2.3420,\n",
      "        2.3420, 2.3420, 2.3420, 2.3420, 1.6940, 1.0895, 0.5270]) return=  167326.75936424857\n",
      "probs of actions:  tensor([0.9970, 0.9941, 0.9981, 0.9980, 0.9979, 0.9981, 0.9981, 0.9989, 0.9977,\n",
      "        0.9984, 0.9982, 0.9962, 0.9969, 0.9986, 0.9983, 0.9977, 0.9953, 0.9971,\n",
      "        0.9968, 0.9987, 0.0032, 0.9990, 0.9999, 1.0000, 0.9967],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.7396, 0.5996, 0.6447, 0.6677, 0.7278])\n",
      "finalReturns:  tensor([0.2978, 0.3462, 0.3061, 0.2008])\n",
      "----------------------------------------\n",
      "iter  0  stage  20  ep  788   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22,  0, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0018, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.8407, 2.8407, 2.8407, 2.8407, 2.8407, 2.8407, 2.8407, 2.8407, 2.8407,\n",
      "        2.8407, 2.8407, 2.8407, 2.8407, 2.8407, 2.8407, 2.8407, 2.8407, 2.8407,\n",
      "        2.8407, 2.8407, 2.8407, 2.1927, 1.5882, 1.0257, 0.4987]) return=  167267.7526299165\n",
      "probs of actions:  tensor([9.9804e-01, 9.9601e-01, 9.9866e-01, 9.9866e-01, 9.9858e-01, 9.9872e-01,\n",
      "        9.9872e-01, 9.9925e-01, 9.9842e-01, 9.9895e-01, 9.9880e-01, 9.9754e-01,\n",
      "        9.9786e-01, 9.9907e-01, 9.9890e-01, 9.9846e-01, 9.9682e-01, 9.9812e-01,\n",
      "        9.9790e-01, 5.2534e-04, 9.9901e-01, 9.9924e-01, 9.9993e-01, 9.9998e-01,\n",
      "        9.9636e-01], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.7396, 0.5996, 0.6447, 0.6677, 0.6794, 0.7337])\n",
      "finalReturns:  tensor([0.4844, 0.5328, 0.4927, 0.3875, 0.2350])\n",
      "----------------------------------------\n",
      "iter  0  stage  19  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0029, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.4906, 3.4906, 3.4906, 3.4906, 3.4906, 3.4906, 3.4906, 3.4906, 3.4906,\n",
      "        3.4906, 3.4906, 3.4906, 3.4906, 3.4906, 3.4906, 3.4906, 3.4906, 3.4906,\n",
      "        3.4906, 3.4906, 2.7510, 2.1030, 1.5197, 0.9827, 0.4792]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9980, 0.9960, 0.9987, 0.9987, 0.9986, 0.9987, 0.9987, 0.9992, 0.9984,\n",
      "        0.9989, 0.9988, 0.9975, 0.9979, 0.9991, 0.9989, 0.9985, 0.9968, 0.9981,\n",
      "        0.9979, 0.9991, 0.9989, 0.9993, 0.9999, 1.0000, 0.9964],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([0.7050, 0.7534, 0.7102, 0.6023, 0.4481, 0.2604])\n",
      "----------------------------------------\n",
      "iter  0  stage  18  ep  349   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,  0,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0040, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.7776, 3.7776, 3.7776, 3.7776, 3.7776, 3.7776, 3.7776, 3.7776, 3.7776,\n",
      "        3.7776, 3.7776, 3.7776, 3.7776, 3.7776, 3.7776, 3.7776, 3.7776, 3.7776,\n",
      "        3.7776, 3.1295, 2.5250, 1.9625, 1.4355, 0.9369, 0.4601]) return=  167223.4464147788\n",
      "probs of actions:  tensor([0.9981, 0.9962, 0.9988, 0.9987, 0.9987, 0.9988, 0.9988, 0.9993, 0.9985,\n",
      "        0.9990, 0.9989, 0.9976, 0.9980, 0.9991, 0.9990, 0.9985, 0.9970, 0.0014,\n",
      "        0.9991, 0.9995, 0.9988, 0.9994, 0.9999, 1.0000, 0.9965],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396,\n",
      "        0.5996, 0.6447, 0.6677, 0.6794, 0.6853, 0.6882, 0.7381])\n",
      "finalReturns:  tensor([0.9256, 0.9740, 0.9338, 0.8286, 0.6761, 0.4895, 0.2780])\n",
      "----------------------------------------\n",
      "iter  0  stage  17  ep  219   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0051, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.3999, 4.3999, 4.3999, 4.3999, 4.3999, 4.3999, 4.3999, 4.3999, 4.3999,\n",
      "        4.3999, 4.3999, 4.3999, 4.3999, 4.3999, 4.3999, 4.3999, 4.3999, 4.3999,\n",
      "        3.6604, 3.0123, 2.4290, 1.8920, 1.3885, 0.9093, 0.4481]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9983, 0.9966, 0.9989, 0.9989, 0.9988, 0.9989, 0.9989, 0.9994, 0.9986,\n",
      "        0.9991, 0.9990, 0.9979, 0.9982, 0.9992, 0.9991, 0.9987, 0.9973, 0.9990,\n",
      "        0.9992, 0.9996, 0.9990, 0.9995, 1.0000, 1.0000, 0.9965],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([1.1780, 1.2264, 1.1833, 1.0754, 0.9212, 0.7335, 0.5215, 0.2915])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  0  stage  16  ep  5672   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,  0, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0037, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.6630, 4.6630, 4.6630, 4.6630, 4.6630, 4.6630, 4.6630, 4.6630, 4.6630,\n",
      "        4.6630, 4.6630, 4.6630, 4.6630, 4.6630, 4.6630, 4.6630, 4.6630, 4.0150,\n",
      "        3.4105, 2.8480, 2.3210, 1.8223, 1.3456, 0.8855, 0.4380]) return=  167212.36521898463\n",
      "probs of actions:  tensor([9.9920e-01, 9.9831e-01, 9.9951e-01, 9.9950e-01, 9.9944e-01, 9.9955e-01,\n",
      "        9.9955e-01, 9.9975e-01, 9.9934e-01, 9.9959e-01, 9.9954e-01, 9.9893e-01,\n",
      "        9.9910e-01, 9.9968e-01, 9.9960e-01, 5.5382e-04, 9.9908e-01, 9.9982e-01,\n",
      "        9.9983e-01, 9.9998e-01, 9.9956e-01, 9.9994e-01, 9.9999e-01, 1.0000e+00,\n",
      "        9.9578e-01], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.7396, 0.5996, 0.6447,\n",
      "        0.6677, 0.6794, 0.6853, 0.6882, 0.6897, 0.6905, 0.7392])\n",
      "finalReturns:  tensor([1.4214, 1.4698, 1.4296, 1.3244, 1.1720, 0.9853, 0.7739, 0.5442, 0.3012])\n",
      "----------------------------------------\n",
      "iter  0  stage  15  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0053, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.2692, 5.2692, 5.2692, 5.2692, 5.2692, 5.2692, 5.2692, 5.2692, 5.2692,\n",
      "        5.2692, 5.2692, 5.2692, 5.2692, 5.2692, 5.2692, 5.2692, 4.5296, 3.8816,\n",
      "        3.2983, 2.7613, 2.2577, 1.7786, 1.3173, 0.8693, 0.4310]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9994, 0.9996, 0.9996, 0.9998, 0.9993,\n",
      "        0.9996, 0.9995, 0.9989, 0.9991, 0.9997, 0.9996, 0.9994, 0.9990, 0.9998,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9958],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([1.6912, 1.7396, 1.6964, 1.5885, 1.4343, 1.2467, 1.0346, 0.8047, 0.5615,\n",
      "        0.3086])\n",
      "----------------------------------------\n",
      "iter  0  stage  14  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0066, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.6948, 5.6948, 5.6948, 5.6948, 5.6948, 5.6948, 5.6948, 5.6948, 5.6948,\n",
      "        5.6948, 5.6948, 5.6948, 5.6948, 5.6948, 5.6948, 4.9552, 4.3072, 3.7239,\n",
      "        3.1869, 2.6834, 2.2042, 1.7430, 1.2949, 0.8566, 0.4256]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9994, 0.9996, 0.9996, 0.9998, 0.9993,\n",
      "        0.9996, 0.9995, 0.9989, 0.9991, 0.9997, 0.9996, 0.9994, 0.9990, 0.9998,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9958],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([1.9567, 2.0051, 1.9620, 1.8541, 1.6999, 1.5122, 1.3002, 1.0702, 0.8271,\n",
      "        0.5742, 0.3140])\n",
      "----------------------------------------\n",
      "iter  0  stage  13  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0078, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.1164, 6.1164, 6.1164, 6.1164, 6.1164, 6.1164, 6.1164, 6.1164, 6.1164,\n",
      "        6.1164, 6.1164, 6.1164, 6.1164, 6.1164, 5.3768, 4.7288, 4.1455, 3.6085,\n",
      "        3.1049, 2.6258, 2.1645, 1.7165, 1.2782, 0.8472, 0.4216]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9994, 0.9996, 0.9996, 0.9998, 0.9993,\n",
      "        0.9996, 0.9995, 0.9989, 0.9991, 0.9997, 0.9996, 0.9994, 0.9990, 0.9998,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9958],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([2.2264, 2.2748, 2.2316, 2.1237, 1.9695, 1.7819, 1.5698, 1.3399, 1.0967,\n",
      "        0.8438, 0.5836, 0.3180])\n",
      "----------------------------------------\n",
      "iter  0  stage  12  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22,  0, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0105, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.5345, 6.5345, 6.5345, 6.5345, 6.5345, 6.5345, 6.5345, 6.5345, 6.5345,\n",
      "        6.5345, 6.5345, 6.5345, 6.5345, 5.7952, 5.1473, 4.5640, 4.0270, 3.5235,\n",
      "        3.0444, 2.5831, 2.1351, 1.6968, 1.2658, 0.8401, 0.4186]) return=  167235.25059974194\n",
      "probs of actions:  tensor([9.9920e-01, 9.9831e-01, 3.7954e-04, 9.9957e-01, 9.9935e-01, 9.9952e-01,\n",
      "        9.9954e-01, 9.9975e-01, 9.9933e-01, 9.9959e-01, 9.9954e-01, 9.9893e-01,\n",
      "        9.9910e-01, 9.9968e-01, 9.9960e-01, 9.9939e-01, 9.9896e-01, 9.9985e-01,\n",
      "        9.9984e-01, 9.9998e-01, 9.9957e-01, 9.9994e-01, 9.9999e-01, 1.0000e+00,\n",
      "        9.9578e-01], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6786, 0.5708, 0.6296, 0.6601, 0.6756, 0.6834, 0.6873,\n",
      "        0.6892, 0.6902, 0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([2.4990, 2.5474, 2.5042, 2.3963, 2.2421, 2.0545, 1.8424, 1.6125, 1.3693,\n",
      "        1.1164, 0.8562, 0.5907, 0.3210])\n",
      "----------------------------------------\n",
      "iter  0  stage  11  ep  13   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0135, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.9511, 6.9511, 6.9511, 6.9511, 6.9511, 6.9511, 6.9511, 6.9511, 6.9511,\n",
      "        6.9511, 6.9511, 6.9511, 6.2116, 5.5636, 4.9804, 4.4434, 3.9398, 3.4607,\n",
      "        2.9994, 2.5514, 2.1131, 1.6821, 1.2565, 0.8349, 0.4163]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9994, 0.9996, 0.9996, 0.9998, 0.9993,\n",
      "        0.9996, 0.9996, 0.9990, 0.9992, 0.9997, 0.9996, 0.9994, 0.9990, 0.9998,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9957],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([2.7739, 2.8223, 2.7791, 2.6712, 2.5170, 2.3294, 2.1173, 1.8874, 1.6442,\n",
      "        1.3913, 1.1311, 0.8655, 0.5959, 0.3233])\n",
      "----------------------------------------\n",
      "iter  0  stage  10  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0155, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.3655, 7.3655, 7.3655, 7.3655, 7.3655, 7.3655, 7.3655, 7.3655, 7.3655,\n",
      "        7.3655, 7.3655, 6.6261, 5.9782, 5.3950, 4.8580, 4.3544, 3.8753, 3.4141,\n",
      "        2.9660, 2.5277, 2.0967, 1.6711, 1.2495, 0.8310, 0.4146]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9994, 0.9996, 0.9996, 0.9998, 0.9993,\n",
      "        0.9996, 0.9996, 0.9990, 0.9992, 0.9997, 0.9996, 0.9994, 0.9990, 0.9998,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9957],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([3.0504, 3.0988, 3.0557, 2.9478, 2.7936, 2.6059, 2.3939, 2.1639, 1.9208,\n",
      "        1.6679, 1.4077, 1.1421, 0.8725, 0.5998, 0.3250])\n",
      "----------------------------------------\n",
      "iter  0  stage  9  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0175, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.7784, 7.7784, 7.7784, 7.7784, 7.7784, 7.7784, 7.7784, 7.7784, 7.7784,\n",
      "        7.7784, 7.0393, 6.3915, 5.8083, 5.2713, 4.7678, 4.2887, 3.8274, 3.3794,\n",
      "        2.9411, 2.5101, 2.0845, 1.6629, 1.2443, 0.8280, 0.4134]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9994, 0.9996, 0.9996, 0.9998, 0.9993,\n",
      "        0.9996, 0.9996, 0.9990, 0.9992, 0.9997, 0.9996, 0.9994, 0.9990, 0.9998,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9957],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([3.3282, 3.3766, 3.3335, 3.2256, 3.0714, 2.8838, 2.6717, 2.4418, 2.1986,\n",
      "        1.9457, 1.6855, 1.4199, 1.1503, 0.8777, 0.6028, 0.3262])\n",
      "----------------------------------------\n",
      "iter  0  stage  8  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0204, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.1899, 8.1899, 8.1899, 8.1899, 8.1899, 8.1899, 8.1899, 8.1899, 8.1899,\n",
      "        7.4513, 6.8037, 6.2206, 5.6837, 5.1802, 4.7011, 4.2398, 3.7918, 3.3535,\n",
      "        2.9225, 2.4969, 2.0753, 1.6567, 1.2404, 0.8258, 0.4124]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9994, 0.9996, 0.9996, 0.9998, 0.9993,\n",
      "        0.9996, 0.9996, 0.9990, 0.9992, 0.9997, 0.9996, 0.9994, 0.9990, 0.9998,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9957],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([3.6069, 3.6553, 3.6122, 3.5043, 3.3502, 3.1625, 2.9505, 2.7205, 2.4774,\n",
      "        2.2245, 1.9643, 1.6987, 1.4291, 1.1565, 0.8816, 0.6050, 0.3272])\n",
      "----------------------------------------\n",
      "iter  0  stage  7  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0220, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.5998, 8.5998, 8.5998, 8.5998, 8.5998, 8.5998, 8.5998, 8.5998, 7.8621,\n",
      "        7.2150, 6.6321, 6.0953, 5.5919, 5.1128, 4.6515, 4.2035, 3.7652, 3.3342,\n",
      "        2.9086, 2.4870, 2.0685, 1.6522, 1.2375, 0.8241, 0.4117]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9994, 0.9996, 0.9996, 0.9998, 0.9994,\n",
      "        0.9996, 0.9996, 0.9990, 0.9992, 0.9997, 0.9996, 0.9995, 0.9990, 0.9998,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9956],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([3.8863, 3.9347, 3.8916, 3.7838, 3.6296, 3.4420, 3.2300, 3.0000, 2.7569,\n",
      "        2.5040, 2.2438, 1.9782, 1.7086, 1.4359, 1.1610, 0.8845, 0.6067, 0.3279])\n",
      "----------------------------------------\n",
      "iter  0  stage  6  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0243, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.0073, 9.0073, 9.0073, 9.0073, 9.0073, 9.0073, 9.0073, 8.2716, 7.6254,\n",
      "        7.0429, 6.5063, 6.0030, 5.5239, 5.0627, 4.6147, 4.1764, 3.7454, 3.3198,\n",
      "        2.8982, 2.4797, 2.0633, 1.6487, 1.2353, 0.8229, 0.4112]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9994, 0.9996, 0.9996, 0.9998, 0.9994,\n",
      "        0.9996, 0.9996, 0.9990, 0.9992, 0.9997, 0.9996, 0.9995, 0.9990, 0.9998,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9956],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([4.1661, 4.2145, 4.1715, 4.0637, 3.9096, 3.7220, 3.5100, 3.2800, 3.0369,\n",
      "        2.7840, 2.5238, 2.2582, 1.9886, 1.7159, 1.4411, 1.1645, 0.8867, 0.6079,\n",
      "        0.3284])\n",
      "----------------------------------------\n",
      "iter  0  stage  5  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0267, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.4107, 9.4107, 9.4107, 9.4107, 9.4107, 9.4107, 8.6789, 8.0345, 7.4529,\n",
      "        6.9167, 6.4136, 5.9346, 5.4735, 5.0255, 4.5872, 4.1562, 3.7306, 3.3090,\n",
      "        2.8904, 2.4741, 2.0595, 1.6461, 1.2337, 0.8220, 0.4108]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9994, 0.9996, 0.9996, 0.9998, 0.9994,\n",
      "        0.9996, 0.9996, 0.9990, 0.9992, 0.9997, 0.9996, 0.9995, 0.9990, 0.9998,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9956],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([4.4461, 4.4945, 4.4516, 4.3440, 4.1899, 4.0023, 3.7903, 3.5604, 3.3173,\n",
      "        3.0644, 2.8042, 2.5386, 2.2690, 1.9964, 1.7215, 1.4449, 1.1671, 0.8883,\n",
      "        0.6088, 0.3288])\n",
      "----------------------------------------\n",
      "iter  0  stage  4  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0297, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.8065, 9.8065, 9.8065, 9.8065, 9.8065, 9.0824, 8.4416, 7.8618, 7.3264,\n",
      "        6.8237, 6.3449, 5.8839, 5.4359, 4.9977, 4.5667, 4.1411, 3.7195, 3.3009,\n",
      "        2.8846, 2.4700, 2.0566, 1.6442, 1.2325, 0.8213, 0.4105]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9994, 0.9996, 0.9996, 0.9998, 0.9994,\n",
      "        0.9996, 0.9996, 0.9990, 0.9992, 0.9997, 0.9996, 0.9995, 0.9990, 0.9999,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9956],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([4.7261, 4.7745, 4.7318, 4.6243, 4.4704, 4.2829, 4.0710, 3.8411, 3.5980,\n",
      "        3.3451, 3.0849, 2.8193, 2.5497, 2.2771, 2.0022, 1.7256, 1.4478, 1.1690,\n",
      "        0.8895, 0.6095, 0.3291])\n",
      "----------------------------------------\n",
      "iter  0  stage  3  ep  0   adversary:  AdversaryModes.myopic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0326, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.1875, 10.1875, 10.1875, 10.1875,  9.4787,  8.8452,  8.2688,  7.7351,\n",
      "         7.2332,  6.7548,  6.2939,  5.8461,  5.4079,  4.9769,  4.5513,  4.1298,\n",
      "         3.7112,  3.2949,  2.8803,  2.4669,  2.0545,  1.6427,  1.2316,  0.8208,\n",
      "         0.4103]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9995, 0.9996, 0.9996, 0.9998, 0.9994,\n",
      "        0.9996, 0.9996, 0.9991, 0.9992, 0.9997, 0.9996, 0.9995, 0.9990, 0.9999,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9956],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([5.0054, 5.0538, 5.0116, 4.9046, 4.7510, 4.5637, 4.3518, 4.1220, 3.8789,\n",
      "        3.6260, 3.3658, 3.1002, 2.8306, 2.5580, 2.2831, 2.0065, 1.7287, 1.4499,\n",
      "        1.1705, 0.8904, 0.6100, 0.3293])\n",
      "----------------------------------------\n",
      "iter  0  stage  2  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0355, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.5398, 10.5398, 10.5398,  9.8612,  9.2420,  8.6725,  8.1421,  7.6417,\n",
      "         7.1642,  6.7037,  6.2560,  5.8179,  5.3870,  4.9614,  4.5399,  4.1213,\n",
      "         3.7050,  3.2904,  2.8770,  2.4646,  2.0528,  1.6417,  1.2309,  0.8204,\n",
      "         0.4101]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9984, 0.9995, 0.9995, 0.9995, 0.9996, 0.9996, 0.9998, 0.9994,\n",
      "        0.9996, 0.9996, 0.9991, 0.9992, 0.9997, 0.9996, 0.9995, 0.9990, 0.9999,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9956],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([5.2833, 5.3317, 5.2905, 5.1844, 5.0313, 4.8444, 4.6327, 4.4030, 4.1599,\n",
      "        3.9071, 3.6469, 3.3813, 3.1117, 2.8391, 2.5642, 2.2876, 2.0098, 1.7310,\n",
      "        1.4516, 1.1715, 0.8911, 0.6104, 0.3295])\n",
      "----------------------------------------\n",
      "iter  0  stage  1  ep  83   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0292, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.8367, 10.8367, 10.2165,  9.6255,  9.0696,  8.5458,  8.0486,  7.5726,\n",
      "         7.1129,  6.6656,  6.2277,  5.7969,  5.3713,  4.9498,  4.5313,  4.1150,\n",
      "         3.7003,  3.2870,  2.8745,  2.4628,  2.0516,  1.6408,  1.2303,  0.8201,\n",
      "         0.4100]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9994, 0.9990, 0.9997, 0.9997, 0.9997, 0.9997, 0.9997, 0.9999, 0.9996,\n",
      "        0.9998, 0.9997, 0.9994, 0.9995, 0.9998, 0.9998, 0.9997, 0.9992, 0.9999,\n",
      "        0.9999, 1.0000, 0.9997, 1.0000, 1.0000, 1.0000, 0.9951],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([5.5581, 5.6065, 5.5674, 5.4630, 5.3111, 5.1248, 4.9135, 4.6840, 4.4410,\n",
      "        4.1882, 3.9281, 3.6625, 3.3929, 3.1203, 2.8454, 2.5689, 2.2910, 2.0123,\n",
      "        1.7328, 1.4528, 1.1724, 0.8917, 0.6107, 0.3296])\n",
      "----------------------------------------\n",
      "iter  0  stage  0  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0333, grad_fn=<NegBackward0>)   ,  base rewards= tensor([11.0309, 10.5197,  9.9832,  9.4539,  8.9431,  8.4524,  7.9795,  7.5213,\n",
      "         7.0747,  6.6372,  6.2066,  5.7811,  5.3597,  4.9411,  4.5248,  4.1102,\n",
      "         3.6968,  3.2844,  2.8727,  2.4615,  2.0507,  1.6402,  1.2300,  0.8199,\n",
      "         0.4099]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9994, 0.9990, 0.9997, 0.9997, 0.9997, 0.9997, 0.9997, 0.9999, 0.9996,\n",
      "        0.9998, 0.9997, 0.9994, 0.9995, 0.9998, 0.9998, 0.9997, 0.9992, 0.9999,\n",
      "        0.9999, 1.0000, 0.9997, 1.0000, 1.0000, 1.0000, 0.9951],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([5.8267, 5.8751, 5.8399, 5.7390, 5.5894, 5.4045, 5.1940, 4.9648, 4.7221,\n",
      "        4.4694, 4.2093, 3.9438, 3.6742, 3.4016, 3.1267, 2.8502, 2.5723, 2.2936,\n",
      "        2.0141, 1.7341, 1.4537, 1.1730, 0.8920, 0.6109, 0.3297])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1682348121 saved\n",
      "[346689, 'tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])', 168576.33348198733, 65680.3332469066, 0.03331249579787254, 1e-05, 1, 0, 'tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\\n        22, 22, 22, 22, 22, 22,  0])', '[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\\n 1.]', '0,[1e-05,1][1, 10000, 1, 1],1682348121', 25, 50, 168576.3334819873, 205322.17620252006, 83998.798421993, 134475.68800000002, 131523.21866666665, 89197.96159612676, 89197.96159612676, 106340.34718300312, 106308.66365021843, 97483.02777699233, 89197.96159612676, 106308.66365021843]\n",
      "policy reset\n",
      "----------------------------------------\n",
      "iter  1  stage  24  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625,\n",
      "        0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625,\n",
      "        0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625]) return=  139739.47699942059\n",
      "probs of actions:  tensor([0.8929, 0.8857, 0.9020, 0.8962, 0.8926, 0.8984, 0.0646, 0.9006, 0.8831,\n",
      "        0.9026, 0.8910, 0.0521, 0.8824, 0.8858, 0.8784, 0.9028, 0.8894, 0.8959,\n",
      "        0.8758, 0.9148, 0.8829, 0.8869, 0.8875, 0.8934, 0.9822],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5366, 0.5495, 0.5560, 0.5592, 0.5609, 0.5616, 0.5658, 0.5642,\n",
      "        0.5633, 0.5629, 0.5626, 0.5664, 0.5644, 0.5635, 0.5630, 0.5627, 0.5626,\n",
      "        0.5626, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  1  stage  23  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([ 1,  0, 11,  0,  0,  0, 16, 19,  0,  0,  0,  4, 14,  8,  0, 12, 11,  0,\n",
      "        10,  4,  0,  0,  0, 14,  0])\n",
      "loss=  tensor(0.0716, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.1330, 1.1330, 1.1330, 1.1330, 1.1330, 1.1330, 1.1330, 1.1330, 1.1330,\n",
      "        1.1330, 1.1330, 1.1330, 1.1330, 1.1330, 1.1330, 1.1330, 1.1330, 1.1330,\n",
      "        1.1330, 1.1330, 1.1330, 1.1330, 1.1330, 1.1330, 0.5652]) return=  146956.28276435166\n",
      "probs of actions:  tensor([0.0785, 0.5092, 0.0653, 0.4961, 0.4847, 0.3295, 0.0636, 0.0042, 0.4223,\n",
      "        0.5699, 0.5365, 0.0700, 0.0100, 0.0034, 0.3717, 0.0786, 0.0824, 0.3663,\n",
      "        0.0485, 0.0709, 0.4142, 0.4306, 0.3190, 0.1280, 0.9781],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5111, 0.5402, 0.5392, 0.5987, 0.5805, 0.5714, 0.5414, 0.5903, 0.6695,\n",
      "        0.6148, 0.5884, 0.5738, 0.5645, 0.6211, 0.6258, 0.5793, 0.6124, 0.6362,\n",
      "        0.5888, 0.6176, 0.6060, 0.5840, 0.5732, 0.5482, 0.6190])\n",
      "finalReturns:  tensor([0.0342, 0.0538])\n",
      "----------------------------------------\n",
      "iter  1  stage  22  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([16, 12, 28,  1,  0, 23, 12, 16, 26, 22,  0, 27, 16, 27, 12, 19, 19, 26,\n",
      "         1, 27, 22, 30, 27, 20,  0])\n",
      "loss=  tensor(0.9044, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.0149, 2.0149, 2.0149, 2.0149, 2.0149, 2.0149, 2.0149, 2.0149, 2.0149,\n",
      "        2.0149, 2.0149, 2.0149, 2.0149, 2.0149, 2.0149, 2.0149, 2.0149, 2.0149,\n",
      "        2.0149, 2.0149, 2.0149, 2.0149, 2.0149, 1.2464, 0.5849]) return=  162124.7477856951\n",
      "probs of actions:  tensor([0.0491, 0.1345, 0.0154, 0.1258, 0.1692, 0.0112, 0.1468, 0.0460, 0.0490,\n",
      "        0.0374, 0.2083, 0.0527, 0.0574, 0.0611, 0.2087, 0.0473, 0.0565, 0.0554,\n",
      "        0.0369, 0.0536, 0.0429, 0.0126, 0.1198, 0.1008, 0.9626],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4856, 0.5824, 0.5477, 0.7066, 0.6365, 0.5460, 0.6571, 0.6382, 0.6087,\n",
      "        0.6761, 0.7320, 0.5716, 0.6866, 0.6276, 0.7269, 0.6619, 0.6699, 0.6424,\n",
      "        0.7418, 0.5802, 0.6683, 0.6381, 0.6956, 0.7358, 0.7489])\n",
      "finalReturns:  tensor([0.1655, 0.2384, 0.1640])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  21  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 18, 27, 27, 26, 27, 29, 26, 27, 23, 27, 27, 19, 19, 20, 16, 25, 19,\n",
      "        26, 23, 30, 27, 28, 29,  0])\n",
      "loss=  tensor(3.3616, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.5627, 2.5627, 2.5627, 2.5627, 2.5627, 2.5627, 2.5627, 2.5627, 2.5627,\n",
      "        2.5627, 2.5627, 2.5627, 2.5627, 2.5627, 2.5627, 2.5627, 2.5627, 2.5627,\n",
      "        2.5627, 2.5627, 2.5627, 2.5627, 1.7836, 1.1172, 0.5300]) return=  170297.27457943128\n",
      "probs of actions:  tensor([0.0463, 0.0392, 0.3955, 0.3738, 0.1785, 0.4352, 0.0531, 0.1506, 0.3757,\n",
      "        0.0076, 0.4171, 0.4271, 0.0406, 0.0422, 0.0334, 0.0191, 0.0565, 0.0451,\n",
      "        0.2052, 0.0074, 0.0165, 0.5405, 0.0105, 0.0368, 0.9833],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.6156, 0.6036, 0.6560, 0.6882, 0.6922, 0.6900, 0.7199, 0.7080,\n",
      "        0.7292, 0.6922, 0.7012, 0.7426, 0.7099, 0.6899, 0.7006, 0.6449, 0.7001,\n",
      "        0.6575, 0.6966, 0.6589, 0.7062, 0.7028, 0.7025, 0.7938])\n",
      "finalReturns:  tensor([0.3426, 0.4155, 0.3792, 0.2638])\n",
      "----------------------------------------\n",
      "iter  1  stage  20  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 27, 27, 25, 27, 27, 27,  0, 26, 27, 26, 27, 27, 27, 27, 27, 26, 25,\n",
      "        26, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.5544, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.0524, 3.0524, 3.0524, 3.0524, 3.0524, 3.0524, 3.0524, 3.0524, 3.0524,\n",
      "        3.0524, 3.0524, 3.0524, 3.0524, 3.0524, 3.0524, 3.0524, 3.0524, 3.0524,\n",
      "        3.0524, 3.0524, 3.0524, 2.2741, 1.6082, 1.0211, 0.4904]) return=  170770.7691723508\n",
      "probs of actions:  tensor([0.1380, 0.6337, 0.6868, 0.0381, 0.6601, 0.7162, 0.7106, 0.0021, 0.1416,\n",
      "        0.6234, 0.1196, 0.6613, 0.6336, 0.6924, 0.6360, 0.6668, 0.1431, 0.0319,\n",
      "        0.1411, 0.7074, 0.7916, 0.8418, 0.8417, 0.6480, 0.9773],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5631, 0.6348, 0.6825, 0.6824, 0.6963, 0.7033, 0.7797, 0.5991,\n",
      "        0.6466, 0.6834, 0.6898, 0.7000, 0.7052, 0.7077, 0.7090, 0.7150, 0.7160,\n",
      "        0.7044, 0.7003, 0.7053, 0.7078, 0.7091, 0.7097, 0.7829])\n",
      "finalReturns:  tensor([0.5625, 0.6354, 0.5935, 0.4715, 0.2926])\n",
      "----------------------------------------\n",
      "iter  1  stage  19  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([27, 27, 27, 29, 27, 27, 27, 27, 20, 27, 26, 27, 27, 27, 22, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.6967, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.5200, 3.5200, 3.5200, 3.5200, 3.5200, 3.5200, 3.5200, 3.5200, 3.5200,\n",
      "        3.5200, 3.5200, 3.5200, 3.5200, 3.5200, 3.5200, 3.5200, 3.5200, 3.5200,\n",
      "        3.5200, 3.5200, 2.7382, 2.0706, 1.4828, 0.9516, 0.4611]) return=  172225.21784845914\n",
      "probs of actions:  tensor([0.7112, 0.6666, 0.7444, 0.0828, 0.7037, 0.7520, 0.7450, 0.6938, 0.0091,\n",
      "        0.6783, 0.1017, 0.6977, 0.6331, 0.7429, 0.0189, 0.6977, 0.6746, 0.7301,\n",
      "        0.7153, 0.8062, 0.8368, 0.8887, 0.8815, 0.7302, 0.9894],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5671, 0.6369, 0.6620, 0.7004, 0.7054, 0.7078, 0.7091, 0.7426,\n",
      "        0.6794, 0.7001, 0.6981, 0.7042, 0.7073, 0.7333, 0.6876, 0.6989, 0.7046,\n",
      "        0.7075, 0.7089, 0.7096, 0.7100, 0.7101, 0.7102, 0.7832])\n",
      "finalReturns:  tensor([0.8120, 0.8849, 0.8429, 0.7208, 0.5418, 0.3221])\n",
      "----------------------------------------\n",
      "iter  1  stage  18  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([27, 27, 27, 29, 27, 26, 26, 27, 29, 29, 27, 27, 27, 27, 29, 27, 27, 27,\n",
      "        27, 27, 27, 25, 27, 19,  0])\n",
      "loss=  tensor(9.2171, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.9682, 3.9682, 3.9682, 3.9682, 3.9682, 3.9682, 3.9682, 3.9682, 3.9682,\n",
      "        3.9682, 3.9682, 3.9682, 3.9682, 3.9682, 3.9682, 3.9682, 3.9682, 3.9682,\n",
      "        3.9682, 3.1839, 2.5150, 1.9267, 1.3953, 0.9046, 0.4417]) return=  172834.79357956513\n",
      "probs of actions:  tensor([0.7195, 0.6850, 0.7542, 0.0809, 0.7215, 0.1352, 0.1281, 0.7134, 0.0822,\n",
      "        0.0737, 0.7545, 0.7188, 0.6647, 0.7534, 0.0863, 0.7126, 0.6873, 0.7424,\n",
      "        0.7456, 0.8016, 0.8489, 0.0049, 0.8710, 0.0048, 0.9992],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5671, 0.6369, 0.6620, 0.7004, 0.7107, 0.7087, 0.7025, 0.6952,\n",
      "        0.7060, 0.7227, 0.7165, 0.7134, 0.7119, 0.6999, 0.7196, 0.7149, 0.7126,\n",
      "        0.7115, 0.7109, 0.7106, 0.7209, 0.7016, 0.7427, 0.7461])\n",
      "finalReturns:  tensor([1.0760, 1.1489, 1.1069, 0.9846, 0.7951, 0.5842, 0.3044])\n",
      "----------------------------------------\n",
      "iter  1  stage  17  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 25, 27, 27, 27, 29, 27, 27, 27, 28, 26, 27, 27,\n",
      "        26, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(4.8350, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.3889, 4.3889, 4.3889, 4.3889, 4.3889, 4.3889, 4.3889, 4.3889, 4.3889,\n",
      "        4.3889, 4.3889, 4.3889, 4.3889, 4.3889, 4.3889, 4.3889, 4.3889, 4.3889,\n",
      "        3.6066, 2.9388, 2.3509, 1.8188, 1.3271, 0.8650, 0.4245]) return=  172634.51909284995\n",
      "probs of actions:  tensor([0.7646, 0.7255, 0.7889, 0.7591, 0.7688, 0.8019, 0.0085, 0.7527, 0.7150,\n",
      "        0.7432, 0.0773, 0.7542, 0.7013, 0.8001, 0.0441, 0.0799, 0.7366, 0.7465,\n",
      "        0.0758, 0.8491, 0.8635, 0.9027, 0.9119, 0.8123, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5671, 0.6369, 0.6732, 0.6916, 0.7010, 0.7160, 0.6992, 0.7047,\n",
      "        0.7075, 0.6977, 0.7185, 0.7144, 0.7124, 0.7058, 0.7206, 0.7084, 0.7093,\n",
      "        0.7151, 0.7057, 0.7080, 0.7092, 0.7097, 0.7100, 0.7831])\n",
      "finalReturns:  tensor([1.3613, 1.4342, 1.3869, 1.2691, 1.0932, 0.8757, 0.6281, 0.3586])\n",
      "----------------------------------------\n",
      "iter  1  stage  16  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([27, 29, 27, 27, 29, 27, 27, 27, 29, 27, 29, 28, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(2.6017, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.7995, 4.7995, 4.7995, 4.7995, 4.7995, 4.7995, 4.7995, 4.7995, 4.7995,\n",
      "        4.7995, 4.7995, 4.7995, 4.7995, 4.7995, 4.7995, 4.7995, 4.7995, 4.0157,\n",
      "        3.3471, 2.7589, 2.2275, 1.7369, 1.2757, 0.8361, 0.4122]) return=  172966.5624334948\n",
      "probs of actions:  tensor([0.7289, 0.1692, 0.7443, 0.7138, 0.1311, 0.7670, 0.7516, 0.7013, 0.1616,\n",
      "        0.7016, 0.1237, 0.0473, 0.6433, 0.7727, 0.6639, 0.7063, 0.7170, 0.6757,\n",
      "        0.8066, 0.8157, 0.8129, 0.8741, 0.9036, 0.8123, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5559, 0.6454, 0.6775, 0.6826, 0.7109, 0.7106, 0.7105, 0.6992,\n",
      "        0.7192, 0.7036, 0.7159, 0.7203, 0.7153, 0.7128, 0.7116, 0.7109, 0.7106,\n",
      "        0.7105, 0.7104, 0.7104, 0.7103, 0.7103, 0.7103, 0.7832])\n",
      "finalReturns:  tensor([1.6675, 1.7404, 1.6984, 1.5761, 1.3971, 1.1774, 0.9282, 0.6575, 0.3710])\n",
      "----------------------------------------\n",
      "iter  1  stage  15  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([27, 29, 27, 27, 27, 29, 29, 27, 28, 29, 29, 27, 29, 27, 29, 27, 27, 29,\n",
      "        29, 29, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(11.3290, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.1989, 5.1989, 5.1989, 5.1989, 5.1989, 5.1989, 5.1989, 5.1989, 5.1989,\n",
      "        5.1989, 5.1989, 5.1989, 5.1989, 5.1989, 5.1989, 5.1989, 4.4036, 3.7298,\n",
      "        3.1391, 2.6065, 2.1171, 1.6595, 1.2254, 0.8070, 0.3996]) return=  173351.18255934515\n",
      "probs of actions:  tensor([0.5099, 0.3595, 0.5167, 0.4838, 0.5129, 0.3033, 0.3360, 0.4651, 0.0750,\n",
      "        0.3347, 0.3011, 0.4653, 0.3972, 0.5718, 0.3602, 0.4803, 0.4625, 0.3816,\n",
      "        0.2773, 0.3271, 0.6014, 0.6833, 0.7747, 0.6614, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5559, 0.6454, 0.6775, 0.6938, 0.6909, 0.7038, 0.7216, 0.7104,\n",
      "        0.7064, 0.7116, 0.7255, 0.7067, 0.7230, 0.7054, 0.7224, 0.7163, 0.7021,\n",
      "        0.7095, 0.7132, 0.7263, 0.7183, 0.7143, 0.7123, 0.7842])\n",
      "finalReturns:  tensor([2.0200, 2.0929, 2.0505, 1.9390, 1.7621, 1.5383, 1.2696, 0.9855, 0.6895,\n",
      "        0.3846])\n",
      "----------------------------------------\n",
      "iter  1  stage  14  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 28, 28, 29, 29, 29, 29, 30, 27, 29, 26, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 26, 29,  0])\n",
      "loss=  tensor(7.7630, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.5612, 5.5612, 5.5612, 5.5612, 5.5612, 5.5612, 5.5612, 5.5612, 5.5612,\n",
      "        5.5612, 5.5612, 5.5612, 5.5612, 5.5612, 5.5612, 4.7621, 4.0865, 3.4969,\n",
      "        2.9681, 2.4827, 2.0287, 1.5976, 1.1832, 0.7812, 0.3884]) return=  173825.5551842266\n",
      "probs of actions:  tensor([0.7370, 0.0655, 0.0614, 0.7672, 0.7414, 0.7641, 0.7839, 0.0485, 0.0853,\n",
      "        0.7527, 0.0260, 0.7726, 0.7826, 0.7109, 0.7644, 0.8093, 0.8081, 0.7662,\n",
      "        0.7728, 0.8169, 0.8252, 0.7849, 0.0323, 0.6756, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5696, 0.6399, 0.6706, 0.6936, 0.7052, 0.7111, 0.7081, 0.7311,\n",
      "        0.7095, 0.7297, 0.7017, 0.7093, 0.7131, 0.7150, 0.7160, 0.7164, 0.7167,\n",
      "        0.7168, 0.7169, 0.7169, 0.7169, 0.7334, 0.7036, 0.7943])\n",
      "finalReturns:  tensor([2.4016, 2.4857, 2.4454, 2.3186, 2.1307, 1.8993, 1.6364, 1.3506, 1.0481,\n",
      "        0.7166, 0.4059])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  13  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 30, 29, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 27, 29, 29, 29,  0])\n",
      "loss=  tensor(8.1818, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.9484, 5.9484, 5.9484, 5.9484, 5.9484, 5.9484, 5.9484, 5.9484, 5.9484,\n",
      "        5.9484, 5.9484, 5.9484, 5.9484, 5.9484, 5.1476, 4.4712, 3.8812, 3.3522,\n",
      "        2.8668, 2.4127, 1.9816, 1.5672, 1.1652, 0.7708, 0.3828]) return=  174002.82550723848\n",
      "probs of actions:  tensor([0.7929, 0.8025, 0.8118, 0.8192, 0.7962, 0.8142, 0.0197, 0.8157, 0.0652,\n",
      "        0.8068, 0.8006, 0.8221, 0.8335, 0.7760, 0.8249, 0.8528, 0.8706, 0.7983,\n",
      "        0.8249, 0.8632, 0.0896, 0.8470, 0.7300, 0.7179, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6384, 0.6772, 0.6969, 0.7069, 0.7060, 0.7189, 0.7236,\n",
      "        0.7129, 0.7149, 0.7159, 0.7164, 0.7167, 0.7168, 0.7169, 0.7169, 0.7169,\n",
      "        0.7169, 0.7169, 0.7281, 0.7080, 0.7125, 0.7147, 0.7999])\n",
      "finalReturns:  tensor([2.7328, 2.8169, 2.7765, 2.6496, 2.4617, 2.2302, 1.9674, 1.6816, 1.3678,\n",
      "        1.0618, 0.7438, 0.4171])\n",
      "----------------------------------------\n",
      "iter  1  stage  12  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 29, 29, 28, 29, 29, 29, 29, 29, 29, 30, 29, 27, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(12.7920, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.3411, 6.3411, 6.3411, 6.3411, 6.3411, 6.3411, 6.3411, 6.3411, 6.3411,\n",
      "        6.3411, 6.3411, 6.3411, 6.3411, 5.5357, 4.8572, 4.2662, 3.7349, 3.2471,\n",
      "        2.7909, 2.3581, 1.9423, 1.5393, 1.1457, 0.7590, 0.3776]) return=  173992.11630517073\n",
      "probs of actions:  tensor([0.8624, 0.8648, 0.8793, 0.8813, 0.0524, 0.8886, 0.8990, 0.8753, 0.8682,\n",
      "        0.8730, 0.8802, 0.0325, 0.8614, 0.0400, 0.8668, 0.9117, 0.9162, 0.8509,\n",
      "        0.9006, 0.9281, 0.9482, 0.9433, 0.8928, 0.8473, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6384, 0.6772, 0.7026, 0.7024, 0.7097, 0.7133, 0.7151,\n",
      "        0.7160, 0.7165, 0.7108, 0.7213, 0.7303, 0.7091, 0.7130, 0.7150, 0.7159,\n",
      "        0.7164, 0.7167, 0.7168, 0.7169, 0.7169, 0.7169, 0.8010])\n",
      "finalReturns:  tensor([3.0651, 3.1492, 3.0974, 2.9793, 2.7976, 2.5705, 2.3107, 2.0271, 1.7261,\n",
      "        1.4124, 1.0892, 0.7589, 0.4235])\n",
      "----------------------------------------\n",
      "iter  1  stage  11  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 29, 29, 30, 27, 29, 30, 30, 28, 29, 28, 29, 29, 29, 29, 29, 30,\n",
      "        30, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(27.1738, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.6912, 6.6912, 6.6912, 6.6912, 6.6912, 6.6912, 6.6912, 6.6912, 6.6912,\n",
      "        6.6912, 6.6912, 6.6912, 5.8911, 5.2150, 4.6242, 4.0942, 3.6077, 3.1528,\n",
      "        2.7210, 2.3062, 1.9046, 1.5132, 1.1288, 0.7493, 0.3734]) return=  174079.33224464272\n",
      "probs of actions:  tensor([0.8209, 0.8254, 0.8419, 0.8423, 0.0719, 0.0270, 0.8679, 0.0696, 0.0810,\n",
      "        0.0624, 0.8521, 0.0596, 0.7923, 0.8248, 0.8013, 0.8720, 0.8975, 0.0851,\n",
      "        0.0489, 0.8874, 0.9414, 0.9314, 0.8985, 0.8402, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6384, 0.6772, 0.6910, 0.7225, 0.7052, 0.7052, 0.7126,\n",
      "        0.7279, 0.7151, 0.7217, 0.7120, 0.7145, 0.7157, 0.7163, 0.7166, 0.7109,\n",
      "        0.7154, 0.7236, 0.7203, 0.7186, 0.7178, 0.7173, 0.8012])\n",
      "finalReturns:  tensor([3.4307, 3.5091, 3.4732, 3.3495, 3.1639, 2.9340, 2.6723, 2.3932, 2.0926,\n",
      "        1.7706, 1.4418, 1.1075, 0.7693, 0.4279])\n",
      "----------------------------------------\n",
      "iter  1  stage  10  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 26, 29, 29, 29, 29, 29, 29, 28, 29, 29, 28, 29, 30, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(21.4632, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.0587, 7.0587, 7.0587, 7.0587, 7.0587, 7.0587, 7.0587, 7.0587, 7.0587,\n",
      "        7.0587, 7.0587, 6.2626, 5.5883, 4.9994, 4.4709, 3.9847, 3.5298, 3.0985,\n",
      "        2.6844, 2.2827, 1.8901, 1.5043, 1.1236, 0.7465, 0.3722]) return=  173927.67848376333\n",
      "probs of actions:  tensor([0.7494, 0.7613, 0.0041, 0.7772, 0.7584, 0.8050, 0.8112, 0.7614, 0.7421,\n",
      "        0.0720, 0.8168, 0.7777, 0.0912, 0.7630, 0.1784, 0.7979, 0.8219, 0.7332,\n",
      "        0.8220, 0.8517, 0.9236, 0.9042, 0.8870, 0.8024, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6549, 0.6641, 0.6903, 0.7036, 0.7102, 0.7136, 0.7152,\n",
      "        0.7218, 0.7120, 0.7145, 0.7214, 0.7118, 0.7085, 0.7201, 0.7185, 0.7177,\n",
      "        0.7173, 0.7171, 0.7170, 0.7170, 0.7170, 0.7169, 0.8010])\n",
      "finalReturns:  tensor([3.7693, 3.8534, 3.8131, 3.6807, 3.4974, 3.2750, 3.0098, 2.7226, 2.4190,\n",
      "        2.1033, 1.7788, 1.4476, 1.1114, 0.7715, 0.4288])\n",
      "----------------------------------------\n",
      "iter  1  stage  9  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 29, 30, 30, 29, 30, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 28,\n",
      "        29, 29, 29, 29, 29, 28,  0])\n",
      "loss=  tensor(16.1739, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.4444, 7.4444, 7.4444, 7.4444, 7.4444, 7.4444, 7.4444, 7.4444, 7.4444,\n",
      "        7.4444, 6.6425, 5.9655, 5.3753, 4.8462, 4.3607, 3.9066, 3.4755, 3.0611,\n",
      "        2.6591, 2.2663, 1.8794, 1.4975, 1.1193, 0.7442, 0.3712]) return=  174131.5237391631\n",
      "probs of actions:  tensor([0.8230, 0.8277, 0.8443, 0.0909, 0.0966, 0.8673, 0.0717, 0.8321, 0.8166,\n",
      "        0.8434, 0.8727, 0.8568, 0.7877, 0.8095, 0.7937, 0.8621, 0.8774, 0.0635,\n",
      "        0.8763, 0.9007, 0.9587, 0.9444, 0.9358, 0.0460, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6384, 0.6713, 0.6954, 0.7136, 0.7093, 0.7206, 0.7187,\n",
      "        0.7178, 0.7174, 0.7172, 0.7170, 0.7170, 0.7170, 0.7169, 0.7169, 0.7226,\n",
      "        0.7125, 0.7147, 0.7158, 0.7164, 0.7166, 0.7225, 0.7965])\n",
      "finalReturns:  tensor([4.1104, 4.1945, 4.1540, 4.0271, 3.8392, 3.6077, 3.3448, 3.0590, 2.7564,\n",
      "        2.4358, 2.1162, 1.7883, 1.4545, 1.1163, 0.7748, 0.4253])\n",
      "----------------------------------------\n",
      "iter  1  stage  8  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 28, 29, 29, 29, 30, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(30.2489, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.8061, 7.8061, 7.8061, 7.8061, 7.8061, 7.8061, 7.8061, 7.8061, 7.8061,\n",
      "        7.0063, 6.3304, 5.7406, 5.2108, 4.7243, 4.2693, 3.8374, 3.4231, 3.0216,\n",
      "        2.6293, 2.2438, 1.8632, 1.4863, 1.1121, 0.7400, 0.3694]) return=  174058.37645850837\n",
      "probs of actions:  tensor([0.8582, 0.8629, 0.8798, 0.8764, 0.8638, 0.9002, 0.9004, 0.8670, 0.8585,\n",
      "        0.0301, 0.9124, 0.8797, 0.8310, 0.0907, 0.8367, 0.8905, 0.9087, 0.8720,\n",
      "        0.9059, 0.9327, 0.9686, 0.9607, 0.9600, 0.9044, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6384, 0.6772, 0.6969, 0.7069, 0.7119, 0.7144, 0.7157,\n",
      "        0.7220, 0.7121, 0.7145, 0.7157, 0.7104, 0.7211, 0.7190, 0.7180, 0.7174,\n",
      "        0.7172, 0.7171, 0.7170, 0.7170, 0.7169, 0.7169, 0.8010])\n",
      "finalReturns:  tensor([4.4630, 4.5471, 4.5011, 4.3787, 4.1940, 3.9647, 3.7093, 3.4202, 3.1154,\n",
      "        2.7989, 2.4738, 2.1422, 1.8057, 1.4656, 1.1228, 0.7779, 0.4316])\n",
      "----------------------------------------\n",
      "iter  1  stage  7  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(4.4280, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.1717, 8.1717, 8.1717, 8.1717, 8.1717, 8.1717, 8.1717, 8.1717, 7.3731,\n",
      "        6.6978, 6.1083, 5.5795, 5.0942, 4.6402, 4.2091, 3.7948, 3.3928, 2.9999,\n",
      "        2.6139, 2.2329, 1.8557, 1.4813, 1.1089, 0.7382, 0.3687]) return=  174060.25019204617\n",
      "probs of actions:  tensor([0.9034, 0.9041, 0.9183, 0.9161, 0.9079, 0.9342, 0.9338, 0.9166, 0.9100,\n",
      "        0.9035, 0.9549, 0.9257, 0.8960, 0.9082, 0.8750, 0.9167, 0.9458, 0.9095,\n",
      "        0.9417, 0.9605, 0.9816, 0.9780, 0.9770, 0.9405, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6384, 0.6772, 0.6969, 0.7069, 0.7119, 0.7144, 0.7157,\n",
      "        0.7163, 0.7166, 0.7168, 0.7168, 0.7169, 0.7169, 0.7169, 0.7169, 0.7169,\n",
      "        0.7169, 0.7169, 0.7169, 0.7169, 0.7169, 0.7169, 0.8010])\n",
      "finalReturns:  tensor([4.8121, 4.8962, 4.8558, 4.7290, 4.5412, 4.3098, 4.0469, 3.7611, 3.4586,\n",
      "        3.1436, 2.8196, 2.4887, 2.1528, 1.8131, 1.4705, 1.1259, 0.7797, 0.4324])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  6  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        28, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(17.8648, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.5390, 8.5390, 8.5390, 8.5390, 8.5390, 8.5390, 8.5390, 7.7430, 7.0688,\n",
      "        6.4798, 5.9513, 5.4661, 5.0122, 4.5811, 4.1668, 3.7648, 3.3719, 2.9859,\n",
      "        2.6049, 2.2277, 1.8533, 1.4802, 1.1085, 0.7381, 0.3686]) return=  174029.23196541704\n",
      "probs of actions:  tensor([0.8938, 0.8960, 0.9100, 0.9065, 0.9003, 0.9289, 0.9158, 0.9051, 0.9186,\n",
      "        0.8903, 0.9594, 0.9238, 0.8900, 0.9022, 0.8511, 0.9066, 0.9395, 0.9038,\n",
      "        0.0062, 0.9561, 0.9805, 0.9749, 0.9774, 0.9416, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6384, 0.6772, 0.6969, 0.7069, 0.7119, 0.7144, 0.7157,\n",
      "        0.7163, 0.7166, 0.7168, 0.7168, 0.7169, 0.7169, 0.7169, 0.7169, 0.7169,\n",
      "        0.7226, 0.7125, 0.7147, 0.7158, 0.7164, 0.7166, 0.8009])\n",
      "finalReturns:  tensor([5.1535, 5.2376, 5.1974, 5.0707, 4.8829, 4.6515, 4.3887, 4.1029, 3.8004,\n",
      "        3.4854, 3.1614, 2.8305, 2.4946, 2.1492, 1.8111, 1.4695, 1.1254, 0.7795,\n",
      "        0.4323])\n",
      "----------------------------------------\n",
      "iter  1  stage  5  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(2.6399, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.8931, 8.8931, 8.8931, 8.8931, 8.8931, 8.8931, 8.1021, 7.4302, 6.8424,\n",
      "        6.3143, 5.8294, 5.3756, 4.9445, 4.5302, 4.1283, 3.7354, 3.3494, 2.9684,\n",
      "        2.5912, 2.2168, 1.8445, 1.4737, 1.1042, 0.7355, 0.3675]) return=  174060.25019204617\n",
      "probs of actions:  tensor([0.9462, 0.9455, 0.9555, 0.9534, 0.9488, 0.9645, 0.9683, 0.9549, 0.9581,\n",
      "        0.9596, 0.9844, 0.9700, 0.9389, 0.9530, 0.9245, 0.9614, 0.9708, 0.9519,\n",
      "        0.9705, 0.9807, 0.9920, 0.9906, 0.9916, 0.9737, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6384, 0.6772, 0.6969, 0.7069, 0.7119, 0.7144, 0.7157,\n",
      "        0.7163, 0.7166, 0.7168, 0.7168, 0.7169, 0.7169, 0.7169, 0.7169, 0.7169,\n",
      "        0.7169, 0.7169, 0.7169, 0.7169, 0.7169, 0.7169, 0.8010])\n",
      "finalReturns:  tensor([5.5094, 5.5935, 5.5535, 5.4270, 5.2393, 5.0080, 4.7452, 4.4594, 4.1569,\n",
      "        3.8420, 3.5179, 3.1870, 2.8511, 2.5114, 2.1689, 1.8243, 1.4781, 1.1307,\n",
      "        0.7824, 0.4335])\n",
      "----------------------------------------\n",
      "iter  1  stage  4  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 30, 29, 29, 29, 29, 29, 30, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(32.6679, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.2297, 9.2297, 9.2297, 9.2297, 9.2297, 8.4486, 7.7814, 7.1957, 6.6687,\n",
      "        6.1842, 5.7315, 5.3016, 4.8883, 4.4871, 4.0950, 3.7094, 3.3296, 2.9536,\n",
      "        2.5804, 2.2090, 1.8390, 1.4701, 1.1018, 0.7342, 0.3669]) return=  174121.3274714346\n",
      "probs of actions:  tensor([0.9370, 0.9376, 0.9482, 0.9457, 0.9450, 0.9597, 0.9723, 0.0570, 0.9488,\n",
      "        0.9538, 0.9850, 0.9677, 0.9243, 0.0438, 0.9097, 0.9542, 0.9587, 0.9397,\n",
      "        0.9704, 0.9754, 0.9915, 0.9905, 0.9908, 0.9710, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6384, 0.6772, 0.6969, 0.7069, 0.7119, 0.7085, 0.7201,\n",
      "        0.7185, 0.7177, 0.7173, 0.7171, 0.7111, 0.7215, 0.7192, 0.7181, 0.7175,\n",
      "        0.7172, 0.7171, 0.7170, 0.7170, 0.7169, 0.7169, 0.8010])\n",
      "finalReturns:  tensor([5.8759, 5.9600, 5.9204, 5.7942, 5.6126, 5.3770, 5.1112, 4.8233, 4.5193,\n",
      "        4.2033, 3.8844, 3.5484, 3.2091, 2.8670, 2.5228, 2.1769, 1.8298, 1.4818,\n",
      "        1.1331, 0.7838, 0.4341])\n",
      "----------------------------------------\n",
      "iter  1  stage  3  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 30, 29, 29, 29, 29, 30, 29, 29, 29, 29, 30, 29, 29, 29, 30, 29,\n",
      "        30, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(41.6779, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.5587, 9.5587, 9.5587, 9.5587, 8.7930, 8.1329, 7.5505, 7.0251, 6.5414,\n",
      "        6.0882, 5.6583, 5.2451, 4.8442, 4.4522, 4.0668, 3.6871, 3.3112, 2.9380,\n",
      "        2.5667, 2.1975, 1.8295, 1.4630, 1.0971, 0.7314, 0.3657]) return=  174209.913456734\n",
      "probs of actions:  tensor([0.8751, 0.8809, 0.1046, 0.8975, 0.8943, 0.9066, 0.9349, 0.1239, 0.8953,\n",
      "        0.9055, 0.9732, 0.9129, 0.1605, 0.9055, 0.8477, 0.9023, 0.0858, 0.8891,\n",
      "        0.0721, 0.9442, 0.9840, 0.9776, 0.9772, 0.9326, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6325, 0.6815, 0.6991, 0.7080, 0.7125, 0.7088, 0.7203,\n",
      "        0.7186, 0.7178, 0.7173, 0.7112, 0.7215, 0.7192, 0.7181, 0.7116, 0.7217,\n",
      "        0.7134, 0.7226, 0.7198, 0.7183, 0.7176, 0.7173, 0.8012])\n",
      "finalReturns:  tensor([6.2388, 6.3229, 6.2839, 6.1583, 5.9712, 5.7461, 5.4791, 5.1904, 4.8858,\n",
      "        4.5694, 4.2501, 3.9140, 3.5745, 3.2323, 2.8939, 2.5435, 2.1993, 1.8447,\n",
      "        1.4914, 1.1390, 0.7871, 0.4355])\n",
      "----------------------------------------\n",
      "iter  1  stage  2  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 30, 29, 29, 30, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 30,  0])\n",
      "loss=  tensor(40.5944, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.8524, 9.8524, 9.8524, 9.1299, 8.4899, 7.9170, 7.3961, 6.9146, 6.4624,\n",
      "        6.0330, 5.6201, 5.2193, 4.8281, 4.4437, 4.0642, 3.6881, 3.3147, 2.9431,\n",
      "        2.5729, 2.2038, 1.8354, 1.4677, 1.1003, 0.7333, 0.3666]) return=  174107.14165860406\n",
      "probs of actions:  tensor([0.8416, 0.8520, 0.8595, 0.8623, 0.8794, 0.8895, 0.0793, 0.8476, 0.8646,\n",
      "        0.1119, 0.9667, 0.8832, 0.7889, 0.8842, 0.7762, 0.8842, 0.8902, 0.8377,\n",
      "        0.9165, 0.9302, 0.9792, 0.9674, 0.9688, 0.0917, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6384, 0.6772, 0.6969, 0.7069, 0.7060, 0.7189, 0.7179,\n",
      "        0.7115, 0.7217, 0.7193, 0.7181, 0.7175, 0.7172, 0.7171, 0.7170, 0.7170,\n",
      "        0.7169, 0.7169, 0.7169, 0.7169, 0.7169, 0.7110, 0.8055])\n",
      "finalReturns:  tensor([6.5673, 6.6514, 6.6142, 6.4902, 6.3042, 6.0797, 5.8130, 5.5245, 5.2259,\n",
      "        4.9051, 4.5769, 4.2432, 3.9053, 3.5641, 3.2205, 2.8751, 2.5283, 2.1805,\n",
      "        1.8319, 1.4827, 1.1331, 0.7832, 0.4389])\n",
      "----------------------------------------\n",
      "iter  1  stage  1  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([30, 30, 30, 29, 29, 29, 29, 30, 29, 29, 29, 29, 29, 30, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(54.3465, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.0714, 10.0714,  9.4193,  8.8129,  8.2569,  7.7457,  7.2699,  6.8214,\n",
      "         6.3935,  5.9812,  5.5814,  5.1904,  4.8059,  4.4262,  4.0499,  3.6762,\n",
      "         3.3052,  2.9359,  2.5675,  2.1999,  1.8327,  1.4658,  1.0991,  0.7327,\n",
      "         0.3663]) return=  174202.79502442433\n",
      "probs of actions:  tensor([0.2038, 0.1953, 0.1608, 0.8084, 0.8417, 0.8700, 0.8805, 0.2281, 0.8222,\n",
      "        0.8522, 0.9632, 0.8519, 0.6879, 0.1521, 0.7490, 0.8204, 0.8533, 0.7903,\n",
      "        0.8795, 0.8923, 0.9703, 0.9507, 0.9572, 0.8827, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5621, 0.6389, 0.6848, 0.7008, 0.7088, 0.7129, 0.7090, 0.7204,\n",
      "        0.7187, 0.7178, 0.7174, 0.7171, 0.7111, 0.7215, 0.7192, 0.7181, 0.7175,\n",
      "        0.7172, 0.7171, 0.7170, 0.7170, 0.7169, 0.7169, 0.8010])\n",
      "finalReturns:  tensor([6.9277, 7.0177, 6.9853, 6.8564, 6.6668, 6.4338, 6.1694, 5.8883, 5.5802,\n",
      "        5.2614, 4.9345, 4.6017, 4.2643, 3.9294, 3.5817, 3.2335, 2.8847, 2.5356,\n",
      "        2.1860, 1.8362, 1.4861, 1.1358, 0.7853, 0.4347])\n",
      "----------------------------------------\n",
      "iter  1  stage  0  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([30, 29, 29, 30, 30, 29, 29, 29, 29, 30, 29, 29, 30, 29, 29, 29, 29, 29,\n",
      "        29, 30, 29, 30, 29, 29,  0])\n",
      "loss=  tensor(69.6359, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.1501,  9.6388,  9.1023,  8.5803,  8.0846,  7.6157,  7.1709,  6.7459,\n",
      "         6.3358,  5.9369,  5.5462,  5.1617,  4.7827,  4.4073,  4.0344,  3.6641,\n",
      "         3.2953,  2.9274,  2.5601,  2.1932,  1.8265,  1.4600,  1.0944,  0.7290,\n",
      "         0.3645]) return=  174253.3722751547\n",
      "probs of actions:  tensor([0.2702, 0.7677, 0.7977, 0.2116, 0.2319, 0.8234, 0.8262, 0.7327, 0.7965,\n",
      "        0.1930, 0.9485, 0.7959, 0.3707, 0.8006, 0.6535, 0.7538, 0.8028, 0.7528,\n",
      "        0.8439, 0.1603, 0.9571, 0.0712, 0.9422, 0.8500, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5680, 0.6405, 0.6723, 0.6960, 0.7139, 0.7154, 0.7162, 0.7165,\n",
      "        0.7108, 0.7213, 0.7191, 0.7121, 0.7220, 0.7194, 0.7182, 0.7176, 0.7172,\n",
      "        0.7171, 0.7111, 0.7214, 0.7133, 0.7225, 0.7197, 0.8024])\n",
      "finalReturns:  tensor([7.2753, 7.3653, 7.3339, 7.2154, 7.0387, 6.8116, 6.5426, 6.2521, 5.9461,\n",
      "        5.6285, 5.3083, 4.9715, 4.6314, 4.2947, 3.9456, 3.5965, 3.2471, 2.8974,\n",
      "        2.5475, 2.1973, 1.8529, 1.4980, 1.1503, 0.7931, 0.4380])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,[1e-05,1][1, 10000, 1, 1],1682423487 saved\n",
      "[3000000, 'tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])', 174210.39338284553, 57605.93776875555, 65.17557525634766, 1e-05, 1, 0, 'tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 29, 29, 29, 30, 29, 29,\\n        29, 30, 29, 29, 29, 29,  0])', '[0.69 0.75 0.77 0.78 0.75 0.8  0.8  0.7  0.77 0.2  0.06 0.24 0.6  0.77\\n 0.62 0.27 0.78 0.72 0.82 0.18 0.95 0.92 0.94 0.84 1.  ]', '0,[1e-05,1][1, 10000, 1, 1],1682423487', 25, 50, 174199.95044967832, 226157.05867704182, 94851.05074168817, 131012.56797668608, 127973.03513660273, 64641.60648389723, 62848.849838023714, 78979.431849868, 79951.69168142487, 109515.23673882235, 64159.865982403535, 79875.30170982817]\n",
      "policy reset\n",
      "----------------------------------------\n",
      "iter  2  stage  24  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625,\n",
      "        0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625,\n",
      "        0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625]) return=  140290.89393391204\n",
      "probs of actions:  tensor([0.9195, 0.0062, 0.9061, 0.9295, 0.9160, 0.9197, 0.9265, 0.9012, 0.8971,\n",
      "        0.9135, 0.9181, 0.9058, 0.9080, 0.9095, 0.0020, 0.9082, 0.9207, 0.9140,\n",
      "        0.9216, 0.8805, 0.9001, 0.9244, 0.8986, 0.9275, 0.9888],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5350, 0.5644, 0.5634, 0.5630, 0.5627, 0.5626, 0.5626, 0.5625,\n",
      "        0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5589, 0.5852, 0.5738, 0.5681,\n",
      "        0.5653, 0.5639, 0.5632, 0.5629, 0.5627, 0.5626, 0.5625])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  2  stage  23  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([17,  0,  1, 22,  8,  0,  9, 17, 19,  7, 14,  7,  0,  1, 14,  0,  7,  1,\n",
      "         0, 10, 17,  0,  2, 21,  0])\n",
      "loss=  tensor(0.1059, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.1688, 1.1688, 1.1688, 1.1688, 1.1688, 1.1688, 1.1688, 1.1688, 1.1688,\n",
      "        1.1688, 1.1688, 1.1688, 1.1688, 1.1688, 1.1688, 1.1688, 1.1688, 1.1688,\n",
      "        1.1688, 1.1688, 1.1688, 1.1688, 1.1688, 1.1688, 0.5770]) return=  150673.14503249113\n",
      "probs of actions:  tensor([0.0195, 0.3042, 0.0644, 0.0075, 0.0056, 0.3819, 0.0345, 0.0280, 0.0135,\n",
      "        0.0520, 0.0902, 0.0551, 0.2481, 0.0579, 0.0623, 0.3358, 0.0548, 0.0627,\n",
      "        0.5367, 0.0444, 0.0219, 0.4604, 0.0402, 0.0654, 0.9866],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4823, 0.6006, 0.5813, 0.5273, 0.6487, 0.6395, 0.5923, 0.5872, 0.6199,\n",
      "        0.6798, 0.6304, 0.6563, 0.6385, 0.5998, 0.5653, 0.6279, 0.5898, 0.6053,\n",
      "        0.5876, 0.5650, 0.5782, 0.6514, 0.6057, 0.5477, 0.6596])\n",
      "finalReturns:  tensor([0.0384, 0.0825])\n"
     ]
    }
   ],
   "source": [
    "for adv in range(len(AdversaryModes)):\n",
    "    adversaryProbs=torch.zeros(len(AdversaryModes))\n",
    "    adversaryProbs[adv]=1\n",
    "    game = Model(totalDemand = 400, \n",
    "                   tupleCosts = (57, 71),\n",
    "                  totalStages = 25, adversaryProbs=adversaryProbs, advHistoryNum=0)\n",
    "    neuralNet=NNBase(num_input=game.T+2+game.advHistoryNum, lr=hyperParams[0],num_actions=50)\n",
    "    algorithm = ReinforceAlgorithm(game, neuralNet, numberIterations=3, numberEpisodes=3_000_000, discountFactor =hyperParams[1])\n",
    "\n",
    "\n",
    "    algorithm.solver(print_step=100_000,options=codeParams,converge_break=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
