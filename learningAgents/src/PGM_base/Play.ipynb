{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BimatrixGame as BG\n",
    "import globals as gl\n",
    "import torch\n",
    "import numpy as np\n",
    "from environmentModelBase import Model, MixedStrategy, Strategy, StrategyType\n",
    "import environmentModelBase as em\n",
    "from learningBase import ReinforceAlgorithm\n",
    "from neuralNetworkSimple import NNBase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "gl.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sjaha\\Documents\\EquiLearn\\PGM_base\\environmentModelBase.py:344: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  adversaryDist = Categorical(torch.tensor(self._strategyProbs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equi: [0.00, 0.00, 1.00], [1.00, 0.00, 0.00], 154997.58, 79665.90\n",
      "Round 0  of  50\n",
      "1684386202 is trained against myopic-1.00,\n",
      "low,1684386202 saved\n",
      "low cost player low,1684386202 added\n",
      "equi: [0.00, 0.00, 0.00, 1.00], [1.00, 0.00, 0.00], 178432.78, 35256.60\n",
      "1684424924 is trained against low,1684386202-1.00,\n",
      "high,1684424924 saved\n",
      "high cost player high,1684424924 added\n",
      "equi: [0.00, 0.00, 0.49, 0.51], [0.06, 0.00, 0.00, 0.94], 125951.71, 57157.89\n",
      "Round 1  of  50\n",
      "1684484716 is trained against myopic-0.06,high,1684424924-0.94,\n",
      "low,1684484716 saved\n",
      "low cost player low,1684484716 added\n",
      "equi: [0.00, 0.00, 0.86, 0.00, 0.14], [0.61, 0.00, 0.39, 0.00], 149731.40, 74148.84\n",
      "1684520477 is trained against guess-0.86,low,1684484716-0.14,\n",
      "Round 2  of  50\n",
      "1684557152 is trained against myopic-0.61,guess-0.39,\n",
      "low,1684557152 saved\n",
      "low cost player low,1684557152 added\n",
      "equi: [0.00, 0.00, 0.00, 0.00, 1.00, 0.00], [0.00, 0.00, 0.00, 1.00], 125382.90, 51182.05\n",
      "1684604536 is trained against low,1684484716-1.00,\n",
      "Round 3  of  50\n",
      "1684660196 is trained against high,1684424924-1.00,\n",
      "1684712683 is trained against low,1684484716-1.00,\n",
      "Round 4  of  50\n",
      "1684767756 is trained against high,1684424924-1.00,\n",
      "1684821735 is trained against low,1684484716-1.00,\n",
      "high,1684821735 saved\n",
      "high cost player high,1684821735 added\n",
      "equi: [0.00, 0.00, 0.91, 0.00, 0.00, 0.09], [0.59, 0.00, 0.41, 0.00, 0.00], 149467.34, 76108.83\n",
      "Round 5  of  50\n",
      "1684856358 is trained against myopic-0.59,guess-0.41,\n",
      "low,1684856358 saved\n",
      "low cost player low,1684856358 added\n",
      "equi: [0.00, 0.00, 0.91, 0.00, 0.00, 0.00, 0.09], [0.58, 0.00, 0.42, 0.00, 0.00], 149349.14, 76358.19\n",
      "1684917192 is trained against guess-0.91,low,1684856358-0.09,\n",
      "Round 6  of  50\n",
      "1685028503 is trained against myopic-0.58,guess-0.42,\n",
      "low,1685028503 saved\n",
      "low cost player low,1685028503 added\n",
      "equi: [0.00, 0.00, 0.94, 0.00, 0.00, 0.00, 0.00, 0.06], [0.51, 0.00, 0.49, 0.00, 0.00], 148434.17, 77473.34\n",
      "1685065319 is trained against guess-0.94,low,1685028503-0.06,\n",
      "Round 7  of  50\n",
      "1685102633 is trained against myopic-0.51,guess-0.49,\n",
      "1685138981 is trained against guess-0.94,low,1685028503-0.06,\n",
      "Round 8  of  50\n",
      "1685191942 is trained against myopic-0.51,guess-0.49,\n",
      "1685257951 is trained against guess-0.94,low,1685028503-0.06,\n",
      "Round 9  of  50\n",
      "1685352041 is trained against myopic-0.51,guess-0.49,\n",
      "1685452188 is trained against guess-0.94,low,1685028503-0.06,\n",
      "Round 10  of  50\n",
      "1685508532 is trained against myopic-0.51,guess-0.49,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlow cost player \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlow_cost_player\u001b[38;5;241m.\u001b[39m_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m added\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m     low_cost_probabilities, high_cost_probabilities, low_cost_payoff, high_cost_payoff \u001b[38;5;241m=\u001b[39m bimatrixGame\u001b[38;5;241m.\u001b[39mcompute_equilibria()\n\u001b[1;32m---> 43\u001b[0m acceptable, agentPayoffs, advPayoffs, high_cost_player \u001b[38;5;241m=\u001b[39m \u001b[43mBG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mgl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhighCost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlowCost\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madvMixedStrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMixedStrategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobablitiesArray\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cost_probabilities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategiesList\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cost_players\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetPayoff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhigh_cost_payoff\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m acceptable:\n\u001b[0;32m     47\u001b[0m     update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\EquiLearn\\PGM_base\\BimatrixGame.py:159\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(costs, advMixedStrategy, targetPayoff)\u001b[0m\n\u001b[0;32m    155\u001b[0m numberEpisodes \u001b[38;5;241m=\u001b[39m gl\u001b[38;5;241m.\u001b[39mnumEpisodes\u001b[38;5;241m+\u001b[39mgl\u001b[38;5;241m.\u001b[39mepisodeIncreaseAdv \u001b[38;5;241m*\u001b[39m \\\n\u001b[0;32m    156\u001b[0m     (support_count(advMixedStrategy\u001b[38;5;241m.\u001b[39m_strategyProbs))\n\u001b[0;32m    157\u001b[0m algorithm \u001b[38;5;241m=\u001b[39m ReinforceAlgorithm(\n\u001b[0;32m    158\u001b[0m     game, neuralNet, numberIterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, numberEpisodes\u001b[38;5;241m=\u001b[39mnumberEpisodes, discountFactor\u001b[38;5;241m=\u001b[39mgl\u001b[38;5;241m.\u001b[39mgamma)\n\u001b[1;32m--> 159\u001b[0m \u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprint_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverge_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m a \u001b[38;5;241m=\u001b[39m algorithm\u001b[38;5;241m.\u001b[39mreturns[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mneuralNet\u001b[38;5;241m.\u001b[39mnn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is trained against \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(advMixedStrategy)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\EquiLearn\\PGM_base\\learningBase.py:118\u001b[0m, in \u001b[0;36mReinforceAlgorithm.solver\u001b[1;34m(self, print_step, options, converge_break)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mappend([])\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stage \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_stage_onwards\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumberEpisodes\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mprob_break_limit_ln\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobBreakLn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconverge_break\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_save\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\EquiLearn\\PGM_base\\learningBase.py:149\u001b[0m, in \u001b[0;36mReinforceAlgorithm.learn_stage_onwards\u001b[1;34m(self, iter, stage, episodes, print_step, prob_break_limit_ln, options, lr, just_stage, write_save)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m    148\u001b[0m     prevState \u001b[38;5;241m=\u001b[39m state\n\u001b[1;32m--> 149\u001b[0m     normPrevState \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalizeState\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprevState\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy(normPrevState)\n\u001b[0;32m    151\u001b[0m     distAction \u001b[38;5;241m=\u001b[39m Categorical(probs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "number_rounds=50\n",
    "\n",
    "# l1=NNBase(lr=gl.lr, num_input=gl.totalStages+2+gl.adversaryHistroy,num_actions=gl.numActions,adv_hist=gl.adversaryHistroy,action_step=gl.actionStep)\n",
    "# l1.load(\"low,1684212411\")\n",
    "# l2=NNBase(lr=gl.lr, num_input=gl.totalStages+2+gl.adversaryHistroy,num_actions=gl.numActions,adv_hist=gl.adversaryHistroy,action_step=gl.actionStep)\n",
    "# l2.load(\"low,1684332617\")\n",
    "# h1=NNBase(lr=gl.lr, num_input=gl.totalStages+2+gl.adversaryHistroy,num_actions=gl.numActions,adv_hist=gl.adversaryHistroy,action_step=gl.actionStep)\n",
    "# h1.load(\"high,1684261807\")\n",
    "equilibria = []\n",
    "low_cost_players = [\n",
    "                    Strategy(StrategyType.static, NNorFunc=em.myopic, name=\"myopic\"), \n",
    "                    Strategy(StrategyType.static, NNorFunc=em.const, name=\"const\", firstPrice=132), \n",
    "                    Strategy(StrategyType.static, NNorFunc=em.guess, name=\"guess\", firstPrice=132)\n",
    "                   ]\n",
    "high_cost_players = [\n",
    "                    Strategy(StrategyType.static, NNorFunc=em.myopic, name=\"myopic\"), \n",
    "                    Strategy(StrategyType.static, NNorFunc=em.const, name=\"const\", firstPrice=132), \n",
    "                    Strategy(StrategyType.static, NNorFunc=em.guess, name=\"guess\", firstPrice=132)\n",
    "                   ]\n",
    "bimatrixGame = BG.BimatrixGame(low_cost_players, high_cost_players)\n",
    "    # bimatrixGame.reset_matrix()\n",
    "bimatrixGame.fill_matrix()\n",
    "low_cost_probabilities, high_cost_probabilities, low_cost_payoff, high_cost_payoff = bimatrixGame.compute_equilibria()\n",
    "for round in range(number_rounds):\n",
    "    print(\"Round\", round, \" of \", number_rounds)\n",
    "\n",
    "    update = False\n",
    "\n",
    "\n",
    "    acceptable, agentPayoffs, advPayoffs, low_cost_player = BG.training([gl.lowCost, gl.highCost], advMixedStrategy=MixedStrategy(\n",
    "        strategiesList=high_cost_players, probablitiesArray=high_cost_probabilities), targetPayoff=low_cost_payoff)\n",
    "    if acceptable:\n",
    "        update = True\n",
    "        low_cost_players.append(low_cost_player)\n",
    "        bimatrixGame.add_low_cost_row(agentPayoffs, advPayoffs)\n",
    "        equilibria.append(\n",
    "            [low_cost_probabilities, high_cost_probabilities, low_cost_payoff, high_cost_payoff])\n",
    "        print(f\"low cost player {low_cost_player._name} added\")\n",
    "\n",
    "        low_cost_probabilities, high_cost_probabilities, low_cost_payoff, high_cost_payoff = bimatrixGame.compute_equilibria()\n",
    "\n",
    "\n",
    "    acceptable, agentPayoffs, advPayoffs, high_cost_player = BG.training(\n",
    "        [gl.highCost, gl.lowCost], advMixedStrategy=MixedStrategy(probablitiesArray=low_cost_probabilities, strategiesList=low_cost_players), targetPayoff=high_cost_payoff)\n",
    "\n",
    "    if acceptable:\n",
    "        update = True\n",
    "        high_cost_players.append(high_cost_player)\n",
    "        bimatrixGame.add_high_cost_col(advPayoffs, agentPayoffs)\n",
    "        equilibria.append(\n",
    "            [low_cost_probabilities, high_cost_probabilities, low_cost_payoff, high_cost_payoff])\n",
    "        print(f\"high cost player {high_cost_player._name} added\")\n",
    "\n",
    "        low_cost_probabilities, high_cost_probabilities, low_cost_payoff, high_cost_payoff = bimatrixGame.compute_equilibria()\n",
    "\n",
    "    if update:\n",
    "        gl.numEpisodes = gl.numEpisodesReset\n",
    "    else:\n",
    "        gl.numEpisodes += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myopic\n",
      "const\n",
      "guess\n",
      "low,1684386202\n",
      "low,1684484716\n",
      "low,1684557152\n",
      "low,1684856358\n",
      "low,1685028503\n"
     ]
    }
   ],
   "source": [
    "for strategy in bimatrixGame._strategies_low:\n",
    "    print(strategy._name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myopic\n",
      "const\n",
      "guess\n",
      "high,1684424924\n",
      "high,1684821735\n"
     ]
    }
   ],
   "source": [
    "for strategy in bimatrixGame._strategies_high:\n",
    "    print(strategy._name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum=0\n",
    "for i in range(26):\n",
    "     sum+=3000*(i**1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1694226.0901287887"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum2=0\n",
    "for i in range(26):\n",
    "    sum2+=4000*i\n",
    "sum2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
