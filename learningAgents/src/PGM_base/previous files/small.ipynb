{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learningBase import ReinforceAlgorithm\n",
    "import environmentModelBase as model\n",
    "from neuralNetworkSimple import NNBase\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "const95= model.Strategy(model.StrategyType.static, model.const,name=\"const95\", firstPrice=95 )\n",
    "actionStep=3\n",
    "adv= model.Adversary()\n",
    "adv._strategies.append(const95)\n",
    "adv._strategyProbs=torch.ones(1)\n",
    "game = model.Model(totalDemand = 400, \n",
    "               tupleCosts = (57, 71),\n",
    "              totalStages = 25, adversary=adv, stateAdvHistory=1, actionStep=actionStep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperParams=[0.000005, 1, 0]\n",
    "codeParams=[1, 10000, 1, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy reset\n",
      "----------------------------------------\n",
      "iter  0  stage  24  ep  29999   adversary:  const95-1.0,\n",
      "  actions:  tensor([ 3, 42,  3, 12,  0,  0,  0,  6,  6, 36,  6, 15,  6, 30,  9,  0,  3,  6,\n",
      "         3, 42,  3,  6,  6,  3,  0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.2113, 0.2113, 0.2113, 0.2113, 0.2113, 0.2113, 0.2113, 0.2113, 0.2113,\n",
      "        0.2113, 0.2113, 0.2113, 0.2113, 0.2113, 0.2113, 0.2113, 0.2113, 0.2113,\n",
      "        0.2113, 0.2113, 0.2113, 0.2113, 0.2113, 0.2113, 0.2113]) return=  62979.42909757305\n",
      "probs of actions:  tensor([0.2569, 0.0078, 0.2615, 0.0434, 0.2830, 0.2847, 0.2901, 0.1838, 0.1792,\n",
      "        0.0102, 0.1705, 0.0234, 0.1853, 0.0105, 0.0954, 0.2553, 0.2298, 0.2041,\n",
      "        0.2421, 0.0096, 0.2458, 0.1886, 0.1877, 0.2351, 0.3951],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5103, 0.2316, 0.4602, 0.3599, 0.3409, 0.2840, 0.2447, 0.2136, 0.2075,\n",
      "        0.0771, 0.2730, 0.2320, 0.2574, 0.1532, 0.2833, 0.2729, 0.2361, 0.2150,\n",
      "        0.2113, 0.0243, 0.2864, 0.2509, 0.2349, 0.2260, 0.2113])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  0  stage  23  ep  29999   adversary:  const95-1.0,\n",
      "  actions:  tensor([3, 0, 0, 3, 0, 0, 9, 0, 6, 3, 0, 0, 0, 6, 0, 0, 3, 6, 3, 0, 3, 6, 3, 3,\n",
      "        3])\n",
      "loss=  tensor(0.0080, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.3337, 0.3337, 0.3337, 0.3337, 0.3337, 0.3337, 0.3337, 0.3337, 0.3337,\n",
      "        0.3337, 0.3337, 0.3337, 0.3337, 0.3337, 0.3337, 0.3337, 0.3337, 0.3337,\n",
      "        0.3337, 0.3337, 0.3337, 0.3337, 0.3337, 0.3337, 0.1635]) return=  52462.04122834946\n",
      "probs of actions:  tensor([0.4363, 0.2692, 0.2836, 0.4190, 0.2666, 0.2884, 0.0413, 0.2971, 0.2062,\n",
      "        0.4231, 0.2851, 0.2973, 0.2680, 0.2101, 0.2771, 0.2523, 0.4025, 0.2236,\n",
      "        0.4208, 0.2826, 0.4316, 0.2136, 0.4160, 0.4585, 0.4117],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5103, 0.4080, 0.3295, 0.2753, 0.2467, 0.2186, 0.1905, 0.2040, 0.1846,\n",
      "        0.1886, 0.1840, 0.1737, 0.1661, 0.1569, 0.1685, 0.1623, 0.1568, 0.1567,\n",
      "        0.1674, 0.1683, 0.1612, 0.1600, 0.1700, 0.1693, 0.1688])\n",
      "finalReturns:  tensor([0.0043, 0.0052])\n",
      "----------------------------------------\n",
      "iter  0  stage  23  ep  59999   adversary:  const95-1.0,\n",
      "  actions:  tensor([ 6,  6,  0,  6,  6,  3,  0,  3, 27,  0,  6,  3,  0,  3,  3,  6,  3,  3,\n",
      "         0,  6,  3,  3,  6,  3,  0])\n",
      "loss=  tensor(0.0133, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.3451, 0.3451, 0.3451, 0.3451, 0.3451, 0.3451, 0.3451, 0.3451, 0.3451,\n",
      "        0.3451, 0.3451, 0.3451, 0.3451, 0.3451, 0.3451, 0.3451, 0.3451, 0.3451,\n",
      "        0.3451, 0.3451, 0.3451, 0.3451, 0.3451, 0.3451, 0.1684]) return=  55844.69544871044\n",
      "probs of actions:  tensor([0.3940, 0.4652, 0.2322, 0.4760, 0.4480, 0.2834, 0.1864, 0.2675, 0.0010,\n",
      "        0.1776, 0.4118, 0.2668, 0.1869, 0.2454, 0.2470, 0.4796, 0.2581, 0.2561,\n",
      "        0.2266, 0.4460, 0.2829, 0.2552, 0.4303, 0.1324, 0.6613],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5076, 0.4140, 0.3537, 0.2891, 0.2624, 0.2459, 0.2257, 0.2028, 0.1216,\n",
      "        0.2433, 0.2126, 0.2095, 0.1994, 0.1839, 0.1797, 0.1738, 0.1805, 0.1771,\n",
      "        0.1755, 0.1638, 0.1729, 0.1715, 0.1677, 0.1758, 0.1746])\n",
      "finalReturns:  tensor([0.0053, 0.0062])\n",
      "----------------------------------------\n",
      "iter  0  stage  23  ep  89999   adversary:  const95-1.0,\n",
      "  actions:  tensor([6, 6, 9, 6, 6, 6, 6, 3, 6, 6, 9, 9, 6, 6, 6, 6, 9, 6, 6, 6, 6, 6, 0, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0300, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.3550, 0.3550, 0.3550, 0.3550, 0.3550, 0.3550, 0.3550, 0.3550, 0.3550,\n",
      "        0.3550, 0.3550, 0.3550, 0.3550, 0.3550, 0.3550, 0.3550, 0.3550, 0.3550,\n",
      "        0.3550, 0.3550, 0.3550, 0.3550, 0.3550, 0.3550, 0.1725]) return=  59413.20879318385\n",
      "probs of actions:  tensor([0.4593, 0.5525, 0.0732, 0.5638, 0.5460, 0.4904, 0.5642, 0.1712, 0.5676,\n",
      "        0.5611, 0.0841, 0.0820, 0.5512, 0.5398, 0.5214, 0.5633, 0.0787, 0.5516,\n",
      "        0.4780, 0.5224, 0.4695, 0.5310, 0.2116, 0.0809, 0.8957],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5076, 0.4140, 0.3456, 0.3139, 0.2801, 0.2560, 0.2386, 0.2286, 0.2097,\n",
      "        0.2047, 0.1964, 0.2005, 0.2080, 0.2034, 0.2000, 0.1975, 0.1911, 0.2009,\n",
      "        0.1982, 0.1961, 0.1946, 0.1934, 0.1962, 0.1744, 0.1917])\n",
      "finalReturns:  tensor([0.0111, 0.0192])\n",
      "----------------------------------------\n",
      "iter  0  stage  22  ep  29999   adversary:  const95-1.0,\n",
      "  actions:  tensor([15,  9,  6,  6,  0,  6,  9,  6,  6,  9,  9,  9,  6,  6,  9,  6,  6,  9,\n",
      "        12,  6,  9,  6,  9,  6,  0])\n",
      "loss=  tensor(0.0575, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5794, 0.5794, 0.5794, 0.5794, 0.5794, 0.5794, 0.5794, 0.5794, 0.5794,\n",
      "        0.5794, 0.5794, 0.5794, 0.5794, 0.5794, 0.5794, 0.5794, 0.5794, 0.5794,\n",
      "        0.5794, 0.5794, 0.5794, 0.5794, 0.5794, 0.3707, 0.1791]) return=  61352.37296508392\n",
      "probs of actions:  tensor([0.0049, 0.2886, 0.5194, 0.5667, 0.0384, 0.5416, 0.2841, 0.5163, 0.5702,\n",
      "        0.3191, 0.3114, 0.2961, 0.5570, 0.5329, 0.3011, 0.5474, 0.5238, 0.2829,\n",
      "        0.0357, 0.5463, 0.2982, 0.5444, 0.3465, 0.6957, 0.9113],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4391, 0.3796, 0.3262, 0.2924, 0.2469, 0.2275, 0.2283, 0.2184,\n",
      "        0.2066, 0.2082, 0.2093, 0.2147, 0.2084, 0.1992, 0.2071, 0.2027, 0.1950,\n",
      "        0.1931, 0.2141, 0.2035, 0.2103, 0.2006, 0.2081, 0.2071])\n",
      "finalReturns:  tensor([0.0365, 0.0446, 0.0280])\n",
      "----------------------------------------\n",
      "iter  0  stage  22  ep  59999   adversary:  const95-1.0,\n",
      "  actions:  tensor([ 9, 12,  3,  9,  9,  9,  6, 12,  9,  6,  9,  3,  6, 12,  9,  9,  9,  6,\n",
      "         9,  6,  9, 12,  9,  9,  0])\n",
      "loss=  tensor(0.0352, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.6075, 0.6075, 0.6075, 0.6075, 0.6075, 0.6075, 0.6075, 0.6075, 0.6075,\n",
      "        0.6075, 0.6075, 0.6075, 0.6075, 0.6075, 0.6075, 0.6075, 0.6075, 0.6075,\n",
      "        0.6075, 0.6075, 0.6075, 0.6075, 0.6075, 0.3862, 0.1857]) return=  62546.458717917136\n",
      "probs of actions:  tensor([0.5353, 0.0431, 0.0359, 0.5870, 0.6315, 0.5678, 0.2807, 0.0454, 0.5922,\n",
      "        0.2558, 0.6168, 0.0411, 0.2766, 0.0482, 0.5834, 0.6188, 0.5877, 0.2788,\n",
      "        0.5710, 0.2691, 0.5920, 0.0468, 0.7153, 0.6523, 0.9644],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4130, 0.3777, 0.3100, 0.2840, 0.2653, 0.2561, 0.2279, 0.2361,\n",
      "        0.2347, 0.2186, 0.2243, 0.2066, 0.1916, 0.2085, 0.2095, 0.2104, 0.2155,\n",
      "        0.2045, 0.2110, 0.2012, 0.1977, 0.2132, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([0.0399, 0.0480, 0.0355])\n",
      "----------------------------------------\n",
      "iter  0  stage  22  ep  89999   adversary:  const95-1.0,\n",
      "  actions:  tensor([ 3,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 15,  6,  9,  9,\n",
      "        12,  9,  9,  9,  6,  9,  0])\n",
      "loss=  tensor(0.0937, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.6155, 0.6155, 0.6155, 0.6155, 0.6155, 0.6155, 0.6155, 0.6155, 0.6155,\n",
      "        0.6155, 0.6155, 0.6155, 0.6155, 0.6155, 0.6155, 0.6155, 0.6155, 0.6155,\n",
      "        0.6155, 0.6155, 0.6155, 0.6155, 0.6155, 0.3906, 0.1875]) return=  63514.70077728213\n",
      "probs of actions:  tensor([0.0296, 0.7851, 0.7652, 0.7515, 0.7909, 0.7315, 0.7606, 0.7464, 0.7608,\n",
      "        0.7813, 0.7735, 0.7258, 0.7492, 0.7762, 0.0049, 0.1191, 0.7491, 0.7561,\n",
      "        0.0529, 0.7435, 0.7497, 0.7477, 0.0676, 0.8504, 0.9806],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5103, 0.3999, 0.3478, 0.3110, 0.2848, 0.2658, 0.2520, 0.2419, 0.2345,\n",
      "        0.2290, 0.2249, 0.2218, 0.2196, 0.2179, 0.2022, 0.2346, 0.2185, 0.2171,\n",
      "        0.2097, 0.2223, 0.2199, 0.2181, 0.2213, 0.2088, 0.2179])\n",
      "finalReturns:  tensor([0.0324, 0.0360, 0.0304])\n",
      "----------------------------------------\n",
      "iter  0  stage  22  ep  119999   adversary:  const95-1.0,\n",
      "  actions:  tensor([ 9,  9,  9,  9, 12,  9,  6,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
      "         9,  9,  9,  3,  9,  9,  0])\n",
      "loss=  tensor(0.0097, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5764, 0.5764, 0.5764, 0.5764, 0.5764, 0.5764, 0.5764, 0.5764, 0.5764,\n",
      "        0.5764, 0.5764, 0.5764, 0.5764, 0.5764, 0.5764, 0.5764, 0.5764, 0.5764,\n",
      "        0.5764, 0.5764, 0.5764, 0.5764, 0.5764, 0.3690, 0.1784]) return=  63600.55185293454\n",
      "probs of actions:  tensor([0.7547, 0.8389, 0.8221, 0.8118, 0.0509, 0.7931, 0.0775, 0.8037, 0.8180,\n",
      "        0.8343, 0.8274, 0.7840, 0.8068, 0.8295, 0.8006, 0.8319, 0.8038, 0.8153,\n",
      "        0.7895, 0.8013, 0.8043, 0.0150, 0.8904, 0.9033, 0.9875],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2854, 0.2788, 0.2660, 0.2413, 0.2340,\n",
      "        0.2286, 0.2246, 0.2216, 0.2194, 0.2177, 0.2165, 0.2156, 0.2149, 0.2144,\n",
      "        0.2140, 0.2137, 0.2135, 0.2205, 0.1993, 0.2026, 0.2132])\n",
      "finalReturns:  tensor([0.0388, 0.0469, 0.0348])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  0  stage  22  ep  149999   adversary:  const95-1.0,\n",
      "  actions:  tensor([ 9,  9,  9,  9, 12,  9,  9,  9,  9,  6, 12,  9,  9,  9,  0,  9,  9,  9,\n",
      "         9,  9,  9,  9,  9,  9,  0])\n",
      "loss=  tensor(0.0138, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.6015, 0.6015, 0.6015, 0.6015, 0.6015, 0.6015, 0.6015, 0.6015, 0.6015,\n",
      "        0.6015, 0.6015, 0.6015, 0.6015, 0.6015, 0.6015, 0.6015, 0.6015, 0.6015,\n",
      "        0.6015, 0.6015, 0.6015, 0.6015, 0.6015, 0.3829, 0.1843]) return=  63367.220552996965\n",
      "probs of actions:  tensor([0.7048, 0.7939, 0.7766, 0.7712, 0.0921, 0.7481, 0.7808, 0.7570, 0.7766,\n",
      "        0.0626, 0.0909, 0.7358, 0.7653, 0.7887, 0.0541, 0.7934, 0.7553, 0.7736,\n",
      "        0.7408, 0.7540, 0.7546, 0.7588, 0.8251, 0.8820, 0.9942],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2854, 0.2788, 0.2615, 0.2489, 0.2396,\n",
      "        0.2373, 0.2142, 0.2257, 0.2225, 0.2200, 0.2263, 0.1960, 0.2001, 0.2033,\n",
      "        0.2056, 0.2074, 0.2088, 0.2098, 0.2105, 0.2111, 0.2196])\n",
      "finalReturns:  tensor([0.0397, 0.0478, 0.0354])\n",
      "----------------------------------------\n",
      "iter  0  stage  21  ep  29999   adversary:  const95-1.0,\n",
      "  actions:  tensor([ 9,  6,  9,  9, 12,  9, 12,  9,  9,  9,  9,  9,  9,  9,  6,  9,  6,  9,\n",
      "         9, 12, 12,  9,  9,  9,  0])\n",
      "loss=  tensor(0.0525, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.8064, 0.8064, 0.8064, 0.8064, 0.8064, 0.8064, 0.8064, 0.8064, 0.8064,\n",
      "        0.8064, 0.8064, 0.8064, 0.8064, 0.8064, 0.8064, 0.8064, 0.8064, 0.8064,\n",
      "        0.8064, 0.8064, 0.8064, 0.8064, 0.5759, 0.3687, 0.1783]) return=  63900.06666813643\n",
      "probs of actions:  tensor([0.7450, 0.0332, 0.8032, 0.7765, 0.1419, 0.7829, 0.1524, 0.7768, 0.7725,\n",
      "        0.7943, 0.7975, 0.7758, 0.7730, 0.7909, 0.0445, 0.7913, 0.0394, 0.7892,\n",
      "        0.7642, 0.1584, 0.1688, 0.7797, 0.7790, 0.8710, 0.9861],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4238, 0.3523, 0.3142, 0.2808, 0.2754, 0.2527, 0.2547, 0.2439,\n",
      "        0.2359, 0.2300, 0.2257, 0.2224, 0.2200, 0.2227, 0.2098, 0.2150, 0.2041,\n",
      "        0.2063, 0.2016, 0.2099, 0.2225, 0.2200, 0.2182, 0.2250])\n",
      "finalReturns:  tensor([0.0792, 0.0873, 0.0745, 0.0466])\n",
      "----------------------------------------\n",
      "iter  0  stage  21  ep  59999   adversary:  const95-1.0,\n",
      "  actions:  tensor([ 9,  9,  9, 12,  9, 12,  9, 12,  9,  9,  9,  9, 12,  9,  9,  9,  3,  9,\n",
      "         9, 12,  9,  9, 12,  9,  0])\n",
      "loss=  tensor(0.1608, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.7875, 0.7875, 0.7875, 0.7875, 0.7875, 0.7875, 0.7875, 0.7875, 0.7875,\n",
      "        0.7875, 0.7875, 0.7875, 0.7875, 0.7875, 0.7875, 0.7875, 0.7875, 0.7875,\n",
      "        0.7875, 0.7875, 0.7875, 0.7875, 0.5642, 0.3623, 0.1756]) return=  64562.60586840481\n",
      "probs of actions:  tensor([0.5976, 0.6168, 0.6466, 0.3209, 0.6422, 0.2996, 0.6183, 0.3199, 0.6098,\n",
      "        0.6397, 0.6462, 0.6327, 0.3156, 0.6326, 0.6199, 0.6341, 0.0044, 0.6398,\n",
      "        0.6093, 0.3274, 0.5959, 0.5767, 0.3889, 0.7362, 0.9915],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3143, 0.2999, 0.2705, 0.2679, 0.2472, 0.2506,\n",
      "        0.2409, 0.2337, 0.2284, 0.2181, 0.2287, 0.2247, 0.2217, 0.2267, 0.2038,\n",
      "        0.2060, 0.2014, 0.2160, 0.2152, 0.2083, 0.2213, 0.2272])\n",
      "finalReturns:  tensor([0.0845, 0.0926, 0.0862, 0.0516])\n",
      "----------------------------------------\n",
      "iter  0  stage  21  ep  89999   adversary:  const95-1.0,\n",
      "  actions:  tensor([ 9,  9, 12, 12, 12,  9, 12,  6,  9,  9,  9,  9,  9, 12, 12, 12, 12, 12,\n",
      "         9,  9, 12, 12, 12,  9,  0])\n",
      "loss=  tensor(0.1576, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.8257, 0.8257, 0.8257, 0.8257, 0.8257, 0.8257, 0.8257, 0.8257, 0.8257,\n",
      "        0.8257, 0.8257, 0.8257, 0.8257, 0.8257, 0.8257, 0.8257, 0.8257, 0.8257,\n",
      "        0.8257, 0.8257, 0.8257, 0.8257, 0.5877, 0.3752, 0.1811]) return=  65901.80171251376\n",
      "probs of actions:  tensor([0.3774, 0.3514, 0.5611, 0.5640, 0.5644, 0.3992, 0.5829, 0.0208, 0.3723,\n",
      "        0.4031, 0.4069, 0.4050, 0.3920, 0.5676, 0.5599, 0.5592, 0.5817, 0.5480,\n",
      "        0.3768, 0.3752, 0.5988, 0.6690, 0.6565, 0.4812, 0.9943],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3550, 0.3230, 0.2999, 0.2895, 0.2629, 0.2668, 0.2419,\n",
      "        0.2344, 0.2289, 0.2248, 0.2218, 0.2132, 0.2187, 0.2229, 0.2260, 0.2284,\n",
      "        0.2365, 0.2305, 0.2197, 0.2236, 0.2266, 0.2351, 0.2375])\n",
      "finalReturns:  tensor([0.0972, 0.1116, 0.0974, 0.0565])\n",
      "----------------------------------------\n",
      "iter  0  stage  21  ep  119999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12,  9,  9,  9, 12, 12, 12,  6, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0851, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.8567, 0.8567, 0.8567, 0.8567, 0.8567, 0.8567, 0.8567, 0.8567, 0.8567,\n",
      "        0.8567, 0.8567, 0.8567, 0.8567, 0.8567, 0.8567, 0.8567, 0.8567, 0.8567,\n",
      "        0.8567, 0.8567, 0.8567, 0.8567, 0.6066, 0.3857, 0.1854]) return=  67404.68062374518\n",
      "probs of actions:  tensor([0.7116, 0.2010, 0.2402, 0.2483, 0.7271, 0.7151, 0.7406, 0.0122, 0.7393,\n",
      "        0.7099, 0.7133, 0.7056, 0.7096, 0.7272, 0.7157, 0.7208, 0.7393, 0.7119,\n",
      "        0.7350, 0.7298, 0.7515, 0.8319, 0.8060, 0.6590, 0.9953],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4292, 0.3682, 0.3255, 0.2889, 0.2751, 0.2649, 0.2682, 0.2366,\n",
      "        0.2364, 0.2362, 0.2360, 0.2359, 0.2358, 0.2358, 0.2357, 0.2357, 0.2357,\n",
      "        0.2357, 0.2356, 0.2356, 0.2356, 0.2356, 0.2356, 0.2500])\n",
      "finalReturns:  tensor([0.1002, 0.1146, 0.0999, 0.0646])\n",
      "----------------------------------------\n",
      "iter  0  stage  21  ep  149999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12,  9, 12, 12, 12, 12, 12,  9, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12,  9, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0369, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.8380, 0.8380, 0.8380, 0.8380, 0.8380, 0.8380, 0.8380, 0.8380, 0.8380,\n",
      "        0.8380, 0.8380, 0.8380, 0.8380, 0.8380, 0.8380, 0.8380, 0.8380, 0.8380,\n",
      "        0.8380, 0.8380, 0.8380, 0.8380, 0.5952, 0.3794, 0.1828]) return=  68049.04485884288\n",
      "probs of actions:  tensor([0.8436, 0.8976, 0.1146, 0.8559, 0.8613, 0.8556, 0.8694, 0.8624, 0.1145,\n",
      "        0.8490, 0.8528, 0.8457, 0.8438, 0.8603, 0.8480, 0.8552, 0.8680, 0.8512,\n",
      "        0.8645, 0.8594, 0.1076, 0.9306, 0.9117, 0.8245, 0.9968],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3775, 0.3257, 0.3019, 0.2846, 0.2719, 0.2626, 0.2620,\n",
      "        0.2430, 0.2411, 0.2397, 0.2387, 0.2379, 0.2373, 0.2369, 0.2366, 0.2363,\n",
      "        0.2361, 0.2360, 0.2422, 0.2284, 0.2302, 0.2315, 0.2469])\n",
      "finalReturns:  tensor([0.0990, 0.1134, 0.0991, 0.0641])\n",
      "----------------------------------------\n",
      "iter  0  stage  21  ep  179999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12,  9, 12, 12, 12, 12,  9, 12,  9, 12, 12, 12, 12,  9,\n",
      "        12, 12,  9, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0208, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.8275, 0.8275, 0.8275, 0.8275, 0.8275, 0.8275, 0.8275, 0.8275, 0.8275,\n",
      "        0.8275, 0.8275, 0.8275, 0.8275, 0.8275, 0.8275, 0.8275, 0.8275, 0.8275,\n",
      "        0.8275, 0.8275, 0.8275, 0.8275, 0.5888, 0.3759, 0.1813]) return=  67652.69436488167\n",
      "probs of actions:  tensor([0.9003, 0.9414, 0.9204, 0.9110, 0.9162, 0.0734, 0.9213, 0.9156, 0.9173,\n",
      "        0.9058, 0.0760, 0.9050, 0.0810, 0.9148, 0.9034, 0.9105, 0.9194, 0.0766,\n",
      "        0.9171, 0.9125, 0.0652, 0.9634, 0.9505, 0.8924, 0.9977],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2955, 0.2673, 0.2592, 0.2532,\n",
      "        0.2487, 0.2517, 0.2354, 0.2418, 0.2280, 0.2299, 0.2313, 0.2324, 0.2395,\n",
      "        0.2264, 0.2287, 0.2367, 0.2243, 0.2271, 0.2292, 0.2452])\n",
      "finalReturns:  tensor([0.0983, 0.1127, 0.0986, 0.0639])\n",
      "----------------------------------------\n",
      "iter  0  stage  20  ep  29999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0203, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 0.7825, 0.5612, 0.3606, 0.1749]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9466, 0.9715, 0.9604, 0.9532, 0.9564, 0.9554, 0.9586, 0.9567, 0.9564,\n",
      "        0.9497, 0.9536, 0.9498, 0.9472, 0.9553, 0.9484, 0.9530, 0.9583, 0.9523,\n",
      "        0.9568, 0.9543, 0.9625, 0.9825, 0.9769, 0.9455, 0.9969],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.1613, 0.1757, 0.1609, 0.1255, 0.0753])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  0  stage  20  ep  59999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0137, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 0.7825, 0.5612, 0.3606, 0.1749]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9620, 0.9807, 0.9729, 0.9670, 0.9694, 0.9691, 0.9710, 0.9699, 0.9693,\n",
      "        0.9641, 0.9674, 0.9646, 0.9624, 0.9685, 0.9634, 0.9670, 0.9707, 0.9663,\n",
      "        0.9698, 0.9678, 0.9753, 0.9893, 0.9836, 0.9615, 0.9975],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.1613, 0.1757, 0.1609, 0.1255, 0.0753])\n",
      "----------------------------------------\n",
      "iter  0  stage  20  ep  89999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0081, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 0.7825, 0.5612, 0.3606, 0.1749]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9754, 0.9882, 0.9832, 0.9790, 0.9807, 0.9806, 0.9816, 0.9810, 0.9803,\n",
      "        0.9769, 0.9792, 0.9774, 0.9758, 0.9799, 0.9765, 0.9789, 0.9813, 0.9785,\n",
      "        0.9809, 0.9793, 0.9857, 0.9934, 0.9898, 0.9773, 0.9982],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.1613, 0.1757, 0.1609, 0.1255, 0.0753])\n",
      "----------------------------------------\n",
      "iter  0  stage  20  ep  119999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0072, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 0.7825, 0.5612, 0.3606, 0.1749]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9778, 0.9895, 0.9850, 0.9812, 0.9826, 0.9827, 0.9834, 0.9828, 0.9822,\n",
      "        0.9793, 0.9814, 0.9798, 0.9781, 0.9820, 0.9790, 0.9810, 0.9832, 0.9807,\n",
      "        0.9829, 0.9813, 0.9868, 0.9942, 0.9910, 0.9807, 0.9991],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.1613, 0.1757, 0.1609, 0.1255, 0.0753])\n",
      "----------------------------------------\n",
      "iter  0  stage  20  ep  149999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0048, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 0.7825, 0.5612, 0.3606, 0.1749]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9843, 0.9929, 0.9897, 0.9868, 0.9879, 0.9880, 0.9884, 0.9880, 0.9875,\n",
      "        0.9853, 0.9870, 0.9858, 0.9845, 0.9873, 0.9851, 0.9867, 0.9882, 0.9864,\n",
      "        0.9880, 0.9869, 0.9912, 0.9964, 0.9940, 0.9866, 0.9994],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.1613, 0.1757, 0.1609, 0.1255, 0.0753])\n",
      "----------------------------------------\n",
      "iter  0  stage  20  ep  179999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0030, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 0.7825, 0.5612, 0.3606, 0.1749]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9895, 0.9955, 0.9934, 0.9913, 0.9921, 0.9922, 0.9924, 0.9922, 0.9918,\n",
      "        0.9902, 0.9915, 0.9906, 0.9896, 0.9916, 0.9901, 0.9913, 0.9923, 0.9911,\n",
      "        0.9922, 0.9914, 0.9946, 0.9980, 0.9964, 0.9908, 0.9998],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.1613, 0.1757, 0.1609, 0.1255, 0.0753])\n",
      "----------------------------------------\n",
      "iter  0  stage  20  ep  209999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0035, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 0.7825, 0.5612, 0.3606, 0.1749]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9880, 0.9947, 0.9924, 0.9900, 0.9908, 0.9909, 0.9912, 0.9910, 0.9905,\n",
      "        0.9885, 0.9903, 0.9891, 0.9878, 0.9903, 0.9884, 0.9900, 0.9912, 0.9897,\n",
      "        0.9910, 0.9901, 0.9938, 0.9978, 0.9962, 0.9878, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.1613, 0.1757, 0.1609, 0.1255, 0.0753])\n",
      "----------------------------------------\n",
      "iter  0  stage  20  ep  239999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0035, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 0.7825, 0.5612, 0.3606, 0.1749]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9881, 0.9948, 0.9924, 0.9900, 0.9908, 0.9911, 0.9913, 0.9911, 0.9906,\n",
      "        0.9887, 0.9903, 0.9892, 0.9880, 0.9903, 0.9886, 0.9900, 0.9913, 0.9897,\n",
      "        0.9911, 0.9902, 0.9942, 0.9978, 0.9961, 0.9881, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.1613, 0.1757, 0.1609, 0.1255, 0.0753])\n",
      "----------------------------------------\n",
      "iter  0  stage  19  ep  29999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  9, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0048, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.1986, 1.1986, 1.1986, 1.1986, 1.1986, 1.1986, 1.1986, 1.1986, 1.1986,\n",
      "        1.1986, 1.1986, 1.1986, 1.1986, 1.1986, 1.1986, 1.1986, 1.1986, 1.1986,\n",
      "        1.1986, 1.1986, 0.9485, 0.7275, 0.5271, 0.3416, 0.1669]) return=  68460.61730250808\n",
      "probs of actions:  tensor([0.9910, 0.9962, 0.9944, 0.9924, 0.9931, 0.9932, 0.9934, 0.9933, 0.9929,\n",
      "        0.9913, 0.0065, 0.9918, 0.9906, 0.9926, 0.9912, 0.9925, 0.9934, 0.9922,\n",
      "        0.9933, 0.9931, 0.9957, 0.9986, 0.9976, 0.9906, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2542, 0.2372, 0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359,\n",
      "        0.2358, 0.2358, 0.2357, 0.2357, 0.2357, 0.2357, 0.2500])\n",
      "finalReturns:  tensor([0.2299, 0.2443, 0.2296, 0.1942, 0.1441, 0.0832])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  0  stage  19  ep  59999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12,  9, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0037, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.1999, 1.1999, 1.1999, 1.1999, 1.1999, 1.1999, 1.1999, 1.1999, 1.1999,\n",
      "        1.1999, 1.1999, 1.1999, 1.1999, 1.1999, 1.1999, 1.1999, 1.1999, 1.1999,\n",
      "        1.1999, 1.1999, 0.9493, 0.7280, 0.5275, 0.3418, 0.1670]) return=  68453.26034712071\n",
      "probs of actions:  tensor([0.9926, 0.9969, 0.9955, 0.9938, 0.9944, 0.9945, 0.9947, 0.0049, 0.9943,\n",
      "        0.9929, 0.9941, 0.9932, 0.9924, 0.9940, 0.9928, 0.9939, 0.9947, 0.9937,\n",
      "        0.9946, 0.9948, 0.9966, 0.9989, 0.9982, 0.9924, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2714, 0.2498,\n",
      "        0.2462, 0.2436, 0.2416, 0.2401, 0.2389, 0.2381, 0.2375, 0.2370, 0.2367,\n",
      "        0.2364, 0.2362, 0.2360, 0.2359, 0.2358, 0.2358, 0.2501])\n",
      "finalReturns:  tensor([0.2300, 0.2444, 0.2297, 0.1943, 0.1441, 0.0832])\n",
      "----------------------------------------\n",
      "iter  0  stage  19  ep  89999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0027, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 0.9500, 0.7285, 0.5277, 0.3420, 0.1670]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9943, 0.9977, 0.9966, 0.9953, 0.9958, 0.9958, 0.9960, 0.9959, 0.9956,\n",
      "        0.9946, 0.9956, 0.9948, 0.9942, 0.9954, 0.9945, 0.9954, 0.9960, 0.9952,\n",
      "        0.9959, 0.9963, 0.9976, 0.9993, 0.9987, 0.9942, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.2301, 0.2445, 0.2297, 0.1944, 0.1441, 0.0832])\n",
      "----------------------------------------\n",
      "iter  0  stage  19  ep  119999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0022, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 0.9500, 0.7285, 0.5277, 0.3420, 0.1670]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9951, 0.9981, 0.9971, 0.9959, 0.9964, 0.9964, 0.9965, 0.9965, 0.9963,\n",
      "        0.9953, 0.9962, 0.9956, 0.9949, 0.9961, 0.9952, 0.9961, 0.9966, 0.9959,\n",
      "        0.9965, 0.9970, 0.9980, 0.9995, 0.9990, 0.9948, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.2301, 0.2445, 0.2297, 0.1944, 0.1441, 0.0832])\n",
      "----------------------------------------\n",
      "iter  0  stage  19  ep  149999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0020, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 0.9500, 0.7285, 0.5277, 0.3420, 0.1670]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9956, 0.9982, 0.9974, 0.9963, 0.9966, 0.9967, 0.9968, 0.9969, 0.9966,\n",
      "        0.9957, 0.9966, 0.9959, 0.9954, 0.9963, 0.9956, 0.9964, 0.9969, 0.9963,\n",
      "        0.9968, 0.9974, 0.9982, 0.9997, 0.9993, 0.9946, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.2301, 0.2445, 0.2297, 0.1944, 0.1441, 0.0832])\n",
      "----------------------------------------\n",
      "iter  0  stage  19  ep  179999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0021, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 0.9500, 0.7285, 0.5277, 0.3420, 0.1670]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9953, 0.9981, 0.9972, 0.9960, 0.9964, 0.9965, 0.9966, 0.9967, 0.9964,\n",
      "        0.9954, 0.9964, 0.9956, 0.9951, 0.9961, 0.9953, 0.9962, 0.9967, 0.9960,\n",
      "        0.9966, 0.9973, 0.9982, 0.9997, 0.9992, 0.9942, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.2301, 0.2445, 0.2297, 0.1944, 0.1441, 0.0832])\n",
      "----------------------------------------\n",
      "iter  0  stage  19  ep  209999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0016, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 0.9500, 0.7285, 0.5277, 0.3420, 0.1670]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9963, 0.9985, 0.9978, 0.9969, 0.9972, 0.9973, 0.9974, 0.9974, 0.9972,\n",
      "        0.9964, 0.9972, 0.9966, 0.9961, 0.9969, 0.9963, 0.9970, 0.9974, 0.9969,\n",
      "        0.9973, 0.9979, 0.9987, 1.0000, 0.9994, 0.9957, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.2301, 0.2445, 0.2297, 0.1944, 0.1441, 0.0832])\n",
      "----------------------------------------\n",
      "iter  0  stage  19  ep  239999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0015, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 0.9500, 0.7285, 0.5277, 0.3420, 0.1670]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9963, 0.9985, 0.9978, 0.9969, 0.9972, 0.9973, 0.9973, 0.9974, 0.9971,\n",
      "        0.9964, 0.9972, 0.9966, 0.9960, 0.9969, 0.9962, 0.9970, 0.9974, 0.9968,\n",
      "        0.9973, 0.9978, 0.9986, 1.0000, 0.9995, 0.9958, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.2301, 0.2445, 0.2297, 0.1944, 0.1441, 0.0832])\n",
      "----------------------------------------\n",
      "iter  0  stage  19  ep  269999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0010, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 0.9500, 0.7285, 0.5277, 0.3420, 0.1670]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9975, 0.9990, 0.9986, 0.9979, 0.9981, 0.9982, 0.9982, 0.9983, 0.9981,\n",
      "        0.9975, 0.9981, 0.9977, 0.9973, 0.9979, 0.9975, 0.9980, 0.9983, 0.9979,\n",
      "        0.9982, 0.9987, 0.9993, 1.0000, 0.9997, 0.9971, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.2301, 0.2445, 0.2297, 0.1944, 0.1441, 0.0832])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  0  stage  19  ep  280092   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0007, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 0.9500, 0.7285, 0.5277, 0.3420, 0.1670]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9981, 0.9993, 0.9989, 0.9984, 0.9986, 0.9987, 0.9987, 0.9987, 0.9985,\n",
      "        0.9981, 0.9986, 0.9982, 0.9979, 0.9984, 0.9981, 0.9985, 0.9987, 0.9984,\n",
      "        0.9987, 0.9990, 0.9995, 1.0000, 0.9998, 0.9979, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.2301, 0.2445, 0.2297, 0.1944, 0.1441, 0.0832])\n",
      "----------------------------------------\n",
      "iter  0  stage  18  ep  607   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0011, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631,\n",
      "        1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631,\n",
      "        1.3631, 1.1119, 0.8901, 0.6892, 0.5034, 0.3283, 0.1613]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9982, 0.9994, 0.9990, 0.9985, 0.9987, 0.9988, 0.9988, 0.9988, 0.9986,\n",
      "        0.9983, 0.9987, 0.9984, 0.9981, 0.9986, 0.9982, 0.9986, 0.9988, 0.9985,\n",
      "        0.9990, 0.9991, 0.9995, 1.0000, 0.9998, 0.9981, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.3047, 0.3191, 0.3044, 0.2690, 0.2187, 0.1578, 0.0890])\n",
      "----------------------------------------\n",
      "iter  0  stage  17  ep  4304   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  9, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0015, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.4969, 1.4969, 1.4969, 1.4969, 1.4969, 1.4969, 1.4969, 1.4969, 1.4969,\n",
      "        1.4969, 1.4969, 1.4969, 1.4969, 1.4969, 1.4969, 1.4969, 1.4969, 1.4969,\n",
      "        1.2527, 1.0359, 0.8386, 0.6553, 0.4822, 0.3165, 0.1562]) return=  68487.85710629346\n",
      "probs of actions:  tensor([0.9984, 0.9994, 0.9991, 0.9987, 0.9989, 0.9989, 0.9989, 0.9990, 0.9988,\n",
      "        0.9985, 0.9989, 0.9986, 0.9983, 0.9988, 0.9984, 0.9988, 0.0010, 0.9990,\n",
      "        0.9992, 0.9992, 0.9996, 1.0000, 0.9999, 0.9983, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2441, 0.2298,\n",
      "        0.2312, 0.2323, 0.2331, 0.2337, 0.2342, 0.2346, 0.2492])\n",
      "finalReturns:  tensor([0.3812, 0.3956, 0.3812, 0.3462, 0.2964, 0.2358, 0.1673, 0.0930])\n",
      "----------------------------------------\n",
      "iter  0  stage  16  ep  30   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0021, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.6770,\n",
      "        1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.4248,\n",
      "        1.2024, 1.0011, 0.8148, 0.6396, 0.4723, 0.3109, 0.1538]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9984, 0.9995, 0.9991, 0.9987, 0.9989, 0.9989, 0.9989, 0.9990, 0.9988,\n",
      "        0.9985, 0.9989, 0.9986, 0.9983, 0.9988, 0.9984, 0.9988, 0.9990, 0.9990,\n",
      "        0.9992, 0.9992, 0.9996, 1.0000, 0.9999, 0.9983, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.4658, 0.4802, 0.4654, 0.4299, 0.3796, 0.3186, 0.2498, 0.1752, 0.0964])\n",
      "----------------------------------------\n",
      "iter  0  stage  15  ep  10043   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0026, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.8309, 1.8309, 1.8309, 1.8309, 1.8309, 1.8309, 1.8309, 1.8309, 1.8309,\n",
      "        1.8309, 1.8309, 1.8309, 1.8309, 1.8309, 1.8309, 1.8309, 1.5780, 1.3551,\n",
      "        1.1534, 0.9669, 0.7914, 0.6240, 0.4625, 0.3054, 0.1515]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9986, 0.9995, 0.9992, 0.9989, 0.9990, 0.9990, 0.9990, 0.9991, 0.9989,\n",
      "        0.9986, 0.9990, 0.9987, 0.9985, 0.9989, 0.9986, 0.9990, 0.9992, 0.9991,\n",
      "        0.9993, 0.9993, 0.9997, 1.0000, 0.9999, 0.9984, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.5504, 0.5648, 0.5499, 0.5144, 0.4641, 0.4030, 0.3342, 0.2596, 0.1807,\n",
      "        0.0987])\n",
      "----------------------------------------\n",
      "iter  0  stage  14  ep  968   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0032, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.9839, 1.9839, 1.9839, 1.9839, 1.9839, 1.9839, 1.9839, 1.9839, 1.9839,\n",
      "        1.9839, 1.9839, 1.9839, 1.9839, 1.9839, 1.9839, 1.7301, 1.5065, 1.3043,\n",
      "        1.1175, 0.9417, 0.7741, 0.6125, 0.4553, 0.3013, 0.1497]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9987, 0.9995, 0.9993, 0.9989, 0.9991, 0.9991, 0.9991, 0.9991, 0.9990,\n",
      "        0.9987, 0.9991, 0.9988, 0.9986, 0.9990, 0.9990, 0.9991, 0.9993, 0.9992,\n",
      "        0.9994, 0.9994, 0.9997, 1.0000, 0.9999, 0.9986, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.6368, 0.6512, 0.6363, 0.6007, 0.5503, 0.4892, 0.4203, 0.3457, 0.2668,\n",
      "        0.1848, 0.1005])\n",
      "----------------------------------------\n",
      "iter  0  stage  13  ep  36   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  9, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(4.9667, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.1368, 2.1368, 2.1368, 2.1368, 2.1368, 2.1368, 2.1368, 2.1368, 2.1368,\n",
      "        2.1368, 2.1368, 2.1368, 2.1368, 2.1368, 1.8817, 1.6572, 1.4543, 1.2670,\n",
      "        1.0910, 0.9231, 0.7613, 0.6040, 0.4499, 0.2982, 0.1484]) return=  68474.29269380601\n",
      "probs of actions:  tensor([9.9870e-01, 9.9955e-01, 9.9930e-01, 9.9896e-01, 9.9907e-01, 9.9912e-01,\n",
      "        9.9911e-01, 9.9914e-01, 9.9902e-01, 9.9874e-01, 9.9907e-01, 9.9884e-01,\n",
      "        9.9861e-01, 9.9900e-01, 9.8588e-04, 9.9909e-01, 9.9929e-01, 9.9916e-01,\n",
      "        9.9939e-01, 9.9939e-01, 9.9970e-01, 1.0000e+00, 9.9995e-01, 9.9856e-01,\n",
      "        1.0000e+00], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2457, 0.2310, 0.2321, 0.2330,\n",
      "        0.2337, 0.2341, 0.2345, 0.2348, 0.2350, 0.2351, 0.2497])\n",
      "finalReturns:  tensor([0.7026, 0.7170, 0.6958, 0.6677, 0.6228, 0.5659, 0.5001, 0.4277, 0.3506,\n",
      "        0.2699, 0.1865, 0.1012])\n",
      "----------------------------------------\n",
      "iter  0  stage  12  ep  7589   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0042, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.2903, 2.2903, 2.2903, 2.2903, 2.2903, 2.2903, 2.2903, 2.2903, 2.2903,\n",
      "        2.2903, 2.2903, 2.2903, 2.2903, 2.0334, 1.8077, 1.6040, 1.4161, 1.2396,\n",
      "        1.0714, 0.9093, 0.7518, 0.5976, 0.4458, 0.2960, 0.1475]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9990, 0.9996, 0.9994, 0.9992, 0.9993, 0.9993, 0.9993, 0.9993, 0.9992,\n",
      "        0.9990, 0.9993, 0.9991, 0.9990, 0.9992, 0.9992, 0.9993, 0.9995, 0.9993,\n",
      "        0.9996, 0.9996, 0.9998, 1.0000, 1.0000, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.8136, 0.8280, 0.8130, 0.7773, 0.7267, 0.6655, 0.5964, 0.5217, 0.4427,\n",
      "        0.3606, 0.2762, 0.1901, 0.1027])\n",
      "----------------------------------------\n",
      "iter  0  stage  11  ep  0   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  9, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(6.1888, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.4451, 2.4451, 2.4451, 2.4451, 2.4451, 2.4451, 2.4451, 2.4451, 2.4451,\n",
      "        2.4451, 2.4451, 2.4451, 2.1859, 1.9586, 1.7537, 1.5650, 1.3879, 1.2192,\n",
      "        1.0569, 0.8991, 0.7447, 0.5928, 0.4428, 0.2943, 0.1467]) return=  68466.13679373146\n",
      "probs of actions:  tensor([9.9895e-01, 9.9965e-01, 9.9944e-01, 9.9917e-01, 9.9926e-01, 9.9930e-01,\n",
      "        9.9929e-01, 9.9931e-01, 9.9921e-01, 9.9899e-01, 9.9926e-01, 9.9907e-01,\n",
      "        9.9921e-04, 9.9925e-01, 9.9920e-01, 9.9927e-01, 9.9947e-01, 9.9932e-01,\n",
      "        9.9959e-01, 9.9959e-01, 9.9979e-01, 1.0000e+00, 9.9997e-01, 9.9888e-01,\n",
      "        1.0000e+00], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2488, 0.2332, 0.2338, 0.2343, 0.2346, 0.2348,\n",
      "        0.2350, 0.2352, 0.2353, 0.2354, 0.2354, 0.2355, 0.2499])\n",
      "finalReturns:  tensor([0.8808, 0.8952, 0.8738, 0.8454, 0.8004, 0.7432, 0.6772, 0.6048, 0.5275,\n",
      "        0.4468, 0.3633, 0.2779, 0.1911, 0.1032])\n",
      "----------------------------------------\n",
      "iter  0  stage  10  ep  0   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0064, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.6021, 2.6021, 2.6021, 2.6021, 2.6021, 2.6021, 2.6021, 2.6021, 2.6021,\n",
      "        2.6021, 2.6021, 2.3399, 2.1104, 1.9040, 1.7141, 1.5362, 1.3670, 1.2041,\n",
      "        1.0460, 0.8914, 0.7394, 0.5893, 0.4406, 0.2930, 0.1462]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9990, 0.9996, 0.9994, 0.9992, 0.9993, 0.9993, 0.9993, 0.9993, 0.9992,\n",
      "        0.9990, 0.9993, 0.9991, 0.9990, 0.9992, 0.9992, 0.9993, 0.9995, 0.9993,\n",
      "        0.9996, 0.9996, 0.9998, 1.0000, 1.0000, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.9944, 1.0088, 0.9935, 0.9575, 0.9066, 0.8451, 0.7758, 0.7009, 0.6218,\n",
      "        0.5396, 0.4551, 0.3689, 0.2815, 0.1931, 0.1040])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  0  stage  9  ep  22   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0078, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.7625, 2.7625, 2.7625, 2.7625, 2.7625, 2.7625, 2.7625, 2.7625, 2.7625,\n",
      "        2.7625, 2.4961, 2.2637, 2.0552, 1.8638, 1.6848, 1.5149, 1.3514, 1.1929,\n",
      "        1.0380, 0.8857, 0.7354, 0.5866, 0.4389, 0.2920, 0.1458]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9989, 0.9996, 0.9994, 0.9992, 0.9992, 0.9993, 0.9993, 0.9993, 0.9992,\n",
      "        0.9990, 0.9993, 0.9991, 0.9990, 0.9992, 0.9992, 0.9993, 0.9995, 0.9993,\n",
      "        0.9996, 0.9996, 0.9998, 1.0000, 1.0000, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.0860, 1.1004, 1.0850, 1.0487, 0.9976, 0.9359, 0.8664, 0.7913, 0.7121,\n",
      "        0.6298, 0.5453, 0.4591, 0.3716, 0.2832, 0.1941, 0.1044])\n",
      "----------------------------------------\n",
      "iter  0  stage  8  ep  0   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0091, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.9276, 2.9276, 2.9276, 2.9276, 2.9276, 2.9276, 2.9276, 2.9276, 2.9276,\n",
      "        2.6556, 2.4193, 2.2080, 2.0147, 1.8342, 1.6632, 1.4990, 1.3399, 1.1845,\n",
      "        1.0319, 0.8814, 0.7324, 0.5846, 0.4376, 0.2913, 0.1455]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9989, 0.9996, 0.9994, 0.9992, 0.9992, 0.9993, 0.9993, 0.9993, 0.9992,\n",
      "        0.9990, 0.9993, 0.9991, 0.9990, 0.9992, 0.9992, 0.9993, 0.9995, 0.9993,\n",
      "        0.9996, 0.9996, 0.9998, 1.0000, 1.0000, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.1785, 1.1929, 1.1773, 1.1406, 1.0892, 1.0272, 0.9575, 0.8823, 0.8029,\n",
      "        0.7205, 0.6359, 0.5496, 0.4620, 0.3736, 0.2844, 0.1948, 0.1047])\n",
      "----------------------------------------\n",
      "iter  0  stage  7  ep  0   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0103, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.0992, 3.0992, 3.0992, 3.0992, 3.0992, 3.0992, 3.0992, 3.0992, 2.8197,\n",
      "        2.5781, 2.3632, 2.1671, 1.9848, 1.8123, 1.6471, 1.4872, 1.3313, 1.1783,\n",
      "        1.0274, 0.8782, 0.7302, 0.5831, 0.4367, 0.2908, 0.1453]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9989, 0.9996, 0.9994, 0.9992, 0.9992, 0.9993, 0.9993, 0.9993, 0.9992,\n",
      "        0.9990, 0.9993, 0.9991, 0.9990, 0.9992, 0.9992, 0.9993, 0.9995, 0.9993,\n",
      "        0.9996, 0.9996, 0.9998, 1.0000, 1.0000, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.2720, 1.2864, 1.2704, 1.2334, 1.1815, 1.1191, 1.0491, 0.9736, 0.8941,\n",
      "        0.8115, 0.7268, 0.6404, 0.5528, 0.4643, 0.3751, 0.2854, 0.1953, 0.1050])\n",
      "----------------------------------------\n",
      "iter  0  stage  6  ep  0   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0116, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.2799, 3.2799, 3.2799, 3.2799, 3.2799, 3.2799, 3.2799, 2.9902, 2.7415,\n",
      "        2.5215, 2.3219, 2.1370, 1.9626, 1.7960, 1.6351, 1.4784, 1.3248, 1.1736,\n",
      "        1.0241, 0.8758, 0.7286, 0.5820, 0.4360, 0.2904, 0.1451]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9989, 0.9996, 0.9994, 0.9992, 0.9992, 0.9993, 0.9993, 0.9993, 0.9992,\n",
      "        0.9990, 0.9993, 0.9991, 0.9990, 0.9992, 0.9992, 0.9993, 0.9995, 0.9993,\n",
      "        0.9996, 0.9996, 0.9998, 1.0000, 1.0000, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.3666, 1.3810, 1.3646, 1.3270, 1.2746, 1.2117, 1.1413, 1.0654, 0.9856,\n",
      "        0.9029, 0.8179, 0.7314, 0.6437, 0.5552, 0.4659, 0.3762, 0.2861, 0.1957,\n",
      "        0.1051])\n",
      "----------------------------------------\n",
      "iter  0  stage  5  ep  0   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0129, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.4731, 3.4731, 3.4731, 3.4731, 3.4731, 3.4731, 3.1695, 2.9111, 2.6844,\n",
      "        2.4799, 2.2915, 2.1146, 1.9461, 1.7839, 1.6262, 1.4718, 1.3200, 1.1701,\n",
      "        1.0215, 0.8740, 0.7273, 0.5812, 0.4355, 0.2901, 0.1450]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9989, 0.9996, 0.9994, 0.9992, 0.9992, 0.9993, 0.9993, 0.9993, 0.9992,\n",
      "        0.9990, 0.9993, 0.9991, 0.9990, 0.9992, 0.9992, 0.9993, 0.9995, 0.9993,\n",
      "        0.9996, 0.9996, 0.9998, 1.0000, 1.0000, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.4627, 1.4771, 1.4601, 1.4217, 1.3686, 1.3050, 1.2341, 1.1578, 1.0776,\n",
      "        0.9945, 0.9094, 0.8228, 0.7349, 0.6463, 0.5570, 0.4672, 0.3770, 0.2866,\n",
      "        0.1960, 0.1053])\n",
      "----------------------------------------\n",
      "iter  0  stage  4  ep  0   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0144, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.6835, 3.6835, 3.6835, 3.6835, 3.6835, 3.3608, 3.0893, 2.8534, 2.6424,\n",
      "        2.4492, 2.2689, 2.0980, 1.9339, 1.7748, 1.6195, 1.4669, 1.3164, 1.1675,\n",
      "        1.0197, 0.8727, 0.7264, 0.5805, 0.4351, 0.2899, 0.1449]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9989, 0.9996, 0.9994, 0.9992, 0.9992, 0.9993, 0.9993, 0.9993, 0.9992,\n",
      "        0.9990, 0.9993, 0.9991, 0.9990, 0.9992, 0.9992, 0.9993, 0.9995, 0.9993,\n",
      "        0.9996, 0.9996, 0.9998, 1.0000, 1.0000, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.5606, 1.5750, 1.5572, 1.5179, 1.4637, 1.3993, 1.3276, 1.2507, 1.1700,\n",
      "        1.0866, 1.0012, 0.9143, 0.8264, 0.7376, 0.6482, 0.5583, 0.4681, 0.3777,\n",
      "        0.2870, 0.1962, 0.1054])\n",
      "----------------------------------------\n",
      "iter  0  stage  3  ep  0   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0161, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.9177, 3.9177, 3.9177, 3.9177, 3.5687, 3.2793, 3.0307, 2.8109, 2.6114,\n",
      "        2.4265, 2.2522, 2.0856, 1.9247, 1.7680, 1.6145, 1.4632, 1.3137, 1.1655,\n",
      "        1.0182, 0.8717, 0.7257, 0.5801, 0.4348, 0.2897, 0.1448]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9989, 0.9996, 0.9994, 0.9992, 0.9992, 0.9993, 0.9993, 0.9993, 0.9992,\n",
      "        0.9990, 0.9993, 0.9991, 0.9990, 0.9992, 0.9992, 0.9993, 0.9995, 0.9993,\n",
      "        0.9996, 0.9996, 0.9998, 1.0000, 1.0000, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.6609, 1.6753, 1.6565, 1.6158, 1.5603, 1.4947, 1.4220, 1.3444, 1.2631,\n",
      "        1.1792, 1.0934, 1.0062, 0.9180, 0.8291, 0.7395, 0.6496, 0.5593, 0.4688,\n",
      "        0.3781, 0.2873, 0.1964, 0.1054])\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  0  stage  2  ep  0   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0174, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.1854, 4.1854, 4.1854, 3.7998, 3.4854, 3.2196, 2.9877, 2.7795, 2.5884,\n",
      "        2.4096, 2.2397, 2.0764, 1.9179, 1.7630, 1.6108, 1.4605, 1.3117, 1.1640,\n",
      "        1.0172, 0.8709, 0.7252, 0.5797, 0.4345, 0.2896, 0.1447]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9989, 0.9996, 0.9994, 0.9992, 0.9992, 0.9993, 0.9993, 0.9993, 0.9992,\n",
      "        0.9990, 0.9993, 0.9991, 0.9990, 0.9992, 0.9992, 0.9993, 0.9995, 0.9993,\n",
      "        0.9996, 0.9996, 0.9998, 1.0000, 1.0000, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.7644, 1.7788, 1.7586, 1.7161, 1.6589, 1.5917, 1.5177, 1.4390, 1.3568,\n",
      "        1.2723, 1.1860, 1.0985, 1.0100, 0.9208, 0.8311, 0.7410, 0.6506, 0.5601,\n",
      "        0.4693, 0.3785, 0.2875, 0.1965, 0.1055])\n",
      "----------------------------------------\n",
      "iter  0  stage  1  ep  0   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0184, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.5005, 4.5005, 4.0633, 3.7141, 3.4244, 3.1758, 2.9558, 2.7562, 2.5713,\n",
      "        2.3970, 2.2304, 2.0695, 1.9128, 1.7592, 1.6080, 1.4584, 1.3102, 1.1629,\n",
      "        1.0164, 0.8704, 0.7248, 0.5795, 0.4344, 0.2895, 0.1447]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9989, 0.9996, 0.9994, 0.9992, 0.9992, 0.9993, 0.9993, 0.9993, 0.9992,\n",
      "        0.9990, 0.9993, 0.9991, 0.9990, 0.9992, 0.9992, 0.9993, 0.9995, 0.9993,\n",
      "        0.9996, 0.9996, 0.9998, 1.0000, 1.0000, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.8720, 1.8864, 1.8645, 1.8196, 1.7600, 1.6907, 1.6150, 1.5348, 1.4516,\n",
      "        1.3662, 1.2792, 1.1911, 1.1022, 1.0128, 0.9228, 0.8326, 0.7421, 0.6514,\n",
      "        0.5606, 0.4697, 0.3787, 0.2877, 0.1966, 0.1055])\n",
      "----------------------------------------\n",
      "iter  0  stage  0  ep  88   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0200, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.8841, 4.3729, 3.9744, 3.6513, 3.3795, 3.1433, 2.9322, 2.7389, 2.5586,\n",
      "        2.3875, 2.2234, 2.0643, 1.9089, 1.7564, 1.6059, 1.4569, 1.3091, 1.1621,\n",
      "        1.0158, 0.8699, 0.7245, 0.5793, 0.4343, 0.2894, 0.1447]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9990, 0.9997, 0.9995, 0.9992, 0.9993, 0.9993, 0.9993, 0.9993, 0.9992,\n",
      "        0.9991, 0.9993, 0.9991, 0.9990, 0.9992, 0.9992, 0.9993, 0.9995, 0.9993,\n",
      "        0.9996, 0.9996, 0.9998, 1.0000, 1.0000, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.9853, 1.9997, 1.9753, 1.9272, 1.8645, 1.7924, 1.7143, 1.6323, 1.5475,\n",
      "        1.4610, 1.3731, 1.2844, 1.1950, 1.1051, 1.0148, 0.9244, 0.8337, 0.7429,\n",
      "        0.6520, 0.5610, 0.4700, 0.3789, 0.2878, 0.1967, 0.1056])\n",
      "policy saved!\n",
      "1,3,[5e-06,1][1, 10000, 1, 1],1683165834 saved\n",
      "policy reset\n",
      "----------------------------------------\n",
      "iter  1  stage  24  ep  29999   adversary:  const95-1.0,\n",
      "  actions:  tensor([42,  3,  3, 45, 18,  0,  3,  0,  3, 24,  3,  9,  3,  3,  0,  6,  3, 12,\n",
      "         3,  6,  3, 45, 12,  3,  6])\n",
      "loss=  tensor(-0.0060, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.2448, 0.2448, 0.2448, 0.2448, 0.2448, 0.2448, 0.2448, 0.2448, 0.2448,\n",
      "        0.2448, 0.2448, 0.2448, 0.2448, 0.2448, 0.2448, 0.2448, 0.2448, 0.2448,\n",
      "        0.2448, 0.2448, 0.2448, 0.2448, 0.2448, 0.2448, 0.2448]) return=  62536.791981005576\n",
      "probs of actions:  tensor([0.0082, 0.2535, 0.2494, 0.0080, 0.0216, 0.2569, 0.2423, 0.2699, 0.2563,\n",
      "        0.0137, 0.2403, 0.1029, 0.2541, 0.2374, 0.2605, 0.1688, 0.2393, 0.0449,\n",
      "        0.2451, 0.1836, 0.2257, 0.0110, 0.0487, 0.2466, 0.1903],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([ 0.3348,  0.5412,  0.4277,  0.1498,  0.3935,  0.3962,  0.3207,  0.2786,\n",
      "         0.2401,  0.1639,  0.2572,  0.2257,  0.2296,  0.2131,  0.2020,  0.1831,\n",
      "         0.1875,  0.1688,  0.1980,  0.1874,  0.1907, -0.0169,  0.2672,  0.2726,\n",
      "         0.2412])\n",
      "finalReturns:  tensor([-0.0036])\n",
      "----------------------------------------\n",
      "iter  1  stage  23  ep  29999   adversary:  const95-1.0,\n",
      "  actions:  tensor([ 3,  0,  0, 24,  3,  0,  6,  0,  3,  9,  0,  3,  3,  3,  3,  3,  3,  6,\n",
      "         3,  3,  3,  0,  3,  6,  3])\n",
      "loss=  tensor(0.0199, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.3255, 0.3255, 0.3255, 0.3255, 0.3255, 0.3255, 0.3255, 0.3255, 0.3255,\n",
      "        0.3255, 0.3255, 0.3255, 0.3255, 0.3255, 0.3255, 0.3255, 0.3255, 0.3255,\n",
      "        0.3255, 0.3255, 0.3255, 0.3255, 0.3255, 0.3255, 0.1601]) return=  54781.59028774685\n",
      "probs of actions:  tensor([0.4734, 0.2537, 0.2662, 0.0015, 0.4600, 0.2560, 0.1655, 0.2745, 0.4745,\n",
      "        0.0526, 0.2590, 0.4529, 0.4598, 0.4480, 0.4363, 0.4599, 0.4514, 0.1761,\n",
      "        0.4566, 0.4411, 0.4323, 0.2937, 0.4565, 0.2538, 0.4391],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5103, 0.4080, 0.3295, 0.2186, 0.3007, 0.2646, 0.2275, 0.2214, 0.1997,\n",
      "        0.1841, 0.1992, 0.1838, 0.1796, 0.1764, 0.1741, 0.1724, 0.1711, 0.1674,\n",
      "        0.1756, 0.1735, 0.1719, 0.1716, 0.1637, 0.1619, 0.1714])\n",
      "finalReturns:  tensor([0.0077, 0.0113])\n",
      "----------------------------------------\n",
      "iter  1  stage  23  ep  59999   adversary:  const95-1.0,\n",
      "  actions:  tensor([ 0,  6,  6,  3,  3,  6,  6,  6,  6,  9,  3,  0, 12,  3,  6,  3,  0,  6,\n",
      "         6,  0,  3,  9,  0,  6,  0])\n",
      "loss=  tensor(0.0099, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.3391, 0.3391, 0.3391, 0.3391, 0.3391, 0.3391, 0.3391, 0.3391, 0.3391,\n",
      "        0.3391, 0.3391, 0.3391, 0.3391, 0.3391, 0.3391, 0.3391, 0.3391, 0.3391,\n",
      "        0.3391, 0.3391, 0.3391, 0.3391, 0.3391, 0.3391, 0.1658]) return=  56244.87614304111\n",
      "probs of actions:  tensor([0.1596, 0.4525, 0.4438, 0.3103, 0.3243, 0.4194, 0.3663, 0.4225, 0.4123,\n",
      "        0.0672, 0.3033, 0.1854, 0.0129, 0.2958, 0.4512, 0.3226, 0.1837, 0.3981,\n",
      "        0.3993, 0.1956, 0.2979, 0.0727, 0.1676, 0.7251, 0.5688],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.3949, 0.3368, 0.2989, 0.2625, 0.2339, 0.2225, 0.2142, 0.2080,\n",
      "        0.1989, 0.2095, 0.1994, 0.1704, 0.1993, 0.1883, 0.1914, 0.1861, 0.1716,\n",
      "        0.1761, 0.1831, 0.1721, 0.1637, 0.1835, 0.1697, 0.1783])\n",
      "finalReturns:  tensor([0.0088, 0.0124])\n",
      "----------------------------------------\n",
      "iter  1  stage  23  ep  89999   adversary:  const95-1.0,\n",
      "  actions:  tensor([3, 6, 6, 6, 6, 0, 0, 6, 6, 3, 3, 0, 3, 3, 3, 9, 6, 0, 6, 6, 6, 6, 6, 6,\n",
      "        0])\n",
      "loss=  tensor(0.0037, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.3663, 0.3663, 0.3663, 0.3663, 0.3663, 0.3663, 0.3663, 0.3663, 0.3663,\n",
      "        0.3663, 0.3663, 0.3663, 0.3663, 0.3663, 0.3663, 0.3663, 0.3663, 0.3663,\n",
      "        0.3663, 0.3663, 0.3663, 0.3663, 0.3663, 0.3663, 0.1773]) return=  55808.994116578215\n",
      "probs of actions:  tensor([0.1982, 0.5378, 0.5276, 0.4819, 0.4884, 0.1662, 0.2449, 0.5050, 0.4959,\n",
      "        0.2201, 0.1924, 0.2021, 0.2375, 0.1857, 0.1872, 0.1046, 0.4413, 0.2141,\n",
      "        0.4682, 0.4636, 0.5007, 0.4585, 0.4863, 0.8265, 0.8623],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5103, 0.4044, 0.3434, 0.3009, 0.2708, 0.2529, 0.2229, 0.1981, 0.1961,\n",
      "        0.1972, 0.1895, 0.1847, 0.1733, 0.1717, 0.1706, 0.1625, 0.1790, 0.1853,\n",
      "        0.1710, 0.1757, 0.1792, 0.1819, 0.1839, 0.1854, 0.1901])\n",
      "finalReturns:  tensor([0.0093, 0.0129])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  22  ep  29999   adversary:  const95-1.0,\n",
      "  actions:  tensor([ 6,  9,  6,  6,  6,  9,  0,  9,  6,  9,  9,  9,  3,  6,  6,  9,  0, 12,\n",
      "         6,  6,  6,  9,  6,  6,  0])\n",
      "loss=  tensor(0.0535, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5674, 0.5674, 0.5674, 0.5674, 0.5674, 0.5674, 0.5674, 0.5674, 0.5674,\n",
      "        0.5674, 0.5674, 0.5674, 0.5674, 0.5674, 0.5674, 0.5674, 0.5674, 0.5674,\n",
      "        0.5674, 0.5674, 0.5674, 0.5674, 0.5674, 0.3640, 0.1763]) return=  59812.40403986201\n",
      "probs of actions:  tensor([0.4401, 0.4532, 0.4122, 0.4032, 0.3970, 0.4278, 0.0902, 0.4619, 0.3996,\n",
      "        0.4055, 0.4287, 0.4020, 0.0991, 0.4210, 0.4286, 0.4193, 0.0713, 0.0278,\n",
      "        0.3751, 0.3976, 0.3955, 0.4142, 0.3706, 0.4904, 0.9183],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5076, 0.4095, 0.3590, 0.3118, 0.2786, 0.2504, 0.2488, 0.2120, 0.2167,\n",
      "        0.2053, 0.2072, 0.2086, 0.2168, 0.2011, 0.1983, 0.1917, 0.2050, 0.1745,\n",
      "        0.1997, 0.1973, 0.1954, 0.1896, 0.1998, 0.1973, 0.1991])\n",
      "finalReturns:  tensor([0.0288, 0.0324, 0.0227])\n",
      "----------------------------------------\n",
      "iter  1  stage  22  ep  59999   adversary:  const95-1.0,\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 6, 9, 0, 9, 6, 6, 0, 9, 6, 9, 9, 9, 9, 0, 9, 9, 6, 9, 9,\n",
      "        9])\n",
      "loss=  tensor(0.1111, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5662, 0.5662, 0.5662, 0.5662, 0.5662, 0.5662, 0.5662, 0.5662, 0.5662,\n",
      "        0.5662, 0.5662, 0.5662, 0.5662, 0.5662, 0.5662, 0.5662, 0.5662, 0.5662,\n",
      "        0.5662, 0.5662, 0.5662, 0.5662, 0.5662, 0.3634, 0.1761]) return=  60649.09398743877\n",
      "probs of actions:  tensor([0.6854, 0.7487, 0.7432, 0.7094, 0.7146, 0.1807, 0.6713, 0.0347, 0.7256,\n",
      "        0.1790, 0.1936, 0.0563, 0.6536, 0.1959, 0.6958, 0.6811, 0.6686, 0.6919,\n",
      "        0.0458, 0.6781, 0.6969, 0.1834, 0.8403, 0.8036, 0.0253],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2753, 0.2480, 0.2471, 0.2107,\n",
      "        0.2158, 0.2092, 0.2079, 0.1828, 0.1946, 0.1890, 0.1948, 0.1992, 0.2026,\n",
      "        0.2132, 0.1867, 0.1930, 0.2024, 0.1948, 0.1992, 0.2026])\n",
      "finalReturns:  tensor([0.0303, 0.0384, 0.0265])\n",
      "----------------------------------------\n",
      "iter  1  stage  22  ep  89999   adversary:  const95-1.0,\n",
      "  actions:  tensor([9, 9, 0, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 6, 9, 6, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0092, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.6015, 0.6015, 0.6015, 0.6015, 0.6015, 0.6015, 0.6015, 0.6015, 0.6015,\n",
      "        0.6015, 0.6015, 0.6015, 0.6015, 0.6015, 0.6015, 0.6015, 0.6015, 0.6015,\n",
      "        0.6015, 0.6015, 0.6015, 0.6015, 0.6015, 0.3829, 0.1843]) return=  62572.5693677297\n",
      "probs of actions:  tensor([0.7904, 0.8445, 0.0282, 0.8092, 0.8090, 0.8079, 0.7612, 0.8194, 0.8199,\n",
      "        0.7612, 0.7964, 0.7755, 0.7516, 0.7987, 0.1141, 0.7796, 0.1087, 0.7876,\n",
      "        0.8168, 0.7721, 0.7937, 0.7671, 0.9175, 0.9006, 0.9779],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3694, 0.2953, 0.2735, 0.2576, 0.2460, 0.2375, 0.2312,\n",
      "        0.2265, 0.2231, 0.2205, 0.2185, 0.2171, 0.2205, 0.2082, 0.2138, 0.2032,\n",
      "        0.2056, 0.2074, 0.2087, 0.2098, 0.2105, 0.2111, 0.2196])\n",
      "finalReturns:  tensor([0.0397, 0.0478, 0.0354])\n",
      "----------------------------------------\n",
      "iter  1  stage  22  ep  119999   adversary:  const95-1.0,\n",
      "  actions:  tensor([9, 9, 3, 9, 9, 9, 9, 6, 9, 9, 6, 9, 9, 9, 9, 9, 6, 9, 9, 9, 0, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0061, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5680, 0.5680, 0.5680, 0.5680, 0.5680, 0.5680, 0.5680, 0.5680, 0.5680,\n",
      "        0.5680, 0.5680, 0.5680, 0.5680, 0.5680, 0.5680, 0.5680, 0.5680, 0.5680,\n",
      "        0.5680, 0.5680, 0.5680, 0.5680, 0.5680, 0.3644, 0.1765]) return=  62135.91864110961\n",
      "probs of actions:  tensor([0.8372, 0.8838, 0.0120, 0.8512, 0.8532, 0.8522, 0.8053, 0.0686, 0.8631,\n",
      "        0.8069, 0.0854, 0.8223, 0.7987, 0.8417, 0.8412, 0.8223, 0.0815, 0.8315,\n",
      "        0.8569, 0.8163, 0.0252, 0.8151, 0.9453, 0.9285, 0.9871],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3685, 0.3037, 0.2795, 0.2620, 0.2492, 0.2444, 0.2256,\n",
      "        0.2224, 0.2245, 0.2111, 0.2115, 0.2118, 0.2121, 0.2123, 0.2169, 0.2055,\n",
      "        0.2073, 0.2087, 0.2178, 0.1899, 0.1955, 0.1998, 0.2111])\n",
      "finalReturns:  tensor([0.0384, 0.0465, 0.0346])\n",
      "----------------------------------------\n",
      "iter  1  stage  22  ep  149999   adversary:  const95-1.0,\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 3, 9, 9, 6, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0045, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.6024, 0.6024, 0.6024, 0.6024, 0.6024, 0.6024, 0.6024, 0.6024, 0.6024,\n",
      "        0.6024, 0.6024, 0.6024, 0.6024, 0.6024, 0.6024, 0.6024, 0.6024, 0.6024,\n",
      "        0.6024, 0.6024, 0.6024, 0.6024, 0.6024, 0.3834, 0.1845]) return=  63171.85534316236\n",
      "probs of actions:  tensor([0.8761, 0.9140, 0.9068, 0.8864, 0.8894, 0.8893, 0.8472, 0.8918, 0.8966,\n",
      "        0.8503, 0.8758, 0.8626, 0.0164, 0.8787, 0.8780, 0.0591, 0.8541, 0.8692,\n",
      "        0.8905, 0.8559, 0.8723, 0.8523, 0.9583, 0.9486, 0.9922],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2274, 0.2043, 0.2064, 0.2125, 0.2022, 0.2049,\n",
      "        0.2068, 0.2083, 0.2094, 0.2103, 0.2109, 0.2114, 0.2198])\n",
      "finalReturns:  tensor([0.0397, 0.0478, 0.0354])\n",
      "----------------------------------------\n",
      "iter  1  stage  21  ep  29999   adversary:  const95-1.0,\n",
      "  actions:  tensor([ 9,  9,  9, 12,  9,  9,  9,  9,  9, 12,  9,  9,  9,  9,  9,  9,  9,  9,\n",
      "         9, 12,  9,  9,  9,  9,  0])\n",
      "loss=  tensor(0.0137, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.7975, 0.7975, 0.7975, 0.7975, 0.7975, 0.7975, 0.7975, 0.7975, 0.7975,\n",
      "        0.7975, 0.7975, 0.7975, 0.7975, 0.7975, 0.7975, 0.7975, 0.7975, 0.7975,\n",
      "        0.7975, 0.7975, 0.7975, 0.7975, 0.5704, 0.3657, 0.1770]) return=  64479.669562396586\n",
      "probs of actions:  tensor([0.9159, 0.9412, 0.9358, 0.0256, 0.9201, 0.9246, 0.9016, 0.9215, 0.9276,\n",
      "        0.0340, 0.9117, 0.9078, 0.8983, 0.9093, 0.9158, 0.9064, 0.9014, 0.9157,\n",
      "        0.9270, 0.0320, 0.9053, 0.9155, 0.9631, 0.9623, 0.9863],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3143, 0.2999, 0.2768, 0.2600, 0.2478, 0.2388,\n",
      "        0.2259, 0.2346, 0.2290, 0.2249, 0.2219, 0.2196, 0.2179, 0.2166, 0.2156,\n",
      "        0.2149, 0.2081, 0.2211, 0.2190, 0.2175, 0.2163, 0.2235])\n",
      "finalReturns:  tensor([0.0788, 0.0869, 0.0741, 0.0465])\n",
      "----------------------------------------\n",
      "iter  1  stage  21  ep  59999   adversary:  const95-1.0,\n",
      "  actions:  tensor([ 9,  9,  9,  9,  9,  9,  9,  9, 12,  9,  9, 12,  9,  9,  9,  9,  9,  6,\n",
      "         9,  0,  9,  9,  9,  6,  0])\n",
      "loss=  tensor(0.2748, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.7363, 0.7363, 0.7363, 0.7363, 0.7363, 0.7363, 0.7363, 0.7363, 0.7363,\n",
      "        0.7363, 0.7363, 0.7363, 0.7363, 0.7363, 0.7363, 0.7363, 0.7363, 0.7363,\n",
      "        0.7363, 0.7363, 0.7363, 0.7363, 0.5326, 0.3447, 0.1682]) return=  63525.18956622784\n",
      "probs of actions:  tensor([0.9149, 0.9400, 0.9339, 0.9228, 0.9178, 0.9228, 0.9010, 0.9185, 0.0332,\n",
      "        0.8975, 0.9112, 0.0388, 0.8970, 0.9078, 0.9157, 0.9048, 0.9007, 0.0332,\n",
      "        0.9261, 0.0118, 0.9043, 0.9107, 0.9565, 0.0225, 0.9908],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2302,\n",
      "        0.2378, 0.2314, 0.2204, 0.2305, 0.2260, 0.2226, 0.2202, 0.2183, 0.2214,\n",
      "        0.2089, 0.2179, 0.1900, 0.1956, 0.1998, 0.2075, 0.2067])\n",
      "finalReturns:  tensor([0.0734, 0.0815, 0.0695, 0.0385])\n",
      "----------------------------------------\n",
      "iter  1  stage  21  ep  89999   adversary:  const95-1.0,\n",
      "  actions:  tensor([ 9,  9,  9,  9, 12,  9,  9,  6,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
      "         9,  9,  9,  9,  9,  9,  0])\n",
      "loss=  tensor(0.0129, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.7825, 0.7825, 0.7825, 0.7825, 0.7825, 0.7825, 0.7825, 0.7825, 0.7825,\n",
      "        0.7825, 0.7825, 0.7825, 0.7825, 0.7825, 0.7825, 0.7825, 0.7825, 0.7825,\n",
      "        0.7825, 0.7825, 0.7825, 0.7825, 0.5611, 0.3605, 0.1749]) return=  63854.30325636411\n",
      "probs of actions:  tensor([0.9253, 0.9474, 0.9409, 0.9324, 0.0420, 0.9313, 0.9130, 0.0223, 0.9336,\n",
      "        0.9088, 0.9203, 0.9165, 0.9102, 0.9164, 0.9261, 0.9145, 0.9124, 0.9255,\n",
      "        0.9341, 0.9146, 0.9132, 0.9181, 0.9589, 0.9694, 0.9937],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2854, 0.2788, 0.2615, 0.2534, 0.2322,\n",
      "        0.2273, 0.2236, 0.2209, 0.2188, 0.2173, 0.2162, 0.2153, 0.2147, 0.2142,\n",
      "        0.2139, 0.2136, 0.2134, 0.2133, 0.2131, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([0.0781, 0.0862, 0.0736, 0.0462])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  21  ep  119999   adversary:  const95-1.0,\n",
      "  actions:  tensor([ 9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
      "         9, 12,  9,  9,  9,  9,  0])\n",
      "loss=  tensor(0.0193, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.7966, 0.7966, 0.7966, 0.7966, 0.7966, 0.7966, 0.7966, 0.7966, 0.7966,\n",
      "        0.7966, 0.7966, 0.7966, 0.7966, 0.7966, 0.7966, 0.7966, 0.7966, 0.7966,\n",
      "        0.7966, 0.7966, 0.7966, 0.7966, 0.5698, 0.3654, 0.1769]) return=  64011.61156289775\n",
      "probs of actions:  tensor([0.9028, 0.9281, 0.9189, 0.9113, 0.9003, 0.9093, 0.8888, 0.9008, 0.9104,\n",
      "        0.8831, 0.8980, 0.8920, 0.8875, 0.8917, 0.9071, 0.8885, 0.8897, 0.9041,\n",
      "        0.9133, 0.0715, 0.8885, 0.8815, 0.9319, 0.9598, 0.9956],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2075, 0.2206, 0.2187, 0.2172, 0.2161, 0.2234])\n",
      "finalReturns:  tensor([0.0788, 0.0869, 0.0741, 0.0465])\n",
      "----------------------------------------\n",
      "iter  1  stage  21  ep  149999   adversary:  const95-1.0,\n",
      "  actions:  tensor([ 9,  9,  9,  9,  9,  9,  9,  9, 12,  9,  9, 15, 12,  9,  9, 12,  9, 12,\n",
      "         9,  9,  9,  9,  9,  9,  0])\n",
      "loss=  tensor(0.0421, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.8001, 0.8001, 0.8001, 0.8001, 0.8001, 0.8001, 0.8001, 0.8001, 0.8001,\n",
      "        0.8001, 0.8001, 0.8001, 0.8001, 0.8001, 0.8001, 0.8001, 0.8001, 0.8001,\n",
      "        0.8001, 0.8001, 0.8001, 0.8001, 0.5720, 0.3666, 0.1774]) return=  65108.245685588154\n",
      "probs of actions:  tensor([0.8282, 0.8623, 0.8464, 0.8418, 0.8199, 0.8360, 0.8128, 0.8229, 0.1314,\n",
      "        0.8013, 0.8239, 0.0089, 0.1378, 0.8145, 0.8448, 0.1504, 0.8174, 0.1274,\n",
      "        0.8468, 0.8121, 0.8161, 0.7587, 0.8521, 0.9195, 0.9976],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2302,\n",
      "        0.2378, 0.2314, 0.2123, 0.2315, 0.2389, 0.2322, 0.2210, 0.2309, 0.2200,\n",
      "        0.2302, 0.2258, 0.2225, 0.2200, 0.2182, 0.2169, 0.2239])\n",
      "finalReturns:  tensor([0.0789, 0.0870, 0.0742, 0.0465])\n",
      "----------------------------------------\n",
      "iter  1  stage  21  ep  179999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12,  9,  9,  9,  9,  9,  9, 12,  9, 12,  9,  9, 12, 12, 12,  9,\n",
      "        15, 12,  9,  9, 12,  9,  0])\n",
      "loss=  tensor(0.1889, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.8339, 0.8339, 0.8339, 0.8339, 0.8339, 0.8339, 0.8339, 0.8339, 0.8339,\n",
      "        0.8339, 0.8339, 0.8339, 0.8339, 0.8339, 0.8339, 0.8339, 0.8339, 0.8339,\n",
      "        0.8339, 0.8339, 0.8339, 0.8339, 0.5927, 0.3780, 0.1822]) return=  66273.57654399677\n",
      "probs of actions:  tensor([0.3208, 0.2918, 0.3097, 0.6656, 0.6256, 0.6527, 0.6345, 0.6362, 0.6539,\n",
      "        0.3379, 0.6420, 0.3300, 0.6465, 0.6312, 0.2827, 0.3408, 0.3112, 0.6632,\n",
      "        0.0122, 0.3303, 0.6451, 0.4837, 0.3308, 0.7926, 0.9987],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3408, 0.3061, 0.2812, 0.2633, 0.2502, 0.2405,\n",
      "        0.2272, 0.2356, 0.2235, 0.2328, 0.2277, 0.2176, 0.2221, 0.2254, 0.2342,\n",
      "        0.2144, 0.2331, 0.2401, 0.2331, 0.2216, 0.2314, 0.2348])\n",
      "finalReturns:  tensor([0.0870, 0.0951, 0.0881, 0.0525])\n",
      "----------------------------------------\n",
      "iter  1  stage  20  ep  29999   adversary:  const95-1.0,\n",
      "  actions:  tensor([ 9, 12, 12, 12,  9, 12, 12, 15, 12,  9,  9, 12,  9, 12, 12,  9, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.2135, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0212, 1.0212, 1.0212, 1.0212, 1.0212, 1.0212, 1.0212, 1.0212, 1.0212,\n",
      "        1.0212, 1.0212, 1.0212, 1.0212, 1.0212, 1.0212, 1.0212, 1.0212, 1.0212,\n",
      "        1.0212, 1.0212, 1.0212, 0.7747, 0.5564, 0.3579, 0.1738]) return=  67469.79672706983\n",
      "probs of actions:  tensor([0.2699, 0.6926, 0.7001, 0.6739, 0.2550, 0.6957, 0.6790, 0.0173, 0.6889,\n",
      "        0.2628, 0.2843, 0.6976, 0.2999, 0.6886, 0.6446, 0.2603, 0.6645, 0.6650,\n",
      "        0.6531, 0.6966, 0.6835, 0.8314, 0.7349, 0.5680, 0.9983],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4130, 0.3642, 0.3296, 0.3110, 0.2784, 0.2674, 0.2512, 0.2611,\n",
      "        0.2609, 0.2484, 0.2330, 0.2399, 0.2267, 0.2289, 0.2369, 0.2244, 0.2272,\n",
      "        0.2293, 0.2309, 0.2320, 0.2329, 0.2336, 0.2341, 0.2489])\n",
      "finalReturns:  tensor([0.1603, 0.1747, 0.1602, 0.1251, 0.0751])\n",
      "----------------------------------------\n",
      "iter  1  stage  20  ep  59999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12,  9, 12, 12, 12, 12,  9, 12, 12, 12,  9, 12, 12,  9,\n",
      "        12, 12, 12, 12,  9, 12,  0])\n",
      "loss=  tensor(0.3982, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0143, 1.0143, 1.0143, 1.0143, 1.0143, 1.0143, 1.0143, 1.0143, 1.0143,\n",
      "        1.0143, 1.0143, 1.0143, 1.0143, 1.0143, 1.0143, 1.0143, 1.0143, 1.0143,\n",
      "        1.0143, 1.0143, 1.0143, 0.7703, 0.5536, 0.3564, 0.1731]) return=  67733.46532960936\n",
      "probs of actions:  tensor([0.8418, 0.8431, 0.8455, 0.8234, 0.8511, 0.1417, 0.8230, 0.8359, 0.8338,\n",
      "        0.8354, 0.1529, 0.8379, 0.8105, 0.8279, 0.1763, 0.8419, 0.8088, 0.1605,\n",
      "        0.8070, 0.8347, 0.8422, 0.9195, 0.1189, 0.7536, 0.9979],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2955, 0.2673, 0.2592, 0.2532,\n",
      "        0.2487, 0.2517, 0.2354, 0.2355, 0.2355, 0.2418, 0.2281, 0.2300, 0.2377,\n",
      "        0.2250, 0.2276, 0.2296, 0.2311, 0.2385, 0.2257, 0.2425])\n",
      "finalReturns:  tensor([0.1531, 0.1675, 0.1531, 0.1118, 0.0694])\n",
      "----------------------------------------\n",
      "iter  1  stage  20  ep  89999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0534, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 0.7825, 0.5612, 0.3606, 0.1749]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9099, 0.9138, 0.9140, 0.8978, 0.9148, 0.9090, 0.8954, 0.9049, 0.9047,\n",
      "        0.9035, 0.8959, 0.9057, 0.8875, 0.8993, 0.8838, 0.9082, 0.8855, 0.8936,\n",
      "        0.8859, 0.9043, 0.9167, 0.9566, 0.9230, 0.8634, 0.9973],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.1613, 0.1757, 0.1609, 0.1255, 0.0753])\n",
      "----------------------------------------\n",
      "iter  1  stage  20  ep  119999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 15,  9, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0448, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0323, 1.0323, 1.0323, 1.0323, 1.0323, 1.0323, 1.0323, 1.0323, 1.0323,\n",
      "        1.0323, 1.0323, 1.0323, 1.0323, 1.0323, 1.0323, 1.0323, 1.0323, 1.0323,\n",
      "        1.0323, 1.0323, 1.0323, 0.7819, 0.5608, 0.3604, 0.1748]) return=  68680.34770364207\n",
      "probs of actions:  tensor([0.9237, 0.9274, 0.9270, 0.9128, 0.9279, 0.9232, 0.9108, 0.9185, 0.9195,\n",
      "        0.9184, 0.9107, 0.9201, 0.0090, 0.0734, 0.9014, 0.9213, 0.9018, 0.9092,\n",
      "        0.9019, 0.9186, 0.9349, 0.9651, 0.9375, 0.8722, 0.9980],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2344, 0.2547, 0.2376, 0.2371, 0.2367, 0.2364,\n",
      "        0.2362, 0.2361, 0.2359, 0.2359, 0.2358, 0.2357, 0.2501])\n",
      "finalReturns:  tensor([0.1612, 0.1756, 0.1609, 0.1255, 0.0753])\n",
      "----------------------------------------\n",
      "iter  1  stage  20  ep  149999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0309, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 0.7825, 0.5612, 0.3606, 0.1749]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9452, 0.9495, 0.9486, 0.9376, 0.9484, 0.9455, 0.9354, 0.9409, 0.9425,\n",
      "        0.9408, 0.9347, 0.9422, 0.9302, 0.9372, 0.9278, 0.9434, 0.9283, 0.9348,\n",
      "        0.9291, 0.9408, 0.9553, 0.9748, 0.9568, 0.9097, 0.9988],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.1613, 0.1757, 0.1609, 0.1255, 0.0753])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  20  ep  179999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0189, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 0.7825, 0.5612, 0.3606, 0.1749]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9643, 0.9682, 0.9674, 0.9595, 0.9663, 0.9648, 0.9575, 0.9611, 0.9624,\n",
      "        0.9606, 0.9569, 0.9619, 0.9534, 0.9585, 0.9525, 0.9628, 0.9520, 0.9574,\n",
      "        0.9532, 0.9609, 0.9724, 0.9833, 0.9721, 0.9466, 0.9995],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.1613, 0.1757, 0.1609, 0.1255, 0.0753])\n",
      "----------------------------------------\n",
      "iter  1  stage  20  ep  209999   adversary:  const95-1.0,\n",
      "  actions:  tensor([ 9, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 15, 12, 12,\n",
      "         9, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0135, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0239, 1.0239, 1.0239, 1.0239, 1.0239, 1.0239, 1.0239, 1.0239, 1.0239,\n",
      "        1.0239, 1.0239, 1.0239, 1.0239, 1.0239, 1.0239, 1.0239, 1.0239, 1.0239,\n",
      "        1.0239, 1.0239, 1.0239, 0.7765, 0.5575, 0.3585, 0.1740]) return=  68418.15387194039\n",
      "probs of actions:  tensor([0.0213, 0.9771, 0.9759, 0.9698, 0.9749, 0.9739, 0.9680, 0.9708, 0.9719,\n",
      "        0.9702, 0.9677, 0.9714, 0.9648, 0.9688, 0.9647, 0.0047, 0.9634, 0.9685,\n",
      "        0.0284, 0.9710, 0.9806, 0.9873, 0.9793, 0.9627, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4130, 0.3642, 0.3296, 0.3047, 0.2866, 0.2734, 0.2637, 0.2565,\n",
      "        0.2512, 0.2473, 0.2443, 0.2421, 0.2405, 0.2393, 0.2302, 0.2452, 0.2428,\n",
      "        0.2473, 0.2321, 0.2330, 0.2337, 0.2341, 0.2345, 0.2492])\n",
      "finalReturns:  tensor([0.1605, 0.1749, 0.1604, 0.1252, 0.0752])\n",
      "----------------------------------------\n",
      "iter  1  stage  20  ep  239999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0093, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 0.7825, 0.5612, 0.3606, 0.1749]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9810, 0.9838, 0.9830, 0.9785, 0.9821, 0.9814, 0.9769, 0.9789, 0.9797,\n",
      "        0.9782, 0.9766, 0.9792, 0.9742, 0.9773, 0.9744, 0.9799, 0.9734, 0.9771,\n",
      "        0.9746, 0.9788, 0.9866, 0.9908, 0.9859, 0.9750, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.1613, 0.1757, 0.1609, 0.1255, 0.0753])\n",
      "----------------------------------------\n",
      "iter  1  stage  19  ep  29999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0147, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 0.9500, 0.7285, 0.5277, 0.3420, 0.1670]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9829, 0.9857, 0.9848, 0.9809, 0.9839, 0.9834, 0.9793, 0.9810, 0.9817,\n",
      "        0.9800, 0.9791, 0.9810, 0.9765, 0.9796, 0.9771, 0.9821, 0.9761, 0.9795,\n",
      "        0.9773, 0.9816, 0.9874, 0.9913, 0.9874, 0.9803, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.2301, 0.2445, 0.2297, 0.1944, 0.1441, 0.0832])\n",
      "----------------------------------------\n",
      "iter  1  stage  19  ep  59999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  9, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0115, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.1881, 1.1881, 1.1881, 1.1881, 1.1881, 1.1881, 1.1881, 1.1881, 1.1881,\n",
      "        1.1881, 1.1881, 1.1881, 1.1881, 1.1881, 1.1881, 1.1881, 1.1881, 1.1881,\n",
      "        1.1881, 1.1881, 0.9414, 0.7229, 0.5243, 0.3400, 0.1662]) return=  68487.85710629346\n",
      "probs of actions:  tensor([0.9863, 0.9887, 0.9880, 0.9849, 0.9871, 0.9869, 0.9833, 0.9847, 0.9852,\n",
      "        0.9837, 0.9833, 0.9847, 0.9810, 0.9835, 0.9818, 0.9857, 0.0142, 0.9837,\n",
      "        0.9816, 0.9863, 0.9894, 0.9925, 0.9901, 0.9853, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2441, 0.2298,\n",
      "        0.2312, 0.2323, 0.2331, 0.2337, 0.2342, 0.2346, 0.2492])\n",
      "finalReturns:  tensor([0.2290, 0.2434, 0.2289, 0.1937, 0.1437, 0.0830])\n",
      "----------------------------------------\n",
      "iter  1  stage  19  ep  89999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  9, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0109, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.1969, 1.1969, 1.1969, 1.1969, 1.1969, 1.1969, 1.1969, 1.1969, 1.1969,\n",
      "        1.1969, 1.1969, 1.1969, 1.1969, 1.1969, 1.1969, 1.1969, 1.1969, 1.1969,\n",
      "        1.1969, 1.1969, 0.9473, 0.7267, 0.5266, 0.3414, 0.1668]) return=  68466.13679373146\n",
      "probs of actions:  tensor([0.9871, 0.9894, 0.9886, 0.9857, 0.9878, 0.9876, 0.9842, 0.9854, 0.9859,\n",
      "        0.9844, 0.9841, 0.9854, 0.0124, 0.9845, 0.9828, 0.9864, 0.9816, 0.9844,\n",
      "        0.9827, 0.9874, 0.9904, 0.9920, 0.9903, 0.9871, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2488, 0.2332, 0.2338, 0.2343, 0.2346, 0.2348,\n",
      "        0.2350, 0.2352, 0.2353, 0.2354, 0.2354, 0.2355, 0.2499])\n",
      "finalReturns:  tensor([0.2297, 0.2441, 0.2295, 0.1941, 0.1440, 0.0831])\n",
      "----------------------------------------\n",
      "iter  1  stage  19  ep  119999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0119, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 0.9500, 0.7285, 0.5277, 0.3420, 0.1670]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9861, 0.9886, 0.9877, 0.9847, 0.9868, 0.9867, 0.9830, 0.9844, 0.9848,\n",
      "        0.9831, 0.9829, 0.9842, 0.9807, 0.9830, 0.9819, 0.9855, 0.9805, 0.9834,\n",
      "        0.9816, 0.9860, 0.9886, 0.9912, 0.9900, 0.9871, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.2301, 0.2445, 0.2297, 0.1944, 0.1441, 0.0832])\n",
      "----------------------------------------\n",
      "iter  1  stage  19  ep  149999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12,  9, 12, 12, 12,  0])\n",
      "loss=  tensor(1.3077, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 0.9500, 0.7285, 0.5277, 0.3420, 0.1670]) return=  68553.0200585037\n",
      "probs of actions:  tensor([0.9907, 0.9926, 0.9919, 0.9898, 0.9912, 0.9911, 0.9885, 0.9896, 0.9898,\n",
      "        0.9885, 0.9884, 0.9893, 0.9868, 0.9885, 0.9877, 0.9902, 0.9867, 0.9888,\n",
      "        0.9876, 0.9906, 0.0035, 0.9945, 0.9941, 0.9918, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2426, 0.2287, 0.2304, 0.2317, 0.2471])\n",
      "finalReturns:  tensor([0.2160, 0.2304, 0.2093, 0.1814, 0.1368, 0.0800])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  19  ep  179999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0061, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 0.9500, 0.7285, 0.5277, 0.3420, 0.1670]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9924, 0.9940, 0.9934, 0.9917, 0.9928, 0.9928, 0.9905, 0.9914, 0.9916,\n",
      "        0.9904, 0.9904, 0.9912, 0.9891, 0.9904, 0.9901, 0.9920, 0.9890, 0.9909,\n",
      "        0.9898, 0.9925, 0.9938, 0.9950, 0.9955, 0.9941, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.2301, 0.2445, 0.2297, 0.1944, 0.1441, 0.0832])\n",
      "----------------------------------------\n",
      "iter  1  stage  19  ep  209999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0064, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 0.9500, 0.7285, 0.5277, 0.3420, 0.1670]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9923, 0.9939, 0.9933, 0.9915, 0.9926, 0.9926, 0.9902, 0.9912, 0.9913,\n",
      "        0.9900, 0.9902, 0.9909, 0.9888, 0.9902, 0.9899, 0.9918, 0.9888, 0.9907,\n",
      "        0.9896, 0.9921, 0.9934, 0.9947, 0.9953, 0.9946, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.2301, 0.2445, 0.2297, 0.1944, 0.1441, 0.0832])\n",
      "----------------------------------------\n",
      "iter  1  stage  19  ep  239999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0052, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 0.9500, 0.7285, 0.5277, 0.3420, 0.1670]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9937, 0.9951, 0.9945, 0.9930, 0.9939, 0.9940, 0.9920, 0.9928, 0.9928,\n",
      "        0.9917, 0.9919, 0.9925, 0.9907, 0.9918, 0.9918, 0.9932, 0.9907, 0.9924,\n",
      "        0.9915, 0.9935, 0.9943, 0.9953, 0.9964, 0.9963, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.2301, 0.2445, 0.2297, 0.1944, 0.1441, 0.0832])\n",
      "----------------------------------------\n",
      "iter  1  stage  19  ep  269999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0042, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 0.9500, 0.7285, 0.5277, 0.3420, 0.1670]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9949, 0.9961, 0.9956, 0.9943, 0.9950, 0.9951, 0.9934, 0.9942, 0.9941,\n",
      "        0.9931, 0.9934, 0.9938, 0.9924, 0.9933, 0.9934, 0.9945, 0.9925, 0.9938,\n",
      "        0.9930, 0.9945, 0.9955, 0.9960, 0.9971, 0.9974, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.2301, 0.2445, 0.2297, 0.1944, 0.1441, 0.0832])\n",
      "----------------------------------------\n",
      "iter  1  stage  19  ep  299999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0036, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 0.9500, 0.7285, 0.5277, 0.3420, 0.1670]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9955, 0.9965, 0.9961, 0.9950, 0.9956, 0.9957, 0.9942, 0.9949, 0.9947,\n",
      "        0.9938, 0.9941, 0.9945, 0.9932, 0.9940, 0.9942, 0.9951, 0.9933, 0.9945,\n",
      "        0.9938, 0.9955, 0.9958, 0.9966, 0.9976, 0.9978, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.2301, 0.2445, 0.2297, 0.1944, 0.1441, 0.0832])\n",
      "----------------------------------------\n",
      "iter  1  stage  18  ep  29999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0060, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631,\n",
      "        1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631,\n",
      "        1.3631, 1.1119, 0.8901, 0.6892, 0.5034, 0.3283, 0.1613]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9956, 0.9966, 0.9962, 0.9951, 0.9957, 0.9958, 0.9943, 0.9950, 0.9948,\n",
      "        0.9939, 0.9942, 0.9946, 0.9934, 0.9942, 0.9944, 0.9952, 0.9935, 0.9947,\n",
      "        0.9950, 0.9955, 0.9961, 0.9962, 0.9977, 0.9982, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.3047, 0.3191, 0.3044, 0.2690, 0.2187, 0.1578, 0.0890])\n",
      "----------------------------------------\n",
      "iter  1  stage  18  ep  59999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0030, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631,\n",
      "        1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631,\n",
      "        1.3631, 1.1119, 0.8901, 0.6892, 0.5034, 0.3283, 0.1613]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9976, 0.9982, 0.9980, 0.9974, 0.9977, 0.9978, 0.9969, 0.9973, 0.9972,\n",
      "        0.9966, 0.9968, 0.9971, 0.9963, 0.9968, 0.9969, 0.9974, 0.9963, 0.9971,\n",
      "        0.9975, 0.9977, 0.9981, 0.9982, 0.9989, 0.9991, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.3047, 0.3191, 0.3044, 0.2690, 0.2187, 0.1578, 0.0890])\n",
      "----------------------------------------\n",
      "iter  1  stage  18  ep  89999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0034, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631,\n",
      "        1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631,\n",
      "        1.3631, 1.1119, 0.8901, 0.6892, 0.5034, 0.3283, 0.1613]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9973, 0.9980, 0.9977, 0.9970, 0.9973, 0.9975, 0.9965, 0.9969, 0.9968,\n",
      "        0.9961, 0.9964, 0.9967, 0.9959, 0.9964, 0.9966, 0.9971, 0.9959, 0.9968,\n",
      "        0.9972, 0.9974, 0.9977, 0.9979, 0.9989, 0.9991, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.3047, 0.3191, 0.3044, 0.2690, 0.2187, 0.1578, 0.0890])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  18  ep  119999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0022, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631,\n",
      "        1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631,\n",
      "        1.3631, 1.1119, 0.8901, 0.6892, 0.5034, 0.3283, 0.1613]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9981, 0.9986, 0.9984, 0.9979, 0.9982, 0.9983, 0.9975, 0.9978, 0.9977,\n",
      "        0.9973, 0.9975, 0.9977, 0.9971, 0.9974, 0.9976, 0.9979, 0.9971, 0.9977,\n",
      "        0.9980, 0.9986, 0.9984, 0.9985, 0.9993, 0.9995, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.3047, 0.3191, 0.3044, 0.2690, 0.2187, 0.1578, 0.0890])\n",
      "----------------------------------------\n",
      "iter  1  stage  18  ep  149999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0017, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631,\n",
      "        1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631,\n",
      "        1.3631, 1.1119, 0.8901, 0.6892, 0.5034, 0.3283, 0.1613]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9984, 0.9989, 0.9987, 0.9983, 0.9984, 0.9986, 0.9979, 0.9982, 0.9981,\n",
      "        0.9977, 0.9979, 0.9980, 0.9975, 0.9978, 0.9980, 0.9983, 0.9975, 0.9981,\n",
      "        0.9985, 0.9989, 0.9987, 0.9988, 0.9994, 0.9997, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.3047, 0.3191, 0.3044, 0.2690, 0.2187, 0.1578, 0.0890])\n",
      "----------------------------------------\n",
      "iter  1  stage  18  ep  179999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0014, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631,\n",
      "        1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631,\n",
      "        1.3631, 1.1119, 0.8901, 0.6892, 0.5034, 0.3283, 0.1613]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9987, 0.9991, 0.9989, 0.9985, 0.9987, 0.9988, 0.9982, 0.9985, 0.9984,\n",
      "        0.9980, 0.9982, 0.9983, 0.9978, 0.9982, 0.9983, 0.9985, 0.9979, 0.9984,\n",
      "        0.9988, 0.9992, 0.9989, 0.9990, 0.9995, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.3047, 0.3191, 0.3044, 0.2690, 0.2187, 0.1578, 0.0890])\n",
      "----------------------------------------\n",
      "iter  1  stage  18  ep  190012   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 15, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0011, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.3810, 1.3810, 1.3810, 1.3810, 1.3810, 1.3810, 1.3810, 1.3810, 1.3810,\n",
      "        1.3810, 1.3810, 1.3810, 1.3810, 1.3810, 1.3810, 1.3810, 1.3810, 1.3810,\n",
      "        1.3810, 1.1241, 0.8984, 0.6947, 0.5068, 0.3302, 0.1621]) return=  68884.88646283273\n",
      "probs of actions:  tensor([0.9989, 0.9992, 0.9990, 0.9987, 0.9988, 0.9989, 0.9984, 0.9987, 0.9986,\n",
      "        0.9982, 0.9984, 0.9985, 0.9981, 0.9984, 0.9985, 0.9987, 0.0012, 0.9986,\n",
      "        0.9990, 0.9993, 0.9991, 0.9992, 0.9996, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2297, 0.2448,\n",
      "        0.2425, 0.2408, 0.2395, 0.2385, 0.2378, 0.2372, 0.2512])\n",
      "finalReturns:  tensor([0.3064, 0.3208, 0.3058, 0.2700, 0.2195, 0.1582, 0.0892])\n",
      "----------------------------------------\n",
      "iter  1  stage  17  ep  1044   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0017, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.5214, 1.5214, 1.5214, 1.5214, 1.5214, 1.5214, 1.5214, 1.5214, 1.5214,\n",
      "        1.5214, 1.5214, 1.5214, 1.5214, 1.5214, 1.5214, 1.5214, 1.5214, 1.5214,\n",
      "        1.2697, 1.0477, 0.8466, 0.6606, 0.4855, 0.3183, 0.1570]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9989, 0.9992, 0.9991, 0.9987, 0.9989, 0.9990, 0.9985, 0.9987, 0.9986,\n",
      "        0.9983, 0.9984, 0.9986, 0.9981, 0.9984, 0.9986, 0.9988, 0.9982, 0.9990,\n",
      "        0.9990, 0.9993, 0.9991, 0.9992, 0.9996, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.3837, 0.3981, 0.3833, 0.3478, 0.2976, 0.2366, 0.1678, 0.0932])\n",
      "----------------------------------------\n",
      "iter  1  stage  16  ep  29999   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0017, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.6770,\n",
      "        1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.4248,\n",
      "        1.2024, 1.0011, 0.8148, 0.6396, 0.4723, 0.3109, 0.1538]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9992, 0.9994, 0.9993, 0.9991, 0.9992, 0.9993, 0.9989, 0.9991, 0.9990,\n",
      "        0.9987, 0.9989, 0.9990, 0.9986, 0.9989, 0.9990, 0.9991, 0.9989, 0.9994,\n",
      "        0.9994, 0.9996, 0.9994, 0.9996, 0.9997, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.4658, 0.4802, 0.4654, 0.4299, 0.3796, 0.3186, 0.2498, 0.1752, 0.0964])\n",
      "----------------------------------------\n",
      "iter  1  stage  16  ep  35741   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 15, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0015, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.6877, 1.6877, 1.6877, 1.6877, 1.6877, 1.6877, 1.6877, 1.6877, 1.6877,\n",
      "        1.6877, 1.6877, 1.6877, 1.6877, 1.6877, 1.6877, 1.6877, 1.6877, 1.4323,\n",
      "        1.2076, 1.0047, 0.8173, 0.6412, 0.4733, 0.3115, 0.1541]) return=  68906.62996763481\n",
      "probs of actions:  tensor([9.9926e-01, 9.9949e-01, 9.9938e-01, 9.9916e-01, 9.9924e-01, 9.9932e-01,\n",
      "        9.9897e-01, 9.9913e-01, 9.9906e-01, 9.9883e-01, 9.9895e-01, 9.9905e-01,\n",
      "        8.9071e-04, 9.9894e-01, 9.9906e-01, 9.9918e-01, 9.9901e-01, 9.9949e-01,\n",
      "        9.9950e-01, 9.9960e-01, 9.9945e-01, 9.9967e-01, 9.9976e-01, 9.9999e-01,\n",
      "        1.0000e+00], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2344, 0.2484, 0.2451, 0.2427, 0.2409, 0.2396,\n",
      "        0.2386, 0.2378, 0.2373, 0.2369, 0.2365, 0.2363, 0.2505])\n",
      "finalReturns:  tensor([0.4669, 0.4813, 0.4663, 0.4307, 0.3802, 0.3190, 0.2501, 0.1754, 0.0964])\n",
      "----------------------------------------\n",
      "iter  1  stage  15  ep  0   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0021, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.8309, 1.8309, 1.8309, 1.8309, 1.8309, 1.8309, 1.8309, 1.8309, 1.8309,\n",
      "        1.8309, 1.8309, 1.8309, 1.8309, 1.8309, 1.8309, 1.8309, 1.5780, 1.3551,\n",
      "        1.1534, 0.9669, 0.7914, 0.6240, 0.4625, 0.3054, 0.1515]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9992, 0.9992, 0.9993, 0.9990, 0.9991, 0.9991,\n",
      "        0.9988, 0.9989, 0.9990, 0.9987, 0.9990, 0.9990, 0.9992, 0.9990, 0.9995,\n",
      "        0.9995, 0.9996, 0.9995, 0.9997, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.5504, 0.5648, 0.5499, 0.5144, 0.4641, 0.4030, 0.3342, 0.2596, 0.1807,\n",
      "        0.0987])\n",
      "----------------------------------------\n",
      "iter  1  stage  14  ep  0   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0030, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.9839, 1.9839, 1.9839, 1.9839, 1.9839, 1.9839, 1.9839, 1.9839, 1.9839,\n",
      "        1.9839, 1.9839, 1.9839, 1.9839, 1.9839, 1.9839, 1.7301, 1.5065, 1.3043,\n",
      "        1.1175, 0.9417, 0.7741, 0.6125, 0.4553, 0.3013, 0.1497]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9992, 0.9992, 0.9993, 0.9990, 0.9991, 0.9991,\n",
      "        0.9988, 0.9989, 0.9990, 0.9987, 0.9990, 0.9990, 0.9992, 0.9990, 0.9995,\n",
      "        0.9995, 0.9996, 0.9995, 0.9997, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.6368, 0.6512, 0.6363, 0.6007, 0.5503, 0.4892, 0.4203, 0.3457, 0.2668,\n",
      "        0.1848, 0.1005])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  13  ep  60   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0039, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.1368, 2.1368, 2.1368, 2.1368, 2.1368, 2.1368, 2.1368, 2.1368, 2.1368,\n",
      "        2.1368, 2.1368, 2.1368, 2.1368, 2.1368, 1.8817, 1.6572, 1.4543, 1.2670,\n",
      "        1.0910, 0.9231, 0.7613, 0.6040, 0.4499, 0.2982, 0.1484]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9992, 0.9992, 0.9993, 0.9990, 0.9991, 0.9991,\n",
      "        0.9988, 0.9989, 0.9990, 0.9987, 0.9990, 0.9991, 0.9992, 0.9990, 0.9995,\n",
      "        0.9995, 0.9996, 0.9994, 0.9997, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.7246, 0.7390, 0.7241, 0.6884, 0.6380, 0.5768, 0.5079, 0.4332, 0.3542,\n",
      "        0.2722, 0.1879, 0.1018])\n",
      "----------------------------------------\n",
      "iter  1  stage  12  ep  674   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0047, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.2903, 2.2903, 2.2903, 2.2903, 2.2903, 2.2903, 2.2903, 2.2903, 2.2903,\n",
      "        2.2903, 2.2903, 2.2903, 2.2903, 2.0334, 1.8077, 1.6040, 1.4161, 1.2396,\n",
      "        1.0714, 0.9093, 0.7518, 0.5976, 0.4458, 0.2960, 0.1475]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9992, 0.9992, 0.9993, 0.9990, 0.9991, 0.9991,\n",
      "        0.9988, 0.9989, 0.9991, 0.9990, 0.9992, 0.9993, 0.9993, 0.9990, 0.9995,\n",
      "        0.9995, 0.9996, 0.9994, 0.9997, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.8136, 0.8280, 0.8130, 0.7773, 0.7267, 0.6655, 0.5964, 0.5217, 0.4427,\n",
      "        0.3606, 0.2762, 0.1901, 0.1027])\n",
      "----------------------------------------\n",
      "iter  1  stage  11  ep  0   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0058, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.4451, 2.4451, 2.4451, 2.4451, 2.4451, 2.4451, 2.4451, 2.4451, 2.4451,\n",
      "        2.4451, 2.4451, 2.4451, 2.1859, 1.9586, 1.7537, 1.5650, 1.3879, 1.2192,\n",
      "        1.0569, 0.8991, 0.7447, 0.5928, 0.4428, 0.2943, 0.1467]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9992, 0.9992, 0.9993, 0.9990, 0.9991, 0.9991,\n",
      "        0.9988, 0.9989, 0.9991, 0.9990, 0.9992, 0.9993, 0.9993, 0.9990, 0.9995,\n",
      "        0.9995, 0.9996, 0.9994, 0.9997, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.9036, 0.9180, 0.9029, 0.8670, 0.8163, 0.7549, 0.6858, 0.6110, 0.5319,\n",
      "        0.4498, 0.3654, 0.2792, 0.1918, 0.1035])\n",
      "----------------------------------------\n",
      "iter  1  stage  10  ep  58   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0070, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.6021, 2.6021, 2.6021, 2.6021, 2.6021, 2.6021, 2.6021, 2.6021, 2.6021,\n",
      "        2.6021, 2.6021, 2.3399, 2.1104, 1.9040, 1.7141, 1.5362, 1.3670, 1.2041,\n",
      "        1.0460, 0.8914, 0.7394, 0.5893, 0.4406, 0.2930, 0.1462]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9992, 0.9992, 0.9993, 0.9990, 0.9991, 0.9991,\n",
      "        0.9988, 0.9990, 0.9991, 0.9990, 0.9992, 0.9993, 0.9993, 0.9990, 0.9995,\n",
      "        0.9995, 0.9996, 0.9994, 0.9997, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.9944, 1.0088, 0.9935, 0.9575, 0.9066, 0.8451, 0.7758, 0.7009, 0.6218,\n",
      "        0.5396, 0.4551, 0.3689, 0.2815, 0.1931, 0.1040])\n",
      "----------------------------------------\n",
      "iter  1  stage  9  ep  794   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 15, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0075, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.7830, 2.7830, 2.7830, 2.7830, 2.7830, 2.7830, 2.7830, 2.7830, 2.7830,\n",
      "        2.7830, 2.5107, 2.2742, 2.0629, 1.8694, 1.6889, 1.5178, 1.3536, 1.1945,\n",
      "        1.0391, 0.8865, 0.7360, 0.5870, 0.4391, 0.2922, 0.1458]) return=  68919.5088490973\n",
      "probs of actions:  tensor([9.9929e-01, 9.9951e-01, 9.9940e-01, 9.9919e-01, 9.9927e-01, 9.9935e-01,\n",
      "        9.9900e-01, 6.2436e-04, 9.9908e-01, 9.9900e-01, 9.9922e-01, 9.9933e-01,\n",
      "        9.9913e-01, 9.9936e-01, 9.9929e-01, 9.9932e-01, 9.9904e-01, 9.9950e-01,\n",
      "        9.9951e-01, 9.9961e-01, 9.9946e-01, 9.9970e-01, 9.9976e-01, 1.0000e+00,\n",
      "        1.0000e+00], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2570, 0.2655,\n",
      "        0.2578, 0.2522, 0.2480, 0.2449, 0.2425, 0.2408, 0.2395, 0.2385, 0.2378,\n",
      "        0.2372, 0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2503])\n",
      "finalReturns:  tensor([1.0883, 1.1027, 1.0870, 1.0504, 0.9989, 0.9369, 0.8672, 0.7919, 0.7126,\n",
      "        0.6302, 0.5455, 0.4592, 0.3717, 0.2833, 0.1941, 0.1044])\n",
      "----------------------------------------\n",
      "iter  1  stage  8  ep  0   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0090, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.9276, 2.9276, 2.9276, 2.9276, 2.9276, 2.9276, 2.9276, 2.9276, 2.9276,\n",
      "        2.6556, 2.4193, 2.2080, 2.0147, 1.8342, 1.6632, 1.4990, 1.3399, 1.1845,\n",
      "        1.0319, 0.8814, 0.7324, 0.5846, 0.4376, 0.2913, 0.1455]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9992, 0.9993, 0.9993, 0.9990, 0.9992, 0.9991,\n",
      "        0.9990, 0.9992, 0.9993, 0.9991, 0.9994, 0.9993, 0.9993, 0.9990, 0.9995,\n",
      "        0.9995, 0.9996, 0.9995, 0.9997, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.1785, 1.1929, 1.1773, 1.1406, 1.0892, 1.0272, 0.9575, 0.8823, 0.8029,\n",
      "        0.7205, 0.6359, 0.5496, 0.4620, 0.3736, 0.2844, 0.1948, 0.1047])\n",
      "----------------------------------------\n",
      "iter  1  stage  7  ep  0   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0103, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.0992, 3.0992, 3.0992, 3.0992, 3.0992, 3.0992, 3.0992, 3.0992, 2.8197,\n",
      "        2.5781, 2.3632, 2.1671, 1.9848, 1.8123, 1.6471, 1.4872, 1.3313, 1.1783,\n",
      "        1.0274, 0.8782, 0.7302, 0.5831, 0.4367, 0.2908, 0.1453]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9992, 0.9993, 0.9993, 0.9990, 0.9992, 0.9991,\n",
      "        0.9990, 0.9992, 0.9993, 0.9991, 0.9994, 0.9993, 0.9993, 0.9990, 0.9995,\n",
      "        0.9995, 0.9996, 0.9995, 0.9997, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.2720, 1.2864, 1.2704, 1.2334, 1.1815, 1.1191, 1.0491, 0.9736, 0.8941,\n",
      "        0.8115, 0.7268, 0.6404, 0.5528, 0.4643, 0.3751, 0.2854, 0.1953, 0.1050])\n",
      "----------------------------------------\n",
      "iter  1  stage  6  ep  2   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0120, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.2799, 3.2799, 3.2799, 3.2799, 3.2799, 3.2799, 3.2799, 2.9902, 2.7415,\n",
      "        2.5215, 2.3219, 2.1370, 1.9626, 1.7960, 1.6351, 1.4784, 1.3248, 1.1736,\n",
      "        1.0241, 0.8758, 0.7286, 0.5820, 0.4360, 0.2904, 0.1451]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9992, 0.9993, 0.9993, 0.9990, 0.9992, 0.9991,\n",
      "        0.9990, 0.9992, 0.9993, 0.9991, 0.9994, 0.9993, 0.9993, 0.9990, 0.9995,\n",
      "        0.9995, 0.9996, 0.9995, 0.9997, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.3666, 1.3810, 1.3646, 1.3270, 1.2746, 1.2117, 1.1413, 1.0654, 0.9856,\n",
      "        0.9029, 0.8179, 0.7314, 0.6437, 0.5552, 0.4659, 0.3762, 0.2861, 0.1957,\n",
      "        0.1051])\n",
      "----------------------------------------\n",
      "iter  1  stage  5  ep  0   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0133, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.4731, 3.4731, 3.4731, 3.4731, 3.4731, 3.4731, 3.1695, 2.9111, 2.6844,\n",
      "        2.4799, 2.2915, 2.1146, 1.9461, 1.7839, 1.6262, 1.4718, 1.3200, 1.1701,\n",
      "        1.0215, 0.8740, 0.7273, 0.5812, 0.4355, 0.2901, 0.1450]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9992, 0.9993, 0.9993, 0.9990, 0.9992, 0.9991,\n",
      "        0.9990, 0.9992, 0.9993, 0.9991, 0.9994, 0.9993, 0.9993, 0.9990, 0.9995,\n",
      "        0.9995, 0.9996, 0.9995, 0.9997, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.4627, 1.4771, 1.4601, 1.4217, 1.3686, 1.3050, 1.2341, 1.1578, 1.0776,\n",
      "        0.9945, 0.9094, 0.8228, 0.7349, 0.6463, 0.5570, 0.4672, 0.3770, 0.2866,\n",
      "        0.1960, 0.1053])\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  1  stage  4  ep  0   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0148, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.6835, 3.6835, 3.6835, 3.6835, 3.6835, 3.3608, 3.0893, 2.8534, 2.6424,\n",
      "        2.4492, 2.2689, 2.0980, 1.9339, 1.7748, 1.6195, 1.4669, 1.3164, 1.1675,\n",
      "        1.0197, 0.8727, 0.7264, 0.5805, 0.4351, 0.2899, 0.1449]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9992, 0.9993, 0.9993, 0.9990, 0.9992, 0.9991,\n",
      "        0.9990, 0.9992, 0.9993, 0.9991, 0.9994, 0.9993, 0.9993, 0.9990, 0.9995,\n",
      "        0.9995, 0.9996, 0.9995, 0.9997, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.5606, 1.5750, 1.5572, 1.5179, 1.4637, 1.3993, 1.3276, 1.2507, 1.1700,\n",
      "        1.0866, 1.0012, 0.9143, 0.8264, 0.7376, 0.6482, 0.5583, 0.4681, 0.3777,\n",
      "        0.2870, 0.1962, 0.1054])\n",
      "----------------------------------------\n",
      "iter  1  stage  3  ep  0   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0165, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.9177, 3.9177, 3.9177, 3.9177, 3.5687, 3.2793, 3.0307, 2.8109, 2.6114,\n",
      "        2.4265, 2.2522, 2.0856, 1.9247, 1.7680, 1.6145, 1.4632, 1.3137, 1.1655,\n",
      "        1.0182, 0.8717, 0.7257, 0.5801, 0.4348, 0.2897, 0.1448]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9992, 0.9993, 0.9993, 0.9990, 0.9992, 0.9991,\n",
      "        0.9990, 0.9992, 0.9993, 0.9991, 0.9994, 0.9993, 0.9993, 0.9990, 0.9995,\n",
      "        0.9995, 0.9996, 0.9995, 0.9997, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.6609, 1.6753, 1.6565, 1.6158, 1.5603, 1.4947, 1.4220, 1.3444, 1.2631,\n",
      "        1.1792, 1.0934, 1.0062, 0.9180, 0.8291, 0.7395, 0.6496, 0.5593, 0.4688,\n",
      "        0.3781, 0.2873, 0.1964, 0.1054])\n",
      "----------------------------------------\n",
      "iter  1  stage  2  ep  0   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0178, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.1854, 4.1854, 4.1854, 3.7998, 3.4854, 3.2196, 2.9877, 2.7795, 2.5884,\n",
      "        2.4096, 2.2397, 2.0764, 1.9179, 1.7630, 1.6108, 1.4605, 1.3117, 1.1640,\n",
      "        1.0172, 0.8709, 0.7252, 0.5797, 0.4345, 0.2896, 0.1447]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9992, 0.9993, 0.9993, 0.9990, 0.9992, 0.9991,\n",
      "        0.9990, 0.9992, 0.9993, 0.9991, 0.9994, 0.9993, 0.9993, 0.9990, 0.9995,\n",
      "        0.9995, 0.9996, 0.9995, 0.9997, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.7644, 1.7788, 1.7586, 1.7161, 1.6589, 1.5917, 1.5177, 1.4390, 1.3568,\n",
      "        1.2723, 1.1860, 1.0985, 1.0100, 0.9208, 0.8311, 0.7410, 0.6506, 0.5601,\n",
      "        0.4693, 0.3785, 0.2875, 0.1965, 0.1055])\n",
      "----------------------------------------\n",
      "iter  1  stage  1  ep  0   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0191, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.5005, 4.5005, 4.0633, 3.7141, 3.4244, 3.1758, 2.9558, 2.7562, 2.5713,\n",
      "        2.3970, 2.2304, 2.0695, 1.9128, 1.7592, 1.6080, 1.4584, 1.3102, 1.1629,\n",
      "        1.0164, 0.8704, 0.7248, 0.5795, 0.4344, 0.2895, 0.1447]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9992, 0.9993, 0.9993, 0.9990, 0.9992, 0.9991,\n",
      "        0.9990, 0.9992, 0.9993, 0.9991, 0.9994, 0.9993, 0.9993, 0.9990, 0.9995,\n",
      "        0.9995, 0.9996, 0.9995, 0.9997, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.8720, 1.8864, 1.8645, 1.8196, 1.7600, 1.6907, 1.6150, 1.5348, 1.4516,\n",
      "        1.3662, 1.2792, 1.1911, 1.1022, 1.0128, 0.9228, 0.8326, 0.7421, 0.6514,\n",
      "        0.5606, 0.4697, 0.3787, 0.2877, 0.1966, 0.1055])\n",
      "----------------------------------------\n",
      "iter  1  stage  0  ep  0   adversary:  const95-1.0,\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0208, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.8841, 4.3729, 3.9744, 3.6513, 3.3795, 3.1433, 2.9322, 2.7389, 2.5586,\n",
      "        2.3875, 2.2234, 2.0643, 1.9089, 1.7564, 1.6059, 1.4569, 1.3091, 1.1621,\n",
      "        1.0158, 0.8699, 0.7245, 0.5793, 0.4343, 0.2894, 0.1447]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9992, 0.9993, 0.9993, 0.9990, 0.9992, 0.9991,\n",
      "        0.9990, 0.9992, 0.9993, 0.9991, 0.9994, 0.9993, 0.9993, 0.9990, 0.9995,\n",
      "        0.9995, 0.9996, 0.9995, 0.9997, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.9853, 1.9997, 1.9753, 1.9272, 1.8645, 1.7924, 1.7143, 1.6323, 1.5475,\n",
      "        1.4610, 1.3731, 1.2844, 1.1950, 1.1051, 1.0148, 0.9244, 0.8337, 0.7429,\n",
      "        0.6520, 0.5610, 0.4700, 0.3789, 0.2878, 0.1967, 0.1056])\n",
      "policy saved!\n",
      "1,3,[5e-06,1][1, 10000, 1, 1],1683196677 saved\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4AAAAK4CAYAAAAlapMjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAACBk0lEQVR4nO39f5hcZZ3n/7/eXamESkboBANLCmIQM2GFQFr6A3HyGS+RwTCi0iIICis7y0eu9eM6Im7vJDN8BR1Y4tWzgvOZ1bnwx4oDCwmYaeOgtCzgzixDosFOaAP0EgRDKqxESaNDClLpfn//qFNNdaWqun6dOvXj+biu0N13nXPqXdVN3fWu+77ft7m7AAAAAACdryfqAAAAAAAAzUECCAAAAABdggQQAAAAALoECSAAAAAAdAkSQAAAAADoEnOiDqDR3vzmN/uyZcuiDgMA0ASPP/74r919cdRxtAv6SADoDuX6x45LAJctW6bt27dHHQYAoAnM7JdRx9BO6CMBoDuU6x+ZAgoAAAAAXYIEEAAAAAC6BAkgAAAAAHQJEkAAAAAA6BIkgAAAAADQJUgAAQAAAKBLkAACAAAAQJcgAQQAAACALkECCAAAAABdYk7UAbSa4dGUhkbGtW8irSW9CQ2uXaGBvmTUYQEAAADoYM3KQ0gA8wyPprR+85jSmUlJUmoirfWbxySJJBAAAABAKJqZhzAFNM/QyPj0k56TzkxqaGQ8oogAAAAAdLpm5iEkgHn2TaSragcAAACAejUzDyEBzLOkN1FVOwAAAADUq5l5CAlgnsG1K5SIx2a0JeIxDa5dEVFEAAAAADpdM/MQisDkyS2wpAooAAAAgGZpZh5CAlhgoC9JwgcAAACgqZqVhzAFFAAAAAC6BAkgAAAAAHQJEkAAAAAA6BKsASwwPJqiCAwAAACAjkQCmGd4NKX1m8eUzkxKklITaa3fPCZJJIEAAAAA2h5TQPMMjYxPJ3856cykhkbGI4oIAAAAABqHBDDPvol0Ve0AAAAA0E5IAPMs6U1U1Q4AAAAA7YQEMM/g2hVKxGMz2hLxmAbXrogoIgAAAABoHIrA5MkVeqEKKAAAAIBORAJYYKAvScIHAGgZZvYtSe+X9JK7nx60LZK0UdIySc9L+oi7H4gqRgBA+5h1CqiZfcvMXjKzn+e1LTKzB83smeDrwrzb1pvZbjMbN7O1ee1nmdlYcNtfm5kF7fPMbGPQvs3MluWdc1VwH8+Y2VUNe9QAALSPb0u6oKBtnaSH3H25pIeCnwEAmFUlawC/rQo7HjN7u6TLJZ0WnPNVM8stqvuapGskLQ/+5a55taQD7v42SbdK+lJwrUWSbpB0jqSzJd2Qn2gCANAN3P0fJb1c0HyRpDuC7++QNNDMmAAA7WvWBLDKjuciSfe4++vu/pyk3ZLONrMTJB3t7o+5u0v6TsE5uWvdJ+m8YHRwraQH3f3lYFrLgzoyEW244dGU1mx4WCevu19rNjys4dFU2HcJAEC1jnf3FyUp+HpcqQPN7Boz225m2/fv39+0AAEAranWKqClOp6kpBfyjtsbtCWD7wvbZ5zj7oclvSLp2DLXOkKjOrfh0ZTWbx5TaiItl5SaSGv95jGSQABA23L329293937Fy9eHHU4AICINXobCCvS5mXaaz1nZmODOrehkXGlM5Mz2tKZSQ2NjNd8TQAAQvCrYHaNgq8vRRwPAKBN1JoAlup49ko6Ke+4EyXtC9pPLNI+4xwzmyPpGGWnnJa6VmhSE+mq2gEAiMgWSbniaFdJ+l6EsQAA2kitCWCpjmeLpMuDyp4nK1vs5SfBNNHfmdnqYH3fxwvOyV3rEkkPB+sERyS918wWBsVf3hu0haan2JhjgGmgAIAomNndkh6TtMLM9prZ1ZI2SDrfzJ6RdH7wMwAAs5p1H8Cg43m3pDeb2V5lK3NukLQp6IT2SLpUktx9l5ltkvSkpMOSPuXuuTmVn1S2omhC0g+Df5L0TUl/Z2a7lR35uzy41stm9peSfhoc90V3LyxG01BTRSeYZg2NjLM/IACg6dz9oyVuOq+pgQAAOsKsCWC1HY+73yzp5iLt2yWdXqT9NQUJZJHbviXpW7PF2Az7mAYKAAAAoM01ughMW+tNxEvetqQ30cRIAAAAAKDxSADznLbkTUXbe0waXLuiydEAAAAAQGORAOb552eLLzGccrH+DwAAAEDbIwHMU6YGDAAAAAC0PRJAAAAAAOgSJIAAAAAA0CVIAAEAAACgS5AAAgAAAECXIAEEAAAAgC5BAggAAAAAXYIEEAAAAAC6BAlghYZHU1GHAAAAAAB1IQGs0PrNT0QdAgAAAADUhQSwQunMVNQhAAAAAEBdSAABAAAAoEuQAAIAAABAlyABBAAAAIAuQQIIAAAAAF2CBLAKbAUBAAAAoJ2RAFZhaGQ86hAAAAAAoGYkgFXYN5GOOgQAAAAAqBkJYBWW9CaiDgEAAAAAakYCWIXBtSuiDgEAAAAAakYCWIWBvmTUIQAAAABAzUgAAQAAAKBLkAACAAAAQJcgAayQRR0AAAAAANSJBLBCLjaCBwAAANDeSACrwEbwAIBWYmafNbNdZvZzM7vbzI6KOiYAQGsjAawCG8EDAFqFmSUl/amkfnc/XVJM0uXRRgUAaHUkgFU4JhGPOgQAAPLNkZQwszmS5kvaF3E8AIAWRwJYBaMSDACgRbh7StJfSdoj6UVJr7j7j6KNCgDQ6kgAq3DgYCbqEAAAkCSZ2UJJF0k6WdISSQvM7Moix11jZtvNbPv+/fubHSYAoMWQAFYhxhAgAKB1/JGk59x9v7tnJG2W9AeFB7n77e7e7+79ixcvbnqQAIDWQgJYhUn3qEMAACBnj6TVZjbfzEzSeZKeijgmAECLIwGsAuN/AIBW4e7bJN0n6WeSxpTt02+PNCgAQMubE3UA7YTxPwBAK3H3GyTdEHUcAID2wQggAAAAAHQJEsAqDY+mog4BAAAAAGpCAliloZHxqEMAAAAAgJrUlQCa2WfM7OdmtsvMrg3aFpnZg2b2TPB1Yd7x681st5mNm9navPazzGwsuO2vg2pmMrN5ZrYxaN9mZsvqibcR9k2kow4BAAAAAGpScwJoZqdL+oSksyWdKen9ZrZc0jpJD7n7ckkPBT/LzN4u6XJJp0m6QNJXzSwWXO5rkq6RtDz4d0HQfrWkA+7+Nkm3SvpSrfE2ypLeRNQhAAAAAEBN6hkB/NeStrr7QXc/LOl/SvqQpIsk3REcc4ekgeD7iyTd4+6vu/tzknZLOtvMTpB0tLs/5u4u6TsF5+SudZ+k83Kjg1EZXLsiyrsHAAAAgJrVkwD+XNK7zOxYM5sv6X2STpJ0vLu/KEnB1+OC45OSXsg7f2/Qlgy+L2yfcU6QZL4i6djCQMzsGjPbbmbb9+/fX8dDKq9H0kBfctbjAAAAAKAV1ZwAuvtTyk7JfFDSA5J2Sjpc5pRiI3depr3cOYWx3O7u/e7ev3jx4rJx12NKVAEFAAAA0L7qKgLj7t9093e4+7skvSzpGUm/CqZ1Kvj6UnD4XmVHCHNOlLQvaD+xSPuMc8xsjqRjgvuJDFVAAQAAALSrequAHhd8XSrpYkl3S9oi6argkKskfS/4fouky4PKnicrW+zlJ8E00d+Z2epgfd/HC87JXesSSQ8H6wQjQxVQAAAAAO1qTp3nf9fMjpWUkfQpdz9gZhskbTKzqyXtkXSpJLn7LjPbJOlJZaeKfsrdJ4PrfFLStyUlJP0w+CdJ35T0d2a2W9mRv8vrjLduVAEFAAAA0K7qSgDd/Q+LtP1G0nkljr9Z0s1F2rdLOr1I+2sKEshWce6p4a0xBAAAAIAw1TUFtBvd/8SLUYcAAAAAADUhAazSgYOZqEMAAAAAgJqQAAIAAABAlyABrFJvIh51CAAAAABQExLAKr3/zBOiDgEAAAAAakICWKW7f/KChkdTUYcBAAAAAFUjAazS5JRraGQ86jAAAAAAoGokgDXYN5GOOgQAAAAAqBoJYA2W9CaiDgEAAAAAqkYCWIPBtSuiDgEAAAAAqjYn6gAAAAAAoFsNj6Y0NDKufRNpLelNaHDtCg30JUO7P0YAa0ARGAAAAAD1Gh5Naf3mMaUm0nJJqYm01m8eC3XXARLAGlAEBgAAAEC9hkbGlc5MzmhLZyZDHXAiAawBRWAAAAAA1KvUwFKYA04kgDWgCAwAAACAepUaWApzwIkEsAZhLsoEAAAA0B0G165QIh47ov3gocOhrQMkAQQAAACACAz0JXXLxSvVm4jPaD9wMBNaMRgSQAAAAACIyEBfUgvmHbk7X1jFYEgAaxBmWVYAACplZr1mdp+ZPW1mT5nZO6OOCQBQvWYWgyEBrMGNW3ZFHQIAAJL0FUkPuPupks6U9FTE8QAAatDMYjAkgDWYSGeiDgEA0OXM7GhJ75L0TUly90PuPhFpUACAmhQrBpOIx0LZfYAEEACA9vRWSfsl/TczGzWzb5jZgsKDzOwaM9tuZtv379/f/CgBALPKFYNJ9iZkkpK9Cd1y8cpQdh8gAQQAoD3NkfQOSV9z9z5Jr0paV3iQu9/u7v3u3r948eJmxwgAaDEkgAAAtKe9kva6+7bg5/uUTQgBAG1meDSlwXt3KjWRlktKTaQ1eO9OtoFoJVQCBQBEyd3/j6QXzCy3QOQ8SU9GGBIAoEY3btmlzJTPaMtMeSjFJ0kAaxTGnhwAAFTp05LuMrMnJK2S9J+jDQcAUItSRSbDKD555I6DqEgqhD05AACohrvvkNQfdRwAgNqd/+UfN/X+GAGsUcws6hAAAAAAtLErvv6Ynnnp1ZK394SQcpAA1mjSffaDAAAAAKCER599ueztUyGkHCSANUr2JqIOAQAAAEAHCyPnIAGs0eDaFbMfBAAAAAA1CiPnIAGs0UBfMuoQAAAAAKAqJIAAAAAA0ILC2HqOBLBGbAQPAAAAIExhbD1HAlgjNoIHAAAAEKYwtp4jAazRPjaCBwAAABCiMLaeIwGs0TGJeNQhAAAAAOhgbAPRQl49dJh1gAAAAABCwzYQLSQz6awDBAAAABCaMLaeqysBNLPPmtkuM/u5md1tZkeZ2SIze9DMngm+Lsw7fr2Z7TazcTNbm9d+lpmNBbf9tVl2taOZzTOzjUH7NjNbVk+8jRZGVR4AAAAA3SFWpsZLGNM/pToSQDNLSvpTSf3ufrqkmKTLJa2T9JC7L5f0UPCzzOztwe2nSbpA0lfNLBZc7muSrpG0PPh3QdB+taQD7v42SbdK+lKt8YYhjKo8AAAAALrDm44qXVckjOmfUv1TQOdISpjZHEnzJe2TdJGkO4Lb75A0EHx/kaR73P11d39O0m5JZ5vZCZKOdvfH3N0lfafgnNy17pN0Xm50sBWEUZUHAAAAQHd4JZ0peVsY0z+lOhJAd09J+itJeyS9KOkVd/+RpOPd/cXgmBclHReckpT0Qt4l9gZtyeD7wvYZ57j7YUmvSDq21pgbLaxhWQAAAACdb0mJfCLMPKOeKaALlR2hO1nSEkkLzOzKcqcUafMy7eXOKYzlGjPbbmbb9+/fXz7wBgprWBYAAABA5xtcu0KJeOyI9hdfSev64bFQ7rOeKaB/JOk5d9/v7hlJmyX9gaRfBdM6FXx9KTh+r6ST8s4/Udkpo3uD7wvbZ5wTTDM9RtLLhYG4++3u3u/u/YsXL67jIVUnrGFZAAAAAJ1voC+pWy5eqUR8Zlo25dKdW/eEkgTWkwDukbTazOYH6/LOk/SUpC2SrgqOuUrS94Lvt0i6PKjsebKyxV5+EkwT/Z2ZrQ6u8/GCc3LXukTSw8E6QQAAAABoewN9SaUzU0Vvu3vbC0Xb6zGn1hPdfZuZ3SfpZ5IOSxqVdLuk35O0ycyuVjZJvDQ4fpeZbZL0ZHD8p9x9MrjcJyV9W1JC0g+Df5L0TUl/Z2a7lR35u7zWeMMwPJpiFBAAAABAzc644YGSt4VRdLLmBFCS3P0GSTcUNL+u7GhgseNvlnRzkfbtkk4v0v6aggSyFQ2NjJMAAgAAAKjJFV9/TL99fbLk7WFsO1fvNhBdbR8bwQMAAACo0aPPHlHeZIaPnnNS2dtrQQJYhzk8ewAAAABCctPAyoZfkxSmDpmp7DpAAAAAAKjGbHlEPKRMjQSwTkMj41GHAAAAAKDN3LhlV9nbhy5dFcr9kgDWKcU6QAAAAABVmkhnSt5222WrQis2SQJYpzAq8wAAAABAGEgA6xTG3hwAAAAAOtvC+fGSt4W5zIwEsE7J3kTUIQAAAABoMzd84LSSt4W53RwJYJ0G166IOgQAAAAAbWagL1lyFHBJiINMJIAAALQxM4uZ2aiZ/UPUsQAAqnPDB05TIh6b0ZaIx0IdZCIBrBPbQAAAIvYZSU9FHQQAoHoDfUndcvFKJXsTMmWXl91y8crQKoBK0pzQrtwlwpyfCwBAOWZ2oqQLJd0s6bqIwwEA1GCgLxlqwleIBLBOvWWq9wAAELLbJP0nSW8qdYCZXSPpGklaunRpc6ICAJR0/fCY7ty654j2BXNjuvlD4Y7+SUwBrRu7QAAAomBm75f0krs/Xu44d7/d3fvdvX/x4sVNig4AUEyp5E+SXj00qc/du1PDo6lQYyABrNMr6UzUIQAAutMaSR80s+cl3SPpPWZ2Z7QhAQDKuXvbC2Vvn5zy0GuMkADWKcwSrQAAlOLu6939RHdfJulySQ+7+5URhwUAKGOygumDqZBrjJAA1ol9AAEAAABUwio8JsxpoCSAddr+y5ejDgEA0OXc/cfu/v6o4wAAlNdTQQbokj63Kby1gCSAdZptHi8AAAAASNJkhQUkJ921fvNYKEkgCWCdKpnHCwAAAADVSGcmQykIwz6AAAAAABpieDSloZFx7ZtIa0lvQoNrVzR1k/NOsy+EgjAkgAAAAADqNjya0vrNY0pnJiVlq1mu3zwmSSSBgUS8R+nMVMXHh7HjAFNAAQAAANRtaGR8OvnLCWsaY7u65eIzKj7WFM6OAySAAAAAAOpWarpiGNMY29VAX1JXrl4663Em6YrVS0MZOSUBbIDrh8eiDgEAAACIVKnpimFMY2xn/W9ZpHiRLCy3Q0SyN6FbL1ulmwZWhnL/JIANwFYQAAAA6HaDa1coEY/NaEvEY6FMY2xXw6MpXbdxh4otA3RJsR4LvXAOCWADsBUEAAAAut1AX1K3XLxSyd6ETNmRrFsuXkkBmDxDI+MqVwJmcspDXzNJFVAAAAAADTHQlyThK6OS9ZBhr5lkBBAAAAAAmqB3fnzWY8JeM8kIIAAAAIBQsDH8TJWsHAt7zSQJYAPY7IcAAAAAXYWN4Y/0Sjoz6zFhPzdMAW0ASsAAAAAAM7Ex/JFmm94Zs/CHlkgAAQAAADQcG8MfaXDtCsV7Sid5Hz3npNBjYAooAAAAgIZb0ptQqkiyV0uRk05ZS5iL+cYtuzSRNx20x6SPnbM0tM3f85EAAgAAAGi4wbUrZqwBlGrbGL6T1hLmEtlX0hklI0pkmQIKAAAAoOEatTF8p6wlHB5N6dqNO5SaSMuVTWSv3bhD1w+PNTUOEsAG6fvijzQ8moo6DAAAAKAlFE7bPPfUxRoaGdfJ6+7Xmg0PV/zeuVPWEl67cUfR9ju37mlqHkEC2CAHDma0fvMYSSAAAAC6Xm7aZv5o151b98z4udL3zqXWDIa9YXozNXM0kwSwgdpxKBoAAABotGLTNgtV+t55cO0KJeKxGW21rCVsZc0czaQITIO121A0AAAAKtcp1ShnU+/jrPQ9ce644dHUjMqYC+fHdcMHTtNAX3L6fjv5eT8mEW/afdWcAJrZCkkb85reKunzkr4TtC+T9Lykj7j7geCc9ZKuljQp6U/dfSRoP0vStyUlJP1A0mfc3c1sXnC9syT9RtJl7v58rTE3QycNRQMAAOANnVSNspxij3Pw3p36wvd3aeJgpqIErNQWEMWOGx5NafDencpM+XT7gYMZDd63U5Kmk8BWeI7D+gDg1UOHNTyaaspjrHkKqLuPu/sqd1+lbIJ2UNLfS1on6SF3Xy7poeBnmdnbJV0u6TRJF0j6qpnlxnK/JukaScuDfxcE7VdLOuDub5N0q6Qv1RpvM3TaUDQAAADe0CnVKGdT7HFmplwHDmYqXr9XbNpmMblKmPnJ3/R9TnpLPbfF1jU2qgZIMx9ro9YAnifpWXf/paSLJN0RtN8haSD4/iJJ97j76+7+nKTdks42sxMkHe3uj7m7Kzvil39O7lr3STrPzKxBMTdc7gWAQjAAAACdp1OqUc6mksczW+Kb2wIiEa8v3UhNpGe8tx4eTWnNhoerriTaCGF/ANCsv6NGrQG8XNLdwffHu/uLkuTuL5rZcUF7UtLWvHP2Bm2Z4PvC9tw5LwTXOmxmr0g6VtKv8+/czK5RdgRRS5cubdBDqk2nTgVA8xSbWiAdOe+9njb+NgEAqF6paY2dtgSo0umb+QlLqfcvr2Wm6o4nNxVUUqRTcOv9ACBmpkk/cqQzp1l/R3UngGY2V9IHJa2f7dAibV6mvdw5Mxvcb5d0uyT19/eXflabJPdJAG+yu9cVX39Mjz77ct3XyU2LaHRbfvv8eI/mxWM6cDAjU5H/wRqg8LoxM330nJPU/5ZFMxZ8F8pfAJ5z/fCY7t72gibdZZLiMdOhySOjjplU2FzsegAAVGpw7YoZCYjUmUuAij3OYnIJS7E1g6X2vKtFZtJLXi+s993FEtp6PwD46Dkn6c6te4re1sy/o0aMAP6xpJ+5+6+Cn39lZicEo38nSHopaN8r6aS8806UtC9oP7FIe/45e81sjqRjJNX/rroJOm0qQLfJ/5/+mERcZppe9HzuqYv1yNP7K/pkrB0czEzpYPDpXFifnhRed9Jdd27dU/JFMOfAwcwRCWvhdYslf9n7qP56rSw/iSaRBYDywirUEXU1ymZVIC18nEfFe5QuMpJ37qmLp4+bLVkMUyPedxe+93v10GFlgjcTuZHGD5+V1HcfT5X9AKDc7+imgZWSNP1Bdk6yyX9H5mWGISu6gNk9kkbc/b8FPw9J+o27bzCzdZIWuft/MrPTJP13SWdLWqJsgZjl7j5pZj+V9GlJ25StAvr/ufsPzOxTkla6+783s8slXezuHykXT39/v2/fvr2mx7Js3f01nVdMsjehR9e9p2HXQ/MUfooFtLJmdxqtxswed/f+qONoF/X0kUC7mK0fb9cP0Yo9rkQ8plsuXln3Y8mfXZObpZNLViRpzYaHi37wHdbMoWrV+7670vd+uT63VII32++omVuIlOsf60oAzWy+smv03ururwRtx0raJGmppD2SLnX3l4Pb/kLSv5N0WNK17v7DoL1fb2wD8UNJnw62gThK0t9J6lN25O9yd/9FuZhaIQFs1P+MiEbfF3+kAweLT0kEEI7nN1xY03kkgNUhAUSnGx5N6XObdpZdZyVllw8MXXJmW71XK5WESfUltaWWreSSu9nWrUWtEe+7yz23+UzSc2X6q1LXySWOYSXwRWMt0z/WNQXU3Q8qW5Qlv+03ylYFLXb8zZJuLtK+XdLpRdpfk3RpPTE2W4+J5K/N5D6N6ZQpnUA7Wrbu/pqTwG5lZicpWzn7X0maknS7u38l2qiA6ORGXypJVnIl99vp/Vq5aY4HDmZ03aYdVe3TJ2Wfs1I1C3LPYisnf42aBVPpFNLZ1vqVKxJTroJos/8OG1UFFIEpp/pnOxgeTZUtPgIAbeCwpM+5+8/M7E2SHjezB939yagDA6JQ7Tq0Ym/WS03Ra+bUvVJmq8w55ZqewVRpdcwvfH9XY4NskkaP4FZS9bSSIi3lisS00hYijdoHEGgbw6MpDd67k+QPQFtz9xfd/WfB97+T9JTe2EYJ6Ar5e8JVO5OncDSn2Cbfn924Q1d8/bHQNv+uRqUbq+dUsj9dKy55me0xLpwfb/j03WLPbbzHtHB+XKbsSGMlM/yKXSeXOJYaPYxiCxFGABusNxGPOgSUwFRPAJ3KzJYpu15+W5HbWmavXKCR6inaFo+Zlh2b0CnrfzC9rZB0ZEETl4pOkYxi6l7uvipZ45jTblXpTdK8OT3Tv9Mw1jYWmzbaqOqus12nVbYQIQFssPefeULUIaBAtoN4omj5YgBod2b2e5K+q2xxtd8W3t5qe+UCOfVOq6xn64FF8+MzkoNa/sdITaR18rr7Q58qWnjN1W9dWPFew+22QX1Pj82YoVXNJvKVLu/J7VH4+eExPfGFC6bbB/qSDdsqpNh1ot5CJB8JYIPduXWP+t+yiHWALSI33TMzxXseoJUd/6a5UYfQlswsrmzyd5e7b446HqBSxTYOr2TNWr5yo1sW/KfYQFki3qNf/e5QtSEXlT8ldPsvX56xR1wtj6kw2Tv31MVHXLPSUb14zNpug/rJgvdrpUZaS43uVeO3r0/qnJsf1La/OL+u61SjUUlmvUgAQ3Djll0t8cvtdsOjKX12446W2J8GQGnHv2luUzvgTmFmJumbkp5y9y9HHQ9QjUZURCxXuOOYRLzkSFA1o0qVSmcmj9jcO9c+22PKX6KSv69eaiKtu7buKTottRIL5s7piPejqYl0Q/fqzteoDwJmc/6Xf6xnXnp1+uc5Paa/ujS6bUhIAENAcZHoDY+mdO3GHVGHASBPj0kfO2fpjM2FUZc1kv6NpDEz2xG0/bm7/yC6kIDKNKIiYrF91XLKvRcL64PhUuvyik0VzSkcCa012StmIp3Rmg0PRz7dsJuVej96eMr12aCdKaARWzg/3pLVkFAdkj/UIv9T19nMm9OjL334jKIv2oVrEHIL2KU35v0fk4jLTNN7NZ176mI98vT+ujvpatee1FvuvBXKonczd/9f0nTtCqCtlCuXX6ntv3y55jWAzZY/VVSauR6slsdQSZ9lwX2qxH1L0tHzYvrt6+3xHLaL64fHio7cFnIpsr0ozVt4c8da9Pf3+/bt22s6t1GJw8L5cY1+/r11XwelFXvjKYm9/TpIufUbjVJPdTG0BjN73N37o46jXdTTRwKNVKyCZyIeq6jUvpR9k33n1j1hhhiaXBXKWiuTJ+IxffispP5h54tVv+fpTcS144Y33qPmqqB2q8IlCI1YW1gNk/TchgvDuXaZ/pERwDwDfcmGJIC5T/s7SSt90l9s4Tgjfq0vmTfSlZpIK2ZWstO5cnV2mmA1H8rMj/doXjymiYOZGSNsxyTievXQYWUm37gvk3TFaqYiAkBU6q2IePe2F8IML1S50bhKR/5MUu/8+PSskdzz9MjT+4smgOVGByfSGQ2Ppqaf525O/qTsGsCw1hdWIqoqrSSAmFUjKnU1MpZq9r9BtCpZ83X98Nj0wvmYmT56zknTxw/0JbX9ly8XnUqxYG5MBw9N1jXVEQAQnfyKiLnX6c9u3FHR63S7vw+oZtqnKzsbJjdt9rMbd5T9cHS2Z+baWc5H80RVpZUpoAUa8SlAPGYauiS6yj6NtmbDw0WnKCR7E3p03XuaEkN2O4cdYiu/cMRMmqzgpWDh/LguPOOEhqxXqwYJHEphCmh1mAKKSpRayxzW624t00G7feoiOsNRMdPTN78vlGszBbTJMpMe2aLOMFRSqasRb9BLretjE/fw5HewrZxktcq+OQA6Qyu/3kWt2P65Bw5mNHjfTknhzPypdkuI4dGUOm0AA93ptUo+fQ8BCWBIalnU26pKVerqMdPJ6+4/Yo1V/oaoxUaKSiV6g/ftnHENpic0Vm7ReKnRO5IsAN2glZY1RGG25HdoZHxG8pcT5ofbpT5ozt//rcekKc8WMfntaxnxsTA6TbHZbmtOWaS7PvHOht8XCSBKKrUxaU5u6kWxBcjpzOSM6ly5hO7ajTuO2OSURC8cpRaNA0A3a8QG5O2qVBG1L3x/1/QUz3L78FWzR181ym3onpPLSan0jU5UuFF8zqPPvqwrvv5Yw5NAEsAQ9X3xR21bYn54NDVjRC4/+ct9ClcrJm2Er5pS2gDQTRqxAXmrqmR0r1jxkQMHM9OjoL1l9kRuZMXC/FiPivc07LpAu5mt/kgY21KQAIYo/wW13d6If+H7u2aUzc9XT/KH2uUKsBTb9yfeY/q9o+Yw2gcAs2jEBuStqNTo3rUbd0wXcSmX5KYzk7POyMmvWFhYKEaaWShsthG9/NlArPMHmosEMGTtOq2k1Kd/aL6YmZ695Y0KUbn98ShgAADVG1y7omjFyWrLsUf5Opx/38ck4nr19UzZKtkHDmb02QYst5ht+4ADBzMVb87OZ8noNrk9jqPcdzCHBLDAgrkxvXqo8r1ZKlHuE7dWeSOfv94vZtb0++82t122SpKO+PS0mI+ec9IRbRRsAYDa1LsBuRRtIZnC+650TRwJFxCN4980V9v+4nxJ0hk3PFD1+WtOWdTokEgAC8VjPZIamwD2zo8XbW+FSmTFpnCwr054TNIVq5fOqLyZr9ym6ACAxqj3Q7QoC8mUWscHIHwm6dbLVtX8//lvX6/u/93j3zSXKqDN8EoI1aVK5VNRVyIrtvEqwmFSRZ8y3zSwkoQPAFpclIVkOqFYDVCr3HrWbpgFlZsyGgYSwAKVlCKuVqmkMupKZHyK2DzPbbgw6hAAAA1SqlJmMwrJhPE+BWiWo+fF9MQXLog6jKYYHk3VvNXZbXWMMlaCBLDA4NoVDd+X7phE8SmgUVciowNpjt4Sv38AQPsZHk3pX147fER7PGZVF5KpxbJjSQDRXkzt/UH4FV9/rKKtGI6KZWtovFaiin4lekz68kfCTf4kEsCmMDuyYpdZtlpW4QbrtVQim02xQjPbf9n4PUVwpHiP6cYPnhZ1GACAQL3F14ZGxpUpsh/SgrlzmjIt7Z9D2BMMqMXzsyR1uc3NXbPvddcJ6kn8lh+3QA9e9+7GBTMLEsACQyPjDb9mbj/AYhW7XG/shZMMoQpoqX2BEJ6F8+PsxwcALahc8TVp9sqgw6OpkqNvldYQqDcBpUwbmuGomM2a0HRDUtcszUz+JBLAI4Sx/s5MZdfaubLTBB9d956G3zfr/JorzAW7AID6lCq+9tmNO2YkVsWqcueSx1KKLd8o3GJp0n3GzJ/Zqn8XSxaBZqhnNAutjwSwQBiLqyvZVWEindHwaGpGR1PrJ4T55/K/b/P0JuIkfwDQwkr178X6ynRmUjdu2TXdn/YECVwxPSYdPHRYJ6+7X0t6Ezr31MX6h50vFt1iqfAKpap/zzZaCaAzHP+muU2/TxLAAmEUganU5zbtnP6+8EV/8N6d+sL3d01PLTz31MV65On9RySIbO0QrkQ8pg+fldR3H0/NeI4T8Rhr/QCgxcXKJHHFTKQz00lcufOmXNNVQVMTad25dU9VcRWbfVRqtBJA58jfJL6ZSAALDPQlI0sAJ92PmIaSk5nykp1L/qeCTPkMT/7eM/1vWVTXGg4AQPNVk/w1U7cUyQAgzZvToy99+IxI3zeSABaxYG5Mrx6KJomqtWtKZyb1uU07W7Zza2fFivMM9CVJ+ACgzVQ7AggA9WjV2hAkgEXEYz2S2m8UjU6tccKoyAoAiBb9JBCtNacs0l2feGfd1XBrdfK6+48YbGnVJC1MJIBFTFRYyhmd6bbLwt+AEwAAoN3NthdgKVHNpGrnDekbiQSwCKaIdCeTdMXqpSR/AACgq9Wa2KE9kAAWQfLXfZjyCQAAQPLXDUgAi0iGsBcgWlM3zvsGgKgU2xidD+CA2vSY9LFzeB+D6pEAFhHlXoBonniPeNEEgCYZHk3N6Ftzs21SE2ldu3GHrt24Qz0mvfOti7Rr3+/KrsdfMDemD70jecRm60A7WzA3pps/tJIPQxA6EsAiotwLEM1hkoYuXRV1GADQNQbv3THrMVMuPfrsy7Me9+qhyao3WweaaU6P6a8uPZNkDi2JBBBdJ39DdwBoZ2Z2gaSvSIpJ+oa7b4g4pJIyU1FHADQO6+QQhmXr7j+iLYy/NRJAdIVEPKZbLmZaBYDOYWYxSf9V0vmS9kr6qZltcfcnw7i/Ym9MgG7F/w9olmXr7m94EtjT0KsBLSjZmyD5A9CJzpa0291/4e6HJN0j6aIw7og3uwDQOepKAM2s18zuM7OnzewpM3unmS0yswfN7Jng68K849eb2W4zGzeztXntZ5nZWHDbX5uZBe3zzGxj0L7NzJbVEy+6T7I3oUfXvYfkD0AnSkp6Ie/nvUEbAAAl1TsC+BVJD7j7qZLOlPSUpHWSHnL35ZIeCn6Wmb1d0uWSTpN0gaSvBtNXJOlrkq6RtDz4d0HQfrWkA+7+Nkm3SvpSnfGiiyTiMQ2uXRF1GAAQFivSdsRGtmZ2jZltN7Pt+/fvb0JYAIBWVvMaQDM7WtK7JP1bSQqmnxwys4skvTs47A5JP5b0Z8pOS7nH3V+X9JyZ7ZZ0tpk9L+lod38suO53JA1I+mFwzo3Bte6T9DdmZu7s1I6Zkr0JnXvqYj3y9H7tm0hrCftKAeh8eyWdlPfziZL2FR7k7rdLul2S+vv76T9RkXJrjs7/8o/1zEuvNjEaAI1UTxGYt0raL+m/mdmZkh6X9BlJx7v7i5Lk7i+a2XHB8UlJW/POz01VyQTfF7bnznkhuNZhM3tF0rGSfp0fiJldo+wIopYuXVrHQ0K7ifeYhiizDKA7/VTScjM7WVJK2Vk2H4s2JLST2y5bVVP/+eB17674WNaPAq2nngRwjqR3SPq0u28zs68omO5ZQqmpKuWmsFQ0vSWMTzd7LLsfEVrPwvlxTRzMMMoHoKsFH4z+B0kjym4D8S133xVxWCiQW4uer96kqJ22IGinWCXp+uEx3b3tBU0y2QwtotW2gdgraa+7bwt+vk/ZBPBXZnZCMPp3gqSX8o4vNlVlb/B9YXv+OXvNbI6kYyTNvkNsA5D8taYrVy/VTQMrow4DAFqCu/9A0g+ijgNZppmfUpdai95uSVE3uWlgJe8z0PFqLgLj7v9H0gtmlntlO0/Sk5K2SLoqaLtK0veC77dIujyo7HmyssVefhJMF/2dma0Oqn9+vOCc3LUukfRws9b/JXsTzbgbVIkXZQBA1ExSPDZzklIiHtMVq5cq2ZuQiS2IALSuejeC/7Sku8xsrqRfSPoTZZPKTWZ2taQ9ki6VJHffZWablE0SD0v6lLtPBtf5pKRvS0ooW/zlh0H7NyX9XVAw5mVl1zc0xeDaFbp2445m3R0qcOVq1ncCAKKViMd0y8XZDyOHRsYpPAag7dSVALr7Dkn9RW46r8TxN0u6uUj7dkmnF2l/TUEC2WwDfUkSwBay/LgFjP4BACKxYG5MBw9NHpHokfABaEf1jgB2rOHRVNQhIM8v9h+MOgQAQJfIreVLMrIHoAORAJYwNDIedQgdpXBhfLWoxgUACFOyN8F0TgBdgQSwhH0T6ahD6BhrTlmkf362dPHWXKd7TCKuiXSm6DExK7YjCAAA9Su2VQMAdCoSwBKW9CaUIgmsy8L5cd3wgdM00JfUmg0PF30+Czvd64fHdOfWPUcc99FzTjqiDQCAepXaqgEAOlXN20B0OjqD2iV7E7rtslUa/fx7p6fQDK5doUQ8NuO4Yp3uTQMrdeXqpdMjfjEz9v4DAISGrRoAdBtGAEugCmhtSk2jyXWulZTMZhNWAEAz5Pb8XbPhYdb/AegaJIBlJJkGWrVyaycH+pJ0qgCAlpCIx3TuqYu1fvOY0pnstsSpibSu3bhDX/j+ruklDADQaZgCWgbTQKu3JPg0FQCAVpXsTeiWi1fqkaf3Tyd/+Q4czGj95jG2hALQkUgAy+CTv+qwkB4A0A7OPXWxBvqSZWetpDOTbAkFoCORAJbBJ39HipnJlP309MrVS5XsTUz/zEJ6AEA7uGvrHg2PpmadtcIyEACdiDWAZfDJ35H+y0fOJMkDgC7z/IYLtWzd/VGH0TCubB8/uHbFjDWAhUzZD4Pp9wB0EkYAy2Az+Jnmx3voBAGgSz2/4cKoQ2iofRNpDfQldcvFK9WbiBc9JpcoAkAnIQEsg4Imb4jHTP/54jOiDgMAEKFOSgJzffxAX1I7bnhvyeP4MBhApyEBLKPY5uXdKNmb0NAlTP0EAHSGYkXLkiU+9OXDYACdhjWAZeQSnm7cED7JZrgAgCLadT1gj0lTXrp/K7YekOrWADoRCeAsBvqSXZcAXrl6qW4aWBl1GAAA1CURj1VcoTp3zNDIuPZNpLWED0IBdCgSQEyLmemj55xE8gcAKKvZo4Cm7FTMwsRseDRVtornUfHqVroM9CVJ+AB0PBLALmeSbr1sFR0eAKAqc0w67M25rytWL1X/WxZNj87lKnMWjtodk4jr1UOHlZnMBnbgYEbrN4/NOBYAuh1FYLpYrMdI/gAANdl9S/gVQWNmujJI/tZvHlNqIi1XdoP29ZvHpvfoe3Tde/Tchgu1YN6c6eQvJ52ZZCsHAMhDAtilYmb6L5dS2RMAULs1pywK7drPb7hQz97yPvW/ZZE+t2nnEdM8iyV2pbZsYCsHAHgDCWAFLOoAQvBfPkLyBwCoz12feGco1zVJw6Op6TV+k158rmlhYldqywa2cgCAN5AAVqBJSxyaiuQPANAIt122quHXdGXX9Q2NjJcs8CIdWeSl2P69bOUAADORAFag1Oaw7Wrh/HjUIQAAOsRAXzKUmTL7JtKzTt1MZ6Z02ucf0PBoajqWWy5eqWRvQqZs/13pNhAA0C2oAlqBc09drDu37ok6jIaIx0w3fOC0qMMAAHSQWy9b1fA9c49JxGWWreRZzquHJmdU+mQrBwAojxHACjzy9P6oQ2iImJmGLmHtHwCgsQb6kg0vCDORzuiVdPnkL4dKnwBQORLACnRK9TAKvwAAwnLXJ96po+fFZj+wClNVLMLvlL4aAMJGAliBTqkeRvIHAAjTE1+4ILL77pS+GgDCRgJYgWJVxQAAiIqZDZnZ02b2hJn9vZn1Rh1TThhVQWdDpU8AqBwJYAUG+pL68FntPXrWm6DyJwB0kAclne7uZ0j635LWRxzPtIG+pK5cvbRp97dwfpxKnwBQBRLACrVzIZgek278IJU/AaBTuPuP3P1w8ONWSSdGGU+hmwZWNrwoTCnz584h+QOAKpAAVqhdF5cvmBvTlz+yis4RADrXv5P0w1I3mtk1ZrbdzLbv39+8DzPv+sQ7m5IEtmv/DABRIQGsULstLk/2JnTbZau064sXkPwBQBsys/9hZj8v8u+ivGP+QtJhSXeVuo673+7u/e7ev3jx4maEPu2uT7wz9Ptot/4ZAKLGRvAVGly7Qus3jymdmYw6lFk9v+HCqEMAANTJ3f+o3O1mdpWk90s6z92r2DChuZYft0DPvPRqXdcwScUeoEkUfwGAKjECWKGBvqRuuXhlyxdTSfJJKAB0PDO7QNKfSfqgux+MOp5yHrzu3Vp+3IKqz1s4Py5Ttl8rld262OIIAKpFAliFgb6kFsyLbtA02ZuY7gyvXL30iK0pKIMNAF3jbyS9SdKDZrbDzP426oDKefC6d+v5DReqxyo7vjcR1+jn36vnNlyoR9e9p+SHm3zoCQDVYwpolaJabH7l6qW6aWDljLb+tyzS0Mi49k2ktaQ3ocG1K/gkFAC6gLu/LeoYajFVwUTVRDx2ROXqYssw+NATAGpDAlilJb0JpZqYBPaY9LFzjkz+pOyIJAkfAKCTFBslzPV1fOgJAPUjAaxSM4vBFBv1AwCgk716aFLXbdohaeb6Pj70BIDGYA1glZpRDGbB3Jhuu2wVyR8AoKNUumZvyqX1m58IORoA6E51jQCa2fOSfidpUtJhd+83s0WSNkpaJul5SR9x9wPB8eslXR0c/6fuPhK0nyXp25ISkn4g6TPu7mY2T9J3JJ0l6TeSLnP35+uJuRFyn0IuW3d/Q6+75pRFTdkzCQCAKAyuXaHPbtxRsqpnvnRmSn1f/JEuPOMEPfL0fqZ+AkCDNGIE8Fx3X+Xu/cHP6yQ95O7LJT0U/Cwze7ukyyWdJukCSV81s1wZy69JukbS8uDfBUH71ZIOBIvdb5X0pQbE27JI/gAAnWygL6krVi+t+PgDBzO6c+sepSbSckmpibTWbx7T8GgqvCABoMOFMQX0Ikl3BN/fIWkgr/0ed3/d3Z+TtFvS2WZ2gqSj3f2xYCPb7xSck7vWfZLOM7MKi0gDAIBWc9PASl25eqlq7czTmUkNjYw3NCYA6Cb1JoAu6Udm9riZXRO0He/uL0pS8PW4oD0p6YW8c/cGbcng+8L2Gee4+2FJr0g6ts6YG2bB3NjsB1Wo1TeYBwCgUW4aWKlbL1tV8z5+zazGDQCdpt4qoGvcfZ+ZHafsZrRPlzm22Id9Xqa93DkzL5xNPq+RpKVLK59aUq94rEfZ5Yx1XqfHjtjzCACATpZf1fP64THdtXVPRWsDpeybg+HRFGsBAaAGdY0Auvu+4OtLkv5e0tmSfhVM61Tw9aXg8L2STso7/URJ+4L2E4u0zzjHzOZIOkbSy0XiuN3d+929f/HixfU8pKq8ks7UfK4F/5K9CQ1deiadGACga+VGBCudDeMS00ABoEY1J4BmtsDM3pT7XtJ7Jf1c0hZJVwWHXSXpe8H3WyRdbmbzzOxkZYu9/CSYJvo7M1sdrO/7eME5uWtdIunhYJ1gS1hS49SVRDymWy9bpec2XKhH172H5A8A0PUG+pLaccN7dVswNTT3IWkp+5gGCgA1qWcK6PGS/j6oyTJH0n939wfM7KeSNpnZ1ZL2SLpUktx9l5ltkvSkpMOSPuXuufmTn9Qb20D8MPgnSd+U9HdmtlvZkb/L64i34WrZFD5JCWsAAEoq3PB9zYaHi675q/VDWADodjUngO7+C0lnFmn/jaTzSpxzs6Sbi7Rvl3R6kfbXFCSQrSjXQQ2NjGc/iTSp1PikSbr1slUkfgAAVGB4NKWhkXGlJtIyzSwAkIjHNLh2RVShAUBbC2MbiK4y0JfUo+veo1svW6U5ZXaouGL1UpI/AAAqMDya0vrNY9Mjf/kV45K9Cd1y8Ur6VACoUb1VQBEYGhlXZurI4T8z6daPMPIHAEClhkbGj1he4comf4+ue080QQFAh2AEsEFKLkZ3kfwBAFCFUn0qhV8AoH4kgA1SajE6i9QBAKgOfSoAhIcEsEEG165QIh6b0cYidQAAqkefCgDhYQ1ggxRWBF3Cdg8AANSEPhUAwkMC2ECFexcBAIDa0KcCQDiYAgoAAAAAXYIEEAAAAAC6BAkgAAAAAHQJEkAAAAAA6BIkgAAAAADQJUgAAQAAAKBLkAACAAAAQJcgAQQAAACALmHuHnUMDWVm+yX9sgGXerOkXzfgOu2M5yCL5yGL54HnIKeVnoe3uPviqINoFw3qI1vp9x8lnocsngeegxyeh9Z6Dkr2jx2XADaKmW139/6o44gSz0EWz0MWzwPPQQ7PQ3fj95/F85DF88BzkMPz0D7PAVNAAQAAAKBLkAACAAAAQJcgASzt9qgDaAE8B1k8D1k8DzwHOTwP3Y3ffxbPQxbPA89BDs9DmzwHrAEEAAAAgC7BCCAAAAAAdAkSQAAAAADoEl2dAJrZBWY2bma7zWxdkdvNzP46uP0JM3tHFHGGrYLn4Yrg8T9hZv9sZmdGEWfYZnse8o77v8xs0swuaWZ8zVDJc2Bm7zazHWa2y8z+Z7NjbIYK/p84xsy+b2Y7g+fhT6KIM0xm9i0ze8nMfl7i9q54fexm9JFZ9JH0jzn0kfSPOW3fR7p7V/6TFJP0rKS3Sporaaektxcc8z5JP5RkklZL2hZ13BE9D38gaWHw/R936/OQd9zDkn4g6ZKo447gb6FX0pOSlgY/Hxd13BE9D38u6UvB94slvSxpbtSxN/h5eJekd0j6eYnbO/71sZv/0UdW9Tx0dB9J/1jV30JH95H0jzMeZ1v3kd08Ani2pN3u/gt3PyTpHkkXFRxzkaTveNZWSb1mdkKzAw3ZrM+Du/+zux8Iftwq6cQmx9gMlfw9SNKnJX1X0kvNDK5JKnkOPiZps7vvkSR379bnwSW9ycxM0u8p28Edbm6Y4XL3f1T2cZXSDa+P3Yw+Mos+kv4xhz6S/nFau/eR3ZwAJiW9kPfz3qCt2mPaXbWP8WplP9HoNLM+D2aWlPQhSX/bxLiaqZK/hd+XtNDMfmxmj5vZx5sWXfNU8jz8jaR/LWmfpDFJn3H3qeaE1zK64fWxm9FHZtFH0j/m0EfSP1ajpV8f50QdQISsSFvhnhiVHNPuKn6MZnausp3b/x1qRNGo5Hm4TdKfuftk9oOtjlPJczBH0lmSzpOUkPSYmW119/8ddnBNVMnzsFbSDknvkXSKpAfN7J/c/bchx9ZKuuH1sZvRR2bRR9I/5tBH0j9Wo6VfH7s5Adwr6aS8n09U9tOKao9pdxU9RjM7Q9I3JP2xu/+mSbE1UyXPQ7+ke4LO7c2S3mdmh919uCkRhq/S/yd+7e6vSnrVzP5R0pmSOqVzkyp7Hv5E0gbPTvTfbWbPSTpV0k+aE2JL6IbXx25GH5lFH0n/mEMfSf9YjZZ+fezmKaA/lbTczE42s7mSLpe0peCYLZI+HlTyWS3pFXd/sdmBhmzW58HMlkraLOnfdNCnWIVmfR7c/WR3X+buyyTdJ+n/7bDOrZL/J74n6Q/NbI6ZzZd0jqSnmhxn2Cp5HvYo+wmvzOx4SSsk/aKpUUavG14fuxl9ZBZ9JP1jDn0k/WM1Wvr1sWtHAN39sJn9B0kjylY1+pa77zKzfx/c/rfKVrJ6n6Tdkg4q+6lGR6nwefi8pGMlfTX4dO+wu/dHFXMYKnweOlolz4G7P2VmD0h6QtKUpG+4e9ESyO2qwr+Fv5T0bTMbU3aax5+5+68jCzoEZna3pHdLerOZ7ZV0g6S41D2vj92MPjKLPpL+MYc+kv4xX7v3kZYdoQUAAAAAdLpungIKAAAAAF2FBBAAAAAAugQJIAAAAAB0CRJAAAAAAOgSJIAAgKYzs2+Z2UtmVlGFPDP7iJk9aWa7zOy/hx0fAABRCbuPpAooAKDpzOxdkv5F0nfc/fRZjl0uaZOk97j7ATM7zt1fakacAAA0W9h9JCOAAICmc/d/lPRyfpuZnWJmD5jZ42b2T2Z2anDTJyT9V3c/EJxL8gcA6Fhh95EkgACAVnG7pE+7+1mS/qOkrwbtvy/p983sUTPbamYXRBYhAADRaFgfOSfEIAEAqIiZ/Z6kP5B0r5nlmucFX+dIWi7p3ZJOlPRPZna6u080OUwAAJqu0X0kCSAAoBX0SJpw91VFbtsraau7ZyQ9Z2bjynZ2P21ifAAARKWhfSRTQAEAkXP33yrbcV0qSZZ1ZnDzsKRzg/Y3Kzvd5RdRxAkAQLM1uo8kAQQANJ2Z3S3pMUkrzGyvmV0t6QpJV5vZTkm7JF0UHD4i6Tdm9qSkRyQNuvtvoogbAICwhd1Hsg0EAAAAAHQJRgABAAAAoEuQAAIAAABAlyABBAAAAIAuQQIIAAAAAF2CBBAAAAAAugQJIAAAAAB0CRJAAAAAAOgSJIAAAAAA0CVIAAEAAACgS5AAAgAAAECXIAEEAAAAgC5BAggAAAAAXYIEEAAAAAC6BAkgAAAAAHQJEkAAAAAA6BIkgAAAAADQJUgAAQAAAKBLkAACAAAAQJcgAQQAAACALkECCAAAAABdggQQAAAAALoECSAAAAAAdAkSQAAAAADoEiSAAAAAANAlSAABAAAAoEuQAAIAAABAlyABBAAAAIAuQQIIAAAAAF2CBBAAAAAAusScqANotDe/+c2+bNmyqMMAADTB448//mt3Xxx1HO2CPhIAukO5/rHjEsBly5Zp+/btUYcBAGgCM/tl1DG0E/pIAOgO5fpHpoACAAAAQJcgAQQAAACALkECCAAAAABdggQQAAAAALoECSAAAAAAdAkSQAAAAADoEiSAAAAAANAlSAABAAAAoEuQAAIAAABAl5gTdQCtZng0paGRce2bSGtJb0KDa1dooC8ZdVgAAAAAOliz8hASwDzDoymt3zymdGZSkpSaSGv95jFJIgkEAAAAEIpm5iFMAc0zNDI+/aTnpDOTGhoZjygiAAAAAJ2umXkICWCefRPpqtoBAAAAoF7NzENIAPMs6U1U1Q4AAAAA9WpmHkICmGdw7Qol4rEZbYl4TINrV0QUEQAAAIBO18w8hCIweXILLKkCCgAAAKBZmpmHkAAWGOhLkvABAAAAaKpm5SEkgAXYBxAAAABAp5p1DaCZfcvMXjKzn+e1LTKzB83smeDrwrzb1pvZbjMbN7O1ee1nmdlYcNtfm5kF7fPMbGPQvs3MluWdc1VwH8+Y2VUNe9QlDI+mdN2mHUpNpOXK7r9x3aYdGh5NhX3XAAAAABC6SorAfFvSBQVt6yQ95O7LJT0U/Cwze7ukyyWdFpzzVTPLrWb8mqRrJC0P/uWuebWkA+7+Nkm3SvpScK1Fkm6QdI6ksyXdkJ9ohuHPNz+hKZ/ZNuXZdgAAAABod7MmgO7+j5JeLmi+SNIdwfd3SBrIa7/H3V939+ck7ZZ0tpmdIOlod3/M3V3SdwrOyV3rPknnBaODayU96O4vu/sBSQ/qyES0oQ5mpqpqBwAAAIB2Uus2EMe7+4uSFHw9LmhPSnoh77i9QVsy+L6wfcY57n5Y0iuSji1zrSOY2TVmtt3Mtu/fv7/GhwQAAAAAna3R+wBakTYv017rOTMb3W93935371+8eHFFgRZjxe6xTDsAAAAAtJNaE8BfBdM6FXx9KWjfK+mkvONOlLQvaD+xSPuMc8xsjqRjlJ1yWupaobninKVVtQMAAABAO6k1AdwiKVeV8ypJ38trvzyo7HmyssVefhJME/2dma0O1vd9vOCc3LUukfRwsE5wRNJ7zWxhUPzlvUFbaG4aWKkrVy9VLBjyi5npytVLddPAyjDvFgAAAACaYtZ9AM3sbknvlvRmM9urbGXODZI2mdnVkvZIulSS3H2XmW2S9KSkw5I+5e6TwaU+qWxF0YSkHwb/JOmbkv7OzHYrO/J3eXCtl83sLyX9NDjui+5eWIym4frfskiPPL1f+ybS+lfHHKX+tywK+y4BAAAAoClmTQDd/aMlbjqvxPE3S7q5SPt2SacXaX9NQQJZ5LZvSfrWbDE2yvBoSus3jymdyeasqYm01m8ekyQ2gwcAAADQ9hpdBKatDY2MTyd/OenMpG7csiuiiAAAAACgcUgA8+ybSBdtn0hnNDyaanI0AAAAANBYJIB5lvQmSt42NDLexEgAAAAAoPFIAPOce2rpPQRLjQ4CAAAAQLsgAczzyNP7S95WbnQQAAAAANoBCWCeVJlRvnKjgwAAAADQDkgAK1RudBAAAAAA2gEJYIVYAwgAaDVm9lkz22VmPzezu83sqKhjAgC0tlk3gkdW7/x41CEAADDNzJKS/lTS2909bWabJF0u6duRBgYAqMnwaEpDI+PaN5HWkt6EBteu0EBfsuH3QwKYxyR5idteK9ggHgCAFjBHUsLMMpLmS9oXcTwAgBoMj6Z03cYdmgp+Tk2kdd3GHZLU8CSQKaB55pR5NtKZqdI3AgDQZO6ekvRXkvZIelHSK+7+o8LjzOwaM9tuZtv372c9OwC0ovWbn1BhtjEVtDcaCWAecjwAQLsws4WSLpJ0sqQlkhaY2ZWFx7n77e7e7+79ixdT0RoAWlGpwaYwBqFIACtkFnUEAADM8EeSnnP3/e6ekbRZ0h9EHBMAoMWRAFbISy0OBAAgGnskrTaz+WZmks6T9FTEMQEAatBTYrCpVHtd99X4S3amZG8i6hAAAJjm7tsk3SfpZ5LGlO3Tb480KABATT52ztKq2utBFdAKnXsq6yYAAK3F3W+QdEPUcQAA6nPTwEpJ0t3bXtCku2Jm+ug5J023NxIJYIUeeZrKaQAAAADCcdPAylASvkJMAa3Qvol01CEAAAAAQF1IACt0TCIedQgAAAAAUBcSwAq9euiwhkdTUYcBAAAAADUjAaxQZtI1NDIedRgAAAAAUDMSwCqwDhAAAABAOyMBrMIS9gIEAAAA0MZIAKswuHZF1CEAAAAAQM1IAKsw0JeMOgQAAAAAqBkJIAAAAAB0CRJAAAAAAOgSJIAAAAAA0CVIAAEAAACgS5AAVmF4NBV1CAAAAABQMxLAKgyNjEcdAgAAAADUjASwCvsm0lGHAAAAAAA1IwGswpLeRNQhAAAAAEDNSACrMLh2RdQhAAAAAEDNSACrMNCXjDoEAAAAAKgZCSAAAAAAdAkSQAAAAADoEiSAVWAfQAAAAADtjASwCjdu2RV1CAAAAABQMxLAKkykM1GHAAAAAAA1IwEEAAAAgC5BAlgFizoAAAAAAKjDnKgDaCcedQAAAAAAOtLwaEpDI+PaN5HWkt6EBteuCGUf8rpGAM3sM2b2czPbZWbXBm2LzOxBM3sm+Low7/j1ZrbbzMbNbG1e+1lmNhbc9tdmZkH7PDPbGLRvM7Nl9cQLAAAAAK1meDSl9ZvHlJpIyyWlJtJav3kslF0Iak4Azex0SZ+QdLakMyW938yWS1on6SF3Xy7poeBnmdnbJV0u6TRJF0j6qpnFgst9TdI1kpYH/y4I2q+WdMDd3ybpVklfqjVeAAAAAGhFQyPjSmcmZ7SlM5MaGhlv+H3VMwL4ryVtdfeD7n5Y0v+U9CFJF0m6IzjmDkkDwfcXSbrH3V939+ck7ZZ0tpmdIOlod3/M3V3SdwrOyV3rPknn5UYHAQAAAKAT7JtIV9Vej3oSwJ9LepeZHWtm8yW9T9JJko539xclKfh6XHB8UtILeefvDdqSwfeF7TPOCZLMVyQdWxiImV1jZtvNbPv+/fvreEgAAAAA0FxLehNVtdej5gTQ3Z9Sdkrmg5IekLRT0uEypxQbufMy7eXOKYzldnfvd/f+xYsXl40bAAAAAFrJ4NoVSsRjM9oS8ZgG165o+H3VVQTG3b/p7u9w93dJelnSM5J+FUzrVPD1peDwvcqOEOacKGlf0H5ikfYZ55jZHEnHBPcDAAAAAB1hoC+pD5+VVCxY7RYz04fPSrZkFdDjgq9LJV0s6W5JWyRdFRxylaTvBd9vkXR5UNnzZGWLvfwkmCb6OzNbHazv+3jBOblrXSLp4WCdIAAAXc/Mes3sPjN72syeMrN3Rh0TAKB6w6MpfffxlCaDVGfSXd99PNVaVUAD3zWzJyV9X9Kn3P2ApA2SzjezZySdH/wsd98laZOkJ5WdMvopd8+VuvmkpG8oWxjmWUk/DNq/KelYM9st6ToFFUWjFMYvAQCAGn1F0gPufqqyFbmfijgeAEANmlkFtK6N4N39D4u0/UbSeSWOv1nSzUXat0s6vUj7a5IurSfGRhsaGQ9lKBYAgGqY2dGS3iXp30qSux+SdCjKmAAAtWmXKqBdKYxfAgAANXirpP2S/puZjZrZN8xsQeFBVMoGgNbXFlVAu1UYvwQAAGowR9I7JH3N3fskvaoiSyWolA0Ara9tqoB2ozB+CQAA1GCvpL3uvi34+T5lE0IAQJsZ6EvqlotXKtmbkElK9iZ0y8UrQ1l6VtcawG60/ZcvswYQABA5d/8/ZvaCma1w93Fl198/GXVcAIDaDPSFs+1DIUYAq3T3theiDgEAgJxPS7rLzJ6QtErSf442HABAq2MEsEqTbEMIAGgR7r5DUn/UcQAA2gcjgAAAAADQJUgAAQAAAKBLkABWaeH8eNQhAAAAAEBNWANYpdcyk1GHAAAAAKDDDI+mNDQyrn0TaS3pTWhw7Qq2gWgF6cxU1CEAAAAA6CDDoykN3rtTmalswcnURFqD9+6UpIYngUwBBQAAAIAI3bhl13Tyl5OZct24ZVfD74sEsEoWdQAAAAAAOspEOlNVez1IAKvELoAAAAAA2hUJIAAAAABEqNROA2HsQEACCAAAAAARuuEDpykem7nYLB4z3fCB0xp+X1QBBQAAAIAI5Sp9sg0EAAAAAHSBgb5kKAlfIaaAAgAAAECXIAGs0ppTFkUdAgAAAADUhASwSj/bMxF1CAAAAABQExLAKqUzU1GHAAAAAAA1IQEEAAAAgC5BAggAAAAAXYJtIAAAAAAgYsOjKfYBBAAAAIBONzya0vrNY0pnJiVJqYm01m8ek6SGJ4FMAa3B8Ggq6hAAAAAAdIihkfHp5C8nnZnU0Mh4w++LBLAG6zePkQQCAAAAaIh9E+mq2utBAliDsLJxAAAAAN1nSW+iqvZ6kADWKIxsHAAAAED3GVy7Qol4bEZbIh7T4NoVDb8visDUKIxsHAAAAEB7aGTVztx5VAFtYWFk4wAAAABaXxhVOwf6kqEkfIVIAGvUjF8OAAAAgNZTrmpnrXkC+wACAAAAQIM0MsFqdNVO9gFscfPm8LQBAAAA7SKXYKUm0nK9kWDVurVbo6t2NnMfQEYAa1BYoQcAAABA65otwap2ZHBw7YoZI3ZSfVU7m7kPIAlgDSbSmahDAAAAAFChUolUbiSw2qmXja7auaQ3oVSRGMPYeYAEsAYxs6hDAAAAAFChUglWzKzmYi6NrNrZ6BHFcljMVoNJ96hDAAAAAFChUhutl3pfH8bUy3K2//JlvZaX/C2YG9MtF68MpQooCSAAAACAjjbQl9QtF69Usjchk5TsTUz/XEwYUy9LuX54THdu3aP8VPTVQ5Pa/suXQ7k/poACANDGzCwmabuklLu/P+p4AKBVlZqy2aypl6Xcve2Fou13bd2jmwZWNvz+SAABAGhvn5H0lKSjow4EAJqhkfv5NbqYSy1KTUN1ZR9ro2OpKwE0s89K+n+UjW9M0p9Imi9po6Rlkp6X9BF3PxAcv17S1ZImJf2pu48E7WdJ+rakhKQfSPqMu7uZzZP0HUlnSfqNpMvc/fl6YgYAoFOY2YmSLpR0s6TrIg4HAEIXxobpjSzmUosek6ZKlBj5wvd3tc5G8GaWlPSnkvrd/XRJMUmXS1on6SF3Xy7poeBnmdnbg9tPk3SBpK8G01Yk6WuSrpG0PPh3QdB+taQD7v42SbdK+lKt8QIA0IFuk/SfJE2VOsDMrjGz7Wa2ff/+/U0LDADC0MwN0xtpeDSlNRse1snr7teaDQ9Pb0A/PJqSytSXPHCw8dvP1VsEZo6khJnNUXbkb5+kiyTdEdx+h6SB4PuLJN3j7q+7+3OSdks628xOkHS0uz/m7q7siF/+Oblr3SfpPDP2YAAAwMzeL+kld3+83HHufru797t7/+LFi5sUHQCEo5kbpjfK8GhKg/fuVGoiLVd21PLajTu0bN39unbjjtKf4IWk5gTQ3VOS/krSHkkvSnrF3X8k6Xh3fzE45kVJxwWnJCXlr3DcG7Qlg+8L22ec4+6HJb0i6dhaY24USqcCAFrAGkkfNLPnJd0j6T1mdme0IQFAuEpV52xm1c5q3bhllzKl5njOojcRb3A09U0BXajsCN3JkpZIWmBmV5Y7pUibl2kvd05hLM2d3mKaHrYFACAK7r7e3U9092XKLrF42N3L9cMA0PZK7efXzKqd1ZpI1z6N88YPntbASLLqGcz6I0nPuft+d89I2izpDyT9KpjWqeDrS8HxeyWdlHf+icpOGd0bfF/YPuOcYJrpMZKO2BCj2dNbplwtP88YAAAA6DSl9vOrtlBKqTV5jVbvdcMoTlNPFdA9klab2XxJaUnnKbsP0auSrpK0Ifj6veD4LZL+u5l9WdkRw+WSfuLuk2b2OzNbLWmbpI9L+v/yzrlK0mOSLlH2083axk8bLNXC84wBAN3F3X8s6ccRhwEATVFv1c4wKomWcuOWXTWfG8b0T6m+NYDblC3M8jNlt4DokXS7sonf+Wb2jKTzg5/l7rskbZL0pKQHJH3K3XMlfD4p6RvKFoZ5VtIPg/ZvSjrWzHYrW956Xa3xNlqMWjQAAABA22lmJdF6pn++euhwKCOTde0D6O43SLqhoPl1ZUcDix1/s7J7FRW2b5d0epH21yRdWk+MYSm1YSMAAACA1tUulUQzk66hkfHW2Qew2yVbuNIQAAAAgOKaWUm03kmDYSw7IwGs0bmnspcSAAAAEIYwi7Q0s5JovZMGw1h2RgJYo3/Y+WLUIQAAAAAdJ1ekJX/j9M9u3KHrh8cacv1GVRKtRL2zBsNYdlbXGsBuVs+CTgAAAADFFSvS4pLu2rpH/W9Z1JBErd5KopVadmyirmmcYSw7YwQQAAAAQMsoVYzF1X57cT/67BFbmFcljGmpJIAAAAAAWka5YiytVqkzTGtOacxoZyESQAAAAAAtY3DtCpUqfRJGpc5WtOaURbrrE+8M5dokgAAAAABaxkBfUlesXnpEEhhWpc5Wc/yb5oaW/EkkgAAAAABazE0DK3XrZauaUqkzTIl4denW0fNi2vYX54cUTRZVQAEAAABMGx5NaWhkXPsm0lrSm9Dg2hWhJF6z3U+zKnWG6bXMVMXHXrl6qW4aWBliNFkkgAAAAEBImpVMNUpuD77cNgypibTWb87uv1dJ3JU83uHRlG7csmvGtmrV3k+7OCreo3SJJPC2y1ZNP9Yrvv6Y7ty6R3du3SMp3DWAJIAAAABACGpJppqZMBa7r2J78KUzkxoaGZ8RR2ESt3B+XBeecYK++3iq7OMtfE5mu592Vyr5kzT9WM+5+UH96neHZtz26LMv64qvPxZKEkgCCAAAAISgVDL1uU07JR2ZBNY7+lZKsURPUtH7KpaYSTO3XxgeTWnw3p3KTPl024GDmenRq3yFSV2x56TU/XS61ERa1w+PHZH85dS7h2ApJIAAAABACEolM5PuMxK7XIKWKnJ8vaNipZLK7NTEI5PTmJkm3Y+4Tv72C0Mj4zOSv9nkPw+zJXjdss1DTrGkOWwkgHUYHk111BA1AAAAGmdJb6JoUie9kdhJKjvyJmWTtmred+aP+MmkwnwunZkseX+T7krEYzNuL9x+odpRuvykrtxzkojHdO6pi7Vmw8Nts2ZyNgvmxvTqodK/2yiwDUQdcv/TAgAAAIUG165QIh4reXtqIq3PbdpZNvnLuXbjDl3x9cdmPS434peaSMt1ZPI3m9x2C72J+HTbUQVbGVQzSleYPJZ6ThbOj+vDZyX13cdT07HnRiuHR1PVPYgWcvOHaq/qWeUOEhUjAaxDN81RBgAAQHUG+pK65eKVilnhluZZJhWdblnKo8++rOuHx2a0DY+mtGbDwzp53f361/+/H+rajTsqSih7E/EjErH8ZO31w28ULzlwMDMjERtcu0LxnuKPSZJiZiX37ss9J/n7+9122SqNfv69euTp/SUL0LSrgb6kyjxVZQ1duqqhseQwBbQO3TZHGQAAANXJJT+F0zxNUpWDc5Kku7e9ML1XXOH6vnIVJwvd+MHTJKloxdG+L/6oaCJ27cYdGhoZ17mnLtaCeXNmbOOQb8pdz224sOR9l9rfr9TgSrsPunzsnKVVrfUzSbfmbRHRaCSAdcgfzgYAAACKya+AuW8irWMS8ZLJ02wm3bVmw8Mlt2yoRG8iPh1TsUqkBw6Wji01kZ41mTkmEa9pHV+p9YHtPuhy08BKbRlN6bevV/a7CjP5k0gA69LOC1IBAABaVbttnl5O4WO5YvVSbfzJC3Vdc7YtG8pJxGPTo3/FNGK65UQ6M53gVrOVxeDaFUc8rsI1hO3iiq8/VtM2DleuXhr63zoJIAAAAFpGWHvhRaHYY7lr656apn4WqiX5k3TEmrxCYUy3LLf3Yb7CkdJ2TP6vHx6bdYQ0EY/pxIVH6ZmXXp1umzenR1/68BlNeawkgAAAAGgZpTZPr2cvvKgUeyyNSP5qlexNlH0Oh0dT6imxD2C9Cvc+LKXU+sBWUuvoXk46M6mDh6b0/IYLdf3wmO7e9oJePzylz23aqe2/fHl6jWdYSADrcP3wWOi/IAAAgG7SSYVA6o05ZqYp94YkZbNNpcyNVoaR/OVUksi34vTf4dGUbtyyq+Z1m8WkJtJHjBZOuuvOrXt059Y9Sob42EkA63DX1j0kgAAAAA0UViGQKBKLcpueVyKXjNWTlFkQx7mnLtbQyLg+u3FH0cdfa0GZapVLiltp+m+9o3yVKDdVNDWR1uB9s0+brQX7ANbBJa3Z8HBbb04JAADQSoptFF5vIZDCzdGbtcF4sccS77Ga94Wr5TSzNyp3lnv8zRphLZfIl5v+G7brh8d08rr7tSz4F3byV4nMpOsL39/V8OsyAlindl6YDAAA0GrCKAQS1brC/MeSmkgrZqbMVPnRvAVzY3r1UPGRuFrGAUvdXa4wS25EsHd+vOz2D40wWyLfyOm/5aZtLpwf1w0fOG3693P+l388oyBLKwnjd0IC2ADtujAZAACg1YQxVTPqdYWvvn5YUmVTOUslf2HIxZOaSCveY4rHTJnJN2KM98yesJbTm4hrwbw5Ff0uyxWgqWb67/BoSoP37lBmqvQxBw5mdO3GHbp2446Kr9tJSAAbpB0XJgMAALSSMNaANSqxqPS+cqN97SYz5UUTtnJJUiIeK7luMB4z3fjB0yr6vZUrQFPp9N9Ktl9oR72JeMOvSQLYII1+AQEAAOg2jZ6q2YjEotL7aXSVyCi8ks5oxw3vnf55eDSlWInkOVelsmTCW8XAYakCNDGzWfctbEaxlqj0mHTjB09r+HVJABugkS8gAAAA3arUjKrURFprNjxcdiphsamj9SQWlSoctWxnx+SNNlWSPOf27Fuz4eEjksDMlFecuJf6vU+5lz3/jBse0G9fb//nvZSwduSgCmgDNOoFBAAAoBMMj6a0ZsPDOnnd/VVVTC83oyq/guXgvTtnXLNUlc9SUzFnSyyq0aztExqlXBXSVw8dnn5ev/D9XRUnz/WusSz1ey/393D+l3/c0cmflB1EDaMCKiOADUDyBwBoNjM7SdJ3JP0rSVOSbnf3r0QbFVD7Or7h0dR0sZTZZKZcn920Y7qC5cFDh4tOHS01fbGRS3farQ7EpHvJyqCZSZ9OOEpVnyyWPNe7d+Pg2hVFR1FTE2ktW3d/RdfoVGH8fZEAAgDQng5L+py7/8zM3iTpcTN70N2fjDowdKdyBVBmW8d3/fCY7tq654hlYz1WehuDXF5XruDKpPsRhUpKLd2ptfpovZu9h21+vEeZKZ+u7jlbUc/URLrs3nPFkrpiCVw1S6QKt8vAG8KoM0ICCABAG3L3FyW9GHz/OzN7SlJSEgkgmq6SdXCl3tgPj6aKJn/S7MnKbPILleybSOuYRFxm0mc37tDQyLjOPXWxHnl6v1ITaZneqFuSP2opld+TsNToVdRiZlr91oXa+osDFW0/ka/c3nPFkrpG7N2YW0/Y98Ufhb4fYbuI9VgodUZIAOu0cH7jS7MCAFANM1smqU/StiK3XSPpGklaunRpcwND16h0HdzwaGo6Kbh+eEx3b3uh6uSkGqmJtP588xOaF4/Jla1ymZ/k5W8bUBhFOjOpG7fs0uuHp8pOZ819bbUqoJPuDa+O2ZuIl0zqcglcvUj+shbMjenmD4VTZ4QEsE4hvmYBADArM/s9Sd+VdK27/7bwdne/XdLtktTf30+vhVBUuk4pNw20mXu2HcxM6WCwK3i1/wMUS+jSmckZm4jnRhkXzJvTUglgo+X29at1qixm12PSlz+yKvTnkyqgderk/9EBAK3NzOLKJn93ufvmqONB96p0nVIuUbx72ws131cYG2PXIzWR1rUbd3T82rUFc7PjRsWqrVZa5bUS8+PdmZ7Mm9PTlORPYgQQAIC2ZGYm6ZuSnnL3L0cdD7pbpevgcvvM1Trt0yw7jdOMWVjN9ko6U3Sq72wFfqo1d05sesS2Ux3/prna9hfnR3b/3ZliAwDQ/tZI+jeS3mNmO4J/74s6KHSngb6k3rH0mFmP+93rh3X98Nisx5Xinp3GSfLXfEt6E3Xv91eJV7pgdt2v/yXT0FHTajECCABAG3L3/yWpxHbOQHNdPzxWUcGRySlv2to/NFaummo9+/0VU2oLkE426V7R3phhIQEEAABAXe4iqeto+dU/69nvT8ptGfKE0h0+zXM2jZ46Ww0SwAbo++KPdMMHTqMCEgAA6DrXD4911ehNs1y5emnTR8biPSaZpjeNl7IJ3o0fPE1Sdfv9NbPSa7tq5NTZapAANsCBg5lIh3EBAACiwpv8xlswN6ZHnt4fevLXY9nCPBMHM9PJnFQ+wZttvz8Sv8rVM3W2HjUngGa2QtLGvKa3Svq8pO8E7cskPS/pI+5+IDhnvaSrJU1K+lN3Hwnaz5L0bUkJST+Q9Bl3dzObF1zvLEm/kXSZuz9fa8xhinIYFwAAAJ3j1UOTevVQ+KNDU/7Gxuu57Szy5doK21E/k6qaOttINVcBdfdxd1/l7quUTdAOSvp7SeskPeTuyyU9FPwsM3u7pMslnSbpAklfNbNYcLmvSbpG0vLg3wVB+9WSDrj72yTdKulLtcbbDFEN4wIAgPY0PJrSmg0P6+R192vNhocjrQwIoDr1VOG6YvXSyAaOGrUNxHmSnnX3X0q6SNIdQfsdkgaC7y+SdI+7v+7uz0naLelsMztB0tHu/pi7u7Ijfvnn5K51n6Tzgn2PWlJUw7joTrxpAID2li2GMXNT7c9u3FHXNgkAGitWJvOodYpubyKumwZW1nh2/Rq1BvBySXcH3x/v7i9Kkru/aGbHBe1JSVvzztkbtGWC7wvbc+e8EFzrsJm9IulYSb/Ov3Mzu0bZEUQtXbq0QQ+pOtVWQEJ3Gx5NVbSAutR5hSWYUxNpDd67U1/4/q7pefznnrpYjzy9X6mJtGJmmnRXssj8/mMScZlpxvz/amKZ7THUc1x+nPmPqdS1an1eKz2/2uvXEk+9jyFMrRwb0I6KbartylbU7H/LIv7/AlrAZAgLMaPe69C8zp00zWyupH2STnP3X5nZhLv35t1+wN0Xmtl/lfSYu98ZtH9T2fV+eyTd4u5/FLT/oaT/5O4fMLNdkta6+97gtmclne3uvykVT39/v2/fvr2mx7Js3f01nTdvTo++9OEzeKHGtFKJGtBovXkJ/DGJuA4dntTBoLT2wvnx6QrFhcnbuacu1j/sfFETRTqhBXNj+tA7kjOS7WXHJvTPz74849PORDymWy5eGelrn5k97u79kQXQZurpI9F4J6+7v+QIQrI3oUfXvaeh91fsdaDch2rlzs0/ttb3T0C3CuP/70Ll+sdGjAD+saSfufuvgp9/ZWYnBKN/J0h6KWjfK+mkvPNOVDZx3Bt8X9ief85eM5sj6RhJs+8y2mSHDnf3PibIGh5N6cYtu4q+oQbCkv/3Vvi3d+Bgpuji/dREumyFtlcPTc64PTWRLvphRjozWVdxAD48Q7db0pso+UFho+sK5Kab5kYcC18HUhPpkhXNi51L9XOgNlEWf8lpxBrAj+qN6Z+StEXSVcH3V0n6Xl775WY2z8xOVrbYy0+C6aK/M7PVwfq+jxeck7vWJZIe9nqHLEPgkm7csivqMBCR4dGUlq27X9du3EHyB1Th9cNTunbjDp36Fz+IOhSgqXJruMvNEjkq3qgyDVnFppsWSmcm9dmNO45YU17s3NwHQH1f/FFD4wQ6XZTFX3LqenUxs/mSzpe0Oa95g6TzzeyZ4LYNkuTuuyRtkvSkpAckfcrdc68mn5T0DWULwzwr6YdB+zclHWtmuyVdp6CiaCuaSGcowtGFrvj6Y5RGBur02qTrjBseiDoMoCnyC7+Uk85M6frhsboLflWSbOZzaTqxy91XudHI3BYCAMpbOD+u2y5bFWnxl5y61wC2mijWAOY0Yz4vosf6PiAcz2+4sOpzWANYHdYARq+aZMwkHRWPzRh9q2btbeHUzVosnB+XRKIHVCp//X2Uwl4DiAD7AHa264fHyq6bAgBgNtV8eOhS0amXQyPjFRVr6QkqQNeDxA8obs0pi3TXJ94ZdRg1IQFsoEbP10frIPkDANSrUfv7pSbSOnnd/UdU/p0f79Frh6c0FeR89SZ/AKQrVy9tiWmbjUQC2EDpzJSGR1ORD/miMfI/RaULBQDU6+5tLzTsWq4jK//mEkEAtTFJt162quPfy5MANtjnNu2URFnkdnfF1x/To8+23I4jAIA2xogc0LoWzI3p5g9Fu7dts5AA5jGp7pGeSXf2xmlz1w+PkfwBTXbbZauiDgEIXawBa/IAVK7HpC9/pPNH9KpFApinUS/Jsy3QRusaHk2x1g9ostu6YLoNIEkfPeck+hggZN00klcrEsCQUBG0/VDoBd0if0H78GhKN27ZNb2WqJry1YXnzo/3aF48pomDGS3pTWhw7Qo6YLSM/HXdUf19Prf/X5p6f0A3OCpmevrm90UdRlshAQzJkt5E1CFgFqzzQxh6E3GZSRMHMzom+P7Awcz01K/egqp9UnaKypRn9xI999TF+oedL85IqjJTrszkG3MUqt0HrNyb3oG+ZM1vgus5Fwhb/p6thUs8UhPphizXqCapHB5N0ecAdejEapxRIQHME++RGlFAKxGPaXDtivovhNCQ/HWWBXNj+tA7kvru43uVzvufONf+yNP7tW8iPZ2Q5Y9QSZrxBu7cUxdPH1/s59wbvDBHEwo7uHruiyQN3ahwA/RiSzxKLdeo9P+3wvuYLakcGhmv81EB3ev4N80l+WsgEsA8hxuQ/FUzfQrRIflrHfkjY4XJWbWJVq2dQy3/vzYzsSKJA6ozNDJ+xAbqxRQu1xgeTWnw3p3KBBvppSbSGrx3p7b/8uUjPggqdh/pzKRu3LKr6P+vLA0BatPOG663KhLAPEt6E0rV+QI9f+4c3qiho8VjNmM6Ym5qVbEqurkPRCQxggWgaSrty13SsnX3lz0mM+Uz1ofnksJcklhoIp3RaZ9/4IgiFI14jwG0O6ZxtgYSwDyDa1fo2o076roGn/C1nuuHx3T3thc06a6YmVa/dWHUIbW83Jq0YqNyUvFkrpK1ZgAQtiu+/ljo91Eq+ct59dCkPnfvzumfc2sRgW5DwteaSADzDPQl604AKf7SWgore066M/0zEO+RJj2b6OUkKxydK3Y7I3UAotZK+7hOTnnd7ymAdvD8hgujDgFVIgFssHNPXRx1CEW1QvnrKNy97YWoQ2g5lSZ5ANBO2McVaAyT9BxJXUcjAWywf9j5YssNdVdbqSx3TqMSxmqvVc/xR8V79PrhKc0yO6cr8YIOoJNRZROYHaN1kEgAG24indHwaKqlRldKVSr77MYd+sL3dxWtujhbwhhGmezCTaVzxw8G6yi2//Jl3bV1T9Fy3m88rgaUcm1jyWAKcrG1JkxPBtDJWGMHkOChMiSAISi2r1CUShWmcWU3qJZmJmZf+P6uoglj7nEVK5N97cYd02sd8rfCKJV8Xrtxhz63aadWv3Whnv9NumzHnenydRTz4z2aF49NJ+qFG4Xn5O8/mZ90F94GAABaz/LjFujB694ddRjoAiSAIWi1TyF758enE71ycqOCpUbYUhPpWctlS9mk8rpNO3TLD57Ur353qORxFGQ5UiIe0y0Xr5z1A4SbBlbOOgrbjWs+AQBoVYzOoVWQAIYgZhZ1CNOuHx6rKPnLadTSuSlX2eQPR+oxVZT85ZSruklFTgCtqluLkqG7kfyhlZAAhmDSo61AkutcW20kEuW5s1cegM6TXTawQ8WWaKcm0kX3y4uZadK9aNXi/ASSel9oVSR8aGUkgCFIxHsiu+/CoitoHxRpAdBphkdTs67hLrZfXu6D1MI15kDUSOzQCUgAC/SY6t5CIMpKlMWKrqD1UaQFQCcicUO7ILFDNyEBLPCxc5a29UayTPtsfYl4TB8+K6lHnt7PGhgAdTGzCyR9RVJM0jfcfUNY91VJETCgXfH3jVYVxocTJIAFbhpY2bYJ4PBoKuoQUMLC+fEj9lsEgHqYWUzSf5V0vqS9kn5qZlvc/clG3xdvjgEgGsvW3d/wJJAEMARzeppfBbSSdRaIRrI3oUfXvSfqMAB0nrMl7Xb3X0iSmd0j6SJJDU8AgVo9v+FCPkAAWgwJYAgOT7mWrbu/aPWyfI0qhU3y17pY2wcgRElJL+T9vFfSOYUHmdk1kq6RpKVLlzYnMkBvTF3LH724fnisbWdaAZ2CBDBEqYm01m8ek3Rkef/Cap3lji1neDSlz23aOfuBaIorVy9lbR+AZik23eSIMmbufruk2yWpv7+fnRMQikqnqN00sFI3Dayc/pnRQaD5SABDls5Mamhk/IgkoFi1zlLHlpJLIqPedxBZ8+b0zOjUACBkeyWdlPfziZL2RRQLukyj1iSVuw7JIRAOEsAm2FekMmexNik7Ejg8mqooCfzC93ex5UMLScRjUYcAoLv8VNJyMztZUkrS5ZI+Fm1I6AStsiVCWHGQWKKdUAW0TR2TiM/4eXg0pR6zkiN3s00FHR5N6cYtuzSRzjQ2UNTlFX4fAJrI3Q+b2X+QNKLsNhDfcvddEYfVkUzScy2SFKF+rZLgAlHpiTqAVhPGVgqvHjo8fd1Kpm3mpoKWim/95jGSvxa0pDcRdQgAuoy7/8Ddf9/dT3H3m6OOp13NVrub13cAnYQRwAKlEq96ZCZ9ulBLsbV/xRRu6J6rGMpG762Jap8A0LriPdKCefHpD0/nx3s0Lx6bsT9rqT7WJF7fAXQUEsACpdbm1WvSXYP37lRmqvKCLdcPj+mRp/crNZGWqUhpN7SEmJluuXgl1T4BoEUNXbqqotfo/OrcUjb5u2L1Ul7fAXQUEsACS3oToY2yVZP8SZqxTw7JX2tKxGMkfwDQwtacsqii1+jcMY3YnxcAWhkJYIHBtSvYVB1lLZwfnzFtiDcHANB6Ymb66DknVbU9z0Bfktd0AB2PBLDAQF9S927fo0effTnqUNBiclOB2OsPAFpXsjehR9e9J+owAKBlkQAW8bM9E1GHgBbRY9KUZ99QMNoHAK0lHjNlJt9YJEFBLgCYHQlgEenMVNQhIGJrTlmkuz7xzqjDAACUsWDuHC2YN4c1ewBQBRJAIE8ta0YAANGYSGe044b3Rh0GALQVEkBAVPMEgHZkyu6Ty2s3AFSuJ+oAgGrFzCRl1+Xddtkq3XbZKvUm4tO399jM40qdn38dkj8AaD8u6cYtu6IOAwDaCiOARSycH9eBg5mow0CemJn+y0fOLJmkFWs/ed39RY+dctfzGy5saHwAgGhMpDOMAgJAFeoaATSzXjO7z8yeNrOnzOydZrbIzB40s2eCrwvzjl9vZrvNbNzM1ua1n2VmY8Ftf22WHZoxs3lmtjFo32Zmy+qJt1I3fOC0ZtwNKpSIx8omf6Us6U1U1Q4AaE+f3bRDw6OpqMMAgLZQ7xTQr0h6wN1PlXSmpKckrZP0kLsvl/RQ8LPM7O2SLpd0mqQLJH3VzGLBdb4m6RpJy4N/FwTtV0s64O5vk3SrpC/VGW9F+BQxfGtOWaRkiUSsNxFXsjchU33TMwfXrlAiHpvRRolwAOg87tLgfTtJAgGgAjVPATWzoyW9S9K/lSR3PyTpkJldJOndwWF3SPqxpD+TdJGke9z9dUnPmdluSWeb2fOSjnb3x4LrfkfSgKQfBufcGFzrPkl/Y2bm7m9s+oO2c9tlqzTQl9TwaErrN48pnZmcvi0Rj+nGD57WkCQ8d42hkXFKhANAHZ7fcKGWlZhW3yoyk66hkXFe4wFgFvWsAXyrpP2S/puZnSnpcUmfkXS8u78oSe7+opkdFxyflLQ17/y9QVsm+L6wPXfOC8G1DpvZK5KOlfTrOuJGRGI9pv9y6RtTOZuRoA30JXkzAABdYt9EOuoQAKDl1ZMAzpH0DkmfdvdtZvYVBdM9SyhWktHLtJc7Z+aFza5Rdgqpli5dWi5mRCg/+cshQQOA9tAOo4Dl1ngPj6aYEQIAqm8N4F5Je919W/DzfcomhL8ysxMkKfj6Ut7xJ+Wdf6KkfUH7iUXaZ5xjZnMkHSPp5cJA3P12d+939/7FixfX8ZAQFhNrKwGg3c0pvrtOyyi1xju35CA1kZZLSk2ktX7zGGsGAXSlmhNAd/8/kl4ws9yr7XmSnpS0RdJVQdtVkr4XfL9F0uVBZc+TlS328pNguujvzGx1UP3z4wXn5K51iaSHWf/Xnq5YzcgsALS73be07hY6V65eWvKDxqGR8RnrzSUpnZnU0Mh4M0IDgJZS7z6An5Z0l5nNlfQLSX+ibFK5ycyulrRH0qWS5O67zGyTskniYUmfcvfcq/EnJX1bUkLZ4i8/DNq/KenvgoIxLytbRbQpYmaaJNdsiES8RzcNrIw6DABAA6w5ZZEeffaIyThNFe8xxWOmg5kpSdn9e/vfsqjk8aXWBrJmEEA3qisBdPcdkvqL3HReieNvlnRzkfbtkk4v0v6aggSy2Uj+GiPWY7rl4jOiDgMA0CB3feKdka4FXDg/rgvPOEHfffyN6ZsHDmZ07cYdunbjDi2cH9cNH5hZTXpJb0KpIske+8IC6Eb17gPYsazF1zm0gwVzY0ULvwAAUEzh3q3FzJ87R488vf+IKZ05Bw5mjtgTkH1hAeAN9U4B7VgMAFYvEe/RU3/5x1GHAQAIWRgVQWNmJZO6fJVM2yzcE5B9YQHgDSSAaIhEPKZbLmadHwCgevGYKTNZ2SevuWmbxaZ05itMFNl2CACymAKKihSblpObJZvsTeiWi1fSsQJAF1l+3IKGXGfh/LiGLjlTyQrW48VjpsG1KzS4dkXRjYLzsb4PAIpjBLCE3kRcE+lM1GG0hGQwVYapMwCAnAeve3dDpoHmF2xZv3ms5DTQwuIu23/5su7cuqfosblEEQBwJBLAEm784Gm6duOOqMOIXG6RPFNnAABh+PPNT8zoYyr9sPGmgZXqf8si3bhl14wPbItVAQUAvIEEsISBvmRXJoDxHtPvHTVHEwczjPQBAMq67bJVdfeVBzNTGh5NTSeB1fQ5fDgJANUjAexya05ZpOd/k2ZqJwCgagN9SV23aYem6qycnV+xEwAQLhLALhUz00fPOUk3DVC5EwBQuy9/pP5RwNkqetZreDTFOnYACJAAdqFkb0KPrntP1GEAADpAqy+ZGB5NzSguk5pIa/3mMUkiCQTQldgGooz58c57eqiMBgBotNsuWxV1CCUNjYwfUVk0nZnU0Mh4RBEBQLQ6L8NpoAr3pG0bZtLQJWfyiScAoKHq7VdiNtuufrUr3BB+tnYA6HQkgGW8fngq6hAa6taPrCL5AwCE4srVS2s+d9JdazY8rOHRVAMjyiq1ITwbxQPoViSAXWLNKYtI/gCgQ5jZkJk9bWZPmNnfm1lv1DHdNLBSy49bUPP5qYm0rt24Q31f/FFDE8HBtSuUiMdmtOX2uAWAbkQC2CXu+sQ7ow4BANA4D0o63d3PkPS/Ja2POB5J0oPXvVtzeuqbznngYEbrN481LAkc6EvqlotXKtmbkClbCO2Wi1fyoSiArkUV0DJMUoctAwQAdAB3/1Hej1slXRJVLIX+6tIz664KmivS0qgkjQ3jAeANjACW0SnJX5J1DgDQyf6dpB9GHUTOQF9Sx79pbt3XoUgLAISDBLCMTkic2PYBANqTmf0PM/t5kX8X5R3zF5IOS7qrzHWuMbPtZrZ9//79zQhd2/7i/LrWA0oUaQGAsJAAltHuidPC+XG2fQCANuXuf+Tupxf59z1JMrOrJL1f0hXuXnLSirvf7u797t6/ePHiZoWvB697t267bJVqWRFIkRYACA8JYBntnDjFe6TRz7+3rR8DAKA4M7tA0p9J+qC7H4w6nlIG+pJ62ywjgcnehG67bJV6E/HptnRmUl/4/q5QtoUAgG5HAtihhi5dFXUIAIDw/I2kN0l60Mx2mNnfRh1QMdcPj+mZl14teXv+SN+rrx+ecduBgxkN3reTJBAAGowqoB3otsvY8B0AOpm7vy3qGCpx97YXSt62YG5MN38oux3Dmg0PKzN15CzWzKQ3tBooAIARwI5D8gcAaBWTpZcm6tVDk9r+y5clla/4STVQAGgsEsAOMj/eQ/IHAGgZMStfAubOrXs0PJoqW/GTaqAA0FgkgB3kP198RtQhAAAwbfVbF856zNDIuAbXrlC858hkka2MAKDxWAPYQRj9AwC0kl37fjfrMfsm0tP9141bdmkinZGU3crohg+cRt8GAA1GAjiL3kR8ujNqZYk4g7kAgNZSSf+Zm+I50Jck2QOAJiBrmMWNHzwt6hAqcgvTPwEAbWa2KZ7Doymt2fCwTl53v9ZseJgtIQCgAUgAZzHQl9SVq5dGHcas+NQUANBqFs6Pl71t6JIzS/Zfw6Mprd88ptREWi4pNZHW+s1jJIEAUCcSwAr0v2VR1CEAANB2bvjAaYrHZhZ3icdMt122SqOff2/ZDy+HRsaVzkzOaEtnJjU0Mh5KrADQLVgDWIFW72zaYYQSANB9cgne0Mi49k2ktaQ3ocG1KyqatVJq/z/2BQSA+pAAVqCVO5srVy/VTQMrow4DAICiai3usqQ3oVSR/pd9AQGgPkwBrUCrdjYL5sZI/gAAHWlw7Qol4rEZbYl4jH0BAaBOJIAVaNXO5uYPkfwBADrTQF9St1y8UsnehExSsjehWy5eSdEzAKgTU0ArMNCX1Be+v0sHDrbOfoBXrl5KJwgA6GjsDQgAjccIYIVu+EDz9wOM99gR1dNMrPsDAAAAUBtGAFuMSXJlp7rkpp7WUj0NAAAAAAqRAFaoGVtB3HbZqqLJHQkfAAAAgEZgCmiFwt4KwoxEDwAAAEC4SAArFPZWEFecw2buAAAAAMJFAlihwbUrZLMfVhOKugAAIA2PprRmw8M6ed39WrPhYQ2PpqIOCQA6DglghQb6krpidTijdCR/AIBuNzya0vrNY0pNpOWSUhNprd88RhIIAA1GAliFmwZW6rbLVjX0mgvnxxt6PQAA2tHQyLjSmckZbenMZFOKsAFAN6krATSz581szMx2mNn2oG2RmT1oZs8EXxfmHb/ezHab2biZrc1rPyu4zm4z+2szs6B9npltDNq3mdmyeuJthIG+pObNaUzeHI9ZJPsLAgDQakoVWwu7CBsAdJtGZDLnuvsqd+8Pfl4n6SF3Xy7poeBnmdnbJV0u6TRJF0j6qpnFgnO+JukaScuDfxcE7VdLOuDub5N0q6QvNSDeun3pw2fUfY1kb0JDl5xJ5U8AAFS62FrYRdgAoNuEsQ/gRZLeHXx/h6QfS/qzoP0ed39d0nNmtlvS2Wb2vKSj3f0xSTKz70gakPTD4Jwbg2vdJ+lvzMzc3UOIu2K5pO3ajTtqOr/Ufn8AAHSq64fHdPe2FzTprpiZPnrOSTPWwA+uXaH1m8dmTANNxGMaXLsiinABoGPVOwLokn5kZo+b2TVB2/Hu/qIkBV+PC9qTkl7IO3dv0JYMvi9sn3GOux+W9IqkYwuDMLNrzGy7mW3fv39/nQ+pMrUmcFeuXkryBwDoKtcPj+nOrXs0GXx+O+muO7fu0fXDY9PHDPQldcvFK5XsTciUnSlzy8Ur6TMBoMHqHQFc4+77zOw4SQ+a2dNlji22i4KXaS93zswG99sl3S5J/f39TRsdjJlNd2azWTA3pps/REcGAOg+d297oWR7/ijgQF+SfhIAQlZXAuju+4KvL5nZ30s6W9KvzOwEd3/RzE6Q9FJw+F5JJ+WdfqKkfUH7iUXa88/Za2ZzJB0j6eV6Ym6kSpM/pnwCALpZqf6y0n4UANA4NU8BNbMFZvam3PeS3ivp55K2SLoqOOwqSd8Lvt8i6fKgsufJyhZ7+UkwTfR3ZrY6qP758YJzcte6RNLDUa//y5esYGH6/HgPyR8AoKvFrNiEntLtAIDw1LMG8HhJ/8vMdkr6iaT73f0BSRsknW9mz0g6P/hZ7r5L0iZJT0p6QNKn3D230vuTkr4habekZ5UtACNJ35R0bFAw5joFFUVbxeDaFUrEYyVvj8dM//ni+iuGAgDQzj56zklVtQMAwlPzFFB3/4WkM4u0/0bSeSXOuVnSzUXat0s6vUj7a5IurTXGsOVG9oZGxrVvIq1jEnGZSRMHM1rSm9Dg2hWM/gEAul5unV+5KqAAgOYIYxuIrsKCdQAAZnfTwEoSPgBoAY3YCB4AAAAA0AZIAAEAAACgS5AAAgAAAECXIAEEAKBNmdl/NDM3szdHHQsAoD2QAAIA0IbM7CRlt1vaE3UsAID2QQIIAEB7ulXSf5LkUQcCAGgfJIAAALQZM/ugpJS776zg2GvMbLuZbd+/f38TogMAtDL2AQQAoAWZ2f+Q9K+K3PQXkv5c0nsruY673y7pdknq7+9ntBAAuhwJIAAALcjd/6hYu5mtlHSypJ1mJkknSvqZmZ3t7v+niSECANqQuXfWh4Fmtl/SLxtwqTdL+nUDrtNqeFzthcfVXnhczfcWd18cdRBRMrPnJfW7+6y/owb1ka3891CPTn1cUuc+Nh5Xe+FxNVfJ/rHjRgAb9UbAzLa7e38jrtVKeFzthcfVXnhcaHWN6CM79e+hUx+X1LmPjcfVXnhcraPjEkAAALqJuy+LOgYAQPugCigAAAAAdAkSwNJujzqAkPC42guPq73wuNANOvXvoVMfl9S5j43H1V54XC2i44rAAAAAAACKYwQQAAAAALoECSAAAAAAdImuTgDN7AIzGzez3Wa2rsjtZmZ/Hdz+hJm9I4o4q1XB47oieDxPmNk/m9mZUcRZi9keW95x/5eZTZrZJc2Mr1aVPC4ze7eZ7TCzXWb2P5sdYy0q+Fs8xsy+b2Y7g8f1J1HEWQ0z+5aZvWRmPy9xe1u+bkgVPba2fe1A9egj2+vvnP6R/rEVdGof2XH9o7t35T9JMUnPSnqrpLmSdkp6e8Ex75P0Q0kmabWkbVHH3aDH9QeSFgbf/3E7PK5KH1vecQ9L+oGkS6KOu0G/s15JT0paGvx8XNRxN+hx/bmkLwXfL5b0sqS5Ucc+y+N6l6R3SPp5idvb7nWjisfWlq8d/Kvpb4E+0tvn75z+kf6xVf51ah/Zaf1jN48Ani1pt7v/wt0PSbpH0kUFx1wk6TuetVVSr5md0OxAqzTr43L3f3b3A8GPWyWd2OQYa1XJ70ySPi3pu5JeamZwdajkcX1M0mZ33yNJ7t4Oj62Sx+WS3mRmJun3lO3gDjc3zOq4+z8qG2cp7fi6IWn2x9bGrx2oHn1kVrv8ndM/0j+2hE7tIzutf+zmBDAp6YW8n/cGbdUe02qqjflqZT+JaQezPjYzS0r6kKS/bWJc9arkd/b7khaa2Y/N7HEz+3jToqtdJY/rbyT9a0n7JI1J+oy7TzUnvNC04+tGLdrptQPVo4/Mape/c/pH+sd20Y6vG9Vq+deNOVEHECEr0la4J0Ylx7SaimM2s3OV/SP9v0ONqHEqeWy3Sfozd5/MfmjWFip5XHMknSXpPEkJSY+Z2VZ3/99hB1eHSh7XWkk7JL1H0imSHjSzf3L334YcW5ja8XWjKm342oHq0Ue21985/SP9Y7tox9eNirXL60Y3J4B7JZ2U9/OJyn7KUu0xraaimM3sDEnfkPTH7v6bJsVWr0oeW7+ke4LO7c2S3mdmh919uCkR1qbSv8Vfu/urkl41s3+UdKakVu7gKnlcfyJpg2cnze82s+cknSrpJ80JMRTt+LpRsTZ97UD16CPb6++c/pH+sV204+tGRdrpdaObp4D+VNJyMzvZzOZKulzSloJjtkj6eFCxaLWkV9z9xWYHWqVZH5eZLZW0WdK/afFPyArN+tjc/WR3X+buyyTdJ+n/bfHOTarsb/F7kv7QzOaY2XxJ50h6qslxVquSx7VH2U9tZWbHS1oh6RdNjbLx2vF1oyJt/NqB6tFHttffOf0j/WO7aMfXjVm12+tG144AuvthM/sPkkaUrcb0LXffZWb/Prj9b5WtkvU+SbslHVT205iWVuHj+rykYyV9Nfgk8LC790cVc6UqfGxtp5LH5e5PmdkDkp6QNCXpG+5etBRxq6jw9/WXkr5tZmPKTgv5M3f/dWRBV8DM7pb0bklvNrO9km6QFJfa93Ujp4LH1pavHagefWR7/Z3TP9I/topO7SM7rX+07MgyAAAAAKDTdfMUUAAAAADoKiSAAAAAANAlSAABAAAAoEuQAAIAAABAlyABBAA0nZl9y8xeMrOKqvWZ2UfM7Ekz22Vm/z3s+AAAiErYfSRVQAEATWdm75L0L5K+4+6nz3LsckmbJL3H3Q+Y2XHu/lIz4gQAoNnC7iMZAQQANJ27/6Okl/PbzOwUM3vAzB43s38ys1ODmz4h6b+6+4HgXJI/AEDHCruPJAEEALSK2yV92t3PkvQfJX01aP99Sb9vZo+a2VYzuyCyCAEAiEbD+sg5IQYJAEBFzOz3JP2BpHvNLNc8L/g6R9JySe+WdKKkfzKz0919oslhAgDQdI3uI0kAAQCtoEfShLuvKnLbXklb3T0j6TkzG1e2s/tpE+MDACAqDe0jmQIKAIicu/9W2Y7rUkmyrDODm4clnRu0v1nZ6S6/iCJOAACardF9JAkgAKDpzOxuSY9JWmFme83saklXSLrazHZK2iXpouDwEUm/MbMnJT0iadDdfxNF3AAAhC3sPpJtIAAAAACgSzACCAAAAABdggQQAAAAALoECSAAAAAAdAkSQAAAAADoEiSAAAAAANAlSAABAAAAoEuQAAIAAABAl/j/A01AGfqZ8sjDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x864 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# neuralNet=NNBase(num_input=game.T+2+game.stateAdvHistory, lr=hyperParams[0],num_actions=18,action_step=actionStep)\n",
    "# algorithm = ReinforceAlgorithm(game, neuralNet, numberIterations=2, numberEpisodes=50_000, discountFactor =hyperParams[1])\n",
    "\n",
    "\n",
    "# algorithm.solver(print_step=30_000,options=codeParams,converge_break=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralNet=NNBase(num_input=game.T+2+game.stateAdvHistory, lr=hyperParams[0],num_actions=18,action_step=actionStep)\n",
    "algorithm = ReinforceAlgorithm(game, neuralNet, numberIterations=2, numberEpisodes=50_000, discountFactor =hyperParams[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy reset\n",
      "policy loaded!\n"
     ]
    }
   ],
   "source": [
    "algorithm.loadPolicyNet(\"1,3,[5e-06,1][1, 10000, 1, 1],1683196677\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x27 and 28x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplayTrainedAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43madv\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\EquiLearn\\PGM_base\\learningBase.py:297\u001b[0m, in \u001b[0;36mReinforceAlgorithm.playTrainedAgent\u001b[1;34m(self, adversary, iterNum)\u001b[0m\n\u001b[0;32m    295\u001b[0m prevState \u001b[38;5;241m=\u001b[39m state\n\u001b[0;32m    296\u001b[0m normPrevState \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnormalizeState(prevState)\n\u001b[1;32m--> 297\u001b[0m probs\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormPrevState\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m distAction \u001b[38;5;241m=\u001b[39m Categorical(probs)\n\u001b[0;32m    299\u001b[0m action \u001b[38;5;241m=\u001b[39m distAction\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[1;32m~\\Documents\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\Documents\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x27 and 28x512)"
     ]
    }
   ],
   "source": [
    "\n",
    "algorithm.playTrainedAgent(adv,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
