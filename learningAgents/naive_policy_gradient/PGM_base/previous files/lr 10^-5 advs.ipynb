{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learningBase import ReinforceAlgorithm\n",
    "from environmentModelBase import Model, AdversaryModes\n",
    "from neuralNetworkSimple import NNBase\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperParams=[0.00001, 1, 0]\n",
    "codeParams=[1, 10000, 1, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy reset\n",
      "----------------------------------------\n",
      "iter  0  stage  24  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5663, 0.5663, 0.5663, 0.5663, 0.5663, 0.5663, 0.5663, 0.5663, 0.5663,\n",
      "        0.5663, 0.5663, 0.5663, 0.5663, 0.5663, 0.5663, 0.5663, 0.5663, 0.5663,\n",
      "        0.5663, 0.5663, 0.5663, 0.5663, 0.5663, 0.5663, 0.5663]) return=  139847.7232627179\n",
      "probs of actions:  tensor([0.8786, 0.8836, 0.8852, 0.9041, 0.9160, 0.8963, 0.8981, 0.8989, 0.9127,\n",
      "        0.9010, 0.9015, 0.9021, 0.0518, 0.8964, 0.8851, 0.9235, 0.9054, 0.0415,\n",
      "        0.9164, 0.9023, 0.9071, 0.9030, 0.0111, 0.9274, 0.9877],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5366, 0.5495, 0.5560, 0.5592, 0.5609, 0.5617, 0.5621, 0.5623,\n",
      "        0.5624, 0.5624, 0.5625, 0.5624, 0.5662, 0.5644, 0.5634, 0.5630, 0.5626,\n",
      "        0.5664, 0.5644, 0.5635, 0.5630, 0.5623, 0.5701, 0.5663])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  0  stage  23  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([ 0,  0,  4,  8, 14,  0,  0,  1,  0, 22, 20,  0,  5,  5,  0,  0, 16,  0,\n",
      "         0,  0,  1,  0,  0, 22,  0])\n",
      "loss=  tensor(0.0097, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.1279, 1.1279, 1.1279, 1.1279, 1.1279, 1.1279, 1.1279, 1.1279, 1.1279,\n",
      "        1.1279, 1.1279, 1.1279, 1.1279, 1.1279, 1.1279, 1.1279, 1.1279, 1.1279,\n",
      "        1.1279, 1.1279, 1.1279, 1.1279, 1.1279, 1.1279, 0.5635]) return=  145876.19064216543\n",
      "probs of actions:  tensor([0.4697, 0.5450, 0.0549, 0.0497, 0.0049, 0.4636, 0.4981, 0.0854, 0.5431,\n",
      "        0.0618, 0.0067, 0.5533, 0.0268, 0.0303, 0.4079, 0.6228, 0.0022, 0.5265,\n",
      "        0.5884, 0.4364, 0.0926, 0.5506, 0.5699, 0.7906, 0.9886],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5366, 0.5479, 0.5646, 0.5776, 0.6343, 0.5978, 0.5799, 0.5750,\n",
      "        0.5203, 0.6114, 0.6865, 0.6204, 0.6092, 0.6062, 0.5841, 0.5477, 0.6298,\n",
      "        0.5957, 0.5790, 0.5706, 0.5704, 0.5664, 0.5161, 0.6491])\n",
      "finalReturns:  tensor([0.0372, 0.0856])\n",
      "----------------------------------------\n",
      "iter  0  stage  22  ep  78431   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22,  0, 22, 22,  0])\n",
      "loss=  tensor(0.0009, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.8150, 1.8150, 1.8150, 1.8150, 1.8150, 1.8150, 1.8150, 1.8150, 1.8150,\n",
      "        1.8150, 1.8150, 1.8150, 1.8150, 1.8150, 1.8150, 1.8150, 1.8150, 1.8150,\n",
      "        1.8150, 1.8150, 1.8150, 1.8150, 1.8150, 1.1670, 0.5625]) return=  167444.53665689877\n",
      "probs of actions:  tensor([0.9807, 0.9663, 0.9871, 0.9885, 0.9861, 0.9875, 0.9866, 0.9924, 0.9865,\n",
      "        0.9892, 0.9886, 0.9758, 0.9819, 0.9908, 0.9891, 0.9866, 0.9716, 0.9830,\n",
      "        0.9787, 0.9912, 0.9762, 0.0128, 0.9990, 0.9996, 0.9954],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.7396, 0.5996, 0.6447, 0.7161])\n",
      "finalReturns:  tensor([0.1454, 0.1938, 0.1536])\n",
      "----------------------------------------\n",
      "iter  0  stage  21  ep  21111   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22,  0, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0010, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.3420, 2.3420, 2.3420, 2.3420, 2.3420, 2.3420, 2.3420, 2.3420, 2.3420,\n",
      "        2.3420, 2.3420, 2.3420, 2.3420, 2.3420, 2.3420, 2.3420, 2.3420, 2.3420,\n",
      "        2.3420, 2.3420, 2.3420, 2.3420, 1.6940, 1.0895, 0.5270]) return=  167326.75936424857\n",
      "probs of actions:  tensor([0.9970, 0.9941, 0.9981, 0.9980, 0.9979, 0.9981, 0.9981, 0.9989, 0.9977,\n",
      "        0.9984, 0.9982, 0.9962, 0.9969, 0.9986, 0.9983, 0.9977, 0.9953, 0.9971,\n",
      "        0.9968, 0.9987, 0.0032, 0.9990, 0.9999, 1.0000, 0.9967],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.7396, 0.5996, 0.6447, 0.6677, 0.7278])\n",
      "finalReturns:  tensor([0.2978, 0.3462, 0.3061, 0.2008])\n",
      "----------------------------------------\n",
      "iter  0  stage  20  ep  788   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22,  0, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0018, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.8407, 2.8407, 2.8407, 2.8407, 2.8407, 2.8407, 2.8407, 2.8407, 2.8407,\n",
      "        2.8407, 2.8407, 2.8407, 2.8407, 2.8407, 2.8407, 2.8407, 2.8407, 2.8407,\n",
      "        2.8407, 2.8407, 2.8407, 2.1927, 1.5882, 1.0257, 0.4987]) return=  167267.7526299165\n",
      "probs of actions:  tensor([9.9804e-01, 9.9601e-01, 9.9866e-01, 9.9866e-01, 9.9858e-01, 9.9872e-01,\n",
      "        9.9872e-01, 9.9925e-01, 9.9842e-01, 9.9895e-01, 9.9880e-01, 9.9754e-01,\n",
      "        9.9786e-01, 9.9907e-01, 9.9890e-01, 9.9846e-01, 9.9682e-01, 9.9812e-01,\n",
      "        9.9790e-01, 5.2534e-04, 9.9901e-01, 9.9924e-01, 9.9993e-01, 9.9998e-01,\n",
      "        9.9636e-01], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.7396, 0.5996, 0.6447, 0.6677, 0.6794, 0.7337])\n",
      "finalReturns:  tensor([0.4844, 0.5328, 0.4927, 0.3875, 0.2350])\n",
      "----------------------------------------\n",
      "iter  0  stage  19  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0029, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.4906, 3.4906, 3.4906, 3.4906, 3.4906, 3.4906, 3.4906, 3.4906, 3.4906,\n",
      "        3.4906, 3.4906, 3.4906, 3.4906, 3.4906, 3.4906, 3.4906, 3.4906, 3.4906,\n",
      "        3.4906, 3.4906, 2.7510, 2.1030, 1.5197, 0.9827, 0.4792]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9980, 0.9960, 0.9987, 0.9987, 0.9986, 0.9987, 0.9987, 0.9992, 0.9984,\n",
      "        0.9989, 0.9988, 0.9975, 0.9979, 0.9991, 0.9989, 0.9985, 0.9968, 0.9981,\n",
      "        0.9979, 0.9991, 0.9989, 0.9993, 0.9999, 1.0000, 0.9964],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([0.7050, 0.7534, 0.7102, 0.6023, 0.4481, 0.2604])\n",
      "----------------------------------------\n",
      "iter  0  stage  18  ep  349   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,  0,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0040, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.7776, 3.7776, 3.7776, 3.7776, 3.7776, 3.7776, 3.7776, 3.7776, 3.7776,\n",
      "        3.7776, 3.7776, 3.7776, 3.7776, 3.7776, 3.7776, 3.7776, 3.7776, 3.7776,\n",
      "        3.7776, 3.1295, 2.5250, 1.9625, 1.4355, 0.9369, 0.4601]) return=  167223.4464147788\n",
      "probs of actions:  tensor([0.9981, 0.9962, 0.9988, 0.9987, 0.9987, 0.9988, 0.9988, 0.9993, 0.9985,\n",
      "        0.9990, 0.9989, 0.9976, 0.9980, 0.9991, 0.9990, 0.9985, 0.9970, 0.0014,\n",
      "        0.9991, 0.9995, 0.9988, 0.9994, 0.9999, 1.0000, 0.9965],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396,\n",
      "        0.5996, 0.6447, 0.6677, 0.6794, 0.6853, 0.6882, 0.7381])\n",
      "finalReturns:  tensor([0.9256, 0.9740, 0.9338, 0.8286, 0.6761, 0.4895, 0.2780])\n",
      "----------------------------------------\n",
      "iter  0  stage  17  ep  219   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0051, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.3999, 4.3999, 4.3999, 4.3999, 4.3999, 4.3999, 4.3999, 4.3999, 4.3999,\n",
      "        4.3999, 4.3999, 4.3999, 4.3999, 4.3999, 4.3999, 4.3999, 4.3999, 4.3999,\n",
      "        3.6604, 3.0123, 2.4290, 1.8920, 1.3885, 0.9093, 0.4481]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9983, 0.9966, 0.9989, 0.9989, 0.9988, 0.9989, 0.9989, 0.9994, 0.9986,\n",
      "        0.9991, 0.9990, 0.9979, 0.9982, 0.9992, 0.9991, 0.9987, 0.9973, 0.9990,\n",
      "        0.9992, 0.9996, 0.9990, 0.9995, 1.0000, 1.0000, 0.9965],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([1.1780, 1.2264, 1.1833, 1.0754, 0.9212, 0.7335, 0.5215, 0.2915])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  0  stage  16  ep  5672   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,  0, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0037, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.6630, 4.6630, 4.6630, 4.6630, 4.6630, 4.6630, 4.6630, 4.6630, 4.6630,\n",
      "        4.6630, 4.6630, 4.6630, 4.6630, 4.6630, 4.6630, 4.6630, 4.6630, 4.0150,\n",
      "        3.4105, 2.8480, 2.3210, 1.8223, 1.3456, 0.8855, 0.4380]) return=  167212.36521898463\n",
      "probs of actions:  tensor([9.9920e-01, 9.9831e-01, 9.9951e-01, 9.9950e-01, 9.9944e-01, 9.9955e-01,\n",
      "        9.9955e-01, 9.9975e-01, 9.9934e-01, 9.9959e-01, 9.9954e-01, 9.9893e-01,\n",
      "        9.9910e-01, 9.9968e-01, 9.9960e-01, 5.5382e-04, 9.9908e-01, 9.9982e-01,\n",
      "        9.9983e-01, 9.9998e-01, 9.9956e-01, 9.9994e-01, 9.9999e-01, 1.0000e+00,\n",
      "        9.9578e-01], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.7396, 0.5996, 0.6447,\n",
      "        0.6677, 0.6794, 0.6853, 0.6882, 0.6897, 0.6905, 0.7392])\n",
      "finalReturns:  tensor([1.4214, 1.4698, 1.4296, 1.3244, 1.1720, 0.9853, 0.7739, 0.5442, 0.3012])\n",
      "----------------------------------------\n",
      "iter  0  stage  15  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0053, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.2692, 5.2692, 5.2692, 5.2692, 5.2692, 5.2692, 5.2692, 5.2692, 5.2692,\n",
      "        5.2692, 5.2692, 5.2692, 5.2692, 5.2692, 5.2692, 5.2692, 4.5296, 3.8816,\n",
      "        3.2983, 2.7613, 2.2577, 1.7786, 1.3173, 0.8693, 0.4310]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9994, 0.9996, 0.9996, 0.9998, 0.9993,\n",
      "        0.9996, 0.9995, 0.9989, 0.9991, 0.9997, 0.9996, 0.9994, 0.9990, 0.9998,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9958],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([1.6912, 1.7396, 1.6964, 1.5885, 1.4343, 1.2467, 1.0346, 0.8047, 0.5615,\n",
      "        0.3086])\n",
      "----------------------------------------\n",
      "iter  0  stage  14  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0066, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.6948, 5.6948, 5.6948, 5.6948, 5.6948, 5.6948, 5.6948, 5.6948, 5.6948,\n",
      "        5.6948, 5.6948, 5.6948, 5.6948, 5.6948, 5.6948, 4.9552, 4.3072, 3.7239,\n",
      "        3.1869, 2.6834, 2.2042, 1.7430, 1.2949, 0.8566, 0.4256]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9994, 0.9996, 0.9996, 0.9998, 0.9993,\n",
      "        0.9996, 0.9995, 0.9989, 0.9991, 0.9997, 0.9996, 0.9994, 0.9990, 0.9998,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9958],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([1.9567, 2.0051, 1.9620, 1.8541, 1.6999, 1.5122, 1.3002, 1.0702, 0.8271,\n",
      "        0.5742, 0.3140])\n",
      "----------------------------------------\n",
      "iter  0  stage  13  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0078, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.1164, 6.1164, 6.1164, 6.1164, 6.1164, 6.1164, 6.1164, 6.1164, 6.1164,\n",
      "        6.1164, 6.1164, 6.1164, 6.1164, 6.1164, 5.3768, 4.7288, 4.1455, 3.6085,\n",
      "        3.1049, 2.6258, 2.1645, 1.7165, 1.2782, 0.8472, 0.4216]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9994, 0.9996, 0.9996, 0.9998, 0.9993,\n",
      "        0.9996, 0.9995, 0.9989, 0.9991, 0.9997, 0.9996, 0.9994, 0.9990, 0.9998,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9958],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([2.2264, 2.2748, 2.2316, 2.1237, 1.9695, 1.7819, 1.5698, 1.3399, 1.0967,\n",
      "        0.8438, 0.5836, 0.3180])\n",
      "----------------------------------------\n",
      "iter  0  stage  12  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22,  0, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0105, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.5345, 6.5345, 6.5345, 6.5345, 6.5345, 6.5345, 6.5345, 6.5345, 6.5345,\n",
      "        6.5345, 6.5345, 6.5345, 6.5345, 5.7952, 5.1473, 4.5640, 4.0270, 3.5235,\n",
      "        3.0444, 2.5831, 2.1351, 1.6968, 1.2658, 0.8401, 0.4186]) return=  167235.25059974194\n",
      "probs of actions:  tensor([9.9920e-01, 9.9831e-01, 3.7954e-04, 9.9957e-01, 9.9935e-01, 9.9952e-01,\n",
      "        9.9954e-01, 9.9975e-01, 9.9933e-01, 9.9959e-01, 9.9954e-01, 9.9893e-01,\n",
      "        9.9910e-01, 9.9968e-01, 9.9960e-01, 9.9939e-01, 9.9896e-01, 9.9985e-01,\n",
      "        9.9984e-01, 9.9998e-01, 9.9957e-01, 9.9994e-01, 9.9999e-01, 1.0000e+00,\n",
      "        9.9578e-01], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6786, 0.5708, 0.6296, 0.6601, 0.6756, 0.6834, 0.6873,\n",
      "        0.6892, 0.6902, 0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([2.4990, 2.5474, 2.5042, 2.3963, 2.2421, 2.0545, 1.8424, 1.6125, 1.3693,\n",
      "        1.1164, 0.8562, 0.5907, 0.3210])\n",
      "----------------------------------------\n",
      "iter  0  stage  11  ep  13   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0135, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.9511, 6.9511, 6.9511, 6.9511, 6.9511, 6.9511, 6.9511, 6.9511, 6.9511,\n",
      "        6.9511, 6.9511, 6.9511, 6.2116, 5.5636, 4.9804, 4.4434, 3.9398, 3.4607,\n",
      "        2.9994, 2.5514, 2.1131, 1.6821, 1.2565, 0.8349, 0.4163]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9994, 0.9996, 0.9996, 0.9998, 0.9993,\n",
      "        0.9996, 0.9996, 0.9990, 0.9992, 0.9997, 0.9996, 0.9994, 0.9990, 0.9998,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9957],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([2.7739, 2.8223, 2.7791, 2.6712, 2.5170, 2.3294, 2.1173, 1.8874, 1.6442,\n",
      "        1.3913, 1.1311, 0.8655, 0.5959, 0.3233])\n",
      "----------------------------------------\n",
      "iter  0  stage  10  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0155, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.3655, 7.3655, 7.3655, 7.3655, 7.3655, 7.3655, 7.3655, 7.3655, 7.3655,\n",
      "        7.3655, 7.3655, 6.6261, 5.9782, 5.3950, 4.8580, 4.3544, 3.8753, 3.4141,\n",
      "        2.9660, 2.5277, 2.0967, 1.6711, 1.2495, 0.8310, 0.4146]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9994, 0.9996, 0.9996, 0.9998, 0.9993,\n",
      "        0.9996, 0.9996, 0.9990, 0.9992, 0.9997, 0.9996, 0.9994, 0.9990, 0.9998,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9957],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([3.0504, 3.0988, 3.0557, 2.9478, 2.7936, 2.6059, 2.3939, 2.1639, 1.9208,\n",
      "        1.6679, 1.4077, 1.1421, 0.8725, 0.5998, 0.3250])\n",
      "----------------------------------------\n",
      "iter  0  stage  9  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0175, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.7784, 7.7784, 7.7784, 7.7784, 7.7784, 7.7784, 7.7784, 7.7784, 7.7784,\n",
      "        7.7784, 7.0393, 6.3915, 5.8083, 5.2713, 4.7678, 4.2887, 3.8274, 3.3794,\n",
      "        2.9411, 2.5101, 2.0845, 1.6629, 1.2443, 0.8280, 0.4134]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9994, 0.9996, 0.9996, 0.9998, 0.9993,\n",
      "        0.9996, 0.9996, 0.9990, 0.9992, 0.9997, 0.9996, 0.9994, 0.9990, 0.9998,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9957],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([3.3282, 3.3766, 3.3335, 3.2256, 3.0714, 2.8838, 2.6717, 2.4418, 2.1986,\n",
      "        1.9457, 1.6855, 1.4199, 1.1503, 0.8777, 0.6028, 0.3262])\n",
      "----------------------------------------\n",
      "iter  0  stage  8  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0204, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.1899, 8.1899, 8.1899, 8.1899, 8.1899, 8.1899, 8.1899, 8.1899, 8.1899,\n",
      "        7.4513, 6.8037, 6.2206, 5.6837, 5.1802, 4.7011, 4.2398, 3.7918, 3.3535,\n",
      "        2.9225, 2.4969, 2.0753, 1.6567, 1.2404, 0.8258, 0.4124]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9994, 0.9996, 0.9996, 0.9998, 0.9993,\n",
      "        0.9996, 0.9996, 0.9990, 0.9992, 0.9997, 0.9996, 0.9994, 0.9990, 0.9998,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9957],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([3.6069, 3.6553, 3.6122, 3.5043, 3.3502, 3.1625, 2.9505, 2.7205, 2.4774,\n",
      "        2.2245, 1.9643, 1.6987, 1.4291, 1.1565, 0.8816, 0.6050, 0.3272])\n",
      "----------------------------------------\n",
      "iter  0  stage  7  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0220, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.5998, 8.5998, 8.5998, 8.5998, 8.5998, 8.5998, 8.5998, 8.5998, 7.8621,\n",
      "        7.2150, 6.6321, 6.0953, 5.5919, 5.1128, 4.6515, 4.2035, 3.7652, 3.3342,\n",
      "        2.9086, 2.4870, 2.0685, 1.6522, 1.2375, 0.8241, 0.4117]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9994, 0.9996, 0.9996, 0.9998, 0.9994,\n",
      "        0.9996, 0.9996, 0.9990, 0.9992, 0.9997, 0.9996, 0.9995, 0.9990, 0.9998,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9956],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([3.8863, 3.9347, 3.8916, 3.7838, 3.6296, 3.4420, 3.2300, 3.0000, 2.7569,\n",
      "        2.5040, 2.2438, 1.9782, 1.7086, 1.4359, 1.1610, 0.8845, 0.6067, 0.3279])\n",
      "----------------------------------------\n",
      "iter  0  stage  6  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0243, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.0073, 9.0073, 9.0073, 9.0073, 9.0073, 9.0073, 9.0073, 8.2716, 7.6254,\n",
      "        7.0429, 6.5063, 6.0030, 5.5239, 5.0627, 4.6147, 4.1764, 3.7454, 3.3198,\n",
      "        2.8982, 2.4797, 2.0633, 1.6487, 1.2353, 0.8229, 0.4112]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9994, 0.9996, 0.9996, 0.9998, 0.9994,\n",
      "        0.9996, 0.9996, 0.9990, 0.9992, 0.9997, 0.9996, 0.9995, 0.9990, 0.9998,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9956],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([4.1661, 4.2145, 4.1715, 4.0637, 3.9096, 3.7220, 3.5100, 3.2800, 3.0369,\n",
      "        2.7840, 2.5238, 2.2582, 1.9886, 1.7159, 1.4411, 1.1645, 0.8867, 0.6079,\n",
      "        0.3284])\n",
      "----------------------------------------\n",
      "iter  0  stage  5  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0267, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.4107, 9.4107, 9.4107, 9.4107, 9.4107, 9.4107, 8.6789, 8.0345, 7.4529,\n",
      "        6.9167, 6.4136, 5.9346, 5.4735, 5.0255, 4.5872, 4.1562, 3.7306, 3.3090,\n",
      "        2.8904, 2.4741, 2.0595, 1.6461, 1.2337, 0.8220, 0.4108]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9994, 0.9996, 0.9996, 0.9998, 0.9994,\n",
      "        0.9996, 0.9996, 0.9990, 0.9992, 0.9997, 0.9996, 0.9995, 0.9990, 0.9998,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9956],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([4.4461, 4.4945, 4.4516, 4.3440, 4.1899, 4.0023, 3.7903, 3.5604, 3.3173,\n",
      "        3.0644, 2.8042, 2.5386, 2.2690, 1.9964, 1.7215, 1.4449, 1.1671, 0.8883,\n",
      "        0.6088, 0.3288])\n",
      "----------------------------------------\n",
      "iter  0  stage  4  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0297, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.8065, 9.8065, 9.8065, 9.8065, 9.8065, 9.0824, 8.4416, 7.8618, 7.3264,\n",
      "        6.8237, 6.3449, 5.8839, 5.4359, 4.9977, 4.5667, 4.1411, 3.7195, 3.3009,\n",
      "        2.8846, 2.4700, 2.0566, 1.6442, 1.2325, 0.8213, 0.4105]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9994, 0.9996, 0.9996, 0.9998, 0.9994,\n",
      "        0.9996, 0.9996, 0.9990, 0.9992, 0.9997, 0.9996, 0.9995, 0.9990, 0.9999,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9956],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([4.7261, 4.7745, 4.7318, 4.6243, 4.4704, 4.2829, 4.0710, 3.8411, 3.5980,\n",
      "        3.3451, 3.0849, 2.8193, 2.5497, 2.2771, 2.0022, 1.7256, 1.4478, 1.1690,\n",
      "        0.8895, 0.6095, 0.3291])\n",
      "----------------------------------------\n",
      "iter  0  stage  3  ep  0   adversary:  AdversaryModes.myopic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0326, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.1875, 10.1875, 10.1875, 10.1875,  9.4787,  8.8452,  8.2688,  7.7351,\n",
      "         7.2332,  6.7548,  6.2939,  5.8461,  5.4079,  4.9769,  4.5513,  4.1298,\n",
      "         3.7112,  3.2949,  2.8803,  2.4669,  2.0545,  1.6427,  1.2316,  0.8208,\n",
      "         0.4103]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9983, 0.9995, 0.9995, 0.9995, 0.9996, 0.9996, 0.9998, 0.9994,\n",
      "        0.9996, 0.9996, 0.9991, 0.9992, 0.9997, 0.9996, 0.9995, 0.9990, 0.9999,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9956],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([5.0054, 5.0538, 5.0116, 4.9046, 4.7510, 4.5637, 4.3518, 4.1220, 3.8789,\n",
      "        3.6260, 3.3658, 3.1002, 2.8306, 2.5580, 2.2831, 2.0065, 1.7287, 1.4499,\n",
      "        1.1705, 0.8904, 0.6100, 0.3293])\n",
      "----------------------------------------\n",
      "iter  0  stage  2  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0355, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.5398, 10.5398, 10.5398,  9.8612,  9.2420,  8.6725,  8.1421,  7.6417,\n",
      "         7.1642,  6.7037,  6.2560,  5.8179,  5.3870,  4.9614,  4.5399,  4.1213,\n",
      "         3.7050,  3.2904,  2.8770,  2.4646,  2.0528,  1.6417,  1.2309,  0.8204,\n",
      "         0.4101]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9992, 0.9984, 0.9995, 0.9995, 0.9995, 0.9996, 0.9996, 0.9998, 0.9994,\n",
      "        0.9996, 0.9996, 0.9991, 0.9992, 0.9997, 0.9996, 0.9995, 0.9990, 0.9999,\n",
      "        0.9998, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 0.9956],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([5.2833, 5.3317, 5.2905, 5.1844, 5.0313, 4.8444, 4.6327, 4.4030, 4.1599,\n",
      "        3.9071, 3.6469, 3.3813, 3.1117, 2.8391, 2.5642, 2.2876, 2.0098, 1.7310,\n",
      "        1.4516, 1.1715, 0.8911, 0.6104, 0.3295])\n",
      "----------------------------------------\n",
      "iter  0  stage  1  ep  83   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0292, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.8367, 10.8367, 10.2165,  9.6255,  9.0696,  8.5458,  8.0486,  7.5726,\n",
      "         7.1129,  6.6656,  6.2277,  5.7969,  5.3713,  4.9498,  4.5313,  4.1150,\n",
      "         3.7003,  3.2870,  2.8745,  2.4628,  2.0516,  1.6408,  1.2303,  0.8201,\n",
      "         0.4100]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9994, 0.9990, 0.9997, 0.9997, 0.9997, 0.9997, 0.9997, 0.9999, 0.9996,\n",
      "        0.9998, 0.9997, 0.9994, 0.9995, 0.9998, 0.9998, 0.9997, 0.9992, 0.9999,\n",
      "        0.9999, 1.0000, 0.9997, 1.0000, 1.0000, 1.0000, 0.9951],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([5.5581, 5.6065, 5.5674, 5.4630, 5.3111, 5.1248, 4.9135, 4.6840, 4.4410,\n",
      "        4.1882, 3.9281, 3.6625, 3.3929, 3.1203, 2.8454, 2.5689, 2.2910, 2.0123,\n",
      "        1.7328, 1.4528, 1.1724, 0.8917, 0.6107, 0.3296])\n",
      "----------------------------------------\n",
      "iter  0  stage  0  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.0333, grad_fn=<NegBackward0>)   ,  base rewards= tensor([11.0309, 10.5197,  9.9832,  9.4539,  8.9431,  8.4524,  7.9795,  7.5213,\n",
      "         7.0747,  6.6372,  6.2066,  5.7811,  5.3597,  4.9411,  4.5248,  4.1102,\n",
      "         3.6968,  3.2844,  2.8727,  2.4615,  2.0507,  1.6402,  1.2300,  0.8199,\n",
      "         0.4099]) return=  168576.33348198733\n",
      "probs of actions:  tensor([0.9994, 0.9990, 0.9997, 0.9997, 0.9997, 0.9997, 0.9997, 0.9999, 0.9996,\n",
      "        0.9998, 0.9997, 0.9994, 0.9995, 0.9998, 0.9998, 0.9997, 0.9992, 0.9999,\n",
      "        0.9999, 1.0000, 0.9997, 1.0000, 1.0000, 1.0000, 0.9951],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5718, 0.6302, 0.6604, 0.6757, 0.6834, 0.6873, 0.6893, 0.6902,\n",
      "        0.6907, 0.6910, 0.6911, 0.6911, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912,\n",
      "        0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.6912, 0.7396])\n",
      "finalReturns:  tensor([5.8267, 5.8751, 5.8399, 5.7390, 5.5894, 5.4045, 5.1940, 4.9648, 4.7221,\n",
      "        4.4694, 4.2093, 3.9438, 3.6742, 3.4016, 3.1267, 2.8502, 2.5723, 2.2936,\n",
      "        2.0141, 1.7341, 1.4537, 1.1730, 0.8920, 0.6109, 0.3297])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1682348121 saved\n",
      "[346689, 'tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])', 168576.33348198733, 65680.3332469066, 0.03331249579787254, 1e-05, 1, 0, 'tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\\n        22, 22, 22, 22, 22, 22,  0])', '[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\\n 1.]', '0,[1e-05,1][1, 10000, 1, 1],1682348121', 25, 50, 168576.3334819873, 205322.17620252006, 83998.798421993, 134475.68800000002, 131523.21866666665, 89197.96159612676, 89197.96159612676, 106340.34718300312, 106308.66365021843, 97483.02777699233, 89197.96159612676, 106308.66365021843]\n",
      "policy reset\n",
      "----------------------------------------\n",
      "iter  1  stage  24  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625,\n",
      "        0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625,\n",
      "        0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625]) return=  139739.47699942059\n",
      "probs of actions:  tensor([0.8929, 0.8857, 0.9020, 0.8962, 0.8926, 0.8984, 0.0646, 0.9006, 0.8831,\n",
      "        0.9026, 0.8910, 0.0521, 0.8824, 0.8858, 0.8784, 0.9028, 0.8894, 0.8959,\n",
      "        0.8758, 0.9148, 0.8829, 0.8869, 0.8875, 0.8934, 0.9822],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5366, 0.5495, 0.5560, 0.5592, 0.5609, 0.5616, 0.5658, 0.5642,\n",
      "        0.5633, 0.5629, 0.5626, 0.5664, 0.5644, 0.5635, 0.5630, 0.5627, 0.5626,\n",
      "        0.5626, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  1  stage  23  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([ 1,  0, 11,  0,  0,  0, 16, 19,  0,  0,  0,  4, 14,  8,  0, 12, 11,  0,\n",
      "        10,  4,  0,  0,  0, 14,  0])\n",
      "loss=  tensor(0.0716, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.1330, 1.1330, 1.1330, 1.1330, 1.1330, 1.1330, 1.1330, 1.1330, 1.1330,\n",
      "        1.1330, 1.1330, 1.1330, 1.1330, 1.1330, 1.1330, 1.1330, 1.1330, 1.1330,\n",
      "        1.1330, 1.1330, 1.1330, 1.1330, 1.1330, 1.1330, 0.5652]) return=  146956.28276435166\n",
      "probs of actions:  tensor([0.0785, 0.5092, 0.0653, 0.4961, 0.4847, 0.3295, 0.0636, 0.0042, 0.4223,\n",
      "        0.5699, 0.5365, 0.0700, 0.0100, 0.0034, 0.3717, 0.0786, 0.0824, 0.3663,\n",
      "        0.0485, 0.0709, 0.4142, 0.4306, 0.3190, 0.1280, 0.9781],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5111, 0.5402, 0.5392, 0.5987, 0.5805, 0.5714, 0.5414, 0.5903, 0.6695,\n",
      "        0.6148, 0.5884, 0.5738, 0.5645, 0.6211, 0.6258, 0.5793, 0.6124, 0.6362,\n",
      "        0.5888, 0.6176, 0.6060, 0.5840, 0.5732, 0.5482, 0.6190])\n",
      "finalReturns:  tensor([0.0342, 0.0538])\n",
      "----------------------------------------\n",
      "iter  1  stage  22  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([16, 12, 28,  1,  0, 23, 12, 16, 26, 22,  0, 27, 16, 27, 12, 19, 19, 26,\n",
      "         1, 27, 22, 30, 27, 20,  0])\n",
      "loss=  tensor(0.9044, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.0149, 2.0149, 2.0149, 2.0149, 2.0149, 2.0149, 2.0149, 2.0149, 2.0149,\n",
      "        2.0149, 2.0149, 2.0149, 2.0149, 2.0149, 2.0149, 2.0149, 2.0149, 2.0149,\n",
      "        2.0149, 2.0149, 2.0149, 2.0149, 2.0149, 1.2464, 0.5849]) return=  162124.7477856951\n",
      "probs of actions:  tensor([0.0491, 0.1345, 0.0154, 0.1258, 0.1692, 0.0112, 0.1468, 0.0460, 0.0490,\n",
      "        0.0374, 0.2083, 0.0527, 0.0574, 0.0611, 0.2087, 0.0473, 0.0565, 0.0554,\n",
      "        0.0369, 0.0536, 0.0429, 0.0126, 0.1198, 0.1008, 0.9626],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4856, 0.5824, 0.5477, 0.7066, 0.6365, 0.5460, 0.6571, 0.6382, 0.6087,\n",
      "        0.6761, 0.7320, 0.5716, 0.6866, 0.6276, 0.7269, 0.6619, 0.6699, 0.6424,\n",
      "        0.7418, 0.5802, 0.6683, 0.6381, 0.6956, 0.7358, 0.7489])\n",
      "finalReturns:  tensor([0.1655, 0.2384, 0.1640])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  21  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 18, 27, 27, 26, 27, 29, 26, 27, 23, 27, 27, 19, 19, 20, 16, 25, 19,\n",
      "        26, 23, 30, 27, 28, 29,  0])\n",
      "loss=  tensor(3.3616, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.5627, 2.5627, 2.5627, 2.5627, 2.5627, 2.5627, 2.5627, 2.5627, 2.5627,\n",
      "        2.5627, 2.5627, 2.5627, 2.5627, 2.5627, 2.5627, 2.5627, 2.5627, 2.5627,\n",
      "        2.5627, 2.5627, 2.5627, 2.5627, 1.7836, 1.1172, 0.5300]) return=  170297.27457943128\n",
      "probs of actions:  tensor([0.0463, 0.0392, 0.3955, 0.3738, 0.1785, 0.4352, 0.0531, 0.1506, 0.3757,\n",
      "        0.0076, 0.4171, 0.4271, 0.0406, 0.0422, 0.0334, 0.0191, 0.0565, 0.0451,\n",
      "        0.2052, 0.0074, 0.0165, 0.5405, 0.0105, 0.0368, 0.9833],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.6156, 0.6036, 0.6560, 0.6882, 0.6922, 0.6900, 0.7199, 0.7080,\n",
      "        0.7292, 0.6922, 0.7012, 0.7426, 0.7099, 0.6899, 0.7006, 0.6449, 0.7001,\n",
      "        0.6575, 0.6966, 0.6589, 0.7062, 0.7028, 0.7025, 0.7938])\n",
      "finalReturns:  tensor([0.3426, 0.4155, 0.3792, 0.2638])\n",
      "----------------------------------------\n",
      "iter  1  stage  20  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 27, 27, 25, 27, 27, 27,  0, 26, 27, 26, 27, 27, 27, 27, 27, 26, 25,\n",
      "        26, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.5544, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.0524, 3.0524, 3.0524, 3.0524, 3.0524, 3.0524, 3.0524, 3.0524, 3.0524,\n",
      "        3.0524, 3.0524, 3.0524, 3.0524, 3.0524, 3.0524, 3.0524, 3.0524, 3.0524,\n",
      "        3.0524, 3.0524, 3.0524, 2.2741, 1.6082, 1.0211, 0.4904]) return=  170770.7691723508\n",
      "probs of actions:  tensor([0.1380, 0.6337, 0.6868, 0.0381, 0.6601, 0.7162, 0.7106, 0.0021, 0.1416,\n",
      "        0.6234, 0.1196, 0.6613, 0.6336, 0.6924, 0.6360, 0.6668, 0.1431, 0.0319,\n",
      "        0.1411, 0.7074, 0.7916, 0.8418, 0.8417, 0.6480, 0.9773],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5631, 0.6348, 0.6825, 0.6824, 0.6963, 0.7033, 0.7797, 0.5991,\n",
      "        0.6466, 0.6834, 0.6898, 0.7000, 0.7052, 0.7077, 0.7090, 0.7150, 0.7160,\n",
      "        0.7044, 0.7003, 0.7053, 0.7078, 0.7091, 0.7097, 0.7829])\n",
      "finalReturns:  tensor([0.5625, 0.6354, 0.5935, 0.4715, 0.2926])\n",
      "----------------------------------------\n",
      "iter  1  stage  19  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([27, 27, 27, 29, 27, 27, 27, 27, 20, 27, 26, 27, 27, 27, 22, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.6967, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.5200, 3.5200, 3.5200, 3.5200, 3.5200, 3.5200, 3.5200, 3.5200, 3.5200,\n",
      "        3.5200, 3.5200, 3.5200, 3.5200, 3.5200, 3.5200, 3.5200, 3.5200, 3.5200,\n",
      "        3.5200, 3.5200, 2.7382, 2.0706, 1.4828, 0.9516, 0.4611]) return=  172225.21784845914\n",
      "probs of actions:  tensor([0.7112, 0.6666, 0.7444, 0.0828, 0.7037, 0.7520, 0.7450, 0.6938, 0.0091,\n",
      "        0.6783, 0.1017, 0.6977, 0.6331, 0.7429, 0.0189, 0.6977, 0.6746, 0.7301,\n",
      "        0.7153, 0.8062, 0.8368, 0.8887, 0.8815, 0.7302, 0.9894],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5671, 0.6369, 0.6620, 0.7004, 0.7054, 0.7078, 0.7091, 0.7426,\n",
      "        0.6794, 0.7001, 0.6981, 0.7042, 0.7073, 0.7333, 0.6876, 0.6989, 0.7046,\n",
      "        0.7075, 0.7089, 0.7096, 0.7100, 0.7101, 0.7102, 0.7832])\n",
      "finalReturns:  tensor([0.8120, 0.8849, 0.8429, 0.7208, 0.5418, 0.3221])\n",
      "----------------------------------------\n",
      "iter  1  stage  18  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([27, 27, 27, 29, 27, 26, 26, 27, 29, 29, 27, 27, 27, 27, 29, 27, 27, 27,\n",
      "        27, 27, 27, 25, 27, 19,  0])\n",
      "loss=  tensor(9.2171, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.9682, 3.9682, 3.9682, 3.9682, 3.9682, 3.9682, 3.9682, 3.9682, 3.9682,\n",
      "        3.9682, 3.9682, 3.9682, 3.9682, 3.9682, 3.9682, 3.9682, 3.9682, 3.9682,\n",
      "        3.9682, 3.1839, 2.5150, 1.9267, 1.3953, 0.9046, 0.4417]) return=  172834.79357956513\n",
      "probs of actions:  tensor([0.7195, 0.6850, 0.7542, 0.0809, 0.7215, 0.1352, 0.1281, 0.7134, 0.0822,\n",
      "        0.0737, 0.7545, 0.7188, 0.6647, 0.7534, 0.0863, 0.7126, 0.6873, 0.7424,\n",
      "        0.7456, 0.8016, 0.8489, 0.0049, 0.8710, 0.0048, 0.9992],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5671, 0.6369, 0.6620, 0.7004, 0.7107, 0.7087, 0.7025, 0.6952,\n",
      "        0.7060, 0.7227, 0.7165, 0.7134, 0.7119, 0.6999, 0.7196, 0.7149, 0.7126,\n",
      "        0.7115, 0.7109, 0.7106, 0.7209, 0.7016, 0.7427, 0.7461])\n",
      "finalReturns:  tensor([1.0760, 1.1489, 1.1069, 0.9846, 0.7951, 0.5842, 0.3044])\n",
      "----------------------------------------\n",
      "iter  1  stage  17  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 25, 27, 27, 27, 29, 27, 27, 27, 28, 26, 27, 27,\n",
      "        26, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(4.8350, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.3889, 4.3889, 4.3889, 4.3889, 4.3889, 4.3889, 4.3889, 4.3889, 4.3889,\n",
      "        4.3889, 4.3889, 4.3889, 4.3889, 4.3889, 4.3889, 4.3889, 4.3889, 4.3889,\n",
      "        3.6066, 2.9388, 2.3509, 1.8188, 1.3271, 0.8650, 0.4245]) return=  172634.51909284995\n",
      "probs of actions:  tensor([0.7646, 0.7255, 0.7889, 0.7591, 0.7688, 0.8019, 0.0085, 0.7527, 0.7150,\n",
      "        0.7432, 0.0773, 0.7542, 0.7013, 0.8001, 0.0441, 0.0799, 0.7366, 0.7465,\n",
      "        0.0758, 0.8491, 0.8635, 0.9027, 0.9119, 0.8123, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5671, 0.6369, 0.6732, 0.6916, 0.7010, 0.7160, 0.6992, 0.7047,\n",
      "        0.7075, 0.6977, 0.7185, 0.7144, 0.7124, 0.7058, 0.7206, 0.7084, 0.7093,\n",
      "        0.7151, 0.7057, 0.7080, 0.7092, 0.7097, 0.7100, 0.7831])\n",
      "finalReturns:  tensor([1.3613, 1.4342, 1.3869, 1.2691, 1.0932, 0.8757, 0.6281, 0.3586])\n",
      "----------------------------------------\n",
      "iter  1  stage  16  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([27, 29, 27, 27, 29, 27, 27, 27, 29, 27, 29, 28, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(2.6017, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.7995, 4.7995, 4.7995, 4.7995, 4.7995, 4.7995, 4.7995, 4.7995, 4.7995,\n",
      "        4.7995, 4.7995, 4.7995, 4.7995, 4.7995, 4.7995, 4.7995, 4.7995, 4.0157,\n",
      "        3.3471, 2.7589, 2.2275, 1.7369, 1.2757, 0.8361, 0.4122]) return=  172966.5624334948\n",
      "probs of actions:  tensor([0.7289, 0.1692, 0.7443, 0.7138, 0.1311, 0.7670, 0.7516, 0.7013, 0.1616,\n",
      "        0.7016, 0.1237, 0.0473, 0.6433, 0.7727, 0.6639, 0.7063, 0.7170, 0.6757,\n",
      "        0.8066, 0.8157, 0.8129, 0.8741, 0.9036, 0.8123, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5559, 0.6454, 0.6775, 0.6826, 0.7109, 0.7106, 0.7105, 0.6992,\n",
      "        0.7192, 0.7036, 0.7159, 0.7203, 0.7153, 0.7128, 0.7116, 0.7109, 0.7106,\n",
      "        0.7105, 0.7104, 0.7104, 0.7103, 0.7103, 0.7103, 0.7832])\n",
      "finalReturns:  tensor([1.6675, 1.7404, 1.6984, 1.5761, 1.3971, 1.1774, 0.9282, 0.6575, 0.3710])\n",
      "----------------------------------------\n",
      "iter  1  stage  15  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([27, 29, 27, 27, 27, 29, 29, 27, 28, 29, 29, 27, 29, 27, 29, 27, 27, 29,\n",
      "        29, 29, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(11.3290, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.1989, 5.1989, 5.1989, 5.1989, 5.1989, 5.1989, 5.1989, 5.1989, 5.1989,\n",
      "        5.1989, 5.1989, 5.1989, 5.1989, 5.1989, 5.1989, 5.1989, 4.4036, 3.7298,\n",
      "        3.1391, 2.6065, 2.1171, 1.6595, 1.2254, 0.8070, 0.3996]) return=  173351.18255934515\n",
      "probs of actions:  tensor([0.5099, 0.3595, 0.5167, 0.4838, 0.5129, 0.3033, 0.3360, 0.4651, 0.0750,\n",
      "        0.3347, 0.3011, 0.4653, 0.3972, 0.5718, 0.3602, 0.4803, 0.4625, 0.3816,\n",
      "        0.2773, 0.3271, 0.6014, 0.6833, 0.7747, 0.6614, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5559, 0.6454, 0.6775, 0.6938, 0.6909, 0.7038, 0.7216, 0.7104,\n",
      "        0.7064, 0.7116, 0.7255, 0.7067, 0.7230, 0.7054, 0.7224, 0.7163, 0.7021,\n",
      "        0.7095, 0.7132, 0.7263, 0.7183, 0.7143, 0.7123, 0.7842])\n",
      "finalReturns:  tensor([2.0200, 2.0929, 2.0505, 1.9390, 1.7621, 1.5383, 1.2696, 0.9855, 0.6895,\n",
      "        0.3846])\n",
      "----------------------------------------\n",
      "iter  1  stage  14  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 28, 28, 29, 29, 29, 29, 30, 27, 29, 26, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 26, 29,  0])\n",
      "loss=  tensor(7.7630, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.5612, 5.5612, 5.5612, 5.5612, 5.5612, 5.5612, 5.5612, 5.5612, 5.5612,\n",
      "        5.5612, 5.5612, 5.5612, 5.5612, 5.5612, 5.5612, 4.7621, 4.0865, 3.4969,\n",
      "        2.9681, 2.4827, 2.0287, 1.5976, 1.1832, 0.7812, 0.3884]) return=  173825.5551842266\n",
      "probs of actions:  tensor([0.7370, 0.0655, 0.0614, 0.7672, 0.7414, 0.7641, 0.7839, 0.0485, 0.0853,\n",
      "        0.7527, 0.0260, 0.7726, 0.7826, 0.7109, 0.7644, 0.8093, 0.8081, 0.7662,\n",
      "        0.7728, 0.8169, 0.8252, 0.7849, 0.0323, 0.6756, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5696, 0.6399, 0.6706, 0.6936, 0.7052, 0.7111, 0.7081, 0.7311,\n",
      "        0.7095, 0.7297, 0.7017, 0.7093, 0.7131, 0.7150, 0.7160, 0.7164, 0.7167,\n",
      "        0.7168, 0.7169, 0.7169, 0.7169, 0.7334, 0.7036, 0.7943])\n",
      "finalReturns:  tensor([2.4016, 2.4857, 2.4454, 2.3186, 2.1307, 1.8993, 1.6364, 1.3506, 1.0481,\n",
      "        0.7166, 0.4059])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  13  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 30, 29, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 27, 29, 29, 29,  0])\n",
      "loss=  tensor(8.1818, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.9484, 5.9484, 5.9484, 5.9484, 5.9484, 5.9484, 5.9484, 5.9484, 5.9484,\n",
      "        5.9484, 5.9484, 5.9484, 5.9484, 5.9484, 5.1476, 4.4712, 3.8812, 3.3522,\n",
      "        2.8668, 2.4127, 1.9816, 1.5672, 1.1652, 0.7708, 0.3828]) return=  174002.82550723848\n",
      "probs of actions:  tensor([0.7929, 0.8025, 0.8118, 0.8192, 0.7962, 0.8142, 0.0197, 0.8157, 0.0652,\n",
      "        0.8068, 0.8006, 0.8221, 0.8335, 0.7760, 0.8249, 0.8528, 0.8706, 0.7983,\n",
      "        0.8249, 0.8632, 0.0896, 0.8470, 0.7300, 0.7179, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6384, 0.6772, 0.6969, 0.7069, 0.7060, 0.7189, 0.7236,\n",
      "        0.7129, 0.7149, 0.7159, 0.7164, 0.7167, 0.7168, 0.7169, 0.7169, 0.7169,\n",
      "        0.7169, 0.7169, 0.7281, 0.7080, 0.7125, 0.7147, 0.7999])\n",
      "finalReturns:  tensor([2.7328, 2.8169, 2.7765, 2.6496, 2.4617, 2.2302, 1.9674, 1.6816, 1.3678,\n",
      "        1.0618, 0.7438, 0.4171])\n",
      "----------------------------------------\n",
      "iter  1  stage  12  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 29, 29, 28, 29, 29, 29, 29, 29, 29, 30, 29, 27, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(12.7920, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.3411, 6.3411, 6.3411, 6.3411, 6.3411, 6.3411, 6.3411, 6.3411, 6.3411,\n",
      "        6.3411, 6.3411, 6.3411, 6.3411, 5.5357, 4.8572, 4.2662, 3.7349, 3.2471,\n",
      "        2.7909, 2.3581, 1.9423, 1.5393, 1.1457, 0.7590, 0.3776]) return=  173992.11630517073\n",
      "probs of actions:  tensor([0.8624, 0.8648, 0.8793, 0.8813, 0.0524, 0.8886, 0.8990, 0.8753, 0.8682,\n",
      "        0.8730, 0.8802, 0.0325, 0.8614, 0.0400, 0.8668, 0.9117, 0.9162, 0.8509,\n",
      "        0.9006, 0.9281, 0.9482, 0.9433, 0.8928, 0.8473, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6384, 0.6772, 0.7026, 0.7024, 0.7097, 0.7133, 0.7151,\n",
      "        0.7160, 0.7165, 0.7108, 0.7213, 0.7303, 0.7091, 0.7130, 0.7150, 0.7159,\n",
      "        0.7164, 0.7167, 0.7168, 0.7169, 0.7169, 0.7169, 0.8010])\n",
      "finalReturns:  tensor([3.0651, 3.1492, 3.0974, 2.9793, 2.7976, 2.5705, 2.3107, 2.0271, 1.7261,\n",
      "        1.4124, 1.0892, 0.7589, 0.4235])\n",
      "----------------------------------------\n",
      "iter  1  stage  11  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 29, 29, 30, 27, 29, 30, 30, 28, 29, 28, 29, 29, 29, 29, 29, 30,\n",
      "        30, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(27.1738, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.6912, 6.6912, 6.6912, 6.6912, 6.6912, 6.6912, 6.6912, 6.6912, 6.6912,\n",
      "        6.6912, 6.6912, 6.6912, 5.8911, 5.2150, 4.6242, 4.0942, 3.6077, 3.1528,\n",
      "        2.7210, 2.3062, 1.9046, 1.5132, 1.1288, 0.7493, 0.3734]) return=  174079.33224464272\n",
      "probs of actions:  tensor([0.8209, 0.8254, 0.8419, 0.8423, 0.0719, 0.0270, 0.8679, 0.0696, 0.0810,\n",
      "        0.0624, 0.8521, 0.0596, 0.7923, 0.8248, 0.8013, 0.8720, 0.8975, 0.0851,\n",
      "        0.0489, 0.8874, 0.9414, 0.9314, 0.8985, 0.8402, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6384, 0.6772, 0.6910, 0.7225, 0.7052, 0.7052, 0.7126,\n",
      "        0.7279, 0.7151, 0.7217, 0.7120, 0.7145, 0.7157, 0.7163, 0.7166, 0.7109,\n",
      "        0.7154, 0.7236, 0.7203, 0.7186, 0.7178, 0.7173, 0.8012])\n",
      "finalReturns:  tensor([3.4307, 3.5091, 3.4732, 3.3495, 3.1639, 2.9340, 2.6723, 2.3932, 2.0926,\n",
      "        1.7706, 1.4418, 1.1075, 0.7693, 0.4279])\n",
      "----------------------------------------\n",
      "iter  1  stage  10  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 26, 29, 29, 29, 29, 29, 29, 28, 29, 29, 28, 29, 30, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(21.4632, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.0587, 7.0587, 7.0587, 7.0587, 7.0587, 7.0587, 7.0587, 7.0587, 7.0587,\n",
      "        7.0587, 7.0587, 6.2626, 5.5883, 4.9994, 4.4709, 3.9847, 3.5298, 3.0985,\n",
      "        2.6844, 2.2827, 1.8901, 1.5043, 1.1236, 0.7465, 0.3722]) return=  173927.67848376333\n",
      "probs of actions:  tensor([0.7494, 0.7613, 0.0041, 0.7772, 0.7584, 0.8050, 0.8112, 0.7614, 0.7421,\n",
      "        0.0720, 0.8168, 0.7777, 0.0912, 0.7630, 0.1784, 0.7979, 0.8219, 0.7332,\n",
      "        0.8220, 0.8517, 0.9236, 0.9042, 0.8870, 0.8024, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6549, 0.6641, 0.6903, 0.7036, 0.7102, 0.7136, 0.7152,\n",
      "        0.7218, 0.7120, 0.7145, 0.7214, 0.7118, 0.7085, 0.7201, 0.7185, 0.7177,\n",
      "        0.7173, 0.7171, 0.7170, 0.7170, 0.7170, 0.7169, 0.8010])\n",
      "finalReturns:  tensor([3.7693, 3.8534, 3.8131, 3.6807, 3.4974, 3.2750, 3.0098, 2.7226, 2.4190,\n",
      "        2.1033, 1.7788, 1.4476, 1.1114, 0.7715, 0.4288])\n",
      "----------------------------------------\n",
      "iter  1  stage  9  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 29, 30, 30, 29, 30, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 28,\n",
      "        29, 29, 29, 29, 29, 28,  0])\n",
      "loss=  tensor(16.1739, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.4444, 7.4444, 7.4444, 7.4444, 7.4444, 7.4444, 7.4444, 7.4444, 7.4444,\n",
      "        7.4444, 6.6425, 5.9655, 5.3753, 4.8462, 4.3607, 3.9066, 3.4755, 3.0611,\n",
      "        2.6591, 2.2663, 1.8794, 1.4975, 1.1193, 0.7442, 0.3712]) return=  174131.5237391631\n",
      "probs of actions:  tensor([0.8230, 0.8277, 0.8443, 0.0909, 0.0966, 0.8673, 0.0717, 0.8321, 0.8166,\n",
      "        0.8434, 0.8727, 0.8568, 0.7877, 0.8095, 0.7937, 0.8621, 0.8774, 0.0635,\n",
      "        0.8763, 0.9007, 0.9587, 0.9444, 0.9358, 0.0460, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6384, 0.6713, 0.6954, 0.7136, 0.7093, 0.7206, 0.7187,\n",
      "        0.7178, 0.7174, 0.7172, 0.7170, 0.7170, 0.7170, 0.7169, 0.7169, 0.7226,\n",
      "        0.7125, 0.7147, 0.7158, 0.7164, 0.7166, 0.7225, 0.7965])\n",
      "finalReturns:  tensor([4.1104, 4.1945, 4.1540, 4.0271, 3.8392, 3.6077, 3.3448, 3.0590, 2.7564,\n",
      "        2.4358, 2.1162, 1.7883, 1.4545, 1.1163, 0.7748, 0.4253])\n",
      "----------------------------------------\n",
      "iter  1  stage  8  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 28, 29, 29, 29, 30, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(30.2489, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.8061, 7.8061, 7.8061, 7.8061, 7.8061, 7.8061, 7.8061, 7.8061, 7.8061,\n",
      "        7.0063, 6.3304, 5.7406, 5.2108, 4.7243, 4.2693, 3.8374, 3.4231, 3.0216,\n",
      "        2.6293, 2.2438, 1.8632, 1.4863, 1.1121, 0.7400, 0.3694]) return=  174058.37645850837\n",
      "probs of actions:  tensor([0.8582, 0.8629, 0.8798, 0.8764, 0.8638, 0.9002, 0.9004, 0.8670, 0.8585,\n",
      "        0.0301, 0.9124, 0.8797, 0.8310, 0.0907, 0.8367, 0.8905, 0.9087, 0.8720,\n",
      "        0.9059, 0.9327, 0.9686, 0.9607, 0.9600, 0.9044, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6384, 0.6772, 0.6969, 0.7069, 0.7119, 0.7144, 0.7157,\n",
      "        0.7220, 0.7121, 0.7145, 0.7157, 0.7104, 0.7211, 0.7190, 0.7180, 0.7174,\n",
      "        0.7172, 0.7171, 0.7170, 0.7170, 0.7169, 0.7169, 0.8010])\n",
      "finalReturns:  tensor([4.4630, 4.5471, 4.5011, 4.3787, 4.1940, 3.9647, 3.7093, 3.4202, 3.1154,\n",
      "        2.7989, 2.4738, 2.1422, 1.8057, 1.4656, 1.1228, 0.7779, 0.4316])\n",
      "----------------------------------------\n",
      "iter  1  stage  7  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(4.4280, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.1717, 8.1717, 8.1717, 8.1717, 8.1717, 8.1717, 8.1717, 8.1717, 7.3731,\n",
      "        6.6978, 6.1083, 5.5795, 5.0942, 4.6402, 4.2091, 3.7948, 3.3928, 2.9999,\n",
      "        2.6139, 2.2329, 1.8557, 1.4813, 1.1089, 0.7382, 0.3687]) return=  174060.25019204617\n",
      "probs of actions:  tensor([0.9034, 0.9041, 0.9183, 0.9161, 0.9079, 0.9342, 0.9338, 0.9166, 0.9100,\n",
      "        0.9035, 0.9549, 0.9257, 0.8960, 0.9082, 0.8750, 0.9167, 0.9458, 0.9095,\n",
      "        0.9417, 0.9605, 0.9816, 0.9780, 0.9770, 0.9405, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6384, 0.6772, 0.6969, 0.7069, 0.7119, 0.7144, 0.7157,\n",
      "        0.7163, 0.7166, 0.7168, 0.7168, 0.7169, 0.7169, 0.7169, 0.7169, 0.7169,\n",
      "        0.7169, 0.7169, 0.7169, 0.7169, 0.7169, 0.7169, 0.8010])\n",
      "finalReturns:  tensor([4.8121, 4.8962, 4.8558, 4.7290, 4.5412, 4.3098, 4.0469, 3.7611, 3.4586,\n",
      "        3.1436, 2.8196, 2.4887, 2.1528, 1.8131, 1.4705, 1.1259, 0.7797, 0.4324])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  6  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        28, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(17.8648, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.5390, 8.5390, 8.5390, 8.5390, 8.5390, 8.5390, 8.5390, 7.7430, 7.0688,\n",
      "        6.4798, 5.9513, 5.4661, 5.0122, 4.5811, 4.1668, 3.7648, 3.3719, 2.9859,\n",
      "        2.6049, 2.2277, 1.8533, 1.4802, 1.1085, 0.7381, 0.3686]) return=  174029.23196541704\n",
      "probs of actions:  tensor([0.8938, 0.8960, 0.9100, 0.9065, 0.9003, 0.9289, 0.9158, 0.9051, 0.9186,\n",
      "        0.8903, 0.9594, 0.9238, 0.8900, 0.9022, 0.8511, 0.9066, 0.9395, 0.9038,\n",
      "        0.0062, 0.9561, 0.9805, 0.9749, 0.9774, 0.9416, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6384, 0.6772, 0.6969, 0.7069, 0.7119, 0.7144, 0.7157,\n",
      "        0.7163, 0.7166, 0.7168, 0.7168, 0.7169, 0.7169, 0.7169, 0.7169, 0.7169,\n",
      "        0.7226, 0.7125, 0.7147, 0.7158, 0.7164, 0.7166, 0.8009])\n",
      "finalReturns:  tensor([5.1535, 5.2376, 5.1974, 5.0707, 4.8829, 4.6515, 4.3887, 4.1029, 3.8004,\n",
      "        3.4854, 3.1614, 2.8305, 2.4946, 2.1492, 1.8111, 1.4695, 1.1254, 0.7795,\n",
      "        0.4323])\n",
      "----------------------------------------\n",
      "iter  1  stage  5  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(2.6399, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.8931, 8.8931, 8.8931, 8.8931, 8.8931, 8.8931, 8.1021, 7.4302, 6.8424,\n",
      "        6.3143, 5.8294, 5.3756, 4.9445, 4.5302, 4.1283, 3.7354, 3.3494, 2.9684,\n",
      "        2.5912, 2.2168, 1.8445, 1.4737, 1.1042, 0.7355, 0.3675]) return=  174060.25019204617\n",
      "probs of actions:  tensor([0.9462, 0.9455, 0.9555, 0.9534, 0.9488, 0.9645, 0.9683, 0.9549, 0.9581,\n",
      "        0.9596, 0.9844, 0.9700, 0.9389, 0.9530, 0.9245, 0.9614, 0.9708, 0.9519,\n",
      "        0.9705, 0.9807, 0.9920, 0.9906, 0.9916, 0.9737, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6384, 0.6772, 0.6969, 0.7069, 0.7119, 0.7144, 0.7157,\n",
      "        0.7163, 0.7166, 0.7168, 0.7168, 0.7169, 0.7169, 0.7169, 0.7169, 0.7169,\n",
      "        0.7169, 0.7169, 0.7169, 0.7169, 0.7169, 0.7169, 0.8010])\n",
      "finalReturns:  tensor([5.5094, 5.5935, 5.5535, 5.4270, 5.2393, 5.0080, 4.7452, 4.4594, 4.1569,\n",
      "        3.8420, 3.5179, 3.1870, 2.8511, 2.5114, 2.1689, 1.8243, 1.4781, 1.1307,\n",
      "        0.7824, 0.4335])\n",
      "----------------------------------------\n",
      "iter  1  stage  4  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 30, 29, 29, 29, 29, 29, 30, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(32.6679, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.2297, 9.2297, 9.2297, 9.2297, 9.2297, 8.4486, 7.7814, 7.1957, 6.6687,\n",
      "        6.1842, 5.7315, 5.3016, 4.8883, 4.4871, 4.0950, 3.7094, 3.3296, 2.9536,\n",
      "        2.5804, 2.2090, 1.8390, 1.4701, 1.1018, 0.7342, 0.3669]) return=  174121.3274714346\n",
      "probs of actions:  tensor([0.9370, 0.9376, 0.9482, 0.9457, 0.9450, 0.9597, 0.9723, 0.0570, 0.9488,\n",
      "        0.9538, 0.9850, 0.9677, 0.9243, 0.0438, 0.9097, 0.9542, 0.9587, 0.9397,\n",
      "        0.9704, 0.9754, 0.9915, 0.9905, 0.9908, 0.9710, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6384, 0.6772, 0.6969, 0.7069, 0.7119, 0.7085, 0.7201,\n",
      "        0.7185, 0.7177, 0.7173, 0.7171, 0.7111, 0.7215, 0.7192, 0.7181, 0.7175,\n",
      "        0.7172, 0.7171, 0.7170, 0.7170, 0.7169, 0.7169, 0.8010])\n",
      "finalReturns:  tensor([5.8759, 5.9600, 5.9204, 5.7942, 5.6126, 5.3770, 5.1112, 4.8233, 4.5193,\n",
      "        4.2033, 3.8844, 3.5484, 3.2091, 2.8670, 2.5228, 2.1769, 1.8298, 1.4818,\n",
      "        1.1331, 0.7838, 0.4341])\n",
      "----------------------------------------\n",
      "iter  1  stage  3  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 30, 29, 29, 29, 29, 30, 29, 29, 29, 29, 30, 29, 29, 29, 30, 29,\n",
      "        30, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(41.6779, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.5587, 9.5587, 9.5587, 9.5587, 8.7930, 8.1329, 7.5505, 7.0251, 6.5414,\n",
      "        6.0882, 5.6583, 5.2451, 4.8442, 4.4522, 4.0668, 3.6871, 3.3112, 2.9380,\n",
      "        2.5667, 2.1975, 1.8295, 1.4630, 1.0971, 0.7314, 0.3657]) return=  174209.913456734\n",
      "probs of actions:  tensor([0.8751, 0.8809, 0.1046, 0.8975, 0.8943, 0.9066, 0.9349, 0.1239, 0.8953,\n",
      "        0.9055, 0.9732, 0.9129, 0.1605, 0.9055, 0.8477, 0.9023, 0.0858, 0.8891,\n",
      "        0.0721, 0.9442, 0.9840, 0.9776, 0.9772, 0.9326, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6325, 0.6815, 0.6991, 0.7080, 0.7125, 0.7088, 0.7203,\n",
      "        0.7186, 0.7178, 0.7173, 0.7112, 0.7215, 0.7192, 0.7181, 0.7116, 0.7217,\n",
      "        0.7134, 0.7226, 0.7198, 0.7183, 0.7176, 0.7173, 0.8012])\n",
      "finalReturns:  tensor([6.2388, 6.3229, 6.2839, 6.1583, 5.9712, 5.7461, 5.4791, 5.1904, 4.8858,\n",
      "        4.5694, 4.2501, 3.9140, 3.5745, 3.2323, 2.8939, 2.5435, 2.1993, 1.8447,\n",
      "        1.4914, 1.1390, 0.7871, 0.4355])\n",
      "----------------------------------------\n",
      "iter  1  stage  2  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 30, 29, 29, 30, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 30,  0])\n",
      "loss=  tensor(40.5944, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.8524, 9.8524, 9.8524, 9.1299, 8.4899, 7.9170, 7.3961, 6.9146, 6.4624,\n",
      "        6.0330, 5.6201, 5.2193, 4.8281, 4.4437, 4.0642, 3.6881, 3.3147, 2.9431,\n",
      "        2.5729, 2.2038, 1.8354, 1.4677, 1.1003, 0.7333, 0.3666]) return=  174107.14165860406\n",
      "probs of actions:  tensor([0.8416, 0.8520, 0.8595, 0.8623, 0.8794, 0.8895, 0.0793, 0.8476, 0.8646,\n",
      "        0.1119, 0.9667, 0.8832, 0.7889, 0.8842, 0.7762, 0.8842, 0.8902, 0.8377,\n",
      "        0.9165, 0.9302, 0.9792, 0.9674, 0.9688, 0.0917, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5639, 0.6384, 0.6772, 0.6969, 0.7069, 0.7060, 0.7189, 0.7179,\n",
      "        0.7115, 0.7217, 0.7193, 0.7181, 0.7175, 0.7172, 0.7171, 0.7170, 0.7170,\n",
      "        0.7169, 0.7169, 0.7169, 0.7169, 0.7169, 0.7110, 0.8055])\n",
      "finalReturns:  tensor([6.5673, 6.6514, 6.6142, 6.4902, 6.3042, 6.0797, 5.8130, 5.5245, 5.2259,\n",
      "        4.9051, 4.5769, 4.2432, 3.9053, 3.5641, 3.2205, 2.8751, 2.5283, 2.1805,\n",
      "        1.8319, 1.4827, 1.1331, 0.7832, 0.4389])\n",
      "----------------------------------------\n",
      "iter  1  stage  1  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([30, 30, 30, 29, 29, 29, 29, 30, 29, 29, 29, 29, 29, 30, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(54.3465, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.0714, 10.0714,  9.4193,  8.8129,  8.2569,  7.7457,  7.2699,  6.8214,\n",
      "         6.3935,  5.9812,  5.5814,  5.1904,  4.8059,  4.4262,  4.0499,  3.6762,\n",
      "         3.3052,  2.9359,  2.5675,  2.1999,  1.8327,  1.4658,  1.0991,  0.7327,\n",
      "         0.3663]) return=  174202.79502442433\n",
      "probs of actions:  tensor([0.2038, 0.1953, 0.1608, 0.8084, 0.8417, 0.8700, 0.8805, 0.2281, 0.8222,\n",
      "        0.8522, 0.9632, 0.8519, 0.6879, 0.1521, 0.7490, 0.8204, 0.8533, 0.7903,\n",
      "        0.8795, 0.8923, 0.9703, 0.9507, 0.9572, 0.8827, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5621, 0.6389, 0.6848, 0.7008, 0.7088, 0.7129, 0.7090, 0.7204,\n",
      "        0.7187, 0.7178, 0.7174, 0.7171, 0.7111, 0.7215, 0.7192, 0.7181, 0.7175,\n",
      "        0.7172, 0.7171, 0.7170, 0.7170, 0.7169, 0.7169, 0.8010])\n",
      "finalReturns:  tensor([6.9277, 7.0177, 6.9853, 6.8564, 6.6668, 6.4338, 6.1694, 5.8883, 5.5802,\n",
      "        5.2614, 4.9345, 4.6017, 4.2643, 3.9294, 3.5817, 3.2335, 2.8847, 2.5356,\n",
      "        2.1860, 1.8362, 1.4861, 1.1358, 0.7853, 0.4347])\n",
      "----------------------------------------\n",
      "iter  1  stage  0  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([30, 29, 29, 30, 30, 29, 29, 29, 29, 30, 29, 29, 30, 29, 29, 29, 29, 29,\n",
      "        29, 30, 29, 30, 29, 29,  0])\n",
      "loss=  tensor(69.6359, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.1501,  9.6388,  9.1023,  8.5803,  8.0846,  7.6157,  7.1709,  6.7459,\n",
      "         6.3358,  5.9369,  5.5462,  5.1617,  4.7827,  4.4073,  4.0344,  3.6641,\n",
      "         3.2953,  2.9274,  2.5601,  2.1932,  1.8265,  1.4600,  1.0944,  0.7290,\n",
      "         0.3645]) return=  174253.3722751547\n",
      "probs of actions:  tensor([0.2702, 0.7677, 0.7977, 0.2116, 0.2319, 0.8234, 0.8262, 0.7327, 0.7965,\n",
      "        0.1930, 0.9485, 0.7959, 0.3707, 0.8006, 0.6535, 0.7538, 0.8028, 0.7528,\n",
      "        0.8439, 0.1603, 0.9571, 0.0712, 0.9422, 0.8500, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5680, 0.6405, 0.6723, 0.6960, 0.7139, 0.7154, 0.7162, 0.7165,\n",
      "        0.7108, 0.7213, 0.7191, 0.7121, 0.7220, 0.7194, 0.7182, 0.7176, 0.7172,\n",
      "        0.7171, 0.7111, 0.7214, 0.7133, 0.7225, 0.7197, 0.8024])\n",
      "finalReturns:  tensor([7.2753, 7.3653, 7.3339, 7.2154, 7.0387, 6.8116, 6.5426, 6.2521, 5.9461,\n",
      "        5.6285, 5.3083, 4.9715, 4.6314, 4.2947, 3.9456, 3.5965, 3.2471, 2.8974,\n",
      "        2.5475, 2.1973, 1.8529, 1.4980, 1.1503, 0.7931, 0.4380])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,[1e-05,1][1, 10000, 1, 1],1682423487 saved\n",
      "[3000000, 'tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])', 174210.39338284553, 57605.93776875555, 65.17557525634766, 1e-05, 1, 0, 'tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 29, 29, 29, 30, 29, 29,\\n        29, 30, 29, 29, 29, 29,  0])', '[0.69 0.75 0.77 0.78 0.75 0.8  0.8  0.7  0.77 0.2  0.06 0.24 0.6  0.77\\n 0.62 0.27 0.78 0.72 0.82 0.18 0.95 0.92 0.94 0.84 1.  ]', '0,[1e-05,1][1, 10000, 1, 1],1682423487', 25, 50, 174199.95044967832, 226157.05867704182, 94851.05074168817, 131012.56797668608, 127973.03513660273, 64641.60648389723, 62848.849838023714, 78979.431849868, 79951.69168142487, 109515.23673882235, 64159.865982403535, 79875.30170982817]\n",
      "policy reset\n",
      "----------------------------------------\n",
      "iter  2  stage  24  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625,\n",
      "        0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625,\n",
      "        0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625]) return=  140290.89393391204\n",
      "probs of actions:  tensor([0.9195, 0.0062, 0.9061, 0.9295, 0.9160, 0.9197, 0.9265, 0.9012, 0.8971,\n",
      "        0.9135, 0.9181, 0.9058, 0.9080, 0.9095, 0.0020, 0.9082, 0.9207, 0.9140,\n",
      "        0.9216, 0.8805, 0.9001, 0.9244, 0.8986, 0.9275, 0.9888],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5350, 0.5644, 0.5634, 0.5630, 0.5627, 0.5626, 0.5626, 0.5625,\n",
      "        0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5589, 0.5852, 0.5738, 0.5681,\n",
      "        0.5653, 0.5639, 0.5632, 0.5629, 0.5627, 0.5626, 0.5625])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  2  stage  23  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([17,  0,  1, 22,  8,  0,  9, 17, 19,  7, 14,  7,  0,  1, 14,  0,  7,  1,\n",
      "         0, 10, 17,  0,  2, 21,  0])\n",
      "loss=  tensor(0.1059, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.1688, 1.1688, 1.1688, 1.1688, 1.1688, 1.1688, 1.1688, 1.1688, 1.1688,\n",
      "        1.1688, 1.1688, 1.1688, 1.1688, 1.1688, 1.1688, 1.1688, 1.1688, 1.1688,\n",
      "        1.1688, 1.1688, 1.1688, 1.1688, 1.1688, 1.1688, 0.5770]) return=  150673.14503249113\n",
      "probs of actions:  tensor([0.0195, 0.3042, 0.0644, 0.0075, 0.0056, 0.3819, 0.0345, 0.0280, 0.0135,\n",
      "        0.0520, 0.0902, 0.0551, 0.2481, 0.0579, 0.0623, 0.3358, 0.0548, 0.0627,\n",
      "        0.5367, 0.0444, 0.0219, 0.4604, 0.0402, 0.0654, 0.9866],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4823, 0.6006, 0.5813, 0.5273, 0.6487, 0.6395, 0.5923, 0.5872, 0.6199,\n",
      "        0.6798, 0.6304, 0.6563, 0.6385, 0.5998, 0.5653, 0.6279, 0.5898, 0.6053,\n",
      "        0.5876, 0.5650, 0.5782, 0.6514, 0.6057, 0.5477, 0.6596])\n",
      "finalReturns:  tensor([0.0384, 0.0825])\n",
      "----------------------------------------\n",
      "iter  2  stage  22  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([23, 19, 13, 24, 23,  1, 23, 25, 24, 24, 23, 17, 24, 24, 13,  7, 21, 16,\n",
      "        23, 23, 24, 23, 24, 24,  0])\n",
      "loss=  tensor(0.3284, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.9812, 1.9812, 1.9812, 1.9812, 1.9812, 1.9812, 1.9812, 1.9812, 1.9812,\n",
      "        1.9812, 1.9812, 1.9812, 1.9812, 1.9812, 1.9812, 1.9812, 1.9812, 1.9812,\n",
      "        1.9812, 1.9812, 1.9812, 1.9812, 1.9812, 1.2344, 0.5830]) return=  165779.32260088876\n",
      "probs of actions:  tensor([0.1627, 0.0385, 0.0941, 0.1479, 0.1835, 0.0145, 0.1909, 0.0031, 0.2096,\n",
      "        0.1863, 0.2033, 0.2533, 0.2315, 0.1846, 0.2161, 0.0282, 0.0494, 0.0190,\n",
      "        0.1273, 0.1832, 0.1901, 0.1711, 0.5221, 0.3500, 0.9898],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4583, 0.5880, 0.6514, 0.6087, 0.6580, 0.7293, 0.5944, 0.6343, 0.6732,\n",
      "        0.6862, 0.6974, 0.7204, 0.6654, 0.6823, 0.7315, 0.7008, 0.6161, 0.6696,\n",
      "        0.6391, 0.6670, 0.6764, 0.6925, 0.6892, 0.6942, 0.7544])\n",
      "finalReturns:  tensor([0.1566, 0.2142, 0.1714])\n",
      "----------------------------------------\n",
      "iter  2  stage  21  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([24, 24, 24, 19, 24, 24, 24, 24, 17, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 23,  0])\n",
      "loss=  tensor(0.6763, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.5329, 2.5329, 2.5329, 2.5329, 2.5329, 2.5329, 2.5329, 2.5329, 2.5329,\n",
      "        2.5329, 2.5329, 2.5329, 2.5329, 2.5329, 2.5329, 2.5329, 2.5329, 2.5329,\n",
      "        2.5329, 2.5329, 2.5329, 2.5329, 1.7760, 1.1199, 0.5347]) return=  169798.62492291527\n",
      "probs of actions:  tensor([0.7248, 0.6904, 0.7508, 0.0141, 0.7384, 0.7358, 0.8043, 0.6820, 0.0194,\n",
      "        0.7494, 0.7018, 0.6975, 0.7651, 0.7216, 0.7007, 0.7041, 0.7644, 0.6917,\n",
      "        0.7048, 0.7520, 0.7286, 0.8208, 0.8729, 0.1889, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4536, 0.5705, 0.6334, 0.6875, 0.6612, 0.6801, 0.6897, 0.6945, 0.7256,\n",
      "        0.6680, 0.6836, 0.6914, 0.6953, 0.6973, 0.6983, 0.6988, 0.6991, 0.6992,\n",
      "        0.6992, 0.6993, 0.6993, 0.6993, 0.6993, 0.7040, 0.7526])\n",
      "finalReturns:  tensor([0.3222, 0.3798, 0.3366, 0.2178])\n",
      "----------------------------------------\n",
      "iter  2  stage  20  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([24, 24, 28, 24, 23, 16, 24, 28, 24, 24, 24, 24, 24, 28, 23, 23, 24, 23,\n",
      "        24, 23, 24, 24, 24, 24,  0])\n",
      "loss=  tensor(0.6225, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.0211, 3.0211, 3.0211, 3.0211, 3.0211, 3.0211, 3.0211, 3.0211, 3.0211,\n",
      "        3.0211, 3.0211, 3.0211, 3.0211, 3.0211, 3.0211, 3.0211, 3.0211, 3.0211,\n",
      "        3.0211, 3.0211, 3.0211, 2.2697, 1.6162, 1.0322, 0.4981]) return=  170179.017451197\n",
      "probs of actions:  tensor([0.6627, 0.6456, 0.1464, 0.6529, 0.1292, 0.0016, 0.7567, 0.1929, 0.6563,\n",
      "        0.6988, 0.6466, 0.6345, 0.7108, 0.1598, 0.1356, 0.1445, 0.7143, 0.1483,\n",
      "        0.6384, 0.1183, 0.6669, 0.7142, 0.8405, 0.7487, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4536, 0.5705, 0.6126, 0.6831, 0.6959, 0.7229, 0.6608, 0.6591, 0.7070,\n",
      "        0.7031, 0.7012, 0.7003, 0.6998, 0.6787, 0.7216, 0.7084, 0.6972, 0.7029,\n",
      "        0.6944, 0.7016, 0.6937, 0.6965, 0.6979, 0.6986, 0.7566])\n",
      "finalReturns:  tensor([0.5223, 0.5799, 0.5369, 0.4229, 0.2585])\n",
      "----------------------------------------\n",
      "iter  2  stage  19  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 24,\n",
      "        28, 28, 28, 28, 28, 24,  0])\n",
      "loss=  tensor(1.3065, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.5129, 3.5129, 3.5129, 3.5129, 3.5129, 3.5129, 3.5129, 3.5129, 3.5129,\n",
      "        3.5129, 3.5129, 3.5129, 3.5129, 3.5129, 3.5129, 3.5129, 3.5129, 3.5129,\n",
      "        3.5129, 3.5129, 2.7297, 2.0614, 1.4742, 0.9450, 0.4574]) return=  173274.47945104944\n",
      "probs of actions:  tensor([0.8751, 0.8362, 0.8357, 0.8808, 0.8721, 0.8655, 0.8324, 0.8766, 0.8592,\n",
      "        0.8549, 0.8751, 0.8901, 0.8497, 0.8566, 0.8846, 0.8587, 0.8459, 0.0844,\n",
      "        0.8948, 0.8662, 0.9174, 0.9180, 0.8195, 0.2077, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4328, 0.5656, 0.6377, 0.6752, 0.6944, 0.7040, 0.7088, 0.7113, 0.7125,\n",
      "        0.7131, 0.7134, 0.7135, 0.7136, 0.7137, 0.7137, 0.7137, 0.7137, 0.7345,\n",
      "        0.6960, 0.7048, 0.7093, 0.7115, 0.7126, 0.7339, 0.7741])\n",
      "finalReturns:  tensor([0.8333, 0.9117, 0.8708, 0.7464, 0.5631, 0.3168])\n",
      "----------------------------------------\n",
      "iter  2  stage  18  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 28,  0])\n",
      "loss=  tensor(0.1852, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.9647, 3.9647, 3.9647, 3.9647, 3.9647, 3.9647, 3.9647, 3.9647, 3.9647,\n",
      "        3.9647, 3.9647, 3.9647, 3.9647, 3.9647, 3.9647, 3.9647, 3.9647, 3.9647,\n",
      "        3.9647, 3.1726, 2.5002, 1.9112, 1.3810, 0.8930, 0.4354]) return=  173387.3335190018\n",
      "probs of actions:  tensor([0.9707, 0.9563, 0.9583, 0.9732, 0.9703, 0.9683, 0.9593, 0.9722, 0.9662,\n",
      "        0.9654, 0.9716, 0.9762, 0.9634, 0.9665, 0.9740, 0.9665, 0.9630, 0.9683,\n",
      "        0.9812, 0.9737, 0.9835, 0.9850, 0.9637, 0.8965, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4328, 0.5656, 0.6377, 0.6752, 0.6944, 0.7040, 0.7088, 0.7113, 0.7125,\n",
      "        0.7131, 0.7134, 0.7135, 0.7136, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137,\n",
      "        0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7921])\n",
      "finalReturns:  tensor([1.1096, 1.1880, 1.1467, 1.0220, 0.8385, 0.6128, 0.3567])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  2  stage  17  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 24,  0])\n",
      "loss=  tensor(2.0490, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.3838, 4.3838, 4.3838, 4.3838, 4.3838, 4.3838, 4.3838, 4.3838, 4.3838,\n",
      "        4.3838, 4.3838, 4.3838, 4.3838, 4.3838, 4.3838, 4.3838, 4.3838, 4.3838,\n",
      "        3.5917, 2.9193, 2.3303, 1.8001, 1.3121, 0.8545, 0.4191]) return=  173418.33352108797\n",
      "probs of actions:  tensor([0.9850, 0.9761, 0.9782, 0.9864, 0.9851, 0.9837, 0.9794, 0.9861, 0.9826,\n",
      "        0.9821, 0.9857, 0.9883, 0.9811, 0.9831, 0.9868, 0.9828, 0.9808, 0.9828,\n",
      "        0.9931, 0.9881, 0.9930, 0.9938, 0.9869, 0.0494, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4328, 0.5656, 0.6377, 0.6752, 0.6944, 0.7040, 0.7088, 0.7113, 0.7125,\n",
      "        0.7131, 0.7134, 0.7135, 0.7136, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137,\n",
      "        0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7345, 0.7744])\n",
      "finalReturns:  tensor([1.4073, 1.4857, 1.4444, 1.3197, 1.1362, 0.9105, 0.6544, 0.3553])\n",
      "----------------------------------------\n",
      "iter  2  stage  16  ep  99999   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 28,  0])\n",
      "loss=  tensor(0.0740, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.7909, 4.7909, 4.7909, 4.7909, 4.7909, 4.7909, 4.7909, 4.7909, 4.7909,\n",
      "        4.7909, 4.7909, 4.7909, 4.7909, 4.7909, 4.7909, 4.7909, 4.7909, 3.9988,\n",
      "        3.3264, 2.7373, 2.2072, 1.7191, 1.2616, 0.8262, 0.4071]) return=  173387.3335190018\n",
      "probs of actions:  tensor([0.9933, 0.9884, 0.9898, 0.9939, 0.9933, 0.9926, 0.9907, 0.9940, 0.9921,\n",
      "        0.9919, 0.9937, 0.9949, 0.9914, 0.9925, 0.9942, 0.9922, 0.9930, 0.9938,\n",
      "        0.9980, 0.9958, 0.9973, 0.9978, 0.9959, 0.9567, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4328, 0.5656, 0.6377, 0.6752, 0.6944, 0.7040, 0.7088, 0.7113, 0.7125,\n",
      "        0.7131, 0.7134, 0.7135, 0.7136, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137,\n",
      "        0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7921])\n",
      "finalReturns:  tensor([1.7108, 1.7892, 1.7479, 1.6233, 1.4397, 1.2141, 0.9579, 0.6796, 0.3850])\n",
      "----------------------------------------\n",
      "iter  2  stage  15  ep  52341   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 28,  0])\n",
      "loss=  tensor(0.0145, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.1891, 5.1891, 5.1891, 5.1891, 5.1891, 5.1891, 5.1891, 5.1891, 5.1891,\n",
      "        5.1891, 5.1891, 5.1891, 5.1891, 5.1891, 5.1891, 5.1891, 4.3970, 3.7246,\n",
      "        3.1355, 2.6054, 2.1173, 1.6598, 1.2244, 0.8053, 0.3982]) return=  173387.3335190018\n",
      "probs of actions:  tensor([0.9987, 0.9975, 0.9980, 0.9989, 0.9988, 0.9986, 0.9983, 0.9989, 0.9985,\n",
      "        0.9985, 0.9988, 0.9991, 0.9984, 0.9986, 0.9990, 0.9990, 0.9989, 0.9991,\n",
      "        0.9998, 0.9995, 0.9998, 0.9999, 0.9996, 0.9912, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4328, 0.5656, 0.6377, 0.6752, 0.6944, 0.7040, 0.7088, 0.7113, 0.7125,\n",
      "        0.7131, 0.7134, 0.7135, 0.7136, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137,\n",
      "        0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7921])\n",
      "finalReturns:  tensor([2.0263, 2.1047, 2.0634, 1.9388, 1.7552, 1.5296, 1.2734, 0.9951, 0.7005,\n",
      "        0.3939])\n",
      "----------------------------------------\n",
      "iter  2  stage  14  ep  17   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 28,  0])\n",
      "loss=  tensor(0.0178, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.5806, 5.5806, 5.5806, 5.5806, 5.5806, 5.5806, 5.5806, 5.5806, 5.5806,\n",
      "        5.5806, 5.5806, 5.5806, 5.5806, 5.5806, 5.5806, 4.7886, 4.1162, 3.5271,\n",
      "        2.9969, 2.5089, 2.0513, 1.6160, 1.1969, 0.7898, 0.3916]) return=  173387.3335190018\n",
      "probs of actions:  tensor([0.9987, 0.9975, 0.9980, 0.9989, 0.9988, 0.9986, 0.9983, 0.9989, 0.9985,\n",
      "        0.9985, 0.9989, 0.9991, 0.9984, 0.9986, 0.9990, 0.9990, 0.9989, 0.9991,\n",
      "        0.9998, 0.9995, 0.9998, 0.9999, 0.9996, 0.9913, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4328, 0.5656, 0.6377, 0.6752, 0.6944, 0.7040, 0.7088, 0.7113, 0.7125,\n",
      "        0.7131, 0.7134, 0.7135, 0.7136, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137,\n",
      "        0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7921])\n",
      "finalReturns:  tensor([2.3484, 2.4268, 2.3855, 2.2609, 2.0774, 1.8517, 1.5956, 1.3172, 1.0226,\n",
      "        0.7160, 0.4005])\n",
      "----------------------------------------\n",
      "iter  2  stage  13  ep  4970   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 28,  0])\n",
      "loss=  tensor(0.0175, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.9673, 5.9673, 5.9673, 5.9673, 5.9673, 5.9673, 5.9673, 5.9673, 5.9673,\n",
      "        5.9673, 5.9673, 5.9673, 5.9673, 5.9673, 5.1752, 4.5028, 3.9138, 3.3836,\n",
      "        2.8956, 2.4380, 2.0026, 1.5835, 1.1764, 0.7783, 0.3867]) return=  173387.3335190018\n",
      "probs of actions:  tensor([0.9990, 0.9979, 0.9984, 0.9991, 0.9990, 0.9989, 0.9986, 0.9991, 0.9988,\n",
      "        0.9988, 0.9991, 0.9993, 0.9987, 0.9990, 0.9992, 0.9992, 0.9992, 0.9993,\n",
      "        0.9999, 0.9996, 0.9999, 0.9999, 0.9997, 0.9930, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4328, 0.5656, 0.6377, 0.6752, 0.6944, 0.7040, 0.7088, 0.7113, 0.7125,\n",
      "        0.7131, 0.7134, 0.7135, 0.7136, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137,\n",
      "        0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7921])\n",
      "finalReturns:  tensor([2.6755, 2.7539, 2.7126, 2.5879, 2.4044, 2.1787, 1.9226, 1.6443, 1.3497,\n",
      "        1.0431, 0.7275, 0.4054])\n",
      "----------------------------------------\n",
      "iter  2  stage  12  ep  205   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 28,  0])\n",
      "loss=  tensor(0.0208, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.3502, 6.3502, 6.3502, 6.3502, 6.3502, 6.3502, 6.3502, 6.3502, 6.3502,\n",
      "        6.3502, 6.3502, 6.3502, 6.3502, 5.5582, 4.8858, 4.2968, 3.7666, 3.2786,\n",
      "        2.8210, 2.3856, 1.9665, 1.5594, 1.1613, 0.7697, 0.3830]) return=  173387.3335190018\n",
      "probs of actions:  tensor([0.9990, 0.9980, 0.9985, 0.9991, 0.9990, 0.9989, 0.9987, 0.9992, 0.9988,\n",
      "        0.9988, 0.9991, 0.9993, 0.9990, 0.9990, 0.9993, 0.9992, 0.9992, 0.9993,\n",
      "        0.9999, 0.9996, 0.9999, 0.9999, 0.9997, 0.9933, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4328, 0.5656, 0.6377, 0.6752, 0.6944, 0.7040, 0.7088, 0.7113, 0.7125,\n",
      "        0.7131, 0.7134, 0.7135, 0.7136, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137,\n",
      "        0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7921])\n",
      "finalReturns:  tensor([3.0062, 3.0846, 3.0433, 2.9186, 2.7351, 2.5094, 2.2533, 1.9750, 1.6804,\n",
      "        1.3738, 1.0582, 0.7361, 0.4091])\n",
      "----------------------------------------\n",
      "iter  2  stage  11  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 28,  0])\n",
      "loss=  tensor(0.0242, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.7303, 6.7303, 6.7303, 6.7303, 6.7303, 6.7303, 6.7303, 6.7303, 6.7303,\n",
      "        6.7303, 6.7303, 6.7303, 5.9384, 5.2660, 4.6770, 4.1469, 3.6588, 3.2013,\n",
      "        2.7659, 2.3468, 1.9397, 1.5415, 1.1499, 0.7633, 0.3803]) return=  173387.3335190018\n",
      "probs of actions:  tensor([0.9990, 0.9980, 0.9985, 0.9991, 0.9990, 0.9989, 0.9987, 0.9992, 0.9988,\n",
      "        0.9988, 0.9991, 0.9993, 0.9990, 0.9990, 0.9993, 0.9992, 0.9992, 0.9993,\n",
      "        0.9999, 0.9996, 0.9999, 0.9999, 0.9997, 0.9933, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4328, 0.5656, 0.6377, 0.6752, 0.6944, 0.7040, 0.7088, 0.7113, 0.7125,\n",
      "        0.7131, 0.7134, 0.7135, 0.7136, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137,\n",
      "        0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7921])\n",
      "finalReturns:  tensor([3.3396, 3.4180, 3.3767, 3.2521, 3.0685, 2.8429, 2.5867, 2.3084, 2.0138,\n",
      "        1.7072, 1.3917, 1.0696, 0.7425, 0.4118])\n",
      "----------------------------------------\n",
      "iter  2  stage  10  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 28,  0])\n",
      "loss=  tensor(0.0286, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.1082, 7.1082, 7.1082, 7.1082, 7.1082, 7.1082, 7.1082, 7.1082, 7.1082,\n",
      "        7.1082, 7.1082, 6.3164, 5.6442, 5.0552, 4.5251, 4.0370, 3.5795, 3.1441,\n",
      "        2.7250, 2.3179, 1.9197, 1.5281, 1.1415, 0.7585, 0.3782]) return=  173387.3335190018\n",
      "probs of actions:  tensor([0.9990, 0.9980, 0.9985, 0.9991, 0.9990, 0.9989, 0.9987, 0.9992, 0.9988,\n",
      "        0.9988, 0.9991, 0.9993, 0.9990, 0.9990, 0.9993, 0.9992, 0.9992, 0.9993,\n",
      "        0.9999, 0.9996, 0.9999, 0.9999, 0.9997, 0.9933, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4328, 0.5656, 0.6377, 0.6752, 0.6944, 0.7040, 0.7088, 0.7113, 0.7125,\n",
      "        0.7131, 0.7134, 0.7135, 0.7136, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137,\n",
      "        0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7921])\n",
      "finalReturns:  tensor([3.6751, 3.7535, 3.7122, 3.5875, 3.4040, 3.1783, 2.9222, 2.6439, 2.3493,\n",
      "        2.0427, 1.7272, 1.4051, 1.0780, 0.7473, 0.4139])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  2  stage  9  ep  72   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 28,  0])\n",
      "loss=  tensor(0.0319, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.4843, 7.4843, 7.4843, 7.4843, 7.4843, 7.4843, 7.4843, 7.4843, 7.4843,\n",
      "        7.4843, 6.6929, 6.0207, 5.4318, 4.9017, 4.4137, 3.9561, 3.5208, 3.1017,\n",
      "        2.6946, 2.2964, 1.9048, 1.5181, 1.1352, 0.7549, 0.3767]) return=  173387.3335190018\n",
      "probs of actions:  tensor([0.9990, 0.9981, 0.9985, 0.9992, 0.9991, 0.9989, 0.9988, 0.9992, 0.9989,\n",
      "        0.9990, 0.9992, 0.9994, 0.9991, 0.9991, 0.9993, 0.9993, 0.9992, 0.9994,\n",
      "        0.9999, 0.9996, 0.9999, 1.0000, 0.9997, 0.9935, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4328, 0.5656, 0.6377, 0.6752, 0.6944, 0.7040, 0.7088, 0.7113, 0.7125,\n",
      "        0.7131, 0.7134, 0.7135, 0.7136, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137,\n",
      "        0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7921])\n",
      "finalReturns:  tensor([4.0120, 4.0904, 4.0492, 3.9245, 3.7410, 3.5154, 3.2592, 2.9809, 2.6863,\n",
      "        2.3797, 2.0642, 1.7421, 1.4151, 1.0843, 0.7509, 0.4154])\n",
      "----------------------------------------\n",
      "iter  2  stage  8  ep  58   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 28,  0])\n",
      "loss=  tensor(0.0356, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.8587, 7.8587, 7.8587, 7.8587, 7.8587, 7.8587, 7.8587, 7.8587, 7.8587,\n",
      "        7.0679, 6.3960, 5.8072, 5.2772, 4.7892, 4.3317, 3.8963, 3.4772, 3.0701,\n",
      "        2.6719, 2.2803, 1.8937, 1.5107, 1.1304, 0.7522, 0.3755]) return=  173387.3335190018\n",
      "probs of actions:  tensor([0.9991, 0.9981, 0.9986, 0.9992, 0.9991, 0.9990, 0.9988, 0.9992, 0.9990,\n",
      "        0.9991, 0.9993, 0.9995, 0.9991, 0.9991, 0.9993, 0.9993, 0.9993, 0.9994,\n",
      "        0.9999, 0.9997, 0.9999, 1.0000, 0.9997, 0.9937, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4328, 0.5656, 0.6377, 0.6752, 0.6944, 0.7040, 0.7088, 0.7113, 0.7125,\n",
      "        0.7131, 0.7134, 0.7135, 0.7136, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137,\n",
      "        0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7921])\n",
      "finalReturns:  tensor([4.3501, 4.4285, 4.3873, 4.2627, 4.0792, 3.8535, 3.5974, 3.3191, 3.0245,\n",
      "        2.7179, 2.4024, 2.0803, 1.7532, 1.4225, 1.0891, 0.7536, 0.4166])\n",
      "----------------------------------------\n",
      "iter  2  stage  7  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 28,  0])\n",
      "loss=  tensor(0.0404, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.2311, 8.2311, 8.2311, 8.2311, 8.2311, 8.2311, 8.2311, 8.2311, 7.4415,\n",
      "        6.7702, 6.1817, 5.6517, 5.1638, 4.7063, 4.2709, 3.8519, 3.4448, 3.0466,\n",
      "        2.6550, 2.2684, 1.8854, 1.5051, 1.1269, 0.7502, 0.3747]) return=  173387.3335190018\n",
      "probs of actions:  tensor([0.9991, 0.9981, 0.9986, 0.9992, 0.9991, 0.9990, 0.9988, 0.9992, 0.9990,\n",
      "        0.9991, 0.9993, 0.9995, 0.9991, 0.9991, 0.9993, 0.9993, 0.9993, 0.9994,\n",
      "        0.9999, 0.9997, 0.9999, 1.0000, 0.9998, 0.9937, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4328, 0.5656, 0.6377, 0.6752, 0.6944, 0.7040, 0.7088, 0.7113, 0.7125,\n",
      "        0.7131, 0.7134, 0.7135, 0.7136, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137,\n",
      "        0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7921])\n",
      "finalReturns:  tensor([4.6890, 4.7674, 4.7262, 4.6016, 4.4182, 4.1925, 3.9364, 3.6581, 3.3635,\n",
      "        3.0569, 2.7414, 2.4193, 2.0922, 1.7615, 1.4281, 1.0926, 0.7556, 0.4174])\n",
      "----------------------------------------\n",
      "iter  2  stage  6  ep  80   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 28,  0])\n",
      "loss=  tensor(0.0427, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.6006, 8.6006, 8.6006, 8.6006, 8.6006, 8.6006, 8.6006, 7.8134, 7.1432,\n",
      "        6.5552, 6.0255, 5.5377, 5.0803, 4.6449, 4.2259, 3.8188, 3.4206, 3.0290,\n",
      "        2.6424, 2.2594, 1.8791, 1.5009, 1.1242, 0.7487, 0.3740]) return=  173387.3335190018\n",
      "probs of actions:  tensor([0.9991, 0.9982, 0.9986, 0.9992, 0.9992, 0.9990, 0.9990, 0.9994, 0.9991,\n",
      "        0.9992, 0.9994, 0.9995, 0.9992, 0.9992, 0.9993, 0.9993, 0.9993, 0.9994,\n",
      "        0.9999, 0.9997, 0.9999, 1.0000, 0.9998, 0.9941, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4328, 0.5656, 0.6377, 0.6752, 0.6944, 0.7040, 0.7088, 0.7113, 0.7125,\n",
      "        0.7131, 0.7134, 0.7135, 0.7136, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137,\n",
      "        0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7921])\n",
      "finalReturns:  tensor([5.0284, 5.1068, 5.0657, 4.9412, 4.7578, 4.5322, 4.2761, 3.9978, 3.7032,\n",
      "        3.3966, 3.0811, 2.7589, 2.4319, 2.1012, 1.7678, 1.4323, 1.0953, 0.7571,\n",
      "        0.4181])\n",
      "----------------------------------------\n",
      "iter  2  stage  5  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 28,  0])\n",
      "loss=  tensor(0.0490, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.9651, 8.9651, 8.9651, 8.9651, 8.9651, 8.9651, 8.1827, 7.5147, 6.9278,\n",
      "        6.3986, 5.9111, 5.4537, 5.0184, 4.5994, 4.1923, 3.7942, 3.4026, 3.0159,\n",
      "        2.6329, 2.2527, 1.8745, 1.4978, 1.1222, 0.7476, 0.3735]) return=  173387.3335190018\n",
      "probs of actions:  tensor([0.9991, 0.9982, 0.9986, 0.9992, 0.9992, 0.9990, 0.9990, 0.9994, 0.9991,\n",
      "        0.9992, 0.9994, 0.9995, 0.9992, 0.9992, 0.9993, 0.9993, 0.9993, 0.9994,\n",
      "        0.9999, 0.9997, 0.9999, 1.0000, 0.9998, 0.9941, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4328, 0.5656, 0.6377, 0.6752, 0.6944, 0.7040, 0.7088, 0.7113, 0.7125,\n",
      "        0.7131, 0.7134, 0.7135, 0.7136, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137,\n",
      "        0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7921])\n",
      "finalReturns:  tensor([5.3679, 5.4463, 5.4054, 5.2811, 5.0978, 4.8722, 4.6162, 4.3379, 4.0433,\n",
      "        3.7367, 3.4212, 3.0991, 2.7721, 2.4414, 2.1079, 1.7724, 1.4354, 1.0973,\n",
      "        0.7582, 0.4186])\n",
      "----------------------------------------\n",
      "iter  2  stage  4  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 28,  0])\n",
      "loss=  tensor(0.0550, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.3201, 9.3201, 9.3201, 9.3201, 9.3201, 8.5474, 7.8839, 7.2990, 6.7709,\n",
      "        6.2838, 5.8267, 5.3915, 4.9725, 4.5655, 4.1673, 3.7758, 3.3891, 3.0061,\n",
      "        2.6259, 2.2476, 1.8710, 1.4954, 1.1208, 0.7467, 0.3732]) return=  173387.3335190018\n",
      "probs of actions:  tensor([0.9991, 0.9982, 0.9986, 0.9992, 0.9992, 0.9990, 0.9990, 0.9994, 0.9991,\n",
      "        0.9992, 0.9994, 0.9995, 0.9992, 0.9992, 0.9993, 0.9993, 0.9993, 0.9994,\n",
      "        0.9999, 0.9997, 0.9999, 1.0000, 0.9998, 0.9941, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4328, 0.5656, 0.6377, 0.6752, 0.6944, 0.7040, 0.7088, 0.7113, 0.7125,\n",
      "        0.7131, 0.7134, 0.7135, 0.7136, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137,\n",
      "        0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7921])\n",
      "finalReturns:  tensor([5.7072, 5.7856, 5.7451, 5.6211, 5.4380, 5.2126, 4.9566, 4.6784, 4.3838,\n",
      "        4.0772, 3.7617, 3.4396, 3.1126, 2.7819, 2.4484, 2.1130, 1.7759, 1.4378,\n",
      "        1.0987, 0.7591, 0.4189])\n",
      "----------------------------------------\n",
      "iter  2  stage  3  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 28,  0])\n",
      "loss=  tensor(0.0607, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.6570, 9.6570, 9.6570, 9.6570, 8.9034, 8.2488, 7.6681, 7.1419, 6.6558,\n",
      "        6.1991, 5.7642, 5.3453, 4.9384, 4.5402, 4.1487, 3.7620, 3.3790, 2.9988,\n",
      "        2.6206, 2.2439, 1.8683, 1.4937, 1.1196, 0.7461, 0.3729]) return=  173387.3335190018\n",
      "probs of actions:  tensor([0.9991, 0.9982, 0.9986, 0.9992, 0.9992, 0.9990, 0.9990, 0.9994, 0.9991,\n",
      "        0.9992, 0.9994, 0.9995, 0.9992, 0.9992, 0.9993, 0.9993, 0.9993, 0.9994,\n",
      "        0.9999, 0.9997, 0.9999, 1.0000, 0.9998, 0.9941, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4328, 0.5656, 0.6377, 0.6752, 0.6944, 0.7040, 0.7088, 0.7113, 0.7125,\n",
      "        0.7131, 0.7134, 0.7135, 0.7136, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137,\n",
      "        0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7921])\n",
      "finalReturns:  tensor([6.0455, 6.1239, 6.0842, 5.9609, 5.7782, 5.5531, 5.2972, 5.0191, 4.7246,\n",
      "        4.4180, 4.1025, 3.7804, 3.4534, 3.1227, 2.7892, 2.4537, 2.1167, 1.7786,\n",
      "        1.4395, 1.0999, 0.7597, 0.4192])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  2  stage  2  ep  339   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 28,  0])\n",
      "loss=  tensor(0.0615, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.9587, 9.9587, 9.9587, 9.2425, 8.6055, 8.0331, 7.5109, 7.0267, 6.5710,\n",
      "        6.1365, 5.7178, 5.3110, 4.9129, 4.5213, 4.1347, 3.7517, 3.3715, 2.9933,\n",
      "        2.6166, 2.2411, 1.8664, 1.4924, 1.1188, 0.7456, 0.3727]) return=  173387.3335190018\n",
      "probs of actions:  tensor([0.9991, 0.9983, 0.9990, 0.9994, 0.9992, 0.9990, 0.9992, 0.9995, 0.9993,\n",
      "        0.9993, 0.9994, 0.9996, 0.9993, 0.9992, 0.9993, 0.9993, 0.9993, 0.9994,\n",
      "        0.9999, 0.9997, 0.9999, 1.0000, 0.9998, 0.9941, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4328, 0.5656, 0.6377, 0.6752, 0.6944, 0.7040, 0.7088, 0.7113, 0.7125,\n",
      "        0.7131, 0.7134, 0.7135, 0.7136, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137,\n",
      "        0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7921])\n",
      "finalReturns:  tensor([6.3816, 6.4600, 6.4218, 6.2998, 6.1181, 5.8934, 5.6379, 5.3599, 5.0655,\n",
      "        4.7589, 4.4435, 4.1214, 3.7944, 3.4637, 3.1302, 2.7947, 2.4577, 2.1195,\n",
      "        1.7805, 1.4408, 1.1007, 0.7602, 0.4194])\n",
      "----------------------------------------\n",
      "iter  2  stage  1  ep  14789   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 28,  0])\n",
      "loss=  tensor(0.0368, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.1929, 10.1929,  9.5489,  8.9464,  8.3904,  7.8761,  7.3956,  6.9417,\n",
      "         6.5082,  6.0900,  5.6833,  5.2853,  4.8939,  4.5072,  4.1243,  3.7440,\n",
      "         3.3658,  2.9891,  2.6136,  2.2389,  1.8649,  1.4914,  1.1182,  0.7453,\n",
      "         0.3726]) return=  173387.3335190018\n",
      "probs of actions:  tensor([0.9994, 0.9990, 0.9995, 0.9998, 0.9995, 0.9994, 0.9997, 0.9999, 0.9997,\n",
      "        1.0000, 0.9997, 1.0000, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9998,\n",
      "        1.0000, 0.9999, 1.0000, 1.0000, 0.9999, 0.9959, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4328, 0.5656, 0.6377, 0.6752, 0.6944, 0.7040, 0.7088, 0.7113, 0.7125,\n",
      "        0.7131, 0.7134, 0.7135, 0.7136, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137,\n",
      "        0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7921])\n",
      "finalReturns:  tensor([6.7130, 6.7914, 6.7562, 6.6369, 6.4569, 6.2333, 5.9784, 5.7007, 5.4064,\n",
      "        5.1000, 4.7846, 4.4625, 4.1355, 3.8048, 3.4714, 3.1359, 2.7989, 2.4607,\n",
      "        2.1217, 1.7820, 1.4418, 1.1013, 0.7605, 0.4195])\n",
      "----------------------------------------\n",
      "iter  2  stage  0  ep  0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 28,  0])\n",
      "loss=  tensor(0.0418, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.3041,  9.7929,  9.2563,  8.7325,  8.2337,  7.7608,  7.3106,  6.8789,\n",
      "         6.4615,  6.0553,  5.6576,  5.2662,  4.8796,  4.4967,  4.1164,  3.7383,\n",
      "         3.3616,  2.9860,  2.6114,  2.2374,  1.8638,  1.4906,  1.1177,  0.7450,\n",
      "         0.3724]) return=  173387.3335190018\n",
      "probs of actions:  tensor([0.9994, 0.9990, 0.9995, 0.9998, 0.9995, 0.9994, 0.9997, 0.9999, 0.9997,\n",
      "        1.0000, 0.9997, 1.0000, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9998,\n",
      "        1.0000, 0.9999, 1.0000, 1.0000, 0.9999, 0.9959, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4328, 0.5656, 0.6377, 0.6752, 0.6944, 0.7040, 0.7088, 0.7113, 0.7125,\n",
      "        0.7131, 0.7134, 0.7135, 0.7136, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137,\n",
      "        0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7137, 0.7921])\n",
      "finalReturns:  tensor([7.0346, 7.1130, 7.0840, 6.9701, 6.7936, 6.5721, 6.3183, 6.0413, 5.7473,\n",
      "        5.4411, 5.1257, 4.8037, 4.4767, 4.1460, 3.8126, 3.4771, 3.1401, 2.8019,\n",
      "        2.4629, 2.1232, 1.7831, 1.4426, 1.1018, 0.7608, 0.4197])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1682452608 saved\n",
      "[1152887, 'tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])', 173387.3335190018, 58923.33323528369, 0.041793305426836014, 1e-05, 1, 0, 'tensor([28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\\n        28, 28, 28, 28, 28, 28,  0])', '[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\\n 1.]', '0,[1e-05,1][1, 10000, 1, 1],1682452608', 25, 50, 173387.33351900178, 222740.53173358264, 93054.95743178323, 131763.112, 128763.66666666667, 68687.38784334851, 68702.99934306633, 84336.85885052304, 84336.85885052304, 107493.35770590571, 68702.99934306633, 84336.85885052304]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x648 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy reset\n",
      "----------------------------------------\n",
      "iter  0  stage  24  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5662, 0.5662, 0.5662, 0.5662, 0.5662, 0.5662, 0.5662, 0.5662, 0.5662,\n",
      "        0.5662, 0.5662, 0.5662, 0.5662, 0.5662, 0.5662, 0.5662, 0.5662, 0.5662,\n",
      "        0.5662, 0.5662, 0.5662, 0.5662, 0.5662, 0.5662, 0.5662]) return=  138591.14106947038\n",
      "probs of actions:  tensor([0.9412, 0.9369, 0.9014, 0.9244, 0.9103, 0.9282, 0.9200, 0.9465, 0.9221,\n",
      "        0.9313, 0.9320, 0.9221, 0.9166, 0.9288, 0.9280, 0.9255, 0.9292, 0.9279,\n",
      "        0.9284, 0.9441, 0.9064, 0.9456, 0.9071, 0.0328, 0.9943],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5238, 0.5334, 0.5406, 0.5460, 0.5501, 0.5532, 0.5555, 0.5573,\n",
      "        0.5586, 0.5595, 0.5603, 0.5608, 0.5613, 0.5616, 0.5618, 0.5620, 0.5621,\n",
      "        0.5622, 0.5623, 0.5623, 0.5624, 0.5624, 0.5623, 0.5662])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  0  stage  23  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([ 1,  0, 23, 15,  2,  0,  0,  0,  0,  0,  0,  0,  0, 15,  0, 14,  1,  0,\n",
      "        13,  1,  1,  3,  0, 15,  0])\n",
      "loss=  tensor(0.0938, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.1960, 1.1960, 1.1960, 1.1960, 1.1960, 1.1960, 1.1960, 1.1960, 1.1960,\n",
      "        1.1960, 1.1960, 1.1960, 1.1960, 1.1960, 1.1960, 1.1960, 1.1960, 1.1960,\n",
      "        1.1960, 1.1960, 1.1960, 1.1960, 1.1960, 1.1960, 0.5929]) return=  150337.40439247817\n",
      "probs of actions:  tensor([0.0699, 0.5186, 0.0031, 0.0702, 0.0520, 0.4937, 0.4496, 0.6094, 0.4886,\n",
      "        0.5172, 0.5479, 0.4457, 0.5186, 0.0648, 0.4107, 0.0057, 0.0700, 0.4843,\n",
      "        0.0029, 0.0619, 0.0670, 0.0427, 0.5052, 0.0788, 0.9895],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5111, 0.5274, 0.4832, 0.6082, 0.6730, 0.6528, 0.6296, 0.6125, 0.5998,\n",
      "        0.5903, 0.5833, 0.5781, 0.5742, 0.5487, 0.6270, 0.5910, 0.6536, 0.6343,\n",
      "        0.5990, 0.6537, 0.6342, 0.6190, 0.6170, 0.5807, 0.6520])\n",
      "finalReturns:  tensor([0.0367, 0.0592])\n",
      "----------------------------------------\n",
      "iter  0  stage  22  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([ 0, 22, 22, 22, 18, 23, 22, 22, 27, 22, 22, 21, 22, 26, 22,  0, 17,  0,\n",
      "        22, 22, 16,  0, 28, 22,  0])\n",
      "loss=  tensor(0.8564, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.1470, 2.1470, 2.1470, 2.1470, 2.1470, 2.1470, 2.1470, 2.1470, 2.1470,\n",
      "        2.1470, 2.1470, 2.1470, 2.1470, 2.1470, 2.1470, 2.1470, 2.1470, 2.1470,\n",
      "        2.1470, 2.1470, 2.1470, 2.1470, 2.1470, 1.3829, 0.6721]) return=  193459.92049804344\n",
      "probs of actions:  tensor([0.2512, 0.3682, 0.3488, 0.2969, 0.0235, 0.0110, 0.3700, 0.3502, 0.0278,\n",
      "        0.3349, 0.3164, 0.0416, 0.3236, 0.0325, 0.4630, 0.1016, 0.0405, 0.1303,\n",
      "        0.3723, 0.4079, 0.0380, 0.3571, 0.0170, 0.6691, 0.9944],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.4754, 0.5683, 0.6430, 0.7178, 0.7252, 0.7735, 0.8025, 0.8000,\n",
      "        0.8650, 0.8719, 0.8813, 0.8760, 0.8609, 0.9026, 0.9485, 0.8137, 0.8435,\n",
      "        0.7196, 0.7612, 0.8159, 0.8381, 0.6857, 0.7853, 0.8599])\n",
      "finalReturns:  tensor([0.1839, 0.2623, 0.1878])\n",
      "----------------------------------------\n",
      "iter  0  stage  21  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([30, 30, 22, 19, 30, 30, 30, 27, 20, 27, 30, 28, 30, 30, 26, 30, 26, 22,\n",
      "        30, 24, 30, 24, 17, 27,  0])\n",
      "loss=  tensor(4.5377, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.5209, 3.5209, 3.5209, 3.5209, 3.5209, 3.5209, 3.5209, 3.5209, 3.5209,\n",
      "        3.5209, 3.5209, 3.5209, 3.5209, 3.5209, 3.5209, 3.5209, 3.5209, 3.5209,\n",
      "        3.5209, 3.5209, 3.5209, 3.5209, 2.4752, 1.5642, 0.7482]) return=  219456.57013693356\n",
      "probs of actions:  tensor([0.3420, 0.3385, 0.2044, 0.0172, 0.4148, 0.3398, 0.4331, 0.0786, 0.0263,\n",
      "        0.0908, 0.4014, 0.0236, 0.3730, 0.3527, 0.1123, 0.3745, 0.1065, 0.2665,\n",
      "        0.3297, 0.0252, 0.3049, 0.0192, 0.0083, 0.0535, 0.9814],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5480, 0.6939, 0.7536, 0.7227, 0.7910, 0.8441, 0.9020, 0.9510,\n",
      "        0.8956, 0.8963, 0.9363, 0.9362, 0.9550, 0.9916, 0.9594, 0.9949, 1.0034,\n",
      "        0.9335, 0.9854, 0.9371, 0.9881, 1.0002, 0.9088, 0.9963])\n",
      "finalReturns:  tensor([0.3725, 0.4301, 0.3409, 0.2480])\n",
      "----------------------------------------\n",
      "iter  0  stage  20  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([30, 30, 30, 30, 30, 30, 30, 30, 27, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30,  0])\n",
      "loss=  tensor(0.1581, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.3714, 4.3714, 4.3714, 4.3714, 4.3714, 4.3714, 4.3714, 4.3714, 4.3714,\n",
      "        4.3714, 4.3714, 4.3714, 4.3714, 4.3714, 4.3714, 4.3714, 4.3714, 4.3714,\n",
      "        4.3714, 4.3714, 4.3714, 3.2718, 2.3232, 1.4805, 0.7132]) return=  228028.54094405496\n",
      "probs of actions:  tensor([0.9125, 0.9169, 0.9037, 0.9342, 0.9368, 0.8968, 0.9510, 0.9254, 0.0124,\n",
      "        0.9645, 0.9396, 0.9534, 0.9234, 0.9067, 0.9477, 0.9371, 0.9293, 0.8873,\n",
      "        0.8996, 0.9431, 0.9692, 0.9912, 0.9642, 0.8582, 0.9988],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5480, 0.6523, 0.7357, 0.8011, 0.8519, 0.8908, 0.9206, 0.9603,\n",
      "        0.9450, 0.9617, 0.9743, 0.9838, 0.9909, 0.9963, 1.0003, 1.0034, 1.0056,\n",
      "        1.0074, 1.0086, 1.0096, 1.0103, 1.0109, 1.0113, 1.1016])\n",
      "finalReturns:  tensor([0.7723, 0.8623, 0.8006, 0.6324, 0.3884])\n",
      "----------------------------------------\n",
      "iter  0  stage  19  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([30, 30, 28, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30,  0])\n",
      "loss=  tensor(0.0536, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.0447, 5.0447, 5.0447, 5.0447, 5.0447, 5.0447, 5.0447, 5.0447, 5.0447,\n",
      "        5.0447, 5.0447, 5.0447, 5.0447, 5.0447, 5.0447, 5.0447, 5.0447, 5.0447,\n",
      "        5.0447, 5.0447, 3.9452, 2.9968, 2.1542, 1.3869, 0.6738]) return=  228200.9420715731\n",
      "probs of actions:  tensor([0.9825, 0.9829, 0.0017, 0.9874, 0.9864, 0.9742, 0.9898, 0.9839, 0.9872,\n",
      "        0.9938, 0.9878, 0.9901, 0.9837, 0.9793, 0.9897, 0.9861, 0.9859, 0.9749,\n",
      "        0.9783, 0.9932, 0.9963, 0.9995, 0.9941, 0.9526, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5480, 0.6639, 0.7266, 0.7941, 0.8464, 0.8867, 0.9174, 0.9408,\n",
      "        0.9585, 0.9719, 0.9820, 0.9895, 0.9953, 0.9996, 1.0028, 1.0052, 1.0070,\n",
      "        1.0084, 1.0094, 1.0102, 1.0108, 1.0112, 1.0115, 1.1018])\n",
      "finalReturns:  tensor([1.1102, 1.2002, 1.1385, 0.9703, 0.7264, 0.4280])\n",
      "----------------------------------------\n",
      "iter  0  stage  18  ep  86927   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30,  0])\n",
      "loss=  tensor(0.0098, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.6869, 5.6869, 5.6869, 5.6869, 5.6869, 5.6869, 5.6869, 5.6869, 5.6869,\n",
      "        5.6869, 5.6869, 5.6869, 5.6869, 5.6869, 5.6869, 5.6869, 5.6869, 5.6869,\n",
      "        5.6869, 4.5884, 3.6405, 2.7984, 2.0314, 1.3185, 0.6449]) return=  228471.31797735966\n",
      "probs of actions:  tensor([0.9982, 0.9981, 0.9972, 0.9989, 0.9985, 0.9973, 0.9989, 0.9984, 0.9987,\n",
      "        0.9994, 0.9987, 0.9990, 0.9983, 0.9981, 0.9990, 0.9985, 0.9986, 0.9974,\n",
      "        0.9990, 0.9997, 1.0000, 1.0000, 0.9995, 0.9908, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5480, 0.6523, 0.7357, 0.8011, 0.8519, 0.8908, 0.9206, 0.9432,\n",
      "        0.9603, 0.9732, 0.9830, 0.9903, 0.9959, 1.0000, 1.0031, 1.0055, 1.0072,\n",
      "        1.0085, 1.0095, 1.0103, 1.0108, 1.0112, 1.0116, 1.1018])\n",
      "finalReturns:  tensor([1.4768, 1.5668, 1.5052, 1.3371, 1.0932, 0.7948, 0.4569])\n",
      "----------------------------------------\n",
      "iter  0  stage  17  ep  20360   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30,  0])\n",
      "loss=  tensor(0.0088, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.3064, 6.3064, 6.3064, 6.3064, 6.3064, 6.3064, 6.3064, 6.3064, 6.3064,\n",
      "        6.3064, 6.3064, 6.3064, 6.3064, 6.3064, 6.3064, 6.3064, 6.3064, 6.3064,\n",
      "        5.2092, 4.2623, 3.4208, 2.6543, 1.9417, 1.2684, 0.6236]) return=  228471.31797735966\n",
      "probs of actions:  tensor([0.9991, 0.9989, 0.9985, 0.9994, 0.9992, 0.9986, 0.9994, 0.9992, 0.9993,\n",
      "        0.9997, 0.9993, 0.9995, 0.9991, 0.9990, 0.9995, 0.9992, 0.9992, 0.9990,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 0.9999, 0.9926, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5480, 0.6523, 0.7357, 0.8011, 0.8519, 0.8908, 0.9206, 0.9432,\n",
      "        0.9603, 0.9732, 0.9830, 0.9903, 0.9959, 1.0000, 1.0031, 1.0055, 1.0072,\n",
      "        1.0085, 1.0095, 1.0103, 1.0108, 1.0112, 1.0116, 1.1018])\n",
      "finalReturns:  tensor([1.8645, 1.9545, 1.8929, 1.7249, 1.4811, 1.1829, 0.8450, 0.4782])\n",
      "----------------------------------------\n",
      "iter  0  stage  16  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30,  0])\n",
      "loss=  tensor(0.0114, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.9087, 6.9087, 6.9087, 6.9087, 6.9087, 6.9087, 6.9087, 6.9087, 6.9087,\n",
      "        6.9087, 6.9087, 6.9087, 6.9087, 6.9087, 6.9087, 6.9087, 6.9087, 5.8133,\n",
      "        4.8675, 4.0269, 3.2610, 2.5489, 1.8759, 1.2314, 0.6079]) return=  228471.31797735966\n",
      "probs of actions:  tensor([0.9991, 0.9989, 0.9985, 0.9994, 0.9992, 0.9986, 0.9994, 0.9992, 0.9993,\n",
      "        0.9997, 0.9993, 0.9995, 0.9991, 0.9990, 0.9995, 0.9992, 0.9992, 0.9990,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 0.9999, 0.9926, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5480, 0.6523, 0.7357, 0.8011, 0.8519, 0.8908, 0.9206, 0.9432,\n",
      "        0.9603, 0.9732, 0.9830, 0.9903, 0.9959, 1.0000, 1.0031, 1.0055, 1.0072,\n",
      "        1.0085, 1.0095, 1.0103, 1.0108, 1.0112, 1.0116, 1.1018])\n",
      "finalReturns:  tensor([2.2677, 2.3577, 2.2962, 2.1283, 1.8847, 1.5865, 1.2487, 0.8820, 0.4939])\n",
      "----------------------------------------\n",
      "iter  0  stage  15  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30,  0])\n",
      "loss=  tensor(0.0146, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.4973, 7.4973, 7.4973, 7.4973, 7.4973, 7.4973, 7.4973, 7.4973, 7.4973,\n",
      "        7.4973, 7.4973, 7.4973, 7.4973, 7.4973, 7.4973, 7.4973, 6.4042, 5.4601,\n",
      "        4.6206, 3.8555, 3.1440, 2.4714, 1.8273, 1.2040, 0.5963]) return=  228471.31797735966\n",
      "probs of actions:  tensor([0.9991, 0.9989, 0.9985, 0.9994, 0.9992, 0.9986, 0.9994, 0.9992, 0.9993,\n",
      "        0.9997, 0.9993, 0.9995, 0.9991, 0.9990, 0.9995, 0.9992, 0.9992, 0.9990,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 0.9999, 0.9926, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5480, 0.6523, 0.7357, 0.8011, 0.8519, 0.8908, 0.9206, 0.9432,\n",
      "        0.9603, 0.9732, 0.9830, 0.9903, 0.9959, 1.0000, 1.0031, 1.0055, 1.0072,\n",
      "        1.0085, 1.0095, 1.0103, 1.0108, 1.0112, 1.0116, 1.1018])\n",
      "finalReturns:  tensor([2.6823, 2.7723, 2.7109, 2.5432, 2.2997, 2.0017, 1.6640, 1.2973, 0.9093,\n",
      "        0.5055])\n",
      "----------------------------------------\n",
      "iter  0  stage  14  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30,  0])\n",
      "loss=  tensor(0.0173, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.0744, 8.0744, 8.0744, 8.0744, 8.0744, 8.0744, 8.0744, 8.0744, 8.0744,\n",
      "        8.0744, 8.0744, 8.0744, 8.0744, 8.0744, 8.0744, 6.9844, 6.0425, 5.2046,\n",
      "        4.4406, 3.7299, 3.0579, 2.4141, 1.7912, 1.1837, 0.5876]) return=  228471.31797735966\n",
      "probs of actions:  tensor([0.9991, 0.9989, 0.9985, 0.9994, 0.9992, 0.9986, 0.9994, 0.9992, 0.9993,\n",
      "        0.9997, 0.9993, 0.9995, 0.9991, 0.9990, 0.9995, 0.9992, 0.9992, 0.9990,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 0.9999, 0.9926, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5480, 0.6523, 0.7357, 0.8011, 0.8519, 0.8908, 0.9206, 0.9432,\n",
      "        0.9603, 0.9732, 0.9830, 0.9903, 0.9959, 1.0000, 1.0031, 1.0055, 1.0072,\n",
      "        1.0085, 1.0095, 1.0103, 1.0108, 1.0112, 1.0116, 1.1018])\n",
      "finalReturns:  tensor([3.1051, 3.1951, 3.1339, 2.9664, 2.7231, 2.4253, 2.0878, 1.7213, 1.3334,\n",
      "        0.9296, 0.5142])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  0  stage  13  ep  2683   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30,  0])\n",
      "loss=  tensor(0.0196, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.6415, 8.6415, 8.6415, 8.6415, 8.6415, 8.6415, 8.6415, 8.6415, 8.6415,\n",
      "        8.6415, 8.6415, 8.6415, 8.6415, 8.6415, 7.5556, 6.6166, 5.7807, 5.0182,\n",
      "        4.3086, 3.6373, 2.9942, 2.3717, 1.7645, 1.1686, 0.5812]) return=  228471.31797735966\n",
      "probs of actions:  tensor([0.9991, 0.9989, 0.9986, 0.9994, 0.9992, 0.9986, 0.9994, 0.9992, 0.9993,\n",
      "        0.9997, 0.9993, 0.9995, 0.9991, 0.9990, 0.9997, 0.9993, 0.9995, 0.9990,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 0.9999, 0.9926, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5480, 0.6523, 0.7357, 0.8011, 0.8519, 0.8908, 0.9206, 0.9432,\n",
      "        0.9603, 0.9732, 0.9830, 0.9903, 0.9959, 1.0000, 1.0031, 1.0055, 1.0072,\n",
      "        1.0085, 1.0095, 1.0103, 1.0108, 1.0112, 1.0116, 1.1018])\n",
      "finalReturns:  tensor([3.5339, 3.6239, 3.5629, 3.3957, 3.1527, 2.8552, 2.5179, 2.1515, 1.7637,\n",
      "        1.3601, 0.9448, 0.5206])\n",
      "----------------------------------------\n",
      "iter  0  stage  12  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30,  0])\n",
      "loss=  tensor(0.0243, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.1989, 9.1989, 9.1989, 9.1989, 9.1989, 9.1989, 9.1989, 9.1989, 9.1989,\n",
      "        9.1989, 9.1989, 9.1989, 9.1989, 8.1186, 7.1834, 6.3502, 5.5897, 4.8815,\n",
      "        4.2112, 3.5688, 2.9469, 2.3401, 1.7445, 1.1573, 0.5763]) return=  228471.31797735966\n",
      "probs of actions:  tensor([0.9991, 0.9989, 0.9986, 0.9994, 0.9992, 0.9986, 0.9994, 0.9992, 0.9993,\n",
      "        0.9997, 0.9993, 0.9995, 0.9991, 0.9990, 0.9997, 0.9993, 0.9995, 0.9990,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 0.9999, 0.9926, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5480, 0.6523, 0.7357, 0.8011, 0.8519, 0.8908, 0.9206, 0.9432,\n",
      "        0.9603, 0.9732, 0.9830, 0.9903, 0.9959, 1.0000, 1.0031, 1.0055, 1.0072,\n",
      "        1.0085, 1.0095, 1.0103, 1.0108, 1.0112, 1.0116, 1.1018])\n",
      "finalReturns:  tensor([3.9668, 4.0568, 3.9962, 3.8293, 3.5867, 3.2895, 2.9525, 2.5864, 2.1988,\n",
      "        1.7953, 1.3801, 0.9560, 0.5255])\n",
      "----------------------------------------\n",
      "iter  0  stage  11  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30,  0])\n",
      "loss=  tensor(0.0279, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.7463, 9.7463, 9.7463, 9.7463, 9.7463, 9.7463, 9.7463, 9.7463, 9.7463,\n",
      "        9.7463, 9.7463, 9.7463, 8.6733, 7.7432, 6.9137, 6.1557, 5.4494, 4.7805,\n",
      "        4.1391, 3.5180, 2.9117, 2.3166, 1.7297, 1.1489, 0.5727]) return=  228471.31797735966\n",
      "probs of actions:  tensor([0.9991, 0.9989, 0.9986, 0.9994, 0.9992, 0.9986, 0.9994, 0.9992, 0.9993,\n",
      "        0.9997, 0.9993, 0.9995, 0.9991, 0.9990, 0.9997, 0.9993, 0.9995, 0.9990,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 0.9999, 0.9926, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5480, 0.6523, 0.7357, 0.8011, 0.8519, 0.8908, 0.9206, 0.9432,\n",
      "        0.9603, 0.9732, 0.9830, 0.9903, 0.9959, 1.0000, 1.0031, 1.0055, 1.0072,\n",
      "        1.0085, 1.0095, 1.0103, 1.0108, 1.0112, 1.0116, 1.1018])\n",
      "finalReturns:  tensor([4.4025, 4.4925, 4.4322, 4.2659, 4.0238, 3.7270, 3.3904, 3.0246, 2.6373,\n",
      "        2.2340, 1.8189, 1.3949, 0.9645, 0.5291])\n",
      "----------------------------------------\n",
      "iter  0  stage  10  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30,  0])\n",
      "loss=  tensor(0.0325, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.2824, 10.2824, 10.2824, 10.2824, 10.2824, 10.2824, 10.2824, 10.2824,\n",
      "        10.2824, 10.2824, 10.2824,  9.2192,  8.2959,  7.4712,  6.7167,  6.0129,\n",
      "         5.3459,  4.7058,  4.0856,  3.4801,  2.8855,  2.2990,  1.7185,  1.1426,\n",
      "         0.5700]) return=  228471.31797735966\n",
      "probs of actions:  tensor([0.9991, 0.9989, 0.9986, 0.9994, 0.9992, 0.9986, 0.9994, 0.9992, 0.9993,\n",
      "        0.9997, 0.9993, 0.9995, 0.9991, 0.9990, 0.9997, 0.9993, 0.9995, 0.9990,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 0.9999, 0.9926, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5480, 0.6523, 0.7357, 0.8011, 0.8519, 0.8908, 0.9206, 0.9432,\n",
      "        0.9603, 0.9732, 0.9830, 0.9903, 0.9959, 1.0000, 1.0031, 1.0055, 1.0072,\n",
      "        1.0085, 1.0095, 1.0103, 1.0108, 1.0112, 1.0116, 1.1018])\n",
      "finalReturns:  tensor([4.8396, 4.9296, 4.8698, 4.7042, 4.4628, 4.1667, 3.8306, 3.4652, 3.0782,\n",
      "        2.6751, 2.2602, 1.8364, 1.4061, 0.9708, 0.5318])\n",
      "----------------------------------------\n",
      "iter  0  stage  9  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30,  0])\n",
      "loss=  tensor(0.0354, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.8052, 10.8052, 10.8052, 10.8052, 10.8052, 10.8052, 10.8052, 10.8052,\n",
      "        10.8052, 10.8052,  9.7549,  8.8407,  8.0224,  7.2725,  6.5720,  5.9074,\n",
      "         5.2692,  4.6503,  4.0457,  3.4519,  2.8659,  2.2859,  1.7102,  1.1379,\n",
      "         0.5680]) return=  228471.31797735966\n",
      "probs of actions:  tensor([0.9991, 0.9989, 0.9986, 0.9994, 0.9992, 0.9986, 0.9994, 0.9992, 0.9993,\n",
      "        0.9997, 0.9993, 0.9995, 0.9991, 0.9990, 0.9997, 0.9993, 0.9995, 0.9990,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 0.9999, 0.9926, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5480, 0.6523, 0.7357, 0.8011, 0.8519, 0.8908, 0.9206, 0.9432,\n",
      "        0.9603, 0.9732, 0.9830, 0.9903, 0.9959, 1.0000, 1.0031, 1.0055, 1.0072,\n",
      "        1.0085, 1.0095, 1.0103, 1.0108, 1.0112, 1.0116, 1.1018])\n",
      "finalReturns:  tensor([5.2771, 5.3671, 5.3080, 5.1433, 4.9029, 4.6075, 4.2721, 3.9073, 3.5207,\n",
      "        3.1180, 2.7034, 2.2798, 1.8496, 1.4144, 0.9755, 0.5338])\n",
      "----------------------------------------\n",
      "iter  0  stage  8  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30,  0])\n",
      "loss=  tensor(0.0407, grad_fn=<NegBackward0>)   ,  base rewards= tensor([11.3115, 11.3115, 11.3115, 11.3115, 11.3115, 11.3115, 11.3115, 11.3115,\n",
      "        11.3115, 10.2783,  9.3761,  8.5663,  7.8226,  7.1265,  6.4652,  5.8293,\n",
      "         5.2122,  4.6089,  4.0160,  3.4308,  2.8513,  2.2760,  1.7040,  1.1344,\n",
      "         0.5665]) return=  228471.31797735966\n",
      "probs of actions:  tensor([0.9991, 0.9989, 0.9986, 0.9994, 0.9992, 0.9986, 0.9994, 0.9992, 0.9993,\n",
      "        0.9997, 0.9993, 0.9995, 0.9991, 0.9990, 0.9997, 0.9993, 0.9995, 0.9990,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 0.9999, 0.9926, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5480, 0.6523, 0.7357, 0.8011, 0.8519, 0.8908, 0.9206, 0.9432,\n",
      "        0.9603, 0.9732, 0.9830, 0.9903, 0.9959, 1.0000, 1.0031, 1.0055, 1.0072,\n",
      "        1.0085, 1.0095, 1.0103, 1.0108, 1.0112, 1.0116, 1.1018])\n",
      "finalReturns:  tensor([5.7139, 5.8039, 5.7458, 5.5824, 5.3432, 5.0489, 4.7144, 4.3503, 3.9643,\n",
      "        3.5620, 3.1477, 2.7244, 2.2944, 1.8594, 1.4206, 0.9790, 0.5353])\n",
      "----------------------------------------\n",
      "iter  0  stage  7  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30,  0])\n",
      "loss=  tensor(0.0468, grad_fn=<NegBackward0>)   ,  base rewards= tensor([11.7970, 11.7970, 11.7970, 11.7970, 11.7970, 11.7970, 11.7970, 11.7970,\n",
      "        10.7864,  9.9001,  9.1016,  8.3659,  7.6757,  7.0187,  6.3860,  5.7712,\n",
      "         5.1697,  4.5781,  3.9938,  3.4150,  2.8403,  2.2687,  1.6994,  1.1317,\n",
      "         0.5654]) return=  228471.31797735966\n",
      "probs of actions:  tensor([0.9991, 0.9989, 0.9986, 0.9994, 0.9992, 0.9986, 0.9994, 0.9992, 0.9993,\n",
      "        0.9997, 0.9993, 0.9995, 0.9991, 0.9990, 0.9997, 0.9993, 0.9995, 0.9990,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 0.9999, 0.9926, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5480, 0.6523, 0.7357, 0.8011, 0.8519, 0.8908, 0.9206, 0.9432,\n",
      "        0.9603, 0.9732, 0.9830, 0.9903, 0.9959, 1.0000, 1.0031, 1.0055, 1.0072,\n",
      "        1.0085, 1.0095, 1.0103, 1.0108, 1.0112, 1.0116, 1.1018])\n",
      "finalReturns:  tensor([6.1490, 6.2390, 6.1822, 6.0204, 5.7828, 5.4900, 5.1567, 4.7936, 4.4084,\n",
      "        4.0067, 3.5929, 3.1699, 2.7402, 2.3054, 1.8667, 1.4252, 0.9816, 0.5364])\n",
      "----------------------------------------\n",
      "iter  0  stage  6  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30,  0])\n",
      "loss=  tensor(0.0518, grad_fn=<NegBackward0>)   ,  base rewards= tensor([12.2557, 12.2557, 12.2557, 12.2557, 12.2557, 12.2557, 12.2557, 11.2749,\n",
      "        10.4095,  9.6259,  8.9010,  8.2186,  7.5673,  6.9388,  6.3271,  5.7279,\n",
      "         5.1380,  4.5550,  3.9772,  3.4032,  2.8321,  2.2632,  1.6959,  1.1297,\n",
      "         0.5645]) return=  228471.31797735966\n",
      "probs of actions:  tensor([0.9991, 0.9989, 0.9986, 0.9994, 0.9992, 0.9986, 0.9994, 0.9992, 0.9993,\n",
      "        0.9997, 0.9993, 0.9995, 0.9991, 0.9990, 0.9997, 0.9993, 0.9995, 0.9990,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 0.9999, 0.9926, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5480, 0.6523, 0.7357, 0.8011, 0.8519, 0.8908, 0.9206, 0.9432,\n",
      "        0.9603, 0.9732, 0.9830, 0.9903, 0.9959, 1.0000, 1.0031, 1.0055, 1.0072,\n",
      "        1.0085, 1.0095, 1.0103, 1.0108, 1.0112, 1.0116, 1.1018])\n",
      "finalReturns:  tensor([6.5812, 6.6712, 6.6160, 6.4564, 6.2210, 5.9301, 5.5985, 5.2366, 4.8524,\n",
      "        4.4516, 4.0384, 3.6159, 3.1865, 2.7520, 2.3136, 1.8722, 1.4287, 0.9836,\n",
      "        0.5373])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  0  stage  5  ep  280   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30,  0])\n",
      "loss=  tensor(0.0541, grad_fn=<NegBackward0>)   ,  base rewards= tensor([12.6800, 12.6800, 12.6800, 12.6800, 12.6800, 12.6800, 11.7381, 10.9002,\n",
      "        10.1362,  9.4255,  8.7535,  8.1098,  7.4869,  6.8794,  6.2833,  5.6957,\n",
      "         5.1144,  4.5378,  3.9648,  3.3944,  2.8260,  2.2591,  1.6933,  1.1283,\n",
      "         0.5639]) return=  228471.31797735966\n",
      "probs of actions:  tensor([0.9991, 0.9989, 0.9985, 0.9994, 0.9992, 0.9990, 0.9994, 0.9994, 0.9995,\n",
      "        0.9998, 0.9995, 0.9996, 0.9993, 0.9990, 0.9997, 0.9993, 0.9995, 0.9990,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 0.9999, 0.9921, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5480, 0.6523, 0.7357, 0.8011, 0.8519, 0.8908, 0.9206, 0.9432,\n",
      "        0.9603, 0.9732, 0.9830, 0.9903, 0.9959, 1.0000, 1.0031, 1.0055, 1.0072,\n",
      "        1.0085, 1.0095, 1.0103, 1.0108, 1.0112, 1.0116, 1.1018])\n",
      "finalReturns:  tensor([7.0088, 7.0988, 7.0459, 6.8892, 6.6567, 6.3684, 6.0389, 5.6789, 5.2960,\n",
      "        4.8963, 4.4839, 4.0621, 3.6332, 3.1990, 2.7608, 2.3197, 1.8764, 1.4313,\n",
      "        0.9851, 0.5379])\n",
      "----------------------------------------\n",
      "iter  0  stage  4  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30,  0])\n",
      "loss=  tensor(0.0616, grad_fn=<NegBackward0>)   ,  base rewards= tensor([13.0598, 13.0598, 13.0598, 13.0598, 13.0598, 12.1686, 11.3667, 10.6286,\n",
      "         9.9366,  9.2783,  8.6446,  8.0291,  7.4271,  6.8351,  6.2506,  5.6715,\n",
      "         5.0967,  4.5249,  3.9555,  3.3878,  2.8214,  2.2560,  1.6913,  1.1272,\n",
      "         0.5634]) return=  228471.31797735966\n",
      "probs of actions:  tensor([0.9991, 0.9989, 0.9985, 0.9994, 0.9992, 0.9990, 0.9994, 0.9994, 0.9995,\n",
      "        0.9998, 0.9995, 0.9996, 0.9993, 0.9990, 0.9997, 0.9993, 0.9995, 0.9990,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 0.9999, 0.9921, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5480, 0.6523, 0.7357, 0.8011, 0.8519, 0.8908, 0.9206, 0.9432,\n",
      "        0.9603, 0.9732, 0.9830, 0.9903, 0.9959, 1.0000, 1.0031, 1.0055, 1.0072,\n",
      "        1.0085, 1.0095, 1.0103, 1.0108, 1.0112, 1.0116, 1.1018])\n",
      "finalReturns:  tensor([7.4301, 7.5201, 7.4702, 7.3174, 7.0888, 6.8040, 6.4773, 6.1196, 5.7386,\n",
      "        5.3403, 4.9290, 4.5080, 4.0798, 3.6460, 3.2083, 2.7674, 2.3243, 1.8794,\n",
      "        1.4333, 0.9862, 0.5384])\n",
      "----------------------------------------\n",
      "iter  0  stage  3  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30,  0])\n",
      "loss=  tensor(0.0675, grad_fn=<NegBackward0>)   ,  base rewards= tensor([13.3829, 13.3829, 13.3829, 13.3829, 12.5572, 11.8020, 11.0977, 10.4303,\n",
      "         9.7900,  9.1696,  8.5639,  7.9692,  7.3826,  6.8021,  6.2261,  5.6535,\n",
      "         5.0835,  4.5153,  3.9485,  3.3828,  2.8180,  2.2537,  1.6898,  1.1263,\n",
      "         0.5631]) return=  228471.31797735966\n",
      "probs of actions:  tensor([0.9991, 0.9989, 0.9985, 0.9994, 0.9992, 0.9990, 0.9994, 0.9994, 0.9995,\n",
      "        0.9998, 0.9995, 0.9996, 0.9993, 0.9990, 0.9997, 0.9993, 0.9995, 0.9990,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 0.9999, 0.9921, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5480, 0.6523, 0.7357, 0.8011, 0.8519, 0.8908, 0.9206, 0.9432,\n",
      "        0.9603, 0.9732, 0.9830, 0.9903, 0.9959, 1.0000, 1.0031, 1.0055, 1.0072,\n",
      "        1.0085, 1.0095, 1.0103, 1.0108, 1.0112, 1.0116, 1.1018])\n",
      "finalReturns:  tensor([7.8427, 7.9327, 7.8868, 7.7392, 7.5158, 7.2355, 6.9127, 6.5581, 6.1795,\n",
      "        5.7831, 5.3733, 4.9534, 4.5260, 4.0930, 3.6557, 3.2152, 2.7724, 2.3277,\n",
      "        1.8818, 1.4348, 0.9870, 0.5387])\n",
      "----------------------------------------\n",
      "iter  0  stage  2  ep  258   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 26, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30,  0])\n",
      "loss=  tensor(34.8480, grad_fn=<NegBackward0>)   ,  base rewards= tensor([13.6344, 13.6344, 13.6344, 12.8921, 12.1971, 11.5366, 10.9012, 10.2846,\n",
      "         9.6816,  9.0890,  8.5039,  7.9245,  7.3494,  6.7774,  6.2078,  5.6400,\n",
      "         5.0735,  4.5081,  3.9433,  3.3791,  2.8154,  2.2519,  1.6887,  1.1257,\n",
      "         0.5628]) return=  227922.18817761683\n",
      "probs of actions:  tensor([9.9912e-01, 9.9896e-01, 9.9900e-01, 9.9960e-01, 9.9942e-01, 9.9922e-01,\n",
      "        9.9947e-01, 9.9954e-01, 9.9955e-01, 9.9982e-01, 9.9957e-01, 9.9970e-01,\n",
      "        9.9947e-01, 9.9904e-01, 9.9969e-01, 4.2794e-04, 9.9954e-01, 9.9905e-01,\n",
      "        9.9980e-01, 9.9997e-01, 1.0000e+00, 1.0000e+00, 9.9995e-01, 9.9227e-01,\n",
      "        1.0000e+00], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5480, 0.6523, 0.7357, 0.8011, 0.8519, 0.8908, 0.9206, 0.9432,\n",
      "        0.9603, 0.9732, 0.9830, 0.9903, 0.9959, 1.0000, 1.0255, 0.9846, 0.9916,\n",
      "        0.9968, 1.0007, 1.0036, 1.0059, 1.0075, 1.0088, 1.0997])\n",
      "finalReturns:  tensor([8.1886, 8.2786, 8.2379, 8.0973, 7.8807, 7.6066, 7.2889, 6.9384, 6.5631,\n",
      "        6.1693, 5.7614, 5.3431, 4.9168, 4.4846, 4.0256, 3.6064, 3.1796, 2.7470,\n",
      "        2.3101, 1.8699, 1.4272, 0.9828, 0.5369])\n",
      "----------------------------------------\n",
      "iter  0  stage  1  ep  23   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30,  0])\n",
      "loss=  tensor(0.0766, grad_fn=<NegBackward0>)   ,  base rewards= tensor([13.7976, 13.7976, 13.1596, 12.5410, 11.9366, 11.3428, 10.7569, 10.1769,\n",
      "         9.6013,  9.0290,  8.4592,  7.8912,  7.3246,  6.7590,  6.1942,  5.6299,\n",
      "         5.0661,  4.5026,  3.9394,  3.3764,  2.8134,  2.2506,  1.6879,  1.1252,\n",
      "         0.5626]) return=  228471.31797735966\n",
      "probs of actions:  tensor([0.9991, 0.9990, 0.9990, 0.9996, 0.9994, 0.9992, 0.9995, 0.9995, 0.9996,\n",
      "        0.9998, 0.9996, 0.9997, 0.9995, 0.9990, 0.9997, 0.9993, 0.9995, 0.9991,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 0.9999, 0.9922, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5480, 0.6523, 0.7357, 0.8011, 0.8519, 0.8908, 0.9206, 0.9432,\n",
      "        0.9603, 0.9732, 0.9830, 0.9903, 0.9959, 1.0000, 1.0031, 1.0055, 1.0072,\n",
      "        1.0085, 1.0095, 1.0103, 1.0108, 1.0112, 1.0116, 1.1018])\n",
      "finalReturns:  tensor([8.6283, 8.7183, 8.6847, 8.5534, 8.3460, 8.0800, 7.7691, 7.4241, 7.0532,\n",
      "        6.6628, 6.2575, 5.8412, 5.4164, 4.9854, 4.5496, 4.1103, 3.6683, 3.2244,\n",
      "        2.7789, 2.3323, 1.8848, 1.4367, 0.9881, 0.5392])\n",
      "----------------------------------------\n",
      "iter  0  stage  0  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30,  0])\n",
      "loss=  tensor(0.0858, grad_fn=<NegBackward0>)   ,  base rewards= tensor([13.8555, 13.3442, 12.8204, 12.2871, 11.7465, 11.2005, 10.6504, 10.0972,\n",
      "         9.5417,  8.9844,  8.4258,  7.8663,  7.3060,  6.7452,  6.1839,  5.6224,\n",
      "         5.0606,  4.4986,  3.9365,  3.3743,  2.8120,  2.2497,  1.6873,  1.1249,\n",
      "         0.5624]) return=  228471.31797735966\n",
      "probs of actions:  tensor([0.9991, 0.9990, 0.9990, 0.9996, 0.9994, 0.9992, 0.9995, 0.9995, 0.9996,\n",
      "        0.9998, 0.9996, 0.9997, 0.9995, 0.9990, 0.9997, 0.9993, 0.9995, 0.9991,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 0.9999, 0.9922, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4212, 0.5480, 0.6523, 0.7357, 0.8011, 0.8519, 0.8908, 0.9206, 0.9432,\n",
      "        0.9603, 0.9732, 0.9830, 0.9903, 0.9959, 1.0000, 1.0031, 1.0055, 1.0072,\n",
      "        1.0085, 1.0095, 1.0103, 1.0108, 1.0112, 1.0116, 1.1018])\n",
      "finalReturns:  tensor([8.9917, 9.0817, 9.0575, 8.9385, 8.7434, 8.4883, 8.1865, 7.8489, 7.4838,\n",
      "        7.0979, 6.6961, 6.2824, 5.8597, 5.4302, 4.9956, 4.5572, 4.1159, 3.6724,\n",
      "        3.2273, 2.7810, 2.3337, 1.8858, 1.4373, 0.9885, 0.5393])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1682476508 saved\n",
      "[830550, 'tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])', 228471.31797735966, 18758.68173295266, 0.08583260327577591, 1e-05, 1, 0, 'tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\\n        30, 30, 30, 30, 30, 30,  0])', '[1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\\n 1.   1.   1.   1.   1.   1.   1.   1.   1.   0.99 1.  ]', '0,[1e-05,1][1, 10000, 1, 1],1682476508', 25, 50, 174696.33353181678, 228471.31797735966, 95930.56714139864, 130490.57866666671, 127475.47466666666, 61768.648600928995, 61760.04937694948, 76910.22725950743, 76910.22725950743, 110767.6026843345, 61768.648600928995, 76910.22725950743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy reset\n",
      "----------------------------------------\n",
      "iter  1  stage  24  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([ 0,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0,  0, 14,  0, 23,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5712, 0.5712, 0.5712, 0.5712, 0.5712, 0.5712, 0.5712, 0.5712, 0.5712,\n",
      "        0.5712, 0.5712, 0.5712, 0.5712, 0.5712, 0.5712, 0.5712, 0.5712, 0.5712,\n",
      "        0.5712, 0.5712, 0.5712, 0.5712, 0.5712, 0.5712, 0.5712]) return=  143712.4363256526\n",
      "probs of actions:  tensor([9.1195e-01, 9.0448e-01, 9.2248e-01, 9.3650e-01, 9.1132e-01, 4.6668e-03,\n",
      "        9.2003e-01, 9.1205e-01, 8.9672e-01, 9.0821e-01, 9.1395e-01, 9.0437e-01,\n",
      "        4.1970e-04, 9.3013e-01, 4.8654e-04, 9.3345e-01, 9.3245e-01, 9.2076e-01,\n",
      "        9.3494e-01, 9.0264e-01, 9.2735e-01, 9.1623e-01, 9.3351e-01, 9.3845e-01,\n",
      "        9.9369e-01], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5238, 0.5334, 0.5406, 0.5460, 0.5492, 0.5644, 0.5639, 0.5636,\n",
      "        0.5633, 0.5631, 0.5630, 0.5432, 0.6165, 0.5499, 0.6844, 0.6528, 0.6296,\n",
      "        0.6125, 0.5998, 0.5903, 0.5833, 0.5781, 0.5742, 0.5712])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  1  stage  23  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([ 2, 13,  8,  3,  8, 11,  1,  0, 10,  1,  4,  0, 11, 10, 11, 11,  0, 11,\n",
      "         3, 19,  5, 11,  0, 16,  0])\n",
      "loss=  tensor(0.1585, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.3100, 1.3100, 1.3100, 1.3100, 1.3100, 1.3100, 1.3100, 1.3100, 1.3100,\n",
      "        1.3100, 1.3100, 1.3100, 1.3100, 1.3100, 1.3100, 1.3100, 1.3100, 1.3100,\n",
      "        1.3100, 1.3100, 1.3100, 1.3100, 1.3100, 1.3100, 0.6413]) return=  158781.70950982286\n",
      "probs of actions:  tensor([0.1258, 0.0380, 0.0482, 0.0496, 0.0497, 0.0659, 0.0877, 0.3758, 0.0628,\n",
      "        0.0816, 0.0299, 0.1993, 0.0989, 0.0494, 0.1139, 0.0767, 0.3202, 0.0930,\n",
      "        0.0514, 0.0096, 0.0470, 0.1015, 0.3275, 0.0204, 0.9615],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5108, 0.5142, 0.5812, 0.6113, 0.6048, 0.6181, 0.6566, 0.6365, 0.6076,\n",
      "        0.6429, 0.6247, 0.6258, 0.5975, 0.6310, 0.6488, 0.6680, 0.6948, 0.6483,\n",
      "        0.6789, 0.6255, 0.7116, 0.6831, 0.7061, 0.6431, 0.7070])\n",
      "finalReturns:  tensor([0.0401, 0.0657])\n",
      "----------------------------------------\n",
      "iter  1  stage  22  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([17, 17, 22,  9, 27, 10, 23,  1, 11, 22, 19, 22, 11, 22, 23, 17, 21, 19,\n",
      "        14, 19,  1, 22, 27, 21,  0])\n",
      "loss=  tensor(0.9555, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.2835, 2.2835, 2.2835, 2.2835, 2.2835, 2.2835, 2.2835, 2.2835, 2.2835,\n",
      "        2.2835, 2.2835, 2.2835, 2.2835, 2.2835, 2.2835, 2.2835, 2.2835, 2.2835,\n",
      "        2.2835, 2.2835, 2.2835, 2.2835, 2.2835, 1.4586, 0.7040]) return=  189588.30780201865\n",
      "probs of actions:  tensor([0.0446, 0.0415, 0.1311, 0.0181, 0.0726, 0.0367, 0.0968, 0.0634, 0.0767,\n",
      "        0.1486, 0.0753, 0.1825, 0.1011, 0.2034, 0.0902, 0.0444, 0.0571, 0.0620,\n",
      "        0.0055, 0.0769, 0.0473, 0.1604, 0.1186, 0.1233, 0.9659],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4823, 0.5582, 0.5991, 0.7076, 0.6403, 0.7792, 0.7194, 0.8173, 0.7414,\n",
      "        0.7016, 0.7596, 0.7689, 0.8353, 0.7713, 0.7963, 0.8475, 0.8247, 0.8458,\n",
      "        0.8627, 0.8232, 0.8652, 0.7396, 0.7520, 0.8324, 0.8876])\n",
      "finalReturns:  tensor([0.1885, 0.2614, 0.1837])\n",
      "----------------------------------------\n",
      "iter  1  stage  21  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([ 2, 23, 29, 27, 27, 33, 22, 22, 29, 27, 27, 27, 29, 27, 27, 23, 19, 23,\n",
      "        27, 27, 29, 29, 21, 29,  0])\n",
      "loss=  tensor(2.9230, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.4631, 3.4631, 3.4631, 3.4631, 3.4631, 3.4631, 3.4631, 3.4631, 3.4631,\n",
      "        3.4631, 3.4631, 3.4631, 3.4631, 3.4631, 3.4631, 3.4631, 3.4631, 3.4631,\n",
      "        3.4631, 3.4631, 3.4631, 3.4631, 2.4400, 1.5449, 0.7401]) return=  213779.62568054372\n",
      "probs of actions:  tensor([0.0017, 0.1390, 0.2575, 0.2755, 0.2901, 0.0024, 0.1132, 0.1030, 0.2525,\n",
      "        0.3007, 0.2977, 0.2952, 0.2668, 0.3061, 0.2577, 0.1412, 0.0256, 0.1252,\n",
      "        0.2708, 0.2615, 0.2655, 0.2885, 0.0294, 0.2154, 0.9868],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5108, 0.4782, 0.5425, 0.6559, 0.7286, 0.7494, 0.8825, 0.8850, 0.8512,\n",
      "        0.8980, 0.9151, 0.9281, 0.9267, 0.9554, 0.9584, 0.9807, 0.9789, 0.9284,\n",
      "        0.9032, 0.9191, 0.9199, 0.9390, 0.9935, 0.9238, 1.0261])\n",
      "finalReturns:  tensor([0.4193, 0.5034, 0.4050, 0.2860])\n",
      "----------------------------------------\n",
      "iter  1  stage  20  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 27, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.3539, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.3058, 4.3058, 4.3058, 4.3058, 4.3058, 4.3058, 4.3058, 4.3058, 4.3058,\n",
      "        4.3058, 4.3058, 4.3058, 4.3058, 4.3058, 4.3058, 4.3058, 4.3058, 4.3058,\n",
      "        4.3058, 4.3058, 4.3058, 3.2296, 2.2973, 1.4662, 0.7072]) return=  225339.17689685564\n",
      "probs of actions:  tensor([0.8154, 0.8062, 0.8327, 0.8258, 0.8574, 0.8441, 0.8384, 0.8128, 0.8276,\n",
      "        0.8399, 0.8353, 0.8444, 0.8512, 0.8138, 0.8433, 0.0926, 0.8387, 0.8597,\n",
      "        0.8749, 0.8681, 0.9106, 0.8920, 0.9257, 0.8106, 0.9973],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9997, 0.9804, 0.9847,\n",
      "        0.9879, 0.9903, 0.9921, 0.9934, 0.9944, 0.9952, 1.0799])\n",
      "finalReturns:  tensor([0.7493, 0.8334, 0.7722, 0.6089, 0.3727])\n",
      "----------------------------------------\n",
      "iter  1  stage  19  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 27, 29, 29, 29, 29, 29, 29, 29, 29, 27, 29, 29, 29,\n",
      "        29, 27, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(3.4990, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.9724, 4.9724, 4.9724, 4.9724, 4.9724, 4.9724, 4.9724, 4.9724, 4.9724,\n",
      "        4.9724, 4.9724, 4.9724, 4.9724, 4.9724, 4.9724, 4.9724, 4.9724, 4.9724,\n",
      "        4.9724, 4.9724, 3.8972, 2.9656, 2.1349, 1.3762, 0.6693]) return=  224839.01113301513\n",
      "probs of actions:  tensor([0.9082, 0.9036, 0.9186, 0.9093, 0.9285, 0.0573, 0.9221, 0.9087, 0.9121,\n",
      "        0.9198, 0.9195, 0.9243, 0.9271, 0.9111, 0.0542, 0.9275, 0.9204, 0.9342,\n",
      "        0.9394, 0.0433, 0.9618, 0.9606, 0.9649, 0.9095, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8542, 0.8707, 0.9017, 0.9252,\n",
      "        0.9431, 0.9565, 0.9667, 0.9744, 0.9801, 0.9957, 0.9774, 0.9824, 0.9862,\n",
      "        0.9890, 1.0023, 0.9824, 0.9861, 0.9890, 0.9911, 1.0768])\n",
      "finalReturns:  tensor([1.0553, 1.1282, 1.0774, 0.9220, 0.6917, 0.4075])\n",
      "----------------------------------------\n",
      "iter  1  stage  18  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.1558, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.6221, 5.6221, 5.6221, 5.6221, 5.6221, 5.6221, 5.6221, 5.6221, 5.6221,\n",
      "        5.6221, 5.6221, 5.6221, 5.6221, 5.6221, 5.6221, 5.6221, 5.6221, 5.6221,\n",
      "        5.6221, 4.5443, 3.6109, 2.7790, 2.0194, 1.3118, 0.6421]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9589, 0.9574, 0.9638, 0.9603, 0.9687, 0.9671, 0.9659, 0.9609, 0.9606,\n",
      "        0.9646, 0.9645, 0.9679, 0.9693, 0.9613, 0.9680, 0.9688, 0.9643, 0.9729,\n",
      "        0.9797, 0.9749, 0.9883, 0.9855, 0.9864, 0.9523, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([1.4313, 1.5154, 1.4541, 1.2907, 1.0544, 0.7657, 0.4389])\n",
      "----------------------------------------\n",
      "iter  1  stage  17  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0674, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.2397, 6.2397, 6.2397, 6.2397, 6.2397, 6.2397, 6.2397, 6.2397, 6.2397,\n",
      "        6.2397, 6.2397, 6.2397, 6.2397, 6.2397, 6.2397, 6.2397, 6.2397, 6.2397,\n",
      "        5.1632, 4.2307, 3.3994, 2.6402, 1.9330, 1.2634, 0.6215]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9847, 0.9841, 0.9869, 0.9858, 0.9886, 0.9888, 0.9882, 0.9863, 0.9859,\n",
      "        0.9873, 0.9876, 0.9891, 0.9897, 0.9864, 0.9889, 0.9893, 0.9873, 0.9919,\n",
      "        0.9960, 0.9936, 0.9962, 0.9956, 0.9956, 0.9809, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([1.8061, 1.8902, 1.8290, 1.6657, 1.4295, 1.1409, 0.8141, 0.4594])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  16  ep  85058   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0069, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.8407, 6.8407, 6.8407, 6.8407, 6.8407, 6.8407, 6.8407, 6.8407, 6.8407,\n",
      "        6.8407, 6.8407, 6.8407, 6.8407, 6.8407, 6.8407, 6.8407, 6.8407, 5.7658,\n",
      "        4.8345, 4.0040, 3.2454, 2.5386, 1.8694, 1.2278, 0.6064]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9980, 0.9978, 0.9983, 0.9983, 0.9986, 0.9987, 0.9986, 0.9983, 0.9983,\n",
      "        0.9985, 0.9986, 0.9988, 0.9989, 0.9985, 0.9988, 0.9988, 0.9990, 0.9992,\n",
      "        0.9998, 0.9997, 1.0000, 0.9997, 0.9997, 0.9987, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([2.1959, 2.2800, 2.2189, 2.0557, 1.8196, 1.5311, 1.2044, 0.8498, 0.4745])\n",
      "----------------------------------------\n",
      "iter  1  stage  15  ep  2201   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0087, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.4283, 7.4283, 7.4283, 7.4283, 7.4283, 7.4283, 7.4283, 7.4283, 7.4283,\n",
      "        7.4283, 7.4283, 7.4283, 7.4283, 7.4283, 7.4283, 7.4283, 6.3558, 5.4260,\n",
      "        4.5967, 3.8389, 3.1326, 2.4638, 1.8225, 1.2013, 0.5951]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9983, 0.9982, 0.9985, 0.9985, 0.9988, 0.9989, 0.9988, 0.9985, 0.9985,\n",
      "        0.9987, 0.9988, 0.9990, 0.9991, 0.9987, 0.9990, 0.9990, 0.9993, 0.9993,\n",
      "        0.9999, 0.9998, 1.0000, 0.9998, 0.9998, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([2.5967, 2.6808, 2.6198, 2.4567, 2.2208, 1.9324, 1.6059, 1.2513, 0.8762,\n",
      "        0.4858])\n",
      "----------------------------------------\n",
      "iter  1  stage  14  ep  15   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0127, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.0050, 8.0050, 8.0050, 8.0050, 8.0050, 8.0050, 8.0050, 8.0050, 8.0050,\n",
      "        8.0050, 8.0050, 8.0050, 8.0050, 8.0050, 8.0050, 6.9354, 6.0078, 5.1799,\n",
      "        4.4232, 3.7177, 3.0495, 2.4086, 1.7877, 1.1817, 0.5868]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9983, 0.9982, 0.9986, 0.9986, 0.9988, 0.9989, 0.9988, 0.9985, 0.9985,\n",
      "        0.9987, 0.9988, 0.9990, 0.9991, 0.9987, 0.9990, 0.9990, 0.9993, 0.9993,\n",
      "        0.9999, 0.9998, 1.0000, 0.9998, 0.9998, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([3.0055, 3.0896, 3.0288, 2.8659, 2.6302, 2.3420, 2.0156, 1.6611, 1.2861,\n",
      "        0.8958, 0.4942])\n",
      "----------------------------------------\n",
      "iter  1  stage  13  ep  280   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0172, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.5720, 8.5720, 8.5720, 8.5720, 8.5720, 8.5720, 8.5720, 8.5720, 8.5720,\n",
      "        8.5720, 8.5720, 8.5720, 8.5720, 8.5720, 7.5064, 6.5815, 5.7556, 5.0003,\n",
      "        4.2958, 3.6284, 2.9880, 2.3676, 1.7619, 1.1671, 0.5805]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9983, 0.9981, 0.9985, 0.9985, 0.9988, 0.9989, 0.9988, 0.9985, 0.9985,\n",
      "        0.9987, 0.9988, 0.9990, 0.9991, 0.9990, 0.9991, 0.9990, 0.9993, 0.9993,\n",
      "        0.9999, 0.9998, 1.0000, 0.9998, 0.9998, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([3.4201, 3.5042, 3.4436, 3.2810, 3.0455, 2.7576, 2.4313, 2.0771, 1.7022,\n",
      "        1.3119, 0.9104, 0.5004])\n",
      "----------------------------------------\n",
      "iter  1  stage  12  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0222, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.1296, 9.1296, 9.1296, 9.1296, 9.1296, 9.1296, 9.1296, 9.1296, 9.1296,\n",
      "        9.1296, 9.1296, 9.1296, 9.1296, 8.0693, 7.1481, 6.3249, 5.5714, 4.8684,\n",
      "        4.2019, 3.5622, 2.9423, 2.3370, 1.7426, 1.1562, 0.5759]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9983, 0.9981, 0.9985, 0.9985, 0.9988, 0.9989, 0.9988, 0.9985, 0.9985,\n",
      "        0.9987, 0.9988, 0.9990, 0.9991, 0.9990, 0.9991, 0.9990, 0.9993, 0.9993,\n",
      "        0.9999, 0.9998, 1.0000, 0.9998, 0.9998, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([3.8386, 3.9227, 3.8624, 3.7002, 3.4651, 3.1775, 2.8515, 2.4975, 2.1227,\n",
      "        1.7327, 1.3312, 0.9213, 0.5051])\n",
      "----------------------------------------\n",
      "iter  1  stage  11  ep  2   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0279, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.6775, 9.6775, 9.6775, 9.6775, 9.6775, 9.6775, 9.6775, 9.6775, 9.6775,\n",
      "        9.6775, 9.6775, 9.6775, 8.6243, 7.7080, 6.8883, 6.1374, 5.4361, 4.7710,\n",
      "        4.1323, 3.5131, 2.9084, 2.3143, 1.7282, 1.1481, 0.5724]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9983, 0.9981, 0.9985, 0.9985, 0.9988, 0.9989, 0.9988, 0.9985, 0.9985,\n",
      "        0.9987, 0.9988, 0.9990, 0.9991, 0.9990, 0.9991, 0.9990, 0.9993, 0.9993,\n",
      "        0.9999, 0.9998, 1.0000, 0.9998, 0.9998, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([4.2598, 4.3439, 4.2840, 4.1223, 3.8877, 3.6005, 3.2749, 2.9211, 2.5466,\n",
      "        2.1567, 1.7554, 1.3456, 0.9294, 0.5085])\n",
      "----------------------------------------\n",
      "iter  1  stage  10  ep  14016   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0265, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.2146, 10.2146, 10.2146, 10.2146, 10.2146, 10.2146, 10.2146, 10.2146,\n",
      "        10.2146, 10.2146, 10.2146,  9.1707,  8.2611,  7.4460,  6.6984,  5.9995,\n",
      "         5.3362,  4.6988,  4.0806,  3.4765,  2.8830,  2.2973,  1.7175,  1.1420,\n",
      "         0.5698]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9982, 0.9981, 0.9985, 0.9985, 0.9987, 0.9989, 0.9987, 0.9984, 0.9984,\n",
      "        0.9987, 0.9990, 1.0000, 0.9997, 0.9990, 0.9990, 0.9989, 0.9993, 0.9993,\n",
      "        1.0000, 0.9998, 1.0000, 0.9998, 0.9998, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([4.6825, 4.7666, 4.7072, 4.5461, 4.3122, 4.0255, 3.7004, 3.3470, 2.9728,\n",
      "        2.5832, 2.1820, 1.7724, 1.3563, 0.9355, 0.5111])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  9  ep  432   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0336, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.7388, 10.7388, 10.7388, 10.7388, 10.7388, 10.7388, 10.7388, 10.7388,\n",
      "        10.7388, 10.7388,  9.7074,  8.8064,  7.9975,  7.2544,  6.5588,  5.8977,\n",
      "         5.2621,  4.6452,  4.0421,  3.4493,  2.8641,  2.2847,  1.7095,  1.1375,\n",
      "         0.5678]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9981, 0.9980, 0.9984, 0.9984, 0.9987, 0.9988, 0.9987, 0.9984, 0.9984,\n",
      "        0.9990, 0.9989, 1.0000, 0.9997, 0.9990, 0.9990, 0.9988, 0.9993, 0.9993,\n",
      "        1.0000, 0.9998, 1.0000, 0.9997, 0.9998, 0.9988, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([5.1056, 5.1897, 5.1309, 4.9707, 4.7377, 4.4518, 4.1273, 3.7745, 3.4007,\n",
      "        3.0113, 2.6104, 2.2010, 1.7851, 1.3643, 0.9401, 0.5131])\n",
      "----------------------------------------\n",
      "iter  1  stage  8  ep  6551   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0253, grad_fn=<NegBackward0>)   ,  base rewards= tensor([11.2472, 11.2472, 11.2472, 11.2472, 11.2472, 11.2472, 11.2472, 11.2472,\n",
      "        11.2472, 10.2322,  9.3428,  8.5420,  7.8048,  7.1135,  6.4556,  5.8223,\n",
      "         5.2071,  4.6052,  4.0134,  3.4289,  2.8500,  2.2752,  1.7035,  1.1341,\n",
      "         0.5664]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9987, 0.9987, 0.9989, 0.9990, 0.9991, 0.9992, 0.9991, 0.9989, 0.9990,\n",
      "        0.9995, 0.9994, 1.0000, 0.9998, 0.9994, 0.9994, 0.9993, 0.9997, 0.9996,\n",
      "        1.0000, 0.9999, 1.0000, 0.9999, 0.9999, 0.9993, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([5.5282, 5.6123, 5.5544, 5.3953, 5.1634, 4.8785, 4.5549, 4.2028, 3.8295,\n",
      "        3.4406, 3.0400, 2.6308, 2.2151, 1.7945, 1.3704, 0.9435, 0.5145])\n",
      "----------------------------------------\n",
      "iter  1  stage  7  ep  30   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0320, grad_fn=<NegBackward0>)   ,  base rewards= tensor([11.7354, 11.7354, 11.7354, 11.7354, 11.7354, 11.7354, 11.7354, 11.7354,\n",
      "        10.7421,  9.8680,  9.0781,  8.3488,  7.6631,  7.0094,  6.3791,  5.7662,\n",
      "         5.1660,  4.5754,  3.9919,  3.4137,  2.8394,  2.2681,  1.6990,  1.1315,\n",
      "         0.5653]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9988, 0.9987, 0.9989, 0.9990, 0.9992, 0.9992, 0.9991, 0.9990, 0.9990,\n",
      "        0.9995, 0.9994, 1.0000, 0.9998, 0.9994, 0.9994, 0.9993, 0.9997, 0.9996,\n",
      "        1.0000, 0.9999, 1.0000, 0.9999, 0.9999, 0.9993, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([5.9491, 6.0332, 5.9765, 5.8190, 5.5886, 5.3051, 4.9826, 4.6314, 4.2589,\n",
      "        3.8705, 3.4704, 3.0615, 2.6460, 2.2257, 1.8016, 1.3749, 0.9460, 0.5156])\n",
      "----------------------------------------\n",
      "iter  1  stage  6  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0387, grad_fn=<NegBackward0>)   ,  base rewards= tensor([12.1977, 12.1977, 12.1977, 12.1977, 12.1977, 12.1977, 12.1977, 11.2331,\n",
      "        10.3791,  9.6037,  8.8847,  8.2066,  7.5584,  6.9322,  6.3223,  5.7244,\n",
      "         5.1354,  4.5532,  3.9759,  3.4023,  2.8315,  2.2627,  1.6956,  1.1296,\n",
      "         0.5645]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9988, 0.9987, 0.9989, 0.9990, 0.9992, 0.9992, 0.9991, 0.9990, 0.9990,\n",
      "        0.9995, 0.9994, 1.0000, 0.9998, 0.9994, 0.9994, 0.9993, 0.9997, 0.9996,\n",
      "        1.0000, 0.9999, 1.0000, 0.9999, 0.9999, 0.9993, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([6.3673, 6.4514, 6.3962, 6.2408, 6.0125, 5.7308, 5.4098, 5.0598, 4.6882,\n",
      "        4.3007, 3.9011, 3.4927, 3.0775, 2.6574, 2.2336, 1.8070, 1.3782, 0.9479,\n",
      "        0.5165])\n",
      "----------------------------------------\n",
      "iter  1  stage  5  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0451, grad_fn=<NegBackward0>)   ,  base rewards= tensor([12.6268, 12.6268, 12.6268, 12.6268, 12.6268, 12.6268, 11.6997, 10.8722,\n",
      "        10.1157,  9.4104,  8.7424,  8.1016,  7.4808,  6.8749,  6.2799,  5.6932,\n",
      "         5.1126,  4.5365,  3.9639,  3.3937,  2.8255,  2.2588,  1.6931,  1.1282,\n",
      "         0.5639]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9988, 0.9987, 0.9990, 0.9990, 0.9992, 0.9992, 0.9991, 0.9990, 0.9990,\n",
      "        0.9995, 0.9994, 1.0000, 0.9998, 0.9994, 0.9994, 0.9993, 0.9997, 0.9996,\n",
      "        1.0000, 0.9999, 1.0000, 0.9999, 0.9999, 0.9993, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([6.7813, 6.8654, 6.8123, 6.6596, 6.4340, 6.1547, 5.8358, 5.4874, 5.1172,\n",
      "        4.7306, 4.3318, 3.9240, 3.5093, 3.0895, 2.6660, 2.2395, 1.8109, 1.3808,\n",
      "        0.9494, 0.5171])\n",
      "----------------------------------------\n",
      "iter  1  stage  4  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0525, grad_fn=<NegBackward0>)   ,  base rewards= tensor([13.0129, 13.0129, 13.0129, 13.0129, 13.0129, 12.1346, 11.3419, 10.6103,\n",
      "         9.9232,  9.2683,  8.6372,  8.0237,  7.4231,  6.8321,  6.2483,  5.6699,\n",
      "         5.0955,  4.5241,  3.9549,  3.3874,  2.8211,  2.2558,  1.6912,  1.1271,\n",
      "         0.5634]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9988, 0.9987, 0.9990, 0.9990, 0.9992, 0.9992, 0.9991, 0.9990, 0.9990,\n",
      "        0.9995, 0.9994, 1.0000, 0.9998, 0.9994, 0.9994, 0.9993, 0.9997, 0.9996,\n",
      "        1.0000, 0.9999, 1.0000, 0.9999, 0.9999, 0.9993, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([7.1893, 7.2734, 7.2232, 7.0741, 6.8522, 6.5761, 6.2599, 5.9137, 5.5452,\n",
      "        5.1599, 4.7622, 4.3551, 3.9411, 3.5218, 3.0985, 2.6724, 2.2440, 1.8139,\n",
      "        1.3826, 0.9504, 0.5175])\n",
      "----------------------------------------\n",
      "iter  1  stage  3  ep  14   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0604, grad_fn=<NegBackward0>)   ,  base rewards= tensor([13.3441, 13.3441, 13.3441, 13.3441, 12.5289, 11.7813, 11.0824, 10.4190,\n",
      "         9.7816,  9.1633,  8.5593,  7.9657,  7.3800,  6.8002,  6.2247,  5.6525,\n",
      "         5.0827,  4.5147,  3.9481,  3.3826,  2.8178,  2.2535,  1.6897,  1.1263,\n",
      "         0.5630]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9988, 0.9987, 0.9990, 0.9990, 0.9992, 0.9993, 0.9992, 0.9990, 0.9990,\n",
      "        0.9995, 0.9995, 1.0000, 0.9998, 0.9994, 0.9994, 0.9993, 0.9997, 0.9996,\n",
      "        1.0000, 0.9999, 1.0000, 0.9999, 0.9999, 0.9993, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([7.5892, 7.6733, 7.6268, 7.4827, 7.2655, 6.9938, 6.6811, 6.3379, 5.9716,\n",
      "        5.5882, 5.1918, 4.7858, 4.3726, 3.9539, 3.5311, 3.1053, 2.6771, 2.2473,\n",
      "        1.8162, 1.3841, 0.9512, 0.5179])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  2  ep  19   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0680, grad_fn=<NegBackward0>)   ,  base rewards= tensor([13.6060, 13.6060, 13.6060, 12.8713, 12.1817, 11.5251, 10.8928, 10.2782,\n",
      "         9.6769,  9.0855,  8.5013,  7.9226,  7.3479,  6.7763,  6.2070,  5.6394,\n",
      "         5.0731,  4.5077,  3.9431,  3.3790,  2.8153,  2.2519,  1.6887,  1.1257,\n",
      "         0.5628]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9988, 0.9987, 0.9990, 0.9990, 0.9992, 0.9993, 0.9992, 0.9991, 0.9990,\n",
      "        0.9995, 0.9995, 1.0000, 0.9998, 0.9994, 0.9994, 0.9993, 0.9997, 0.9996,\n",
      "        1.0000, 0.9999, 1.0000, 0.9999, 0.9999, 0.9993, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([7.9780, 8.0621, 8.0205, 7.8829, 7.6723, 7.4063, 7.0984, 6.7590, 6.3958,\n",
      "        6.0148, 5.6203, 5.2157, 4.8035, 4.3856, 3.9635, 3.5381, 3.1103, 2.6807,\n",
      "        2.2498, 1.8178, 1.3851, 0.9518, 0.5181])\n",
      "----------------------------------------\n",
      "iter  1  stage  1  ep  2390   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0622, grad_fn=<NegBackward0>)   ,  base rewards= tensor([13.7821, 13.7821, 13.1481, 12.5324, 11.9302, 11.3380, 10.7533, 10.1743,\n",
      "         9.5993,  9.0276,  8.4581,  7.8904,  7.3239,  6.7585,  6.1938,  5.6297,\n",
      "         5.0659,  4.5025,  3.9393,  3.3763,  2.8134,  2.2506,  1.6879,  1.1252,\n",
      "         0.5626]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9989, 0.9990, 0.9991, 0.9992, 0.9994, 0.9997, 0.9997, 0.9993, 0.9991,\n",
      "        0.9995, 0.9995, 1.0000, 0.9999, 0.9995, 0.9995, 0.9994, 0.9998, 0.9996,\n",
      "        1.0000, 0.9999, 1.0000, 0.9999, 0.9999, 0.9994, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([8.3518, 8.4359, 8.4010, 8.2721, 8.0700, 7.8117, 7.5102, 7.1760, 6.8169,\n",
      "        6.4390, 6.0470, 5.6443, 5.2335, 4.8167, 4.3954, 3.9706, 3.5433, 3.1141,\n",
      "        2.6834, 2.2517, 1.8191, 1.3859, 0.9523, 0.5183])\n",
      "----------------------------------------\n",
      "iter  1  stage  0  ep  100   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0722, grad_fn=<NegBackward0>)   ,  base rewards= tensor([13.8555, 13.3442, 12.8204, 12.2871, 11.7465, 11.2005, 10.6504, 10.0972,\n",
      "         9.5417,  8.9844,  8.4258,  7.8663,  7.3060,  6.7452,  6.1839,  5.6224,\n",
      "         5.0606,  4.4986,  3.9365,  3.3743,  2.8120,  2.2497,  1.6873,  1.1249,\n",
      "         0.5624]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9990, 0.9990, 0.9991, 0.9992, 0.9994, 0.9997, 0.9997, 0.9993, 0.9991,\n",
      "        0.9995, 0.9995, 1.0000, 0.9999, 0.9995, 0.9995, 0.9994, 0.9998, 0.9996,\n",
      "        1.0000, 0.9999, 1.0000, 0.9999, 0.9999, 0.9994, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([8.7056, 8.7897, 8.7636, 8.6463, 8.4558, 8.2076, 7.9147, 7.5873, 7.2337,\n",
      "        6.8601, 6.4713, 6.0711, 5.6622, 5.2469, 4.8266, 4.4027, 3.9760, 3.5472,\n",
      "        3.1169, 2.6854, 2.2531, 1.8200, 1.3865, 0.9526, 0.5185])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1682506150 saved\n",
      "[1071125, 'tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])', 225610.63312213228, 21262.174331621536, 0.07217718660831451, 1e-05, 1, 0, 'tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\\n        29, 29, 29, 29, 29, 29,  0])', '[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\\n 1.]', '0,[1e-05,1][1, 10000, 1, 1],1682506150', 25, 50, 174060.25019204617, 225610.63312213225, 94497.47055325202, 131154.89666666667, 128142.59199999999, 65231.33608163523, 65242.24251310914, 80629.96159612676, 80629.96159612676, 109128.78816076377, 65242.24251310914, 80629.96159612676]\n",
      "policy reset\n",
      "----------------------------------------\n",
      "iter  2  stage  24  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 1, 0, 0, 4, 0, 0, 0,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5700, 0.5700, 0.5700, 0.5700, 0.5700, 0.5700, 0.5700, 0.5700, 0.5700,\n",
      "        0.5700, 0.5700, 0.5700, 0.5700, 0.5700, 0.5700, 0.5700, 0.5700, 0.5700,\n",
      "        0.5700, 0.5700, 0.5700, 0.5700, 0.5700, 0.5700, 0.5700]) return=  139801.3687034911\n",
      "probs of actions:  tensor([0.0586, 0.8760, 0.8988, 0.8761, 0.8584, 0.9093, 0.8702, 0.8950, 0.8991,\n",
      "        0.8972, 0.9039, 0.0053, 0.8843, 0.0620, 0.8891, 0.8950, 0.8928, 0.0581,\n",
      "        0.9003, 0.8979, 0.0050, 0.9160, 0.9029, 0.8696, 0.9885],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5111, 0.5274, 0.5361, 0.5426, 0.5476, 0.5513, 0.5541, 0.5562, 0.5578,\n",
      "        0.5589, 0.5598, 0.5596, 0.5723, 0.5697, 0.5718, 0.5694, 0.5677, 0.5663,\n",
      "        0.5692, 0.5675, 0.5647, 0.5805, 0.5759, 0.5726, 0.5700])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  2  stage  23  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([22,  0,  0, 15, 13,  0, 11,  0,  0,  0, 20,  5,  3,  0,  3,  0, 26, 25,\n",
      "         0, 11,  0,  6,  2, 12,  0])\n",
      "loss=  tensor(0.1463, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2831, 1.2831, 1.2831, 1.2831, 1.2831, 1.2831, 1.2831, 1.2831, 1.2831,\n",
      "        1.2831, 1.2831, 1.2831, 1.2831, 1.2831, 1.2831, 1.2831, 1.2831, 1.2831,\n",
      "        1.2831, 1.2831, 1.2831, 1.2831, 1.2831, 1.2831, 0.6299]) return=  159276.00506941756\n",
      "probs of actions:  tensor([0.0025, 0.2966, 0.3466, 0.0218, 0.0535, 0.3252, 0.0605, 0.3943, 0.3766,\n",
      "        0.3660, 0.0257, 0.0161, 0.0400, 0.3518, 0.0500, 0.3720, 0.0043, 0.0210,\n",
      "        0.4237, 0.0692, 0.2962, 0.0161, 0.0380, 0.0140, 0.9845],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.6065, 0.5953, 0.5645, 0.6225, 0.6719, 0.6316, 0.6670, 0.6401,\n",
      "        0.6202, 0.5655, 0.6717, 0.6647, 0.6510, 0.6274, 0.6233, 0.5402, 0.6384,\n",
      "        0.7707, 0.7035, 0.7216, 0.6763, 0.6735, 0.6388, 0.6784])\n",
      "finalReturns:  tensor([0.0341, 0.0485])\n",
      "----------------------------------------\n",
      "iter  2  stage  22  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([ 1, 23,  0, 24,  0, 23, 25,  0,  0,  1, 23, 23,  1, 23, 31, 20,  0, 25,\n",
      "        23, 15, 20, 18, 23, 23,  0])\n",
      "loss=  tensor(0.1068, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.3746, 2.3746, 2.3746, 2.3746, 2.3746, 2.3746, 2.3746, 2.3746, 2.3746,\n",
      "        2.3746, 2.3746, 2.3746, 2.3746, 2.3746, 2.3746, 2.3746, 2.3746, 2.3746,\n",
      "        2.3746, 2.3746, 2.3746, 2.3746, 2.3746, 1.5088, 0.7250]) return=  180250.60124621165\n",
      "probs of actions:  tensor([0.0518, 0.3167, 0.1523, 0.0385, 0.0575, 0.4545, 0.1042, 0.2132, 0.0763,\n",
      "        0.1119, 0.3796, 0.3270, 0.1043, 0.3741, 0.0090, 0.0816, 0.1816, 0.1185,\n",
      "        0.2634, 0.0165, 0.0844, 0.0138, 0.7915, 0.7690, 0.9828],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5111, 0.4745, 0.6236, 0.5504, 0.6928, 0.6060, 0.6665, 0.7927, 0.7315,\n",
      "        0.6871, 0.6060, 0.6760, 0.7837, 0.6764, 0.6880, 0.8233, 0.8730, 0.7265,\n",
      "        0.7865, 0.8464, 0.8140, 0.8336, 0.8128, 0.8360, 0.9065])\n",
      "finalReturns:  tensor([0.1808, 0.2337, 0.1815])\n",
      "----------------------------------------\n",
      "iter  2  stage  21  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([31, 23, 25, 25, 25, 25, 23, 25, 23, 23, 23, 25, 25, 23, 25, 23, 25, 25,\n",
      "        27, 25, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(0.7501, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.4017, 3.4017, 3.4017, 3.4017, 3.4017, 3.4017, 3.4017, 3.4017, 3.4017,\n",
      "        3.4017, 3.4017, 3.4017, 3.4017, 3.4017, 3.4017, 3.4017, 3.4017, 3.4017,\n",
      "        3.4017, 3.4017, 3.4017, 3.4017, 2.4026, 1.5243, 0.7315]) return=  213022.8652278334\n",
      "probs of actions:  tensor([0.0111, 0.2492, 0.4587, 0.4976, 0.4984, 0.5060, 0.2525, 0.4668, 0.3060,\n",
      "        0.2324, 0.2723, 0.5297, 0.4819, 0.2750, 0.5488, 0.2651, 0.5455, 0.4659,\n",
      "        0.0139, 0.5334, 0.5165, 0.6597, 0.5791, 0.4736, 0.9465],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4151, 0.5891, 0.6531, 0.7198, 0.7717, 0.8117, 0.8520, 0.8561, 0.8857,\n",
      "        0.8912, 0.8952, 0.8887, 0.9008, 0.9195, 0.9069, 0.9241, 0.9103, 0.9171,\n",
      "        0.9118, 0.9360, 0.9363, 0.9366, 0.9369, 0.9370, 0.9996])\n",
      "finalReturns:  tensor([0.4084, 0.4709, 0.4124, 0.2681])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  2  stage  20  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 25, 24, 25, 25, 31, 23, 25, 23, 24, 25, 29, 25, 25, 25, 25, 23, 25,\n",
      "        25, 25, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(0.3533, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.0790, 4.0790, 4.0790, 4.0790, 4.0790, 4.0790, 4.0790, 4.0790, 4.0790,\n",
      "        4.0790, 4.0790, 4.0790, 4.0790, 4.0790, 4.0790, 4.0790, 4.0790, 4.0790,\n",
      "        4.0790, 4.0790, 4.0790, 3.0833, 2.2075, 1.4165, 0.6863]) return=  214816.26798917833\n",
      "probs of actions:  tensor([0.0574, 0.6796, 0.0284, 0.7525, 0.7721, 0.0030, 0.1141, 0.6917, 0.1161,\n",
      "        0.0350, 0.8065, 0.0533, 0.7493, 0.7699, 0.8023, 0.8069, 0.0802, 0.7284,\n",
      "        0.7459, 0.8124, 0.8481, 0.9153, 0.8995, 0.8177, 0.9781],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5715, 0.6601, 0.7170, 0.7696, 0.7765, 0.8795, 0.8768, 0.9014,\n",
      "        0.8982, 0.8994, 0.8872, 0.9358, 0.9363, 0.9366, 0.9368, 0.9466, 0.9271,\n",
      "        0.9297, 0.9317, 0.9331, 0.9342, 0.9350, 0.9357, 0.9986])\n",
      "finalReturns:  tensor([0.6577, 0.7202, 0.6618, 0.5178, 0.3124])\n",
      "----------------------------------------\n",
      "iter  2  stage  19  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 25, 25, 25, 25, 29, 29, 27, 29, 25, 29, 29, 25, 25, 29, 25, 25,\n",
      "        25, 29, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(2.2123, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.7950, 4.7950, 4.7950, 4.7950, 4.7950, 4.7950, 4.7950, 4.7950, 4.7950,\n",
      "        4.7950, 4.7950, 4.7950, 4.7950, 4.7950, 4.7950, 4.7950, 4.7950, 4.7950,\n",
      "        4.7950, 4.7950, 3.7789, 2.8886, 2.0874, 1.3497, 0.6581]) return=  219146.74992654673\n",
      "probs of actions:  tensor([0.3673, 0.4170, 0.5130, 0.5435, 0.5898, 0.5895, 0.4646, 0.4325, 0.0031,\n",
      "        0.4868, 0.5454, 0.4070, 0.3855, 0.5566, 0.5777, 0.3509, 0.5618, 0.4698,\n",
      "        0.4669, 0.3693, 0.5363, 0.7391, 0.8429, 0.8211, 0.9920],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6723, 0.7348, 0.7833, 0.8206, 0.8276, 0.8687, 0.9113,\n",
      "        0.9140, 0.9562, 0.9299, 0.9466, 0.9808, 0.9699, 0.9401, 0.9759, 0.9663,\n",
      "        0.9590, 0.9320, 0.9698, 0.9617, 0.9556, 0.9511, 1.0102])\n",
      "finalReturns:  tensor([0.9853, 1.0694, 0.9899, 0.8295, 0.6115, 0.3521])\n",
      "----------------------------------------\n",
      "iter  2  stage  18  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 25,  0])\n",
      "loss=  tensor(1.4374, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.6221, 5.6221, 5.6221, 5.6221, 5.6221, 5.6221, 5.6221, 5.6221, 5.6221,\n",
      "        5.6221, 5.6221, 5.6221, 5.6221, 5.6221, 5.6221, 5.6221, 5.6221, 5.6221,\n",
      "        5.6221, 4.5443, 3.6109, 2.7790, 2.0194, 1.3118, 0.6421]) return=  225619.69834256533\n",
      "probs of actions:  tensor([0.9429, 0.9464, 0.9397, 0.9376, 0.9270, 0.9454, 0.9617, 0.9620, 0.9437,\n",
      "        0.9725, 0.9606, 0.9671, 0.9557, 0.9405, 0.9506, 0.9565, 0.9554, 0.9611,\n",
      "        0.9799, 0.9698, 0.9807, 0.9492, 0.8315, 0.2472, 0.9998],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 1.0182, 1.0602])\n",
      "finalReturns:  tensor([1.4322, 1.5163, 1.4550, 1.2916, 1.0553, 0.7666, 0.4182])\n",
      "----------------------------------------\n",
      "iter  2  stage  17  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.1282, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.2397, 6.2397, 6.2397, 6.2397, 6.2397, 6.2397, 6.2397, 6.2397, 6.2397,\n",
      "        6.2397, 6.2397, 6.2397, 6.2397, 6.2397, 6.2397, 6.2397, 6.2397, 6.2397,\n",
      "        5.1632, 4.2307, 3.3994, 2.6402, 1.9330, 1.2634, 0.6215]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9887, 0.9903, 0.9890, 0.9885, 0.9857, 0.9906, 0.9939, 0.9941, 0.9898,\n",
      "        0.9961, 0.9934, 0.9951, 0.9932, 0.9894, 0.9918, 0.9928, 0.9927, 0.9959,\n",
      "        0.9982, 0.9969, 0.9978, 0.9934, 0.9720, 0.9220, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([1.8061, 1.8902, 1.8290, 1.6657, 1.4295, 1.1409, 0.8141, 0.4594])\n",
      "----------------------------------------\n",
      "iter  2  stage  16  ep  99999   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.2565, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.8407, 6.8407, 6.8407, 6.8407, 6.8407, 6.8407, 6.8407, 6.8407, 6.8407,\n",
      "        6.8407, 6.8407, 6.8407, 6.8407, 6.8407, 6.8407, 6.8407, 6.8407, 5.7658,\n",
      "        4.8345, 4.0040, 3.2454, 2.5386, 1.8694, 1.2278, 0.6064]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9848, 0.9873, 0.9857, 0.9847, 0.9815, 0.9870, 0.9919, 0.9923, 0.9860,\n",
      "        0.9947, 0.9908, 0.9933, 0.9906, 0.9856, 0.9885, 0.9898, 0.9932, 0.9950,\n",
      "        0.9983, 0.9976, 0.9975, 0.9915, 0.9593, 0.8347, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([2.1959, 2.2800, 2.2189, 2.0557, 1.8196, 1.5311, 1.2044, 0.8498, 0.4745])\n",
      "----------------------------------------\n",
      "iter  2  stage  15  ep  66414   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0475, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.4283, 7.4283, 7.4283, 7.4283, 7.4283, 7.4283, 7.4283, 7.4283, 7.4283,\n",
      "        7.4283, 7.4283, 7.4283, 7.4283, 7.4283, 7.4283, 7.4283, 6.3558, 5.4260,\n",
      "        4.5967, 3.8389, 3.1326, 2.4638, 1.8225, 1.2013, 0.5951]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9969, 0.9974, 0.9972, 0.9971, 0.9963, 0.9978, 0.9985, 0.9986, 0.9973,\n",
      "        0.9991, 0.9984, 0.9989, 0.9985, 0.9973, 0.9980, 0.9990, 0.9994, 0.9998,\n",
      "        1.0000, 0.9998, 1.0000, 0.9991, 0.9926, 0.9648, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([2.5967, 2.6808, 2.6198, 2.4567, 2.2208, 1.9324, 1.6059, 1.2513, 0.8762,\n",
      "        0.4858])\n",
      "----------------------------------------\n",
      "iter  2  stage  14  ep  1469   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0459, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.0050, 8.0050, 8.0050, 8.0050, 8.0050, 8.0050, 8.0050, 8.0050, 8.0050,\n",
      "        8.0050, 8.0050, 8.0050, 8.0050, 8.0050, 8.0050, 6.9354, 6.0078, 5.1799,\n",
      "        4.4232, 3.7177, 3.0495, 2.4086, 1.7877, 1.1817, 0.5868]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9972, 0.9976, 0.9975, 0.9974, 0.9967, 0.9980, 0.9986, 0.9988, 0.9976,\n",
      "        0.9992, 0.9986, 0.9990, 0.9986, 0.9976, 0.9990, 0.9991, 0.9994, 0.9998,\n",
      "        1.0000, 0.9998, 1.0000, 0.9993, 0.9937, 0.9690, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([3.0055, 3.0896, 3.0288, 2.8659, 2.6302, 2.3420, 2.0156, 1.6611, 1.2861,\n",
      "        0.8958, 0.4942])\n",
      "----------------------------------------\n",
      "iter  2  stage  13  ep  39701   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0311, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.5720, 8.5720, 8.5720, 8.5720, 8.5720, 8.5720, 8.5720, 8.5720, 8.5720,\n",
      "        8.5720, 8.5720, 8.5720, 8.5720, 8.5720, 7.5064, 6.5815, 5.7556, 5.0003,\n",
      "        4.2958, 3.6284, 2.9880, 2.3676, 1.7619, 1.1671, 0.5805]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9982, 0.9985, 0.9985, 0.9984, 0.9979, 0.9988, 0.9992, 0.9993, 0.9985,\n",
      "        0.9995, 0.9992, 0.9994, 0.9992, 0.9990, 0.9994, 1.0000, 0.9999, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9998, 0.9960, 0.9792, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([3.4201, 3.5042, 3.4436, 3.2810, 3.0455, 2.7576, 2.4313, 2.0771, 1.7022,\n",
      "        1.3119, 0.9104, 0.5004])\n",
      "----------------------------------------\n",
      "iter  2  stage  12  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0353, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.1296, 9.1296, 9.1296, 9.1296, 9.1296, 9.1296, 9.1296, 9.1296, 9.1296,\n",
      "        9.1296, 9.1296, 9.1296, 9.1296, 8.0693, 7.1481, 6.3249, 5.5714, 4.8684,\n",
      "        4.2019, 3.5622, 2.9423, 2.3370, 1.7426, 1.1562, 0.5759]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9982, 0.9985, 0.9985, 0.9984, 0.9979, 0.9988, 0.9992, 0.9993, 0.9985,\n",
      "        0.9995, 0.9992, 0.9994, 0.9992, 0.9990, 0.9994, 1.0000, 0.9999, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9998, 0.9960, 0.9792, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([3.8386, 3.9227, 3.8624, 3.7002, 3.4651, 3.1775, 2.8515, 2.4975, 2.1227,\n",
      "        1.7327, 1.3312, 0.9213, 0.5051])\n",
      "----------------------------------------\n",
      "iter  2  stage  11  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0389, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.6775, 9.6775, 9.6775, 9.6775, 9.6775, 9.6775, 9.6775, 9.6775, 9.6775,\n",
      "        9.6775, 9.6775, 9.6775, 8.6243, 7.7080, 6.8883, 6.1374, 5.4361, 4.7710,\n",
      "        4.1323, 3.5131, 2.9084, 2.3143, 1.7282, 1.1481, 0.5724]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9982, 0.9985, 0.9985, 0.9984, 0.9979, 0.9988, 0.9992, 0.9993, 0.9985,\n",
      "        0.9995, 0.9992, 0.9994, 0.9992, 0.9990, 0.9994, 1.0000, 0.9999, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9998, 0.9960, 0.9792, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([4.2598, 4.3439, 4.2840, 4.1223, 3.8877, 3.6005, 3.2749, 2.9211, 2.5466,\n",
      "        2.1567, 1.7554, 1.3456, 0.9294, 0.5085])\n",
      "----------------------------------------\n",
      "iter  2  stage  10  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0440, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.2146, 10.2146, 10.2146, 10.2146, 10.2146, 10.2146, 10.2146, 10.2146,\n",
      "        10.2146, 10.2146, 10.2146,  9.1707,  8.2611,  7.4460,  6.6984,  5.9995,\n",
      "         5.3362,  4.6988,  4.0806,  3.4765,  2.8830,  2.2973,  1.7175,  1.1420,\n",
      "         0.5698]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9982, 0.9985, 0.9985, 0.9984, 0.9980, 0.9988, 0.9992, 0.9993, 0.9985,\n",
      "        0.9995, 0.9992, 0.9994, 0.9992, 0.9990, 0.9994, 1.0000, 0.9999, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9998, 0.9961, 0.9792, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([4.6825, 4.7666, 4.7072, 4.5461, 4.3122, 4.0255, 3.7004, 3.3470, 2.9728,\n",
      "        2.5832, 2.1820, 1.7724, 1.3563, 0.9355, 0.5111])\n",
      "----------------------------------------\n",
      "iter  2  stage  9  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0476, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.7388, 10.7388, 10.7388, 10.7388, 10.7388, 10.7388, 10.7388, 10.7388,\n",
      "        10.7388, 10.7388,  9.7074,  8.8064,  7.9975,  7.2544,  6.5588,  5.8977,\n",
      "         5.2621,  4.6452,  4.0421,  3.4493,  2.8641,  2.2847,  1.7095,  1.1375,\n",
      "         0.5678]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9982, 0.9985, 0.9985, 0.9984, 0.9980, 0.9988, 0.9992, 0.9993, 0.9985,\n",
      "        0.9995, 0.9992, 0.9994, 0.9992, 0.9990, 0.9994, 1.0000, 0.9999, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9998, 0.9961, 0.9792, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([5.1056, 5.1897, 5.1309, 4.9707, 4.7377, 4.4518, 4.1273, 3.7745, 3.4007,\n",
      "        3.0113, 2.6104, 2.2010, 1.7851, 1.3643, 0.9401, 0.5131])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  2  stage  8  ep  12279   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0442, grad_fn=<NegBackward0>)   ,  base rewards= tensor([11.2472, 11.2472, 11.2472, 11.2472, 11.2472, 11.2472, 11.2472, 11.2472,\n",
      "        11.2472, 10.2322,  9.3428,  8.5420,  7.8048,  7.1135,  6.4556,  5.8223,\n",
      "         5.2071,  4.6052,  4.0134,  3.4289,  2.8500,  2.2752,  1.7035,  1.1341,\n",
      "         0.5664]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9982, 0.9985, 0.9984, 0.9984, 0.9979, 0.9988, 0.9991, 0.9993, 0.9990,\n",
      "        1.0000, 1.0000, 0.9997, 0.9994, 0.9990, 0.9994, 1.0000, 0.9999, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9998, 0.9958, 0.9800, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([5.5282, 5.6123, 5.5544, 5.3953, 5.1634, 4.8785, 4.5549, 4.2028, 3.8295,\n",
      "        3.4406, 3.0400, 2.6308, 2.2151, 1.7945, 1.3704, 0.9435, 0.5145])\n",
      "----------------------------------------\n",
      "iter  2  stage  7  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0494, grad_fn=<NegBackward0>)   ,  base rewards= tensor([11.7354, 11.7354, 11.7354, 11.7354, 11.7354, 11.7354, 11.7354, 11.7354,\n",
      "        10.7421,  9.8680,  9.0781,  8.3488,  7.6631,  7.0094,  6.3791,  5.7662,\n",
      "         5.1660,  4.5754,  3.9919,  3.4137,  2.8394,  2.2681,  1.6990,  1.1315,\n",
      "         0.5653]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9982, 0.9985, 0.9984, 0.9984, 0.9979, 0.9988, 0.9991, 0.9993, 0.9990,\n",
      "        1.0000, 1.0000, 0.9997, 0.9994, 0.9990, 0.9994, 1.0000, 0.9999, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9998, 0.9958, 0.9800, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([5.9491, 6.0332, 5.9765, 5.8190, 5.5886, 5.3051, 4.9826, 4.6314, 4.2589,\n",
      "        3.8705, 3.4704, 3.0615, 2.6460, 2.2257, 1.8016, 1.3749, 0.9460, 0.5156])\n",
      "----------------------------------------\n",
      "iter  2  stage  6  ep  0   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0560, grad_fn=<NegBackward0>)   ,  base rewards= tensor([12.1977, 12.1977, 12.1977, 12.1977, 12.1977, 12.1977, 12.1977, 11.2331,\n",
      "        10.3791,  9.6037,  8.8847,  8.2066,  7.5584,  6.9322,  6.3223,  5.7244,\n",
      "         5.1354,  4.5532,  3.9759,  3.4023,  2.8315,  2.2627,  1.6956,  1.1296,\n",
      "         0.5645]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9982, 0.9985, 0.9984, 0.9984, 0.9979, 0.9988, 0.9991, 0.9993, 0.9990,\n",
      "        1.0000, 1.0000, 0.9997, 0.9994, 0.9990, 0.9994, 1.0000, 0.9999, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9998, 0.9958, 0.9800, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([6.3673, 6.4514, 6.3962, 6.2408, 6.0125, 5.7308, 5.4098, 5.0598, 4.6882,\n",
      "        4.3007, 3.9011, 3.4927, 3.0775, 2.6574, 2.2336, 1.8070, 1.3782, 0.9479,\n",
      "        0.5165])\n",
      "----------------------------------------\n",
      "iter  2  stage  5  ep  90   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0596, grad_fn=<NegBackward0>)   ,  base rewards= tensor([12.6268, 12.6268, 12.6268, 12.6268, 12.6268, 12.6268, 11.6997, 10.8722,\n",
      "        10.1157,  9.4104,  8.7424,  8.1016,  7.4808,  6.8749,  6.2799,  5.6932,\n",
      "         5.1126,  4.5365,  3.9639,  3.3937,  2.8255,  2.2588,  1.6931,  1.1282,\n",
      "         0.5639]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9983, 0.9985, 0.9985, 0.9985, 0.9980, 0.9990, 0.9993, 0.9994, 0.9991,\n",
      "        1.0000, 1.0000, 0.9997, 0.9994, 0.9991, 0.9994, 1.0000, 0.9999, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9998, 0.9960, 0.9811, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([6.7813, 6.8654, 6.8123, 6.6596, 6.4340, 6.1547, 5.8358, 5.4874, 5.1172,\n",
      "        4.7306, 4.3318, 3.9240, 3.5093, 3.0895, 2.6660, 2.2395, 1.8109, 1.3808,\n",
      "        0.9494, 0.5171])\n",
      "----------------------------------------\n",
      "iter  2  stage  4  ep  882   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0599, grad_fn=<NegBackward0>)   ,  base rewards= tensor([13.0129, 13.0129, 13.0129, 13.0129, 13.0129, 12.1346, 11.3419, 10.6103,\n",
      "         9.9232,  9.2683,  8.6372,  8.0237,  7.4231,  6.8321,  6.2483,  5.6699,\n",
      "         5.0955,  4.5241,  3.9549,  3.3874,  2.8211,  2.2558,  1.6912,  1.1271,\n",
      "         0.5634]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9984, 0.9987, 0.9986, 0.9986, 0.9990, 0.9991, 0.9995, 0.9995, 0.9992,\n",
      "        1.0000, 1.0000, 0.9997, 0.9995, 0.9992, 0.9995, 1.0000, 0.9999, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9998, 0.9963, 0.9830, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([7.1893, 7.2734, 7.2232, 7.0741, 6.8522, 6.5761, 6.2599, 5.9137, 5.5452,\n",
      "        5.1599, 4.7622, 4.3551, 3.9411, 3.5218, 3.0985, 2.6724, 2.2440, 1.8139,\n",
      "        1.3826, 0.9504, 0.5175])\n",
      "----------------------------------------\n",
      "iter  2  stage  3  ep  26351   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0595, grad_fn=<NegBackward0>)   ,  base rewards= tensor([13.3441, 13.3441, 13.3441, 13.3441, 12.5289, 11.7813, 11.0824, 10.4190,\n",
      "         9.7816,  9.1633,  8.5593,  7.9657,  7.3800,  6.8002,  6.2247,  5.6525,\n",
      "         5.0827,  4.5147,  3.9481,  3.3826,  2.8178,  2.2535,  1.6897,  1.1263,\n",
      "         0.5630]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9984, 0.9986, 0.9986, 0.9990, 0.9992, 0.9989, 0.9997, 0.9997, 0.9993,\n",
      "        1.0000, 1.0000, 1.0000, 0.9998, 0.9993, 0.9996, 1.0000, 0.9999, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 0.9961, 0.9837, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([7.5892, 7.6733, 7.6268, 7.4827, 7.2655, 6.9938, 6.6811, 6.3379, 5.9716,\n",
      "        5.5882, 5.1918, 4.7858, 4.3726, 3.9539, 3.5311, 3.1053, 2.6771, 2.2473,\n",
      "        1.8162, 1.3841, 0.9512, 0.5179])\n",
      "----------------------------------------\n",
      "iter  2  stage  2  ep  289   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 25,  0])\n",
      "loss=  tensor(3.9982, grad_fn=<NegBackward0>)   ,  base rewards= tensor([13.6060, 13.6060, 13.6060, 12.8713, 12.1817, 11.5251, 10.8928, 10.2782,\n",
      "         9.6769,  9.0855,  8.5013,  7.9226,  7.3479,  6.7763,  6.2070,  5.6394,\n",
      "         5.0731,  4.5077,  3.9431,  3.3790,  2.8153,  2.2519,  1.6887,  1.1257,\n",
      "         0.5628]) return=  225619.69834256533\n",
      "probs of actions:  tensor([0.9984, 0.9987, 0.9990, 0.9990, 0.9992, 0.9989, 0.9997, 0.9997, 0.9993,\n",
      "        1.0000, 1.0000, 1.0000, 0.9998, 0.9994, 0.9996, 1.0000, 0.9999, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 0.9961, 0.0159, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 1.0182, 1.0602])\n",
      "finalReturns:  tensor([7.9789, 8.0630, 8.0214, 7.8838, 7.6732, 7.4072, 7.0993, 6.7599, 6.3967,\n",
      "        6.0157, 5.6212, 5.2166, 4.8044, 4.3865, 3.9644, 3.5390, 3.1112, 2.6816,\n",
      "        2.2507, 1.8188, 1.3860, 0.9528, 0.4974])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  2  stage  1  ep  7933   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([25, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0748, grad_fn=<NegBackward0>)   ,  base rewards= tensor([13.7203, 13.7203, 13.1021, 12.4981, 11.9046, 11.3189, 10.7391, 10.1636,\n",
      "         9.5914,  9.0216,  8.4536,  7.8870,  7.3215,  6.7567,  6.1925,  5.6287,\n",
      "         5.0652,  4.5020,  3.9389,  3.3760,  2.8132,  2.2505,  1.6878,  1.1252,\n",
      "         0.5626]) return=  225109.1821149025\n",
      "probs of actions:  tensor([0.0017, 0.9990, 0.9990, 0.9991, 0.9993, 0.9989, 0.9997, 0.9999, 0.9993,\n",
      "        1.0000, 1.0000, 1.0000, 0.9999, 0.9995, 0.9996, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 0.9959, 0.9832, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5341, 0.6379, 0.7210, 0.7863, 0.8369, 0.8759, 0.9056, 0.9282,\n",
      "        0.9453, 0.9582, 0.9680, 0.9753, 0.9808, 0.9850, 0.9881, 0.9905, 0.9922,\n",
      "        0.9935, 0.9945, 0.9953, 0.9958, 0.9962, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([8.3419, 8.4260, 8.3922, 8.2647, 8.0641, 7.8069, 7.5065, 7.1732, 6.8147,\n",
      "        6.4374, 6.0458, 5.6433, 5.2328, 4.8162, 4.3950, 3.9703, 3.5431, 3.1139,\n",
      "        2.6833, 2.2516, 1.8191, 1.3859, 0.9523, 0.5183])\n",
      "----------------------------------------\n",
      "iter  2  stage  0  ep  8878   adversary:  AdversaryModes.constant_132\n",
      "  actions:  tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29,  0])\n",
      "loss=  tensor(0.0617, grad_fn=<NegBackward0>)   ,  base rewards= tensor([13.8555, 13.3442, 12.8204, 12.2871, 11.7465, 11.2005, 10.6504, 10.0972,\n",
      "         9.5417,  8.9844,  8.4258,  7.8663,  7.3060,  6.7452,  6.1839,  5.6224,\n",
      "         5.0606,  4.4986,  3.9365,  3.3743,  2.8120,  2.2497,  1.6873,  1.1249,\n",
      "         0.5624]) return=  225610.63312213228\n",
      "probs of actions:  tensor([0.9990, 0.9992, 0.9993, 0.9995, 0.9995, 0.9992, 0.9998, 0.9999, 0.9996,\n",
      "        1.0000, 1.0000, 1.0000, 0.9999, 0.9999, 0.9997, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 0.9972, 0.9874, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4271, 0.5499, 0.6507, 0.7311, 0.7942, 0.8430, 0.8805, 0.9091, 0.9309,\n",
      "        0.9473, 0.9598, 0.9691, 0.9762, 0.9815, 0.9855, 0.9885, 0.9907, 0.9924,\n",
      "        0.9937, 0.9946, 0.9954, 0.9959, 0.9963, 0.9966, 1.0809])\n",
      "finalReturns:  tensor([8.7056, 8.7897, 8.7636, 8.6463, 8.4558, 8.2076, 7.9147, 7.5873, 7.2337,\n",
      "        6.8601, 6.4713, 6.0711, 5.6622, 5.2469, 4.8266, 4.4027, 3.9760, 3.5472,\n",
      "        3.1169, 2.6854, 2.2531, 1.8200, 1.3865, 0.9526, 0.5185])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1682631035 saved\n",
      "[1244302, 'tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])', 225610.63312213228, 21262.174331621536, 0.061683181673288345, 1e-05, 1, 0, 'tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\\n        29, 29, 29, 29, 29, 29,  0])', '[1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\\n 1.   1.   1.   1.   1.   1.   1.   1.   1.   0.99 1.  ]', '0,[1e-05,1][1, 10000, 1, 1],1682631035', 25, 50, 174060.25019204617, 225611.5396441756, 94497.47055325202, 131149.86666666667, 128142.59199999999, 65242.24251310914, 65242.24251310914, 80629.96159612676, 80629.96159612676, 109136.08826110291, 65218.96207421804, 80629.96159612676]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x648 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy reset\n",
      "----------------------------------------\n",
      "iter  0  stage  24  ep  99999   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([ 0,  0, 38,  0,  1,  0,  2,  0,  0,  1,  0,  0,  1,  0,  0,  0,  0,  0,\n",
      "         1,  0,  1,  0,  0,  1,  0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.1482, 0.1482, 0.1482, 0.1482, 0.1482, 0.1482, 0.1482, 0.1482, 0.1482,\n",
      "        0.1482, 0.1482, 0.1482, 0.1482, 0.1482, 0.1482, 0.1482, 0.1482, 0.1482,\n",
      "        0.1482, 0.1482, 0.1482, 0.1482, 0.1482, 0.1482, 0.1482]) return=  51639.70510999265\n",
      "probs of actions:  tensor([8.0034e-01, 8.2695e-01, 3.4396e-04, 8.0664e-01, 1.2781e-01, 7.9923e-01,\n",
      "        1.9496e-02, 7.3233e-01, 7.9346e-01, 1.3482e-01, 7.8419e-01, 7.5348e-01,\n",
      "        1.3699e-01, 7.8668e-01, 7.3267e-01, 7.2963e-01, 7.8819e-01, 7.6218e-01,\n",
      "        1.3528e-01, 7.8927e-01, 1.3168e-01, 7.7874e-01, 7.6106e-01, 1.3310e-01,\n",
      "        9.4036e-01], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.3985, 0.1787, 0.3799, 0.3104, 0.2657, 0.2315, 0.2127, 0.1944,\n",
      "        0.1811, 0.1737, 0.1661, 0.1604, 0.1584, 0.1548, 0.1522, 0.1502, 0.1488,\n",
      "        0.1476, 0.1488, 0.1476, 0.1488, 0.1477, 0.1468, 0.1482])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  0  stage  23  ep  99999   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([ 6,  4,  8,  0,  6,  6,  7,  0, 13,  3,  7,  0,  5,  6,  7,  7,  4,  4,\n",
      "         0,  7,  6,  0,  1,  4,  0])\n",
      "loss=  tensor(0.0107, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.3310, 0.3310, 0.3310, 0.3310, 0.3310, 0.3310, 0.3310, 0.3310, 0.3310,\n",
      "        0.3310, 0.3310, 0.3310, 0.3310, 0.3310, 0.3310, 0.3310, 0.3310, 0.3310,\n",
      "        0.3310, 0.3310, 0.3310, 0.3310, 0.3310, 0.3310, 0.1624]) return=  56826.748788899706\n",
      "probs of actions:  tensor([0.2833, 0.2137, 0.0058, 0.1687, 0.2598, 0.2771, 0.2007, 0.1315, 0.0016,\n",
      "        0.0535, 0.2105, 0.1792, 0.0410, 0.2486, 0.2121, 0.2264, 0.1918, 0.1917,\n",
      "        0.1469, 0.1903, 0.2626, 0.1697, 0.0305, 0.1974, 0.9888],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5076, 0.4160, 0.3413, 0.3105, 0.2595, 0.2411, 0.2265, 0.2240, 0.1856,\n",
      "        0.2153, 0.1987, 0.2033, 0.1852, 0.1834, 0.1837, 0.1871, 0.1930, 0.1884,\n",
      "        0.1865, 0.1706, 0.1785, 0.1849, 0.1742, 0.1670, 0.1706])\n",
      "finalReturns:  tensor([0.0066, 0.0082])\n",
      "----------------------------------------\n",
      "iter  0  stage  22  ep  99999   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([ 7,  7,  7,  7,  7, 14,  7,  7,  7,  7, 12,  7,  7,  6,  7,  7,  7,  7,\n",
      "         7,  7,  7,  7,  7,  7,  0])\n",
      "loss=  tensor(0.0059, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5673, 0.5673, 0.5673, 0.5673, 0.5673, 0.5673, 0.5673, 0.5673, 0.5673,\n",
      "        0.5673, 0.5673, 0.5673, 0.5673, 0.5673, 0.5673, 0.5673, 0.5673, 0.5673,\n",
      "        0.5673, 0.5673, 0.5673, 0.5673, 0.5673, 0.3640, 0.1763]) return=  61396.95122420932\n",
      "probs of actions:  tensor([0.9033, 0.9047, 0.9016, 0.8795, 0.8839, 0.0039, 0.8793, 0.8599, 0.8975,\n",
      "        0.8653, 0.0223, 0.8265, 0.8999, 0.0598, 0.8802, 0.8758, 0.8795, 0.8736,\n",
      "        0.8427, 0.8736, 0.8772, 0.8745, 0.9073, 0.9292, 0.9986],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5063, 0.4160, 0.3540, 0.3107, 0.2801, 0.2435, 0.2600, 0.2436, 0.2317,\n",
      "        0.2229, 0.2069, 0.2234, 0.2168, 0.2132, 0.2060, 0.2039, 0.2023, 0.2011,\n",
      "        0.2002, 0.1996, 0.1991, 0.1987, 0.1984, 0.1982, 0.2030])\n",
      "finalReturns:  tensor([0.0323, 0.0372, 0.0267])\n",
      "----------------------------------------\n",
      "iter  0  stage  21  ep  99999   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([12, 12, 12, 12, 12,  9, 12, 12, 12, 12, 12,  7, 12, 12,  7,  7, 12, 12,\n",
      "        10, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0066, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.8348, 0.8348, 0.8348, 0.8348, 0.8348, 0.8348, 0.8348, 0.8348, 0.8348,\n",
      "        0.8348, 0.8348, 0.8348, 0.8348, 0.8348, 0.8348, 0.8348, 0.8348, 0.8348,\n",
      "        0.8348, 0.8348, 0.8348, 0.8348, 0.5933, 0.3783, 0.1824]) return=  67207.34217323098\n",
      "probs of actions:  tensor([0.9530, 0.9572, 0.9566, 0.9385, 0.9367, 0.0018, 0.9516, 0.9293, 0.9447,\n",
      "        0.9415, 0.9464, 0.0281, 0.9350, 0.9386, 0.0333, 0.0270, 0.9371, 0.9286,\n",
      "        0.0046, 0.9552, 0.9292, 0.9849, 0.9793, 0.9724, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2955, 0.2673, 0.2592, 0.2532,\n",
      "        0.2487, 0.2454, 0.2525, 0.2286, 0.2304, 0.2412, 0.2299, 0.2121, 0.2179,\n",
      "        0.2266, 0.2207, 0.2244, 0.2271, 0.2292, 0.2308, 0.2464])\n",
      "finalReturns:  tensor([0.0988, 0.1132, 0.0989, 0.0641])\n",
      "----------------------------------------\n",
      "iter  0  stage  20  ep  99999   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0034, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332, 1.0332,\n",
      "        1.0332, 1.0332, 1.0332, 0.7825, 0.5612, 0.3606, 0.1749]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9908, 0.9920, 0.9920, 0.9888, 0.9875, 0.9919, 0.9900, 0.9860, 0.9891,\n",
      "        0.9871, 0.9900, 0.9854, 0.9871, 0.9887, 0.9851, 0.9900, 0.9875, 0.9859,\n",
      "        0.9865, 0.9914, 0.9903, 0.9968, 0.9954, 0.9957, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.1613, 0.1757, 0.1609, 0.1255, 0.0753])\n",
      "----------------------------------------\n",
      "iter  0  stage  19  ep  99999   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0025, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009, 1.2009,\n",
      "        1.2009, 1.2009, 0.9500, 0.7285, 0.5277, 0.3420, 0.1670]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9955, 0.9963, 0.9964, 0.9952, 0.9943, 0.9963, 0.9954, 0.9935, 0.9951,\n",
      "        0.9938, 0.9956, 0.9930, 0.9943, 0.9949, 0.9932, 0.9956, 0.9942, 0.9936,\n",
      "        0.9938, 0.9969, 0.9960, 0.9991, 0.9979, 0.9984, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.2301, 0.2445, 0.2297, 0.1944, 0.1441, 0.0832])\n",
      "----------------------------------------\n",
      "iter  0  stage  18  ep  99999   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0020, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631,\n",
      "        1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631, 1.3631,\n",
      "        1.3631, 1.1119, 0.8901, 0.6892, 0.5034, 0.3283, 0.1613]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9976, 0.9981, 0.9982, 0.9974, 0.9970, 0.9981, 0.9975, 0.9964, 0.9973,\n",
      "        0.9966, 0.9977, 0.9961, 0.9969, 0.9973, 0.9962, 0.9976, 0.9969, 0.9965,\n",
      "        0.9972, 0.9989, 0.9987, 0.9997, 0.9990, 0.9994, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.3047, 0.3191, 0.3044, 0.2690, 0.2187, 0.1578, 0.0890])\n",
      "----------------------------------------\n",
      "iter  0  stage  17  ep  45969   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0007, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.5214, 1.5214, 1.5214, 1.5214, 1.5214, 1.5214, 1.5214, 1.5214, 1.5214,\n",
      "        1.5214, 1.5214, 1.5214, 1.5214, 1.5214, 1.5214, 1.5214, 1.5214, 1.5214,\n",
      "        1.2697, 1.0477, 0.8466, 0.6606, 0.4855, 0.3183, 0.1570]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9993, 0.9994, 0.9995, 0.9992, 0.9991, 0.9994, 0.9992, 0.9988, 0.9991,\n",
      "        0.9989, 0.9993, 0.9987, 0.9990, 0.9992, 0.9989, 0.9992, 0.9991, 0.9990,\n",
      "        0.9998, 0.9998, 0.9998, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.3837, 0.3981, 0.3833, 0.3478, 0.2976, 0.2366, 0.1678, 0.0932])\n",
      "----------------------------------------\n",
      "iter  0  stage  16  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0012, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.6770,\n",
      "        1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.6770, 1.4248,\n",
      "        1.2024, 1.0011, 0.8148, 0.6396, 0.4723, 0.3109, 0.1538]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9993, 0.9994, 0.9995, 0.9992, 0.9991, 0.9994, 0.9992, 0.9988, 0.9991,\n",
      "        0.9989, 0.9993, 0.9987, 0.9990, 0.9992, 0.9989, 0.9992, 0.9991, 0.9990,\n",
      "        0.9998, 0.9998, 0.9998, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.4658, 0.4802, 0.4654, 0.4299, 0.3796, 0.3186, 0.2498, 0.1752, 0.0964])\n",
      "----------------------------------------\n",
      "iter  0  stage  15  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0019, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.8309, 1.8309, 1.8309, 1.8309, 1.8309, 1.8309, 1.8309, 1.8309, 1.8309,\n",
      "        1.8309, 1.8309, 1.8309, 1.8309, 1.8309, 1.8309, 1.8309, 1.5780, 1.3551,\n",
      "        1.1534, 0.9669, 0.7914, 0.6240, 0.4625, 0.3054, 0.1515]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9993, 0.9994, 0.9995, 0.9992, 0.9991, 0.9994, 0.9992, 0.9988, 0.9991,\n",
      "        0.9989, 0.9993, 0.9987, 0.9990, 0.9992, 0.9989, 0.9992, 0.9991, 0.9990,\n",
      "        0.9998, 0.9998, 0.9998, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.5504, 0.5648, 0.5499, 0.5144, 0.4641, 0.4030, 0.3342, 0.2596, 0.1807,\n",
      "        0.0987])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  0  stage  14  ep  66   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0025, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.9839, 1.9839, 1.9839, 1.9839, 1.9839, 1.9839, 1.9839, 1.9839, 1.9839,\n",
      "        1.9839, 1.9839, 1.9839, 1.9839, 1.9839, 1.9839, 1.7301, 1.5065, 1.3043,\n",
      "        1.1175, 0.9417, 0.7741, 0.6125, 0.4553, 0.3013, 0.1497]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9995, 0.9992, 0.9991, 0.9994, 0.9992, 0.9989, 0.9992,\n",
      "        0.9989, 0.9993, 0.9987, 0.9991, 0.9992, 0.9990, 0.9993, 0.9992, 0.9990,\n",
      "        0.9998, 0.9998, 0.9998, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.6368, 0.6512, 0.6363, 0.6007, 0.5503, 0.4892, 0.4203, 0.3457, 0.2668,\n",
      "        0.1848, 0.1005])\n",
      "----------------------------------------\n",
      "iter  0  stage  13  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0034, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.1368, 2.1368, 2.1368, 2.1368, 2.1368, 2.1368, 2.1368, 2.1368, 2.1368,\n",
      "        2.1368, 2.1368, 2.1368, 2.1368, 2.1368, 1.8817, 1.6572, 1.4543, 1.2670,\n",
      "        1.0910, 0.9231, 0.7613, 0.6040, 0.4499, 0.2982, 0.1484]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9995, 0.9992, 0.9991, 0.9994, 0.9992, 0.9989, 0.9992,\n",
      "        0.9989, 0.9993, 0.9987, 0.9991, 0.9992, 0.9990, 0.9993, 0.9992, 0.9990,\n",
      "        0.9998, 0.9998, 0.9998, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.7246, 0.7390, 0.7241, 0.6884, 0.6380, 0.5768, 0.5079, 0.4332, 0.3542,\n",
      "        0.2722, 0.1879, 0.1018])\n",
      "----------------------------------------\n",
      "iter  0  stage  12  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0044, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.2903, 2.2903, 2.2903, 2.2903, 2.2903, 2.2903, 2.2903, 2.2903, 2.2903,\n",
      "        2.2903, 2.2903, 2.2903, 2.2903, 2.0334, 1.8077, 1.6040, 1.4161, 1.2396,\n",
      "        1.0714, 0.9093, 0.7518, 0.5976, 0.4458, 0.2960, 0.1475]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9995, 0.9992, 0.9991, 0.9994, 0.9992, 0.9989, 0.9992,\n",
      "        0.9989, 0.9993, 0.9987, 0.9991, 0.9992, 0.9990, 0.9993, 0.9992, 0.9990,\n",
      "        0.9998, 0.9998, 0.9998, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.8136, 0.8280, 0.8130, 0.7773, 0.7267, 0.6655, 0.5964, 0.5217, 0.4427,\n",
      "        0.3606, 0.2762, 0.1901, 0.1027])\n",
      "----------------------------------------\n",
      "iter  0  stage  11  ep  5857   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0029, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.4451, 2.4451, 2.4451, 2.4451, 2.4451, 2.4451, 2.4451, 2.4451, 2.4451,\n",
      "        2.4451, 2.4451, 2.4451, 2.1859, 1.9586, 1.7537, 1.5650, 1.3879, 1.2192,\n",
      "        1.0569, 0.8991, 0.7447, 0.5928, 0.4428, 0.2943, 0.1467]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9995, 0.9996, 0.9996, 0.9994, 0.9993, 0.9996, 0.9994, 0.9992, 0.9994,\n",
      "        0.9991, 0.9995, 0.9990, 0.9998, 0.9997, 0.9995, 0.9998, 0.9997, 0.9993,\n",
      "        0.9998, 0.9999, 0.9999, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.9036, 0.9180, 0.9029, 0.8670, 0.8163, 0.7549, 0.6858, 0.6110, 0.5319,\n",
      "        0.4498, 0.3654, 0.2792, 0.1918, 0.1035])\n",
      "----------------------------------------\n",
      "iter  0  stage  10  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0036, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.6021, 2.6021, 2.6021, 2.6021, 2.6021, 2.6021, 2.6021, 2.6021, 2.6021,\n",
      "        2.6021, 2.6021, 2.3399, 2.1104, 1.9040, 1.7141, 1.5362, 1.3670, 1.2041,\n",
      "        1.0460, 0.8914, 0.7394, 0.5893, 0.4406, 0.2930, 0.1462]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9995, 0.9996, 0.9996, 0.9994, 0.9993, 0.9996, 0.9994, 0.9992, 0.9994,\n",
      "        0.9991, 0.9995, 0.9990, 0.9998, 0.9997, 0.9995, 0.9998, 0.9997, 0.9993,\n",
      "        0.9998, 0.9999, 0.9999, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([0.9944, 1.0088, 0.9935, 0.9575, 0.9066, 0.8451, 0.7758, 0.7009, 0.6218,\n",
      "        0.5396, 0.4551, 0.3689, 0.2815, 0.1931, 0.1040])\n",
      "----------------------------------------\n",
      "iter  0  stage  9  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0047, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.7625, 2.7625, 2.7625, 2.7625, 2.7625, 2.7625, 2.7625, 2.7625, 2.7625,\n",
      "        2.7625, 2.4961, 2.2637, 2.0552, 1.8638, 1.6848, 1.5149, 1.3514, 1.1929,\n",
      "        1.0380, 0.8857, 0.7354, 0.5866, 0.4389, 0.2920, 0.1458]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9995, 0.9996, 0.9996, 0.9994, 0.9993, 0.9996, 0.9994, 0.9992, 0.9994,\n",
      "        0.9991, 0.9995, 0.9990, 0.9998, 0.9997, 0.9995, 0.9998, 0.9997, 0.9993,\n",
      "        0.9998, 0.9999, 0.9999, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.0860, 1.1004, 1.0850, 1.0487, 0.9976, 0.9359, 0.8664, 0.7913, 0.7121,\n",
      "        0.6298, 0.5453, 0.4591, 0.3716, 0.2832, 0.1941, 0.1044])\n",
      "----------------------------------------\n",
      "iter  0  stage  8  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0057, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.9276, 2.9276, 2.9276, 2.9276, 2.9276, 2.9276, 2.9276, 2.9276, 2.9276,\n",
      "        2.6556, 2.4193, 2.2080, 2.0147, 1.8342, 1.6632, 1.4990, 1.3399, 1.1845,\n",
      "        1.0319, 0.8814, 0.7324, 0.5846, 0.4376, 0.2913, 0.1455]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9995, 0.9996, 0.9996, 0.9994, 0.9993, 0.9996, 0.9994, 0.9992, 0.9994,\n",
      "        0.9991, 0.9995, 0.9990, 0.9998, 0.9997, 0.9995, 0.9998, 0.9997, 0.9993,\n",
      "        0.9998, 0.9999, 0.9999, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.1785, 1.1929, 1.1773, 1.1406, 1.0892, 1.0272, 0.9575, 0.8823, 0.8029,\n",
      "        0.7205, 0.6359, 0.5496, 0.4620, 0.3736, 0.2844, 0.1948, 0.1047])\n",
      "----------------------------------------\n",
      "iter  0  stage  7  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0070, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.0992, 3.0992, 3.0992, 3.0992, 3.0992, 3.0992, 3.0992, 3.0992, 2.8197,\n",
      "        2.5781, 2.3632, 2.1671, 1.9848, 1.8123, 1.6471, 1.4872, 1.3313, 1.1783,\n",
      "        1.0274, 0.8782, 0.7302, 0.5831, 0.4367, 0.2908, 0.1453]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9995, 0.9996, 0.9996, 0.9994, 0.9993, 0.9996, 0.9994, 0.9992, 0.9994,\n",
      "        0.9991, 0.9995, 0.9990, 0.9998, 0.9997, 0.9995, 0.9998, 0.9997, 0.9993,\n",
      "        0.9998, 0.9999, 0.9999, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.2720, 1.2864, 1.2704, 1.2334, 1.1815, 1.1191, 1.0491, 0.9736, 0.8941,\n",
      "        0.8115, 0.7268, 0.6404, 0.5528, 0.4643, 0.3751, 0.2854, 0.1953, 0.1050])\n",
      "----------------------------------------\n",
      "iter  0  stage  6  ep  0   adversary:  AdversaryModes.constant_95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0080, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.2799, 3.2799, 3.2799, 3.2799, 3.2799, 3.2799, 3.2799, 2.9902, 2.7415,\n",
      "        2.5215, 2.3219, 2.1370, 1.9626, 1.7960, 1.6351, 1.4784, 1.3248, 1.1736,\n",
      "        1.0241, 0.8758, 0.7286, 0.5820, 0.4360, 0.2904, 0.1451]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9995, 0.9996, 0.9996, 0.9994, 0.9993, 0.9996, 0.9994, 0.9992, 0.9994,\n",
      "        0.9991, 0.9995, 0.9990, 0.9998, 0.9997, 0.9995, 0.9998, 0.9997, 0.9993,\n",
      "        0.9998, 0.9999, 0.9999, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.3666, 1.3810, 1.3646, 1.3270, 1.2746, 1.2117, 1.1413, 1.0654, 0.9856,\n",
      "        0.9029, 0.8179, 0.7314, 0.6437, 0.5552, 0.4659, 0.3762, 0.2861, 0.1957,\n",
      "        0.1051])\n",
      "----------------------------------------\n",
      "iter  0  stage  5  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0089, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.4731, 3.4731, 3.4731, 3.4731, 3.4731, 3.4731, 3.1695, 2.9111, 2.6844,\n",
      "        2.4799, 2.2915, 2.1146, 1.9461, 1.7839, 1.6262, 1.4718, 1.3200, 1.1701,\n",
      "        1.0215, 0.8740, 0.7273, 0.5812, 0.4355, 0.2901, 0.1450]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9995, 0.9996, 0.9996, 0.9994, 0.9993, 0.9996, 0.9994, 0.9992, 0.9994,\n",
      "        0.9991, 0.9995, 0.9990, 0.9998, 0.9997, 0.9995, 0.9998, 0.9997, 0.9993,\n",
      "        0.9998, 0.9999, 0.9999, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.4627, 1.4771, 1.4601, 1.4217, 1.3686, 1.3050, 1.2341, 1.1578, 1.0776,\n",
      "        0.9945, 0.9094, 0.8228, 0.7349, 0.6463, 0.5570, 0.4672, 0.3770, 0.2866,\n",
      "        0.1960, 0.1053])\n",
      "----------------------------------------\n",
      "iter  0  stage  4  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0102, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.6835, 3.6835, 3.6835, 3.6835, 3.6835, 3.3608, 3.0893, 2.8534, 2.6424,\n",
      "        2.4492, 2.2689, 2.0980, 1.9339, 1.7748, 1.6195, 1.4669, 1.3164, 1.1675,\n",
      "        1.0197, 0.8727, 0.7264, 0.5805, 0.4351, 0.2899, 0.1449]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9995, 0.9996, 0.9996, 0.9994, 0.9993, 0.9996, 0.9994, 0.9992, 0.9994,\n",
      "        0.9991, 0.9995, 0.9990, 0.9998, 0.9997, 0.9995, 0.9998, 0.9997, 0.9993,\n",
      "        0.9998, 0.9999, 0.9999, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.5606, 1.5750, 1.5572, 1.5179, 1.4637, 1.3993, 1.3276, 1.2507, 1.1700,\n",
      "        1.0866, 1.0012, 0.9143, 0.8264, 0.7376, 0.6482, 0.5583, 0.4681, 0.3777,\n",
      "        0.2870, 0.1962, 0.1054])\n",
      "----------------------------------------\n",
      "iter  0  stage  3  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0114, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.9177, 3.9177, 3.9177, 3.9177, 3.5687, 3.2793, 3.0307, 2.8109, 2.6114,\n",
      "        2.4265, 2.2522, 2.0856, 1.9247, 1.7680, 1.6145, 1.4632, 1.3137, 1.1655,\n",
      "        1.0182, 0.8717, 0.7257, 0.5801, 0.4348, 0.2897, 0.1448]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9995, 0.9996, 0.9996, 0.9994, 0.9993, 0.9996, 0.9994, 0.9992, 0.9994,\n",
      "        0.9991, 0.9995, 0.9990, 0.9998, 0.9997, 0.9995, 0.9998, 0.9997, 0.9993,\n",
      "        0.9998, 0.9999, 0.9999, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.6609, 1.6753, 1.6565, 1.6158, 1.5603, 1.4947, 1.4220, 1.3444, 1.2631,\n",
      "        1.1792, 1.0934, 1.0062, 0.9180, 0.8291, 0.7395, 0.6496, 0.5593, 0.4688,\n",
      "        0.3781, 0.2873, 0.1964, 0.1054])\n",
      "----------------------------------------\n",
      "iter  0  stage  2  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0123, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.1854, 4.1854, 4.1854, 3.7998, 3.4854, 3.2196, 2.9877, 2.7795, 2.5884,\n",
      "        2.4096, 2.2397, 2.0764, 1.9179, 1.7630, 1.6108, 1.4605, 1.3117, 1.1640,\n",
      "        1.0172, 0.8709, 0.7252, 0.5797, 0.4345, 0.2896, 0.1447]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9995, 0.9996, 0.9996, 0.9994, 0.9993, 0.9996, 0.9994, 0.9992, 0.9994,\n",
      "        0.9991, 0.9995, 0.9990, 0.9998, 0.9997, 0.9995, 0.9998, 0.9997, 0.9993,\n",
      "        0.9998, 0.9999, 0.9999, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.7644, 1.7788, 1.7586, 1.7161, 1.6589, 1.5917, 1.5177, 1.4390, 1.3568,\n",
      "        1.2723, 1.1860, 1.0985, 1.0100, 0.9208, 0.8311, 0.7410, 0.6506, 0.5601,\n",
      "        0.4693, 0.3785, 0.2875, 0.1965, 0.1055])\n",
      "----------------------------------------\n",
      "iter  0  stage  1  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0133, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.5005, 4.5005, 4.0633, 3.7141, 3.4244, 3.1758, 2.9558, 2.7562, 2.5713,\n",
      "        2.3970, 2.2304, 2.0695, 1.9128, 1.7592, 1.6080, 1.4584, 1.3102, 1.1629,\n",
      "        1.0164, 0.8704, 0.7248, 0.5795, 0.4344, 0.2895, 0.1447]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9995, 0.9996, 0.9996, 0.9994, 0.9993, 0.9996, 0.9994, 0.9992, 0.9994,\n",
      "        0.9992, 0.9995, 0.9990, 0.9998, 0.9997, 0.9995, 0.9998, 0.9997, 0.9993,\n",
      "        0.9998, 0.9999, 0.9999, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.8720, 1.8864, 1.8645, 1.8196, 1.7600, 1.6907, 1.6150, 1.5348, 1.4516,\n",
      "        1.3662, 1.2792, 1.1911, 1.1022, 1.0128, 0.9228, 0.8326, 0.7421, 0.6514,\n",
      "        0.5606, 0.4697, 0.3787, 0.2877, 0.1966, 0.1055])\n",
      "----------------------------------------\n",
      "iter  0  stage  0  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0145, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.8841, 4.3729, 3.9744, 3.6513, 3.3795, 3.1433, 2.9322, 2.7389, 2.5586,\n",
      "        2.3875, 2.2234, 2.0643, 1.9089, 1.7564, 1.6059, 1.4569, 1.3091, 1.1621,\n",
      "        1.0158, 0.8699, 0.7245, 0.5793, 0.4343, 0.2894, 0.1447]) return=  68694.09895647192\n",
      "probs of actions:  tensor([0.9995, 0.9996, 0.9996, 0.9994, 0.9993, 0.9996, 0.9994, 0.9992, 0.9994,\n",
      "        0.9992, 0.9995, 0.9990, 0.9998, 0.9997, 0.9995, 0.9998, 0.9997, 0.9993,\n",
      "        0.9998, 0.9999, 0.9999, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.4229, 0.3712, 0.3345, 0.3083, 0.2892, 0.2753, 0.2651, 0.2576,\n",
      "        0.2520, 0.2479, 0.2448, 0.2425, 0.2407, 0.2394, 0.2385, 0.2378, 0.2372,\n",
      "        0.2368, 0.2365, 0.2363, 0.2361, 0.2360, 0.2359, 0.2502])\n",
      "finalReturns:  tensor([1.9853, 1.9997, 1.9753, 1.9272, 1.8645, 1.7924, 1.7143, 1.6323, 1.5475,\n",
      "        1.4610, 1.3731, 1.2844, 1.1950, 1.1051, 1.0148, 0.9244, 0.8337, 0.7429,\n",
      "        0.6520, 0.5610, 0.4700, 0.3789, 0.2878, 0.1967, 0.1056])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1682652460 saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[891910, 'tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])', 68694.09895647192, 88516.4319236399, 0.014486568048596382, 1e-05, 1, 0, 'tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\\n        12, 12, 12, 12, 12, 12,  0])', '[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\\n 1.]', '0,[1e-05,1][1, 10000, 1, 1],1682652460', 25, 50, 157611.33342506486, 175538.2609849781, 68707.8951114289, 135313.23466666666, 132439.05866666668, 122329.26544005591, 122329.26544005591, 130254.64915679736, 130254.64915679736, 80045.82189636558, 122329.26544005591, 130254.64915679736]\n",
      "policy reset\n",
      "----------------------------------------\n",
      "iter  1  stage  24  ep  99999   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.1455, 0.1455, 0.1455, 0.1455, 0.1455, 0.1455, 0.1455, 0.1455, 0.1455,\n",
      "        0.1455, 0.1455, 0.1455, 0.1455, 0.1455, 0.1455, 0.1455, 0.1455, 0.1455,\n",
      "        0.1455, 0.1455, 0.1455, 0.1455, 0.1455, 0.1455, 0.1455]) return=  48892.64112506197\n",
      "probs of actions:  tensor([0.8787, 0.8940, 0.8694, 0.8601, 0.8593, 0.8601, 0.8369, 0.8052, 0.8556,\n",
      "        0.8616, 0.8490, 0.8605, 0.8540, 0.8323, 0.8056, 0.8109, 0.8350, 0.8404,\n",
      "        0.8529, 0.8607, 0.0669, 0.8381, 0.8490, 0.8036, 0.9799],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.3985, 0.3231, 0.2718, 0.2362, 0.2111, 0.1933, 0.1804, 0.1710,\n",
      "        0.1641, 0.1591, 0.1554, 0.1526, 0.1505, 0.1490, 0.1478, 0.1470, 0.1463,\n",
      "        0.1458, 0.1455, 0.1451, 0.1469, 0.1463, 0.1458, 0.1455])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  1  stage  23  ep  99999   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([ 5,  0,  9,  6,  4,  4,  3,  6,  9,  6,  7,  6,  9,  6, 15,  9,  6,  6,\n",
      "         5,  5,  6,  7,  5,  9,  0])\n",
      "loss=  tensor(0.0071, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.3767, 0.3767, 0.3767, 0.3767, 0.3767, 0.3767, 0.3767, 0.3767, 0.3767,\n",
      "        0.3767, 0.3767, 0.3767, 0.3767, 0.3767, 0.3767, 0.3767, 0.3767, 0.3767,\n",
      "        0.3767, 0.3767, 0.3767, 0.3767, 0.3767, 0.3767, 0.1817]) return=  59134.4246096995\n",
      "probs of actions:  tensor([0.1258, 0.1556, 0.3641, 0.1753, 0.0548, 0.0523, 0.0173, 0.1730, 0.3084,\n",
      "        0.1926, 0.1241, 0.1785, 0.2978, 0.1695, 0.0034, 0.2990, 0.1839, 0.1763,\n",
      "        0.1376, 0.1150, 0.2031, 0.1178, 0.1148, 0.5464, 0.9951],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5087, 0.4144, 0.3258, 0.2998, 0.2721, 0.2458, 0.2276, 0.2089, 0.1996,\n",
      "        0.2074, 0.2017, 0.2019, 0.1944, 0.2034, 0.1811, 0.2137, 0.2180, 0.2108,\n",
      "        0.2066, 0.2004, 0.1947, 0.1922, 0.1960, 0.1869, 0.2014])\n",
      "finalReturns:  tensor([0.0116, 0.0197])\n",
      "----------------------------------------\n",
      "iter  1  stage  22  ep  99999   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 7, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0002, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.6042, 0.6042, 0.6042, 0.6042, 0.6042, 0.6042, 0.6042, 0.6042, 0.6042,\n",
      "        0.6042, 0.6042, 0.6042, 0.6042, 0.6042, 0.6042, 0.6042, 0.6042, 0.6042,\n",
      "        0.6042, 0.6042, 0.6042, 0.6042, 0.6042, 0.3844, 0.1849]) return=  63727.41292382953\n",
      "probs of actions:  tensor([0.9949, 0.9901, 0.9940, 0.9882, 0.9920, 0.9925, 0.9934, 0.9894, 0.9914,\n",
      "        0.9911, 0.9930, 0.9908, 0.9882, 0.9847, 0.9907, 0.9878, 0.9905, 0.0035,\n",
      "        0.9919, 0.9858, 0.9927, 0.9911, 0.9983, 0.9988, 0.9993],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2177,\n",
      "        0.2094, 0.2103, 0.2109, 0.2114, 0.2117, 0.2120, 0.2203])\n",
      "finalReturns:  tensor([0.0398, 0.0479, 0.0354])\n",
      "----------------------------------------\n",
      "iter  1  stage  22  ep  113089   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 0, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(7.9259e-05, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5613, 0.5613, 0.5613, 0.5613, 0.5613, 0.5613, 0.5613, 0.5613, 0.5613,\n",
      "        0.5613, 0.5613, 0.5613, 0.5613, 0.5613, 0.5613, 0.5613, 0.5613, 0.5613,\n",
      "        0.5613, 0.5613, 0.5613, 0.5613, 0.5613, 0.3606, 0.1749]) return=  63459.11549104476\n",
      "probs of actions:  tensor([9.9744e-01, 9.9532e-01, 9.9700e-01, 9.9432e-01, 9.9599e-01, 9.9599e-01,\n",
      "        9.9645e-01, 9.9433e-01, 9.9540e-01, 9.9529e-01, 9.9630e-01, 9.9524e-01,\n",
      "        9.9393e-01, 9.9210e-01, 9.9491e-01, 9.9326e-01, 9.9492e-01, 9.9538e-01,\n",
      "        9.9567e-01, 9.9287e-01, 9.9599e-01, 8.9096e-04, 9.9901e-01, 9.9946e-01,\n",
      "        9.9952e-01], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2214, 0.1925, 0.1975, 0.2094])\n",
      "finalReturns:  tensor([0.0382, 0.0463, 0.0345])\n",
      "----------------------------------------\n",
      "iter  1  stage  21  ep  25407   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0001, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.7827, 0.7827, 0.7827, 0.7827, 0.7827, 0.7827, 0.7827, 0.7827, 0.7827,\n",
      "        0.7827, 0.7827, 0.7827, 0.7827, 0.7827, 0.7827, 0.7827, 0.7827, 0.7827,\n",
      "        0.7827, 0.7827, 0.7827, 0.7827, 0.5613, 0.3606, 0.1749]) return=  63858.06679267008\n",
      "probs of actions:  tensor([0.9994, 0.9990, 0.9993, 0.9987, 0.9991, 0.9989, 0.9990, 0.9985, 0.9987,\n",
      "        0.9987, 0.9990, 0.9988, 0.9985, 0.9981, 0.9986, 0.9981, 0.9986, 0.9988,\n",
      "        0.9988, 0.9983, 0.9989, 0.9990, 0.9998, 1.0000, 0.9996],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2133, 0.2132, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([0.0781, 0.0862, 0.0736, 0.0462])\n",
      "----------------------------------------\n",
      "iter  1  stage  20  ep  42   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0003, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.9502, 0.9502, 0.9502, 0.9502, 0.9502, 0.9502, 0.9502, 0.9502, 0.9502,\n",
      "        0.9502, 0.9502, 0.9502, 0.9502, 0.9502, 0.9502, 0.9502, 0.9502, 0.9502,\n",
      "        0.9502, 0.9502, 0.9502, 0.7286, 0.5278, 0.3420, 0.1671]) return=  63858.06679267008\n",
      "probs of actions:  tensor([0.9994, 0.9990, 0.9993, 0.9987, 0.9991, 0.9989, 0.9991, 0.9985, 0.9987,\n",
      "        0.9987, 0.9990, 0.9988, 0.9985, 0.9981, 0.9986, 0.9982, 0.9987, 0.9988,\n",
      "        0.9988, 0.9983, 0.9990, 0.9990, 0.9998, 1.0000, 0.9996],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2133, 0.2132, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([0.1241, 0.1322, 0.1196, 0.0922, 0.0541])\n",
      "----------------------------------------\n",
      "iter  1  stage  19  ep  280   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0005, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.1122, 1.1122, 1.1122, 1.1122, 1.1122, 1.1122, 1.1122, 1.1122, 1.1122,\n",
      "        1.1122, 1.1122, 1.1122, 1.1122, 1.1122, 1.1122, 1.1122, 1.1122, 1.1122,\n",
      "        1.1122, 1.1122, 0.8904, 0.6894, 0.5035, 0.3284, 0.1613]) return=  63858.06679267008\n",
      "probs of actions:  tensor([0.9995, 0.9992, 0.9994, 0.9989, 0.9992, 0.9991, 0.9992, 0.9987, 0.9989,\n",
      "        0.9989, 0.9992, 0.9990, 0.9987, 0.9985, 0.9989, 0.9985, 0.9989, 0.9990,\n",
      "        0.9990, 0.9990, 0.9994, 0.9992, 0.9998, 1.0000, 0.9995],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2133, 0.2132, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([0.1759, 0.1840, 0.1714, 0.1440, 0.1058, 0.0598])\n",
      "----------------------------------------\n",
      "iter  1  stage  18  ep  1   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0008, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2703, 1.2703, 1.2703, 1.2703, 1.2703, 1.2703, 1.2703, 1.2703, 1.2703,\n",
      "        1.2703, 1.2703, 1.2703, 1.2703, 1.2703, 1.2703, 1.2703, 1.2703, 1.2703,\n",
      "        1.2703, 1.0481, 0.8469, 0.6608, 0.4856, 0.3184, 0.1570]) return=  63858.06679267008\n",
      "probs of actions:  tensor([0.9995, 0.9992, 0.9994, 0.9989, 0.9992, 0.9991, 0.9992, 0.9987, 0.9989,\n",
      "        0.9990, 0.9992, 0.9990, 0.9987, 0.9985, 0.9989, 0.9985, 0.9989, 0.9990,\n",
      "        0.9990, 0.9990, 0.9994, 0.9992, 0.9998, 1.0000, 0.9995],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2133, 0.2132, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([0.2319, 0.2400, 0.2274, 0.2000, 0.1619, 0.1159, 0.0641])\n",
      "----------------------------------------\n",
      "iter  1  stage  17  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0013, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.4255, 1.4255, 1.4255, 1.4255, 1.4255, 1.4255, 1.4255, 1.4255, 1.4255,\n",
      "        1.4255, 1.4255, 1.4255, 1.4255, 1.4255, 1.4255, 1.4255, 1.4255, 1.4255,\n",
      "        1.2029, 1.0014, 0.8151, 0.6397, 0.4724, 0.3110, 0.1539]) return=  63858.06679267008\n",
      "probs of actions:  tensor([0.9995, 0.9992, 0.9994, 0.9989, 0.9992, 0.9991, 0.9992, 0.9987, 0.9990,\n",
      "        0.9990, 0.9992, 0.9990, 0.9987, 0.9985, 0.9989, 0.9985, 0.9989, 0.9990,\n",
      "        0.9990, 0.9990, 0.9994, 0.9992, 0.9998, 1.0000, 0.9995],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2133, 0.2132, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([0.2912, 0.2993, 0.2867, 0.2593, 0.2211, 0.1751, 0.1233, 0.0673])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  16  ep  27   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0017, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.5790, 1.5790, 1.5790, 1.5790, 1.5790, 1.5790, 1.5790, 1.5790, 1.5790,\n",
      "        1.5790, 1.5790, 1.5790, 1.5790, 1.5790, 1.5790, 1.5790, 1.5790, 1.3558,\n",
      "        1.1539, 0.9672, 0.7917, 0.6242, 0.4626, 0.3055, 0.1515]) return=  63858.06679267008\n",
      "probs of actions:  tensor([0.9995, 0.9992, 0.9995, 0.9990, 0.9993, 0.9992, 0.9993, 0.9988, 0.9990,\n",
      "        0.9990, 0.9992, 0.9991, 0.9988, 0.9985, 0.9989, 0.9985, 0.9990, 0.9991,\n",
      "        0.9991, 0.9991, 0.9994, 0.9992, 0.9998, 1.0000, 0.9995],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2133, 0.2132, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([0.3529, 0.3610, 0.3483, 0.3209, 0.2827, 0.2366, 0.1848, 0.1288, 0.0696])\n",
      "----------------------------------------\n",
      "iter  1  stage  15  ep  96   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0018, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.7314, 1.7314, 1.7314, 1.7314, 1.7314, 1.7314, 1.7314, 1.7314, 1.7314,\n",
      "        1.7314, 1.7314, 1.7314, 1.7314, 1.7314, 1.7314, 1.7314, 1.5074, 1.3050,\n",
      "        1.1179, 0.9421, 0.7744, 0.6127, 0.4554, 0.3014, 0.1498]) return=  63858.06679267008\n",
      "probs of actions:  tensor([0.9996, 0.9994, 0.9996, 0.9992, 0.9994, 0.9993, 0.9994, 0.9990, 0.9992,\n",
      "        0.9992, 0.9994, 0.9993, 0.9991, 0.9988, 0.9991, 0.9990, 0.9993, 0.9994,\n",
      "        0.9994, 0.9993, 0.9996, 0.9994, 0.9999, 1.0000, 0.9995],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2133, 0.2132, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([0.4163, 0.4244, 0.4118, 0.3843, 0.3460, 0.3000, 0.2481, 0.1921, 0.1329,\n",
      "        0.0714])\n",
      "----------------------------------------\n",
      "iter  1  stage  14  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0024, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.8835, 1.8835, 1.8835, 1.8835, 1.8835, 1.8835, 1.8835, 1.8835, 1.8835,\n",
      "        1.8835, 1.8835, 1.8835, 1.8835, 1.8835, 1.8835, 1.6585, 1.4553, 1.2677,\n",
      "        1.0915, 0.9235, 0.7615, 0.6041, 0.4500, 0.2983, 0.1485]) return=  63858.06679267008\n",
      "probs of actions:  tensor([0.9996, 0.9994, 0.9996, 0.9992, 0.9994, 0.9993, 0.9994, 0.9990, 0.9992,\n",
      "        0.9992, 0.9994, 0.9993, 0.9991, 0.9988, 0.9991, 0.9990, 0.9993, 0.9994,\n",
      "        0.9994, 0.9993, 0.9996, 0.9994, 0.9999, 1.0000, 0.9994],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2133, 0.2132, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([0.4812, 0.4893, 0.4766, 0.4490, 0.4108, 0.3646, 0.3128, 0.2567, 0.1975,\n",
      "        0.1359, 0.0727])\n",
      "----------------------------------------\n",
      "iter  1  stage  13  ep  29   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0028, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.0358, 2.0358, 2.0358, 2.0358, 2.0358, 2.0358, 2.0358, 2.0358, 2.0358,\n",
      "        2.0358, 2.0358, 2.0358, 2.0358, 2.0358, 1.8094, 1.6052, 1.4170, 1.2402,\n",
      "        1.0719, 0.9097, 0.7520, 0.5977, 0.4459, 0.2960, 0.1475]) return=  63858.06679267008\n",
      "probs of actions:  tensor([0.9997, 0.9994, 0.9996, 0.9993, 0.9995, 0.9994, 0.9995, 0.9991, 0.9993,\n",
      "        0.9993, 0.9995, 0.9993, 0.9991, 0.9990, 0.9993, 0.9991, 0.9994, 0.9995,\n",
      "        0.9994, 0.9994, 0.9996, 0.9994, 0.9999, 1.0000, 0.9994],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2133, 0.2132, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([0.5471, 0.5552, 0.5425, 0.5149, 0.4765, 0.4303, 0.3784, 0.3223, 0.2631,\n",
      "        0.2015, 0.1382, 0.0736])\n",
      "----------------------------------------\n",
      "iter  1  stage  12  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0035, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.1892, 2.1892, 2.1892, 2.1892, 2.1892, 2.1892, 2.1892, 2.1892, 2.1892,\n",
      "        2.1892, 2.1892, 2.1892, 2.1892, 1.9609, 1.7554, 1.5662, 1.3887, 1.2199,\n",
      "        1.0573, 0.8994, 0.7449, 0.5930, 0.4429, 0.2943, 0.1468]) return=  63858.06679267008\n",
      "probs of actions:  tensor([0.9997, 0.9994, 0.9996, 0.9993, 0.9995, 0.9994, 0.9995, 0.9991, 0.9993,\n",
      "        0.9993, 0.9995, 0.9993, 0.9991, 0.9990, 0.9993, 0.9991, 0.9994, 0.9995,\n",
      "        0.9994, 0.9994, 0.9996, 0.9994, 0.9999, 1.0000, 0.9994],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2133, 0.2132, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([0.6140, 0.6221, 0.6093, 0.5815, 0.5431, 0.4969, 0.4449, 0.3887, 0.3294,\n",
      "        0.2678, 0.2045, 0.1399, 0.0744])\n",
      "----------------------------------------\n",
      "iter  1  stage  11  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0042, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.3442, 2.3442, 2.3442, 2.3442, 2.3442, 2.3442, 2.3442, 2.3442, 2.3442,\n",
      "        2.3442, 2.3442, 2.3442, 2.1135, 1.9062, 1.7157, 1.5374, 1.3678, 1.2048,\n",
      "        1.0465, 0.8917, 0.7396, 0.5894, 0.4407, 0.2930, 0.1462]) return=  63858.06679267008\n",
      "probs of actions:  tensor([0.9997, 0.9994, 0.9996, 0.9993, 0.9995, 0.9994, 0.9995, 0.9991, 0.9993,\n",
      "        0.9993, 0.9995, 0.9993, 0.9991, 0.9990, 0.9993, 0.9991, 0.9994, 0.9995,\n",
      "        0.9994, 0.9994, 0.9996, 0.9994, 0.9999, 1.0000, 0.9994],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2133, 0.2132, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([0.6815, 0.6896, 0.6767, 0.6489, 0.6104, 0.5640, 0.5120, 0.4557, 0.3964,\n",
      "        0.3347, 0.2714, 0.2068, 0.1412, 0.0749])\n",
      "----------------------------------------\n",
      "iter  1  stage  10  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0048, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.5019, 2.5019, 2.5019, 2.5019, 2.5019, 2.5019, 2.5019, 2.5019, 2.5019,\n",
      "        2.5019, 2.5019, 2.2679, 2.0583, 1.8661, 1.6865, 1.5160, 1.3523, 1.1935,\n",
      "        1.0384, 0.8860, 0.7356, 0.5867, 0.4390, 0.2921, 0.1458]) return=  63858.06679267008\n",
      "probs of actions:  tensor([0.9997, 0.9994, 0.9996, 0.9993, 0.9995, 0.9994, 0.9995, 0.9991, 0.9993,\n",
      "        0.9993, 0.9995, 0.9994, 0.9991, 0.9990, 0.9993, 0.9991, 0.9994, 0.9995,\n",
      "        0.9994, 0.9994, 0.9996, 0.9994, 0.9999, 1.0000, 0.9994],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2133, 0.2132, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([0.7498, 0.7579, 0.7449, 0.7169, 0.6782, 0.6317, 0.5795, 0.5232, 0.4638,\n",
      "        0.4021, 0.3387, 0.2740, 0.2085, 0.1422, 0.0753])\n",
      "----------------------------------------\n",
      "iter  1  stage  9  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0055, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.6635, 2.6635, 2.6635, 2.6635, 2.6635, 2.6635, 2.6635, 2.6635, 2.6635,\n",
      "        2.6635, 2.4249, 2.2121, 2.0177, 1.8364, 1.6648, 1.5002, 1.3408, 1.1852,\n",
      "        1.0324, 0.8817, 0.7327, 0.5848, 0.4377, 0.2914, 0.1455]) return=  63858.06679267008\n",
      "probs of actions:  tensor([0.9997, 0.9994, 0.9996, 0.9993, 0.9995, 0.9994, 0.9995, 0.9991, 0.9993,\n",
      "        0.9993, 0.9995, 0.9994, 0.9991, 0.9990, 0.9993, 0.9991, 0.9994, 0.9995,\n",
      "        0.9994, 0.9994, 0.9996, 0.9994, 0.9999, 1.0000, 0.9994],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2133, 0.2132, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([0.8187, 0.8268, 0.8136, 0.7854, 0.7465, 0.6999, 0.6475, 0.5911, 0.5316,\n",
      "        0.4698, 0.4064, 0.3417, 0.2760, 0.2097, 0.1429, 0.0756])\n",
      "----------------------------------------\n",
      "iter  1  stage  8  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0063, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.8303, 2.8303, 2.8303, 2.8303, 2.8303, 2.8303, 2.8303, 2.8303, 2.8303,\n",
      "        2.5858, 2.3687, 2.1712, 1.9877, 1.8145, 1.6487, 1.4884, 1.3321, 1.1789,\n",
      "        1.0279, 0.8785, 0.7304, 0.5833, 0.4368, 0.2908, 0.1453]) return=  63858.06679267008\n",
      "probs of actions:  tensor([0.9997, 0.9994, 0.9996, 0.9993, 0.9995, 0.9994, 0.9995, 0.9991, 0.9993,\n",
      "        0.9993, 0.9995, 0.9994, 0.9992, 0.9990, 0.9993, 0.9991, 0.9994, 0.9995,\n",
      "        0.9994, 0.9994, 0.9996, 0.9994, 0.9999, 1.0000, 0.9994],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2133, 0.2132, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([0.8883, 0.8964, 0.8830, 0.8546, 0.8154, 0.7685, 0.7160, 0.6594, 0.5997,\n",
      "        0.5378, 0.4743, 0.4096, 0.3439, 0.2775, 0.2107, 0.1434, 0.0759])\n",
      "----------------------------------------\n",
      "iter  1  stage  7  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0073, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.0045, 3.0045, 3.0045, 3.0045, 3.0045, 3.0045, 3.0045, 3.0045, 2.7518,\n",
      "        2.5290, 2.3274, 2.1410, 1.9656, 1.7982, 1.6367, 1.4796, 1.3257, 1.1742,\n",
      "        1.0245, 0.8761, 0.7288, 0.5822, 0.4361, 0.2904, 0.1451]) return=  63858.06679267008\n",
      "probs of actions:  tensor([0.9997, 0.9995, 0.9996, 0.9993, 0.9995, 0.9994, 0.9995, 0.9991, 0.9993,\n",
      "        0.9993, 0.9995, 0.9994, 0.9992, 0.9990, 0.9993, 0.9991, 0.9994, 0.9995,\n",
      "        0.9994, 0.9994, 0.9996, 0.9994, 0.9999, 1.0000, 0.9994],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2133, 0.2132, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([0.9587, 0.9668, 0.9531, 0.9243, 0.8848, 0.8376, 0.7848, 0.7280, 0.6682,\n",
      "        0.6062, 0.5425, 0.4777, 0.4120, 0.3456, 0.2786, 0.2114, 0.1438, 0.0760])\n",
      "----------------------------------------\n",
      "iter  1  stage  6  ep  0   adversary:  AdversaryModes.constant_95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0080, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.1888, 3.1888, 3.1888, 3.1888, 3.1888, 3.1888, 3.1888, 2.9251, 2.6945,\n",
      "        2.4873, 2.2969, 2.1186, 1.9491, 1.7860, 1.6278, 1.4730, 1.3209, 1.1707,\n",
      "        1.0220, 0.8744, 0.7275, 0.5813, 0.4356, 0.2901, 0.1450]) return=  63858.06679267008\n",
      "probs of actions:  tensor([0.9997, 0.9995, 0.9996, 0.9993, 0.9995, 0.9994, 0.9995, 0.9991, 0.9993,\n",
      "        0.9993, 0.9995, 0.9994, 0.9992, 0.9990, 0.9993, 0.9991, 0.9994, 0.9995,\n",
      "        0.9994, 0.9994, 0.9996, 0.9994, 0.9999, 1.0000, 0.9994],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2133, 0.2132, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([1.0301, 1.0382, 1.0241, 0.9949, 0.9548, 0.9072, 0.8541, 0.7969, 0.7369,\n",
      "        0.6747, 0.6110, 0.5460, 0.4802, 0.4138, 0.3468, 0.2795, 0.2119, 0.1441,\n",
      "        0.0762])\n",
      "----------------------------------------\n",
      "iter  1  stage  5  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0089, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.3871, 3.3871, 3.3871, 3.3871, 3.3871, 3.3871, 3.1081, 2.8670, 2.6523,\n",
      "        2.4565, 2.2743, 2.1019, 1.9368, 1.7770, 1.6211, 1.4681, 1.3173, 1.1681,\n",
      "        1.0201, 0.8730, 0.7266, 0.5807, 0.4352, 0.2899, 0.1449]) return=  63858.06679267008\n",
      "probs of actions:  tensor([0.9997, 0.9995, 0.9996, 0.9993, 0.9995, 0.9994, 0.9995, 0.9991, 0.9993,\n",
      "        0.9993, 0.9995, 0.9994, 0.9992, 0.9990, 0.9993, 0.9991, 0.9994, 0.9995,\n",
      "        0.9995, 0.9994, 0.9996, 0.9994, 0.9999, 1.0000, 0.9994],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2133, 0.2132, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([1.1027, 1.1108, 1.0963, 1.0663, 1.0257, 0.9775, 0.9238, 0.8663, 0.8060,\n",
      "        0.7436, 0.6797, 0.6146, 0.5487, 0.4821, 0.4151, 0.3477, 0.2801, 0.2123,\n",
      "        0.1443, 0.0763])\n",
      "----------------------------------------\n",
      "iter  1  stage  4  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0096, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.6046, 3.6046, 3.6046, 3.6046, 3.6046, 3.3048, 3.0491, 2.8242, 2.6211,\n",
      "        2.4337, 2.2575, 2.0895, 1.9276, 1.7702, 1.6161, 1.4644, 1.3146, 1.1661,\n",
      "        1.0187, 0.8720, 0.7259, 0.5802, 0.4349, 0.2897, 0.1448]) return=  63858.06679267008\n",
      "probs of actions:  tensor([0.9997, 0.9995, 0.9996, 0.9993, 0.9995, 0.9994, 0.9995, 0.9991, 0.9993,\n",
      "        0.9993, 0.9995, 0.9994, 0.9992, 0.9991, 0.9993, 0.9992, 0.9994, 0.9995,\n",
      "        0.9995, 0.9994, 0.9996, 0.9994, 0.9999, 1.0000, 0.9994],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2133, 0.2132, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([1.1768, 1.1849, 1.1698, 1.1390, 1.0975, 1.0485, 0.9943, 0.9362, 0.8755,\n",
      "        0.8128, 0.7486, 0.6833, 0.6173, 0.5506, 0.4835, 0.4161, 0.3484, 0.2806,\n",
      "        0.2126, 0.1445, 0.0763])\n",
      "----------------------------------------\n",
      "iter  1  stage  3  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0105, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.8490, 3.8490, 3.8490, 3.8490, 3.5203, 3.2446, 3.0057, 2.7927, 2.5981,\n",
      "        2.4167, 2.2449, 2.0803, 1.9208, 1.7651, 1.6123, 1.4617, 1.3126, 1.1647,\n",
      "        1.0176, 0.8713, 0.7254, 0.5799, 0.4346, 0.2896, 0.1447]) return=  63858.06679267008\n",
      "probs of actions:  tensor([0.9997, 0.9995, 0.9996, 0.9993, 0.9995, 0.9994, 0.9995, 0.9992, 0.9993,\n",
      "        0.9993, 0.9995, 0.9994, 0.9992, 0.9991, 0.9993, 0.9992, 0.9994, 0.9995,\n",
      "        0.9995, 0.9994, 0.9996, 0.9995, 0.9999, 1.0000, 0.9994],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2133, 0.2132, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([1.2530, 1.2611, 1.2451, 1.2132, 1.1705, 1.1206, 1.0655, 1.0068, 0.9455,\n",
      "        0.8824, 0.8178, 0.7523, 0.6861, 0.6193, 0.5521, 0.4846, 0.4169, 0.3490,\n",
      "        0.2809, 0.2128, 0.1446, 0.0764])\n",
      "----------------------------------------\n",
      "iter  1  stage  2  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0111, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.1314, 4.1314, 4.1314, 3.7620, 3.4586, 3.2004, 2.9737, 2.7693, 2.5809,\n",
      "        2.4040, 2.2356, 2.0733, 1.9156, 1.7613, 1.6095, 1.4596, 1.3111, 1.1635,\n",
      "        1.0168, 0.8707, 0.7250, 0.5796, 0.4345, 0.2895, 0.1447]) return=  63858.06679267008\n",
      "probs of actions:  tensor([0.9997, 0.9995, 0.9996, 0.9993, 0.9995, 0.9994, 0.9995, 0.9992, 0.9993,\n",
      "        0.9993, 0.9995, 0.9994, 0.9992, 0.9991, 0.9993, 0.9992, 0.9994, 0.9995,\n",
      "        0.9995, 0.9994, 0.9996, 0.9995, 0.9999, 1.0000, 0.9994],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2133, 0.2132, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([1.3319, 1.3400, 1.3228, 1.2894, 1.2452, 1.1939, 1.1377, 1.0781, 1.0161,\n",
      "        0.9524, 0.8875, 0.8216, 0.7551, 0.6881, 0.6208, 0.5532, 0.4854, 0.4174,\n",
      "        0.3493, 0.2812, 0.2130, 0.1447, 0.0764])\n",
      "----------------------------------------\n",
      "iter  1  stage  1  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0120, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.4683, 4.4683, 4.0409, 3.6983, 3.4131, 3.1676, 2.9499, 2.7519, 2.5681,\n",
      "        2.3946, 2.2286, 2.0682, 1.9118, 1.7585, 1.6074, 1.4580, 1.3099, 1.1627,\n",
      "        1.0162, 0.8703, 0.7247, 0.5794, 0.4343, 0.2894, 0.1447]) return=  63858.06679267008\n",
      "probs of actions:  tensor([0.9997, 0.9995, 0.9996, 0.9993, 0.9995, 0.9994, 0.9995, 0.9992, 0.9993,\n",
      "        0.9993, 0.9995, 0.9994, 0.9992, 0.9991, 0.9993, 0.9992, 0.9994, 0.9995,\n",
      "        0.9995, 0.9994, 0.9996, 0.9995, 0.9999, 1.0000, 0.9994],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2133, 0.2132, 0.2131, 0.2211])\n",
      "finalReturns:  tensor([1.4144, 1.4225, 1.4038, 1.3683, 1.3221, 1.2690, 1.2113, 1.1505, 1.0876,\n",
      "        1.0231, 0.9576, 0.8913, 0.8245, 0.7572, 0.6897, 0.6219, 0.5540, 0.4860,\n",
      "        0.4178, 0.3496, 0.2814, 0.2131, 0.1448, 0.0765])\n",
      "----------------------------------------\n",
      "iter  1  stage  0  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        0])\n",
      "loss=  tensor(0.0125, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.8841, 4.3729, 3.9744, 3.6513, 3.3795, 3.1433, 2.9322, 2.7389, 2.5586,\n",
      "        2.3875, 2.2234, 2.0643, 1.9089, 1.7564, 1.6059, 1.4569, 1.3091, 1.1621,\n",
      "        1.0158, 0.8699, 0.7245, 0.5793, 0.4343, 0.2894, 0.1447]) return=  63858.06679267008\n",
      "probs of actions:  tensor([0.9997, 0.9995, 0.9996, 0.9993, 0.9995, 0.9994, 0.9995, 0.9992, 0.9993,\n",
      "        0.9993, 0.9995, 0.9994, 0.9992, 0.9991, 0.9993, 0.9992, 0.9994, 0.9995,\n",
      "        0.9995, 0.9994, 0.9996, 0.9995, 0.9999, 1.0000, 0.9994],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.4193, 0.3613, 0.3206, 0.2917, 0.2708, 0.2557, 0.2446, 0.2365,\n",
      "        0.2304, 0.2260, 0.2226, 0.2202, 0.2183, 0.2169, 0.2159, 0.2151, 0.2145,\n",
      "        0.2141, 0.2138, 0.2135, 0.2133, 0.2132, 0.2131, 0.2211])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalReturns:  tensor([1.5017, 1.5098, 1.4889, 1.4507, 1.4019, 1.3464, 1.2867, 1.2243, 1.1600,\n",
      "        1.0946, 1.0283, 0.9615, 0.8942, 0.8266, 0.7588, 0.6909, 0.6228, 0.5546,\n",
      "        0.4864, 0.4182, 0.3499, 0.2815, 0.2132, 0.1448, 0.0765])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1682661769 saved\n",
      "[378994, 'tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])', 63858.06679267008, 91921.34476753633, 0.012501846998929977, 1e-05, 1, 0, 'tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\\n        0])', '[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\\n 1.]', '0,[1e-05,1][1, 10000, 1, 1],1682661769', 25, 50, 153603.58340915042, 166411.3640199337, 63840.19497842904, 134666.6666666667, 131815.9786666667, 116903.27057727946, 116903.27057727946, 134357.5795423984, 134357.5795423984, 74631.0377323958, 116903.27057727946, 134357.5795423984]\n",
      "policy reset\n",
      "----------------------------------------\n",
      "iter  2  stage  24  ep  99999   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.1469, 0.1469, 0.1469, 0.1469, 0.1469, 0.1469, 0.1469, 0.1469, 0.1469,\n",
      "        0.1469, 0.1469, 0.1469, 0.1469, 0.1469, 0.1469, 0.1469, 0.1469, 0.1469,\n",
      "        0.1469, 0.1469, 0.1469, 0.1469, 0.1469, 0.1469, 0.1469]) return=  49505.75711012355\n",
      "probs of actions:  tensor([0.0101, 0.8958, 0.8695, 0.8334, 0.8086, 0.8203, 0.8380, 0.8371, 0.8358,\n",
      "        0.8111, 0.8296, 0.8712, 0.0919, 0.0101, 0.8609, 0.0820, 0.8421, 0.8506,\n",
      "        0.8459, 0.8274, 0.0171, 0.8530, 0.8225, 0.8343, 0.9739],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5108, 0.4048, 0.3274, 0.2747, 0.2382, 0.2126, 0.1943, 0.1811, 0.1716,\n",
      "        0.1646, 0.1594, 0.1556, 0.1526, 0.1517, 0.1564, 0.1532, 0.1530, 0.1509,\n",
      "        0.1492, 0.1480, 0.1467, 0.1503, 0.1488, 0.1477, 0.1469])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  2  stage  23  ep  99999   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([3, 5, 5, 5, 6, 0, 5, 6, 4, 5, 2, 3, 4, 7, 4, 5, 5, 0, 3, 5, 5, 0, 2, 5,\n",
      "        0])\n",
      "loss=  tensor(0.0054, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.3278, 0.3278, 0.3278, 0.3278, 0.3278, 0.3278, 0.3278, 0.3278, 0.3278,\n",
      "        0.3278, 0.3278, 0.3278, 0.3278, 0.3278, 0.3278, 0.3278, 0.3278, 0.3278,\n",
      "        0.3278, 0.3278, 0.3278, 0.3278, 0.3278, 0.3278, 0.1610]) return=  55563.48757478318\n",
      "probs of actions:  tensor([0.0395, 0.4441, 0.3476, 0.4419, 0.0984, 0.2259, 0.3980, 0.0929, 0.1221,\n",
      "        0.3735, 0.0178, 0.0503, 0.1371, 0.0706, 0.1347, 0.3186, 0.3156, 0.1818,\n",
      "        0.0519, 0.4018, 0.3445, 0.2042, 0.0182, 0.4977, 0.9952],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5103, 0.4055, 0.3416, 0.2972, 0.2648, 0.2486, 0.2174, 0.2073, 0.2049,\n",
      "        0.1962, 0.1948, 0.1852, 0.1799, 0.1753, 0.1841, 0.1808, 0.1812, 0.1840,\n",
      "        0.1728, 0.1698, 0.1729, 0.1777, 0.1687, 0.1643, 0.1712])\n",
      "finalReturns:  tensor([0.0077, 0.0102])\n",
      "----------------------------------------\n",
      "iter  2  stage  22  ep  99999   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([11,  9, 11, 12, 12,  9, 15, 12, 12,  9, 11, 11, 11, 17, 12, 11, 12, 11,\n",
      "         9, 12, 12,  9, 12,  9,  0])\n",
      "loss=  tensor(0.1197, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.6472, 0.6472, 0.6472, 0.6472, 0.6472, 0.6472, 0.6472, 0.6472, 0.6472,\n",
      "        0.6472, 0.6472, 0.6472, 0.6472, 0.6472, 0.6472, 0.6472, 0.6472, 0.6472,\n",
      "        0.6472, 0.6472, 0.6472, 0.6472, 0.6472, 0.4080, 0.1948]) return=  67657.67510325903\n",
      "probs of actions:  tensor([0.3362, 0.1559, 0.3439, 0.2915, 0.2712, 0.1769, 0.0220, 0.2707, 0.2781,\n",
      "        0.1934, 0.3567, 0.3823, 0.3320, 0.0054, 0.2852, 0.3265, 0.2812, 0.3373,\n",
      "        0.1603, 0.2767, 0.2689, 0.1686, 0.4401, 0.2747, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.4259, 0.3619, 0.3234, 0.3002, 0.2896, 0.2550, 0.2639, 0.2567,\n",
      "        0.2576, 0.2420, 0.2385, 0.2359, 0.2171, 0.2452, 0.2451, 0.2384, 0.2400,\n",
      "        0.2410, 0.2275, 0.2295, 0.2373, 0.2248, 0.2338, 0.2365])\n",
      "finalReturns:  tensor([0.0479, 0.0623, 0.0417])\n",
      "----------------------------------------\n",
      "iter  2  stage  21  ep  99999   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([15, 15, 12, 13, 15, 15, 12, 12, 12, 13,  9, 12, 15, 11, 11, 12, 11, 12,\n",
      "        13, 12, 11, 11, 11,  9,  0])\n",
      "loss=  tensor(0.6629, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.8528, 0.8528, 0.8528, 0.8528, 0.8528, 0.8528, 0.8528, 0.8528, 0.8528,\n",
      "        0.8528, 0.8528, 0.8528, 0.8528, 0.8528, 0.8528, 0.8528, 0.8528, 0.8528,\n",
      "        0.8528, 0.8528, 0.8528, 0.8528, 0.6043, 0.3844, 0.1849]) return=  69577.64726088839\n",
      "probs of actions:  tensor([0.2457, 0.2400, 0.4711, 0.0916, 0.2515, 0.2377, 0.4269, 0.4518, 0.4405,\n",
      "        0.0901, 0.0234, 0.4779, 0.2212, 0.1748, 0.1601, 0.4600, 0.1729, 0.4251,\n",
      "        0.0977, 0.4363, 0.1895, 0.1269, 0.1451, 0.0535, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4247, 0.3876, 0.3438, 0.3115, 0.2978, 0.2958, 0.2801, 0.2687,\n",
      "        0.2577, 0.2628, 0.2436, 0.2335, 0.2500, 0.2444, 0.2379, 0.2397, 0.2344,\n",
      "        0.2322, 0.2374, 0.2393, 0.2364, 0.2343, 0.2367, 0.2387])\n",
      "finalReturns:  tensor([0.0934, 0.1055, 0.0910, 0.0538])\n",
      "----------------------------------------\n",
      "iter  2  stage  20  ep  99999   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([15, 15, 15, 11, 13, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0408, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.1196, 1.1196, 1.1196, 1.1196, 1.1196, 1.1196, 1.1196, 1.1196, 1.1196,\n",
      "        1.1196, 1.1196, 1.1196, 1.1196, 1.1196, 1.1196, 1.1196, 1.1196, 1.1196,\n",
      "        1.1196, 1.1196, 1.1196, 0.8382, 0.5954, 0.3795, 0.1828]) return=  72938.84118085599\n",
      "probs of actions:  tensor([0.9609, 0.9638, 0.9470, 0.0121, 0.0154, 0.9355, 0.9524, 0.9420, 0.9463,\n",
      "        0.9441, 0.9459, 0.9427, 0.9293, 0.9476, 0.9522, 0.9450, 0.9424, 0.9520,\n",
      "        0.9369, 0.9393, 0.9634, 0.9825, 0.9491, 0.8879, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4247, 0.3795, 0.3576, 0.3178, 0.2927, 0.2839, 0.2774, 0.2726,\n",
      "        0.2690, 0.2664, 0.2644, 0.2629, 0.2617, 0.2609, 0.2603, 0.2598, 0.2595,\n",
      "        0.2592, 0.2590, 0.2588, 0.2587, 0.2587, 0.2586, 0.2810])\n",
      "finalReturns:  tensor([0.1963, 0.2188, 0.2029, 0.1601, 0.0982])\n",
      "----------------------------------------\n",
      "iter  2  stage  19  ep  99999   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0125, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2935, 1.2935, 1.2935, 1.2935, 1.2935, 1.2935, 1.2935, 1.2935, 1.2935,\n",
      "        1.2935, 1.2935, 1.2935, 1.2935, 1.2935, 1.2935, 1.2935, 1.2935, 1.2935,\n",
      "        1.2935, 1.2935, 1.0118, 0.7687, 0.5526, 0.3558, 0.1729]) return=  73445.38232037451\n",
      "probs of actions:  tensor([0.9926, 0.9932, 0.9889, 0.9853, 0.9862, 0.9855, 0.9897, 0.9877, 0.9884,\n",
      "        0.9879, 0.9882, 0.9876, 0.9833, 0.9891, 0.9900, 0.9879, 0.9871, 0.9900,\n",
      "        0.9858, 0.9914, 0.9953, 0.9967, 0.9902, 0.9719, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4247, 0.3795, 0.3472, 0.3239, 0.3069, 0.2944, 0.2852, 0.2784,\n",
      "        0.2733, 0.2696, 0.2667, 0.2646, 0.2631, 0.2619, 0.2610, 0.2604, 0.2599,\n",
      "        0.2595, 0.2592, 0.2590, 0.2589, 0.2587, 0.2587, 0.2811])\n",
      "finalReturns:  tensor([0.2821, 0.3046, 0.2887, 0.2459, 0.1840, 0.1082])\n",
      "----------------------------------------\n",
      "iter  2  stage  18  ep  75370   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 12,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0013, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.4352, 1.4352, 1.4352, 1.4352, 1.4352, 1.4352, 1.4352, 1.4352, 1.4352,\n",
      "        1.4352, 1.4352, 1.4352, 1.4352, 1.4352, 1.4352, 1.4352, 1.4352, 1.4352,\n",
      "        1.4352, 1.1611, 0.9233, 0.7110, 0.5169, 0.3359, 0.1645]) return=  73251.7417815017\n",
      "probs of actions:  tensor([9.9940e-01, 9.9947e-01, 9.9895e-01, 9.9849e-01, 9.9850e-01, 9.9845e-01,\n",
      "        9.9896e-01, 9.9875e-01, 9.9878e-01, 9.9872e-01, 9.9879e-01, 9.9881e-01,\n",
      "        9.9813e-01, 9.9887e-01, 9.9903e-01, 9.9874e-01, 9.9859e-01, 6.3129e-04,\n",
      "        9.9900e-01, 9.9951e-01, 9.9974e-01, 9.9997e-01, 9.9944e-01, 9.9743e-01,\n",
      "        1.0000e+00], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4247, 0.3795, 0.3472, 0.3239, 0.3069, 0.2944, 0.2852, 0.2784,\n",
      "        0.2733, 0.2696, 0.2667, 0.2646, 0.2631, 0.2619, 0.2610, 0.2604, 0.2680,\n",
      "        0.2516, 0.2533, 0.2546, 0.2555, 0.2562, 0.2568, 0.2797])\n",
      "finalReturns:  tensor([0.3725, 0.3950, 0.3795, 0.3372, 0.2758, 0.2005, 0.1152])\n",
      "----------------------------------------\n",
      "iter  2  stage  17  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0020, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.6212, 1.6212, 1.6212, 1.6212, 1.6212, 1.6212, 1.6212, 1.6212, 1.6212,\n",
      "        1.6212, 1.6212, 1.6212, 1.6212, 1.6212, 1.6212, 1.6212, 1.6212, 1.6212,\n",
      "        1.3388, 1.0953, 0.8789, 0.6819, 0.4988, 0.3258, 0.1602]) return=  73445.38232037451\n",
      "probs of actions:  tensor([0.9994, 0.9995, 0.9990, 0.9985, 0.9985, 0.9984, 0.9990, 0.9987, 0.9988,\n",
      "        0.9987, 0.9988, 0.9988, 0.9981, 0.9989, 0.9990, 0.9987, 0.9986, 0.9990,\n",
      "        0.9990, 0.9995, 0.9997, 1.0000, 0.9994, 0.9974, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4247, 0.3795, 0.3472, 0.3239, 0.3069, 0.2944, 0.2852, 0.2784,\n",
      "        0.2733, 0.2696, 0.2667, 0.2646, 0.2631, 0.2619, 0.2610, 0.2604, 0.2599,\n",
      "        0.2595, 0.2592, 0.2590, 0.2589, 0.2587, 0.2587, 0.2811])\n",
      "finalReturns:  tensor([0.4738, 0.4963, 0.4804, 0.4375, 0.3755, 0.2997, 0.2140, 0.1209])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  2  stage  16  ep  368   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0027, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.7790, 1.7790, 1.7790, 1.7790, 1.7790, 1.7790, 1.7790, 1.7790, 1.7790,\n",
      "        1.7790, 1.7790, 1.7790, 1.7790, 1.7790, 1.7790, 1.7790, 1.7790, 1.4961,\n",
      "        1.2522, 1.0356, 0.8384, 0.6552, 0.4821, 0.3164, 0.1562]) return=  73445.38232037451\n",
      "probs of actions:  tensor([0.9994, 0.9995, 0.9990, 0.9985, 0.9985, 0.9985, 0.9990, 0.9988, 0.9988,\n",
      "        0.9987, 0.9988, 0.9988, 0.9982, 0.9989, 0.9991, 0.9988, 0.9990, 0.9993,\n",
      "        0.9990, 0.9995, 0.9998, 1.0000, 0.9995, 0.9975, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4247, 0.3795, 0.3472, 0.3239, 0.3069, 0.2944, 0.2852, 0.2784,\n",
      "        0.2733, 0.2696, 0.2667, 0.2646, 0.2631, 0.2619, 0.2610, 0.2604, 0.2599,\n",
      "        0.2595, 0.2592, 0.2590, 0.2589, 0.2587, 0.2587, 0.2811])\n",
      "finalReturns:  tensor([0.5764, 0.5989, 0.5829, 0.5401, 0.4780, 0.4022, 0.3164, 0.2234, 0.1249])\n",
      "----------------------------------------\n",
      "iter  2  stage  15  ep  147   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0035, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.9344, 1.9344, 1.9344, 1.9344, 1.9344, 1.9344, 1.9344, 1.9344, 1.9344,\n",
      "        1.9344, 1.9344, 1.9344, 1.9344, 1.9344, 1.9344, 1.9344, 1.6509, 1.4065,\n",
      "        1.1895, 0.9921, 0.8087, 0.6355, 0.4697, 0.3095, 0.1532]) return=  73445.38232037451\n",
      "probs of actions:  tensor([0.9994, 0.9995, 0.9990, 0.9985, 0.9986, 0.9985, 0.9990, 0.9988, 0.9988,\n",
      "        0.9988, 0.9988, 0.9989, 0.9982, 0.9989, 0.9991, 0.9990, 0.9991, 0.9994,\n",
      "        0.9990, 0.9995, 0.9998, 1.0000, 0.9995, 0.9976, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4247, 0.3795, 0.3472, 0.3239, 0.3069, 0.2944, 0.2852, 0.2784,\n",
      "        0.2733, 0.2696, 0.2667, 0.2646, 0.2631, 0.2619, 0.2610, 0.2604, 0.2599,\n",
      "        0.2595, 0.2592, 0.2590, 0.2589, 0.2587, 0.2587, 0.2811])\n",
      "finalReturns:  tensor([0.6820, 0.7045, 0.6885, 0.6456, 0.5835, 0.5077, 0.4219, 0.3288, 0.2303,\n",
      "        0.1279])\n",
      "----------------------------------------\n",
      "iter  2  stage  14  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0046, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.0884, 2.0884, 2.0884, 2.0884, 2.0884, 2.0884, 2.0884, 2.0884, 2.0884,\n",
      "        2.0884, 2.0884, 2.0884, 2.0884, 2.0884, 2.0884, 1.8040, 1.5590, 1.3416,\n",
      "        1.1439, 0.9602, 0.7869, 0.6210, 0.4606, 0.3043, 0.1510]) return=  73445.38232037451\n",
      "probs of actions:  tensor([0.9994, 0.9995, 0.9990, 0.9985, 0.9986, 0.9985, 0.9990, 0.9988, 0.9988,\n",
      "        0.9988, 0.9988, 0.9989, 0.9982, 0.9989, 0.9991, 0.9990, 0.9991, 0.9994,\n",
      "        0.9990, 0.9995, 0.9998, 1.0000, 0.9995, 0.9976, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4247, 0.3795, 0.3472, 0.3239, 0.3069, 0.2944, 0.2852, 0.2784,\n",
      "        0.2733, 0.2696, 0.2667, 0.2646, 0.2631, 0.2619, 0.2610, 0.2604, 0.2599,\n",
      "        0.2595, 0.2592, 0.2590, 0.2589, 0.2587, 0.2587, 0.2811])\n",
      "finalReturns:  tensor([0.7900, 0.8125, 0.7964, 0.7534, 0.6913, 0.6154, 0.5295, 0.4364, 0.3379,\n",
      "        0.2354, 0.1301])\n",
      "----------------------------------------\n",
      "iter  2  stage  13  ep  58   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0058, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.2417, 2.2417, 2.2417, 2.2417, 2.2417, 2.2417, 2.2417, 2.2417, 2.2417,\n",
      "        2.2417, 2.2417, 2.2417, 2.2417, 2.2417, 1.9561, 1.7103, 1.4924, 1.2942,\n",
      "        1.1103, 0.9367, 0.7707, 0.6102, 0.4538, 0.3005, 0.1494]) return=  73445.38232037451\n",
      "probs of actions:  tensor([0.9994, 0.9995, 0.9990, 0.9985, 0.9985, 0.9985, 0.9990, 0.9988, 0.9988,\n",
      "        0.9987, 0.9988, 0.9988, 0.9982, 0.9990, 0.9991, 0.9990, 0.9991, 0.9994,\n",
      "        0.9990, 0.9995, 0.9998, 1.0000, 0.9994, 0.9975, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4247, 0.3795, 0.3472, 0.3239, 0.3069, 0.2944, 0.2852, 0.2784,\n",
      "        0.2733, 0.2696, 0.2667, 0.2646, 0.2631, 0.2619, 0.2610, 0.2604, 0.2599,\n",
      "        0.2595, 0.2592, 0.2590, 0.2589, 0.2587, 0.2587, 0.2811])\n",
      "finalReturns:  tensor([0.8997, 0.9222, 0.9061, 0.8630, 0.8008, 0.7248, 0.6389, 0.5457, 0.4472,\n",
      "        0.3447, 0.2393, 0.1317])\n",
      "----------------------------------------\n",
      "iter  2  stage  12  ep  8390   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0049, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.3953, 2.3953, 2.3953, 2.3953, 2.3953, 2.3953, 2.3953, 2.3953, 2.3953,\n",
      "        2.3953, 2.3953, 2.3953, 2.3953, 2.1081, 1.8612, 1.6425, 1.4438, 1.2595,\n",
      "        1.0856, 0.9193, 0.7587, 0.6022, 0.4488, 0.2976, 0.1482]) return=  73445.38232037451\n",
      "probs of actions:  tensor([0.9996, 0.9996, 0.9992, 0.9988, 0.9989, 0.9988, 0.9992, 0.9990, 0.9990,\n",
      "        0.9990, 0.9991, 0.9991, 0.9990, 0.9993, 0.9995, 0.9994, 0.9994, 0.9999,\n",
      "        0.9993, 0.9997, 0.9999, 1.0000, 0.9996, 0.9979, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4247, 0.3795, 0.3472, 0.3239, 0.3069, 0.2944, 0.2852, 0.2784,\n",
      "        0.2733, 0.2696, 0.2667, 0.2646, 0.2631, 0.2619, 0.2610, 0.2604, 0.2599,\n",
      "        0.2595, 0.2592, 0.2590, 0.2589, 0.2587, 0.2587, 0.2811])\n",
      "finalReturns:  tensor([1.0108, 1.0333, 1.0171, 0.9739, 0.9116, 0.8355, 0.7495, 0.6563, 0.5577,\n",
      "        0.4552, 0.3497, 0.2421, 0.1329])\n",
      "----------------------------------------\n",
      "iter  2  stage  11  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0062, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.5497, 2.5497, 2.5497, 2.5497, 2.5497, 2.5497, 2.5497, 2.5497, 2.5497,\n",
      "        2.5497, 2.5497, 2.5497, 2.2605, 2.0121, 1.7924, 1.5929, 1.4081, 1.2338,\n",
      "        1.0673, 0.9064, 0.7498, 0.5962, 0.4450, 0.2955, 0.1473]) return=  73445.38232037451\n",
      "probs of actions:  tensor([0.9996, 0.9996, 0.9992, 0.9988, 0.9989, 0.9988, 0.9992, 0.9990, 0.9990,\n",
      "        0.9990, 0.9991, 0.9991, 0.9990, 0.9993, 0.9995, 0.9994, 0.9994, 0.9999,\n",
      "        0.9993, 0.9997, 0.9999, 1.0000, 0.9996, 0.9979, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4247, 0.3795, 0.3472, 0.3239, 0.3069, 0.2944, 0.2852, 0.2784,\n",
      "        0.2733, 0.2696, 0.2667, 0.2646, 0.2631, 0.2619, 0.2610, 0.2604, 0.2599,\n",
      "        0.2595, 0.2592, 0.2590, 0.2589, 0.2587, 0.2587, 0.2811])\n",
      "finalReturns:  tensor([1.1231, 1.1456, 1.1293, 1.0859, 1.0235, 0.9473, 0.8612, 0.7678, 0.6692,\n",
      "        0.5666, 0.4611, 0.3535, 0.2443, 0.1338])\n",
      "----------------------------------------\n",
      "iter  2  stage  10  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0078, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.7060, 2.7060, 2.7060, 2.7060, 2.7060, 2.7060, 2.7060, 2.7060, 2.7060,\n",
      "        2.7060, 2.7060, 2.4139, 2.1636, 1.9425, 1.7421, 1.5565, 1.3817, 1.2148,\n",
      "        1.0537, 0.8968, 0.7431, 0.5918, 0.4422, 0.2939, 0.1466]) return=  73445.38232037451\n",
      "probs of actions:  tensor([0.9996, 0.9996, 0.9992, 0.9988, 0.9989, 0.9988, 0.9992, 0.9990, 0.9990,\n",
      "        0.9990, 0.9991, 0.9991, 0.9990, 0.9993, 0.9995, 0.9994, 0.9994, 0.9999,\n",
      "        0.9993, 0.9997, 0.9999, 1.0000, 0.9996, 0.9979, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4247, 0.3795, 0.3472, 0.3239, 0.3069, 0.2944, 0.2852, 0.2784,\n",
      "        0.2733, 0.2696, 0.2667, 0.2646, 0.2631, 0.2619, 0.2610, 0.2604, 0.2599,\n",
      "        0.2595, 0.2592, 0.2590, 0.2589, 0.2587, 0.2587, 0.2811])\n",
      "finalReturns:  tensor([1.2364, 1.2589, 1.2424, 1.1989, 1.1362, 1.0599, 0.9736, 0.8802, 0.7815,\n",
      "        0.6788, 0.5733, 0.4656, 0.3563, 0.2459, 0.1345])\n",
      "----------------------------------------\n",
      "iter  2  stage  9  ep  3   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0095, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.8650, 2.8650, 2.8650, 2.8650, 2.8650, 2.8650, 2.8650, 2.8650, 2.8650,\n",
      "        2.8650, 2.5692, 2.3163, 2.0933, 1.8916, 1.7051, 1.5296, 1.3622, 1.2007,\n",
      "        1.0436, 0.8897, 0.7382, 0.5884, 0.4401, 0.2927, 0.1461]) return=  73445.38232037451\n",
      "probs of actions:  tensor([0.9996, 0.9996, 0.9992, 0.9988, 0.9989, 0.9988, 0.9992, 0.9990, 0.9991,\n",
      "        0.9990, 0.9991, 0.9991, 0.9990, 0.9993, 0.9995, 0.9994, 0.9994, 0.9999,\n",
      "        0.9993, 0.9997, 0.9999, 1.0000, 0.9996, 0.9979, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4247, 0.3795, 0.3472, 0.3239, 0.3069, 0.2944, 0.2852, 0.2784,\n",
      "        0.2733, 0.2696, 0.2667, 0.2646, 0.2631, 0.2619, 0.2610, 0.2604, 0.2599,\n",
      "        0.2595, 0.2592, 0.2590, 0.2589, 0.2587, 0.2587, 0.2811])\n",
      "finalReturns:  tensor([1.3506, 1.3731, 1.3565, 1.3127, 1.2498, 1.1732, 1.0868, 0.9932, 0.8943,\n",
      "        0.7916, 0.6860, 0.5782, 0.4689, 0.3584, 0.2471, 0.1350])\n",
      "----------------------------------------\n",
      "iter  2  stage  8  ep  0   adversary:  AdversaryModes.constant_95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0113, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.0282, 3.0282, 3.0282, 3.0282, 3.0282, 3.0282, 3.0282, 3.0282, 3.0282,\n",
      "        2.7273, 2.4709, 2.2454, 2.0419, 1.8542, 1.6778, 1.5097, 1.3477, 1.1902,\n",
      "        1.0360, 0.8843, 0.7344, 0.5860, 0.4385, 0.2918, 0.1457]) return=  73445.38232037451\n",
      "probs of actions:  tensor([0.9996, 0.9996, 0.9992, 0.9988, 0.9989, 0.9988, 0.9992, 0.9990, 0.9991,\n",
      "        0.9990, 0.9991, 0.9991, 0.9990, 0.9993, 0.9995, 0.9994, 0.9994, 0.9999,\n",
      "        0.9993, 0.9997, 0.9999, 1.0000, 0.9996, 0.9979, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4247, 0.3795, 0.3472, 0.3239, 0.3069, 0.2944, 0.2852, 0.2784,\n",
      "        0.2733, 0.2696, 0.2667, 0.2646, 0.2631, 0.2619, 0.2610, 0.2604, 0.2599,\n",
      "        0.2595, 0.2592, 0.2590, 0.2589, 0.2587, 0.2587, 0.2811])\n",
      "finalReturns:  tensor([1.4659, 1.4884, 1.4715, 1.4274, 1.3641, 1.2872, 1.2005, 1.1067, 1.0077,\n",
      "        0.9048, 0.7991, 0.6913, 0.5820, 0.4714, 0.3600, 0.2480, 0.1354])\n",
      "----------------------------------------\n",
      "iter  2  stage  7  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0132, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.1970, 3.1970, 3.1970, 3.1970, 3.1970, 3.1970, 3.1970, 3.1970, 2.8893,\n",
      "        2.6282, 2.3995, 2.1936, 2.0042, 1.8265, 1.6575, 1.4949, 1.3369, 1.1823,\n",
      "        1.0304, 0.8803, 0.7317, 0.5841, 0.4373, 0.2911, 0.1454]) return=  73445.38232037451\n",
      "probs of actions:  tensor([0.9996, 0.9996, 0.9992, 0.9988, 0.9989, 0.9988, 0.9992, 0.9990, 0.9991,\n",
      "        0.9990, 0.9991, 0.9991, 0.9990, 0.9993, 0.9995, 0.9994, 0.9994, 0.9999,\n",
      "        0.9993, 0.9997, 0.9999, 1.0000, 0.9996, 0.9979, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4247, 0.3795, 0.3472, 0.3239, 0.3069, 0.2944, 0.2852, 0.2784,\n",
      "        0.2733, 0.2696, 0.2667, 0.2646, 0.2631, 0.2619, 0.2610, 0.2604, 0.2599,\n",
      "        0.2595, 0.2592, 0.2590, 0.2589, 0.2587, 0.2587, 0.2811])\n",
      "finalReturns:  tensor([1.5822, 1.6047, 1.5874, 1.5429, 1.4792, 1.4019, 1.3149, 1.2208, 1.1215,\n",
      "        1.0185, 0.9127, 0.8048, 0.6953, 0.5847, 0.4733, 0.3612, 0.2486, 0.1357])\n",
      "----------------------------------------\n",
      "iter  2  stage  6  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0150, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.3738, 3.3738, 3.3738, 3.3738, 3.3738, 3.3738, 3.3738, 3.0569, 2.7895,\n",
      "        2.5563, 2.3473, 2.1555, 1.9762, 1.8060, 1.6425, 1.4838, 1.3288, 1.1765,\n",
      "        1.0261, 0.8773, 0.7296, 0.5827, 0.4364, 0.2906, 0.1452]) return=  73445.38232037451\n",
      "probs of actions:  tensor([0.9996, 0.9996, 0.9992, 0.9988, 0.9989, 0.9988, 0.9992, 0.9990, 0.9991,\n",
      "        0.9990, 0.9991, 0.9991, 0.9990, 0.9993, 0.9995, 0.9994, 0.9994, 0.9999,\n",
      "        0.9993, 0.9997, 0.9999, 1.0000, 0.9996, 0.9979, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4247, 0.3795, 0.3472, 0.3239, 0.3069, 0.2944, 0.2852, 0.2784,\n",
      "        0.2733, 0.2696, 0.2667, 0.2646, 0.2631, 0.2619, 0.2610, 0.2604, 0.2599,\n",
      "        0.2595, 0.2592, 0.2590, 0.2589, 0.2587, 0.2587, 0.2811])\n",
      "finalReturns:  tensor([1.6998, 1.7223, 1.7046, 1.6594, 1.5951, 1.5173, 1.4298, 1.3353, 1.2358,\n",
      "        1.1326, 1.0266, 0.9185, 0.8090, 0.6983, 0.5868, 0.4747, 0.3621, 0.2491,\n",
      "        0.1359])\n",
      "----------------------------------------\n",
      "iter  2  stage  5  ep  18525   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0105, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.5615, 3.5615, 3.5615, 3.5615, 3.5615, 3.5615, 3.2322, 2.9561, 2.7169,\n",
      "        2.5036, 2.3089, 2.1274, 1.9555, 1.7908, 1.6313, 1.4756, 1.3228, 1.1721,\n",
      "        1.0230, 0.8751, 0.7280, 0.5816, 0.4358, 0.2903, 0.1450]) return=  73445.38232037451\n",
      "probs of actions:  tensor([0.9996, 0.9996, 0.9993, 0.9990, 0.9990, 0.9990, 0.9993, 0.9997, 0.9994,\n",
      "        0.9992, 1.0000, 0.9999, 0.9993, 0.9994, 0.9995, 0.9996, 0.9997, 1.0000,\n",
      "        0.9995, 1.0000, 1.0000, 1.0000, 0.9997, 0.9984, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4247, 0.3795, 0.3472, 0.3239, 0.3069, 0.2944, 0.2852, 0.2784,\n",
      "        0.2733, 0.2696, 0.2667, 0.2646, 0.2631, 0.2619, 0.2610, 0.2604, 0.2599,\n",
      "        0.2595, 0.2592, 0.2590, 0.2589, 0.2587, 0.2587, 0.2811])\n",
      "finalReturns:  tensor([1.8189, 1.8414, 1.8231, 1.7771, 1.7120, 1.6335, 1.5454, 1.4505, 1.3506,\n",
      "        1.2470, 1.1408, 1.0326, 0.9229, 0.8122, 0.7006, 0.5884, 0.4757, 0.3627,\n",
      "        0.2495, 0.1361])\n",
      "----------------------------------------\n",
      "iter  2  stage  4  ep  15   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0127, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.7642, 3.7642, 3.7642, 3.7642, 3.7642, 3.4179, 3.1301, 2.8828, 2.6638,\n",
      "        2.4649, 2.2805, 2.1065, 1.9402, 1.7795, 1.6229, 1.4694, 1.3183, 1.1688,\n",
      "        1.0206, 0.8734, 0.7269, 0.5809, 0.4353, 0.2900, 0.1449]) return=  73445.38232037451\n",
      "probs of actions:  tensor([0.9996, 0.9996, 0.9993, 0.9990, 0.9990, 0.9990, 0.9993, 0.9997, 0.9994,\n",
      "        0.9992, 1.0000, 0.9999, 0.9993, 0.9994, 0.9995, 0.9996, 0.9997, 1.0000,\n",
      "        0.9995, 1.0000, 1.0000, 1.0000, 0.9997, 0.9984, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4247, 0.3795, 0.3472, 0.3239, 0.3069, 0.2944, 0.2852, 0.2784,\n",
      "        0.2733, 0.2696, 0.2667, 0.2646, 0.2631, 0.2619, 0.2610, 0.2604, 0.2599,\n",
      "        0.2595, 0.2592, 0.2590, 0.2589, 0.2587, 0.2587, 0.2811])\n",
      "finalReturns:  tensor([1.9401, 1.9626, 1.9435, 1.8964, 1.8302, 1.7507, 1.6619, 1.5663, 1.4659,\n",
      "        1.3619, 1.2554, 1.1470, 1.0371, 0.9262, 0.8145, 0.7022, 0.5895, 0.4765,\n",
      "        0.3632, 0.2498, 0.1362])\n",
      "----------------------------------------\n",
      "iter  2  stage  3  ep  7   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0151, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.9877, 3.9877, 3.9877, 3.9877, 3.6180, 3.3143, 3.0560, 2.8292, 2.6248,\n",
      "        2.4363, 2.2594, 2.0910, 1.9287, 1.7710, 1.6167, 1.4648, 1.3149, 1.1664,\n",
      "        1.0188, 0.8721, 0.7260, 0.5803, 0.4349, 0.2898, 0.1448]) return=  73445.38232037451\n",
      "probs of actions:  tensor([0.9996, 0.9996, 0.9993, 0.9990, 0.9990, 0.9990, 0.9993, 0.9997, 0.9994,\n",
      "        0.9992, 1.0000, 0.9999, 0.9993, 0.9994, 0.9995, 0.9996, 0.9997, 1.0000,\n",
      "        0.9995, 1.0000, 1.0000, 1.0000, 0.9997, 0.9984, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4247, 0.3795, 0.3472, 0.3239, 0.3069, 0.2944, 0.2852, 0.2784,\n",
      "        0.2733, 0.2696, 0.2667, 0.2646, 0.2631, 0.2619, 0.2610, 0.2604, 0.2599,\n",
      "        0.2595, 0.2592, 0.2590, 0.2589, 0.2587, 0.2587, 0.2811])\n",
      "finalReturns:  tensor([2.0639, 2.0864, 2.0661, 2.0176, 1.9500, 1.8693, 1.7793, 1.6829, 1.5818,\n",
      "        1.4773, 1.3704, 1.2617, 1.1516, 1.0405, 0.9287, 0.8163, 0.7035, 0.5904,\n",
      "        0.4771, 0.3636, 0.2500, 0.1363])\n",
      "----------------------------------------\n",
      "iter  2  stage  2  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0170, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.2401, 4.2401, 4.2401, 3.8380, 3.5125, 3.2391, 3.0017, 2.7898, 2.5959,\n",
      "        2.4151, 2.2438, 2.0794, 1.9201, 1.7646, 1.6120, 1.4614, 1.3124, 1.1645,\n",
      "        1.0175, 0.8712, 0.7253, 0.5798, 0.4346, 0.2896, 0.1447]) return=  73445.38232037451\n",
      "probs of actions:  tensor([0.9996, 0.9996, 0.9993, 0.9990, 0.9990, 0.9990, 0.9993, 0.9997, 0.9994,\n",
      "        0.9992, 1.0000, 0.9999, 0.9993, 0.9994, 0.9995, 0.9996, 0.9997, 1.0000,\n",
      "        0.9995, 1.0000, 1.0000, 1.0000, 0.9997, 0.9984, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4247, 0.3795, 0.3472, 0.3239, 0.3069, 0.2944, 0.2852, 0.2784,\n",
      "        0.2733, 0.2696, 0.2667, 0.2646, 0.2631, 0.2619, 0.2610, 0.2604, 0.2599,\n",
      "        0.2595, 0.2592, 0.2590, 0.2589, 0.2587, 0.2587, 0.2811])\n",
      "finalReturns:  tensor([2.1910, 2.2135, 2.1918, 2.1414, 2.0719, 1.9895, 1.8981, 1.8006, 1.6986,\n",
      "        1.5934, 1.4859, 1.3767, 1.2663, 1.1550, 1.0430, 0.9305, 0.8176, 0.7044,\n",
      "        0.5911, 0.4775, 0.3639, 0.2502, 0.1364])\n",
      "----------------------------------------\n",
      "iter  2  stage  1  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0182, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.5331, 4.5331, 4.0859, 3.7300, 3.4358, 3.1839, 2.9618, 2.7606, 2.5745,\n",
      "        2.3993, 2.2321, 2.0708, 1.9137, 1.7599, 1.6085, 1.4588, 1.3105, 1.1631,\n",
      "        1.0165, 0.8705, 0.7248, 0.5795, 0.4344, 0.2895, 0.1447]) return=  73445.38232037451\n",
      "probs of actions:  tensor([0.9996, 0.9996, 0.9993, 0.9990, 0.9990, 0.9990, 0.9993, 0.9997, 0.9994,\n",
      "        0.9992, 1.0000, 0.9999, 0.9993, 0.9994, 0.9995, 0.9996, 0.9997, 1.0000,\n",
      "        0.9995, 1.0000, 1.0000, 1.0000, 0.9997, 0.9984, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4247, 0.3795, 0.3472, 0.3239, 0.3069, 0.2944, 0.2852, 0.2784,\n",
      "        0.2733, 0.2696, 0.2667, 0.2646, 0.2631, 0.2619, 0.2610, 0.2604, 0.2599,\n",
      "        0.2595, 0.2592, 0.2590, 0.2589, 0.2587, 0.2587, 0.2811])\n",
      "finalReturns:  tensor([2.3227, 2.3452, 2.3216, 2.2686, 2.1965, 2.1118, 2.0186, 1.9195, 1.8163,\n",
      "        1.7102, 1.6020, 1.4923, 1.3815, 1.2698, 1.1576, 1.0449, 0.9319, 0.8186,\n",
      "        0.7052, 0.5916, 0.4779, 0.3641, 0.2503, 0.1364])\n",
      "----------------------------------------\n",
      "iter  2  stage  0  ep  0   adversary:  AdversaryModes.constant_95\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0196, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.8841, 4.3729, 3.9744, 3.6513, 3.3795, 3.1433, 2.9322, 2.7389, 2.5586,\n",
      "        2.3875, 2.2234, 2.0643, 1.9089, 1.7564, 1.6059, 1.4569, 1.3091, 1.1621,\n",
      "        1.0158, 0.8699, 0.7245, 0.5793, 0.4343, 0.2894, 0.1447]) return=  73445.38232037451\n",
      "probs of actions:  tensor([0.9996, 0.9996, 0.9993, 0.9990, 0.9990, 0.9990, 0.9993, 0.9997, 0.9994,\n",
      "        0.9992, 1.0000, 0.9999, 0.9993, 0.9994, 0.9996, 0.9996, 0.9997, 1.0000,\n",
      "        0.9995, 1.0000, 1.0000, 1.0000, 0.9997, 0.9984, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.4247, 0.3795, 0.3472, 0.3239, 0.3069, 0.2944, 0.2852, 0.2784,\n",
      "        0.2733, 0.2696, 0.2667, 0.2646, 0.2631, 0.2619, 0.2610, 0.2604, 0.2599,\n",
      "        0.2595, 0.2592, 0.2590, 0.2589, 0.2587, 0.2587, 0.2811])\n",
      "finalReturns:  tensor([2.4604, 2.4829, 2.4566, 2.4002, 2.3248, 2.2371, 2.1414, 2.0403, 1.9355,\n",
      "        1.8281, 1.7189, 1.6085, 1.4971, 1.3850, 1.2725, 1.1595, 1.0463, 0.9329,\n",
      "        0.8194, 0.7057, 0.5919, 0.4781, 0.3643, 0.2504, 0.1364])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1682682010 saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[822902, 'tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])', 73445.38232037451, 85129.48297577976, 0.01956232078373432, 1e-05, 1, 0, 'tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\\n        15, 15, 15, 15, 15, 15,  0])', '[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\\n 1.]', '0,[1e-05,1][1, 10000, 1, 1],1682682010', 25, 50, 161287.58344151574, 184572.3091501231, 73445.38232037451, 135544.55463053397, 132647.75466666667, 112524.66365021843, 112524.66365021843, 117051.59338141435, 117051.59338141435, 85375.85726043607, 112524.66365021843, 117051.59338141435]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x648 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy reset\n",
      "----------------------------------------\n",
      "iter  0  stage  24  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([ 1,  1,  0, 28,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  1,  0,  0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5204, 0.5204, 0.5204, 0.5204, 0.5204, 0.5204, 0.5204, 0.5204, 0.5204,\n",
      "        0.5204, 0.5204, 0.5204, 0.5204, 0.5204, 0.5204, 0.5204, 0.5204, 0.5204,\n",
      "        0.5204, 0.5204, 0.5204, 0.5204, 0.5204, 0.5204, 0.5204]) return=  130427.71758286069\n",
      "probs of actions:  tensor([1.2898e-01, 1.4863e-01, 7.8200e-01, 5.0099e-04, 8.0009e-01, 8.2463e-01,\n",
      "        7.8552e-01, 7.7865e-01, 8.1916e-01, 7.8481e-01, 7.8624e-01, 7.9122e-01,\n",
      "        8.0141e-01, 1.2257e-01, 8.0664e-01, 8.0212e-01, 8.3226e-01, 8.0869e-01,\n",
      "        8.1746e-01, 8.0829e-01, 8.0075e-01, 8.0096e-01, 1.2746e-01, 8.1773e-01,\n",
      "        9.4606e-01], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5111, 0.5273, 0.5234, 0.4424, 0.6274, 0.4963, 0.5276, 0.5197, 0.5217,\n",
      "        0.5212, 0.5213, 0.5213, 0.5213, 0.5212, 0.5249, 0.5204, 0.5215, 0.5212,\n",
      "        0.5213, 0.5213, 0.5213, 0.5213, 0.5212, 0.5249, 0.5204])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  0  stage  23  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([16,  0,  1, 16,  0, 16,  0, 16,  0,  0,  4, 18,  0,  0, 16,  7, 13, 18,\n",
      "        16, 16, 16,  0, 16, 16,  0])\n",
      "loss=  tensor(0.0165, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0899, 1.0899, 1.0899, 1.0899, 1.0899, 1.0899, 1.0899, 1.0899, 1.0899,\n",
      "        1.0899, 1.0899, 1.0899, 1.0899, 1.0899, 1.0899, 1.0899, 1.0899, 1.0899,\n",
      "        1.0899, 1.0899, 1.0899, 1.0899, 1.0899, 1.0899, 0.5062]) return=  133455.20582218055\n",
      "probs of actions:  tensor([0.3193, 0.3793, 0.0503, 0.2874, 0.5107, 0.3032, 0.3821, 0.2562, 0.4683,\n",
      "        0.3529, 0.0051, 0.1597, 0.2929, 0.4627, 0.3191, 0.0089, 0.0342, 0.1184,\n",
      "        0.2100, 0.3366, 0.3129, 0.4630, 0.3278, 0.6132, 0.9927],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4856, 0.5833, 0.5062, 0.5031, 0.5787, 0.4818, 0.5843, 0.4805, 0.5847,\n",
      "        0.5060, 0.5235, 0.5024, 0.5847, 0.5060, 0.4995, 0.5747, 0.5155, 0.5340,\n",
      "        0.5510, 0.5409, 0.5434, 0.5684, 0.4842, 0.5581, 0.5648])\n",
      "finalReturns:  tensor([0.0329, 0.0585])\n",
      "----------------------------------------\n",
      "iter  0  stage  22  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 16, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 16,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0506, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.5443, 1.5443, 1.5443, 1.5443, 1.5443, 1.5443, 1.5443, 1.5443, 1.5443,\n",
      "        1.5443, 1.5443, 1.5443, 1.5443, 1.5443, 1.5443, 1.5443, 1.5443, 1.5443,\n",
      "        1.5443, 1.5443, 1.5443, 1.5443, 1.5443, 0.9698, 0.4614]) return=  135378.46941865046\n",
      "probs of actions:  tensor([0.7424, 0.7524, 0.7040, 0.7659, 0.7584, 0.2114, 0.7666, 0.7316, 0.7425,\n",
      "        0.7602, 0.7405, 0.7840, 0.7915, 0.7560, 0.7886, 0.7388, 0.7687, 0.1881,\n",
      "        0.7565, 0.7638, 0.7718, 0.7267, 0.8800, 0.7854, 0.9964],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5490, 0.5346, 0.5441, 0.5417,\n",
      "        0.5423, 0.5421, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5490,\n",
      "        0.5346, 0.5441, 0.5417, 0.5423, 0.5421, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([0.1146, 0.1470, 0.1132])\n",
      "----------------------------------------\n",
      "iter  0  stage  21  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0087, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.9719, 1.9719, 1.9719, 1.9719, 1.9719, 1.9719, 1.9719, 1.9719, 1.9719,\n",
      "        1.9719, 1.9719, 1.9719, 1.9719, 1.9719, 1.9719, 1.9719, 1.9719, 1.9719,\n",
      "        1.9719, 1.9719, 1.9719, 1.9719, 1.3974, 0.8890, 0.4276]) return=  135363.21866666665\n",
      "probs of actions:  tensor([0.9761, 0.9773, 0.9695, 0.9786, 0.9814, 0.9793, 0.9797, 0.9766, 0.9777,\n",
      "        0.9793, 0.9776, 0.9815, 0.9828, 0.9794, 0.9828, 0.9772, 0.9826, 0.9781,\n",
      "        0.9808, 0.9789, 0.9796, 0.9874, 0.9920, 0.9846, 0.9985],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([0.2291, 0.2615, 0.2277, 0.1469])\n",
      "----------------------------------------\n",
      "iter  0  stage  20  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0133, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.3751, 2.3751, 2.3751, 2.3751, 2.3751, 2.3751, 2.3751, 2.3751, 2.3751,\n",
      "        2.3751, 2.3751, 2.3751, 2.3751, 2.3751, 2.3751, 2.3751, 2.3751, 2.3751,\n",
      "        2.3751, 2.3751, 2.3751, 1.8005, 1.2922, 0.8308, 0.4032]) return=  135363.21866666665\n",
      "probs of actions:  tensor([0.9824, 0.9830, 0.9781, 0.9842, 0.9869, 0.9853, 0.9850, 0.9833, 0.9834,\n",
      "        0.9845, 0.9833, 0.9865, 0.9871, 0.9851, 0.9869, 0.9831, 0.9873, 0.9838,\n",
      "        0.9868, 0.9844, 0.9890, 0.9896, 0.9933, 0.9911, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([0.3681, 0.4005, 0.3667, 0.2859, 0.1714])\n",
      "----------------------------------------\n",
      "iter  0  stage  19  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0049, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.7604, 2.7604, 2.7604, 2.7604, 2.7604, 2.7604, 2.7604, 2.7604, 2.7604,\n",
      "        2.7604, 2.7604, 2.7604, 2.7604, 2.7604, 2.7604, 2.7604, 2.7604, 2.7604,\n",
      "        2.7604, 2.7604, 2.1858, 1.6775, 1.2161, 0.7885, 0.3853]) return=  135363.21866666665\n",
      "probs of actions:  tensor([0.9953, 0.9953, 0.9940, 0.9958, 0.9968, 0.9965, 0.9960, 0.9958, 0.9957,\n",
      "        0.9960, 0.9956, 0.9964, 0.9968, 0.9963, 0.9966, 0.9957, 0.9969, 0.9957,\n",
      "        0.9966, 0.9966, 0.9979, 0.9984, 0.9988, 0.9983, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([0.5250, 0.5574, 0.5236, 0.4428, 0.3283, 0.1893])\n",
      "----------------------------------------\n",
      "iter  0  stage  18  ep  27994   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0030, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.1325, 3.1325, 3.1325, 3.1325, 3.1325, 3.1325, 3.1325, 3.1325, 3.1325,\n",
      "        3.1325, 3.1325, 3.1325, 3.1325, 3.1325, 3.1325, 3.1325, 3.1325, 3.1325,\n",
      "        3.1325, 2.5580, 2.0496, 1.5882, 1.1606, 0.7574, 0.3721]) return=  135363.21866666665\n",
      "probs of actions:  tensor([0.9977, 0.9977, 0.9971, 0.9980, 0.9985, 0.9984, 0.9980, 0.9980, 0.9980,\n",
      "        0.9981, 0.9979, 0.9982, 0.9985, 0.9983, 0.9984, 0.9980, 0.9986, 0.9980,\n",
      "        0.9990, 0.9989, 0.9989, 0.9993, 0.9996, 0.9995, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([0.6950, 0.7274, 0.6936, 0.6128, 0.4983, 0.3593, 0.2024])\n",
      "----------------------------------------\n",
      "iter  0  stage  17  ep  19492   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0024, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.4950, 3.4950, 3.4950, 3.4950, 3.4950, 3.4950, 3.4950, 3.4950, 3.4950,\n",
      "        3.4950, 3.4950, 3.4950, 3.4950, 3.4950, 3.4950, 3.4950, 3.4950, 3.4950,\n",
      "        2.9204, 2.4120, 1.9507, 1.5230, 1.1199, 0.7346, 0.3624]) return=  135363.21866666665\n",
      "probs of actions:  tensor([0.9987, 0.9987, 0.9983, 0.9988, 0.9992, 0.9991, 0.9989, 0.9988, 0.9989,\n",
      "        0.9989, 0.9988, 0.9990, 0.9992, 0.9990, 0.9991, 0.9988, 0.9992, 0.9990,\n",
      "        0.9997, 0.9995, 0.9996, 0.9996, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([0.8747, 0.9071, 0.8733, 0.7926, 0.6780, 0.5390, 0.3821, 0.2121])\n",
      "----------------------------------------\n",
      "iter  0  stage  16  ep  0   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0036, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.8502, 3.8502, 3.8502, 3.8502, 3.8502, 3.8502, 3.8502, 3.8502, 3.8502,\n",
      "        3.8502, 3.8502, 3.8502, 3.8502, 3.8502, 3.8502, 3.8502, 3.8502, 3.2757,\n",
      "        2.7673, 2.3059, 1.8783, 1.4751, 1.0898, 0.7177, 0.3552]) return=  135363.21866666665\n",
      "probs of actions:  tensor([0.9987, 0.9987, 0.9983, 0.9988, 0.9992, 0.9991, 0.9989, 0.9988, 0.9989,\n",
      "        0.9989, 0.9988, 0.9990, 0.9992, 0.9990, 0.9991, 0.9989, 0.9992, 0.9990,\n",
      "        0.9997, 0.9995, 0.9996, 0.9996, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([1.0617, 1.0941, 1.0603, 0.9795, 0.8649, 0.7259, 0.5691, 0.3991, 0.2193])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  0  stage  15  ep  79   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0051, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.2001, 4.2001, 4.2001, 4.2001, 4.2001, 4.2001, 4.2001, 4.2001, 4.2001,\n",
      "        4.2001, 4.2001, 4.2001, 4.2001, 4.2001, 4.2001, 4.2001, 3.6255, 3.1172,\n",
      "        2.6558, 2.2282, 1.8250, 1.4397, 1.0676, 0.7051, 0.3499]) return=  135363.21866666665\n",
      "probs of actions:  tensor([0.9988, 0.9987, 0.9984, 0.9989, 0.9992, 0.9991, 0.9989, 0.9989, 0.9989,\n",
      "        0.9990, 0.9988, 0.9990, 0.9992, 0.9991, 0.9991, 0.9990, 0.9993, 0.9990,\n",
      "        0.9997, 0.9995, 0.9996, 0.9997, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([1.2539, 1.2863, 1.2525, 1.1718, 1.0572, 0.9182, 0.7613, 0.5913, 0.4116,\n",
      "        0.2247])\n",
      "----------------------------------------\n",
      "iter  0  stage  14  ep  0   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0069, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.5460, 4.5460, 4.5460, 4.5460, 4.5460, 4.5460, 4.5460, 4.5460, 4.5460,\n",
      "        4.5460, 4.5460, 4.5460, 4.5460, 4.5460, 4.5460, 3.9715, 3.4631, 3.0017,\n",
      "        2.5741, 2.1709, 1.7856, 1.4135, 1.0510, 0.6958, 0.3459]) return=  135363.21866666665\n",
      "probs of actions:  tensor([0.9988, 0.9987, 0.9984, 0.9989, 0.9992, 0.9992, 0.9989, 0.9989, 0.9989,\n",
      "        0.9990, 0.9988, 0.9990, 0.9992, 0.9991, 0.9991, 0.9990, 0.9993, 0.9990,\n",
      "        0.9997, 0.9995, 0.9996, 0.9997, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([1.4502, 1.4826, 1.4488, 1.3680, 1.2535, 1.1145, 0.9576, 0.7876, 0.6079,\n",
      "        0.4209, 0.2287])\n",
      "----------------------------------------\n",
      "iter  0  stage  13  ep  0   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0090, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.8889, 4.8889, 4.8889, 4.8889, 4.8889, 4.8889, 4.8889, 4.8889, 4.8889,\n",
      "        4.8889, 4.8889, 4.8889, 4.8889, 4.8889, 4.3144, 3.8060, 3.3446, 2.9170,\n",
      "        2.5138, 2.1285, 1.7564, 1.3940, 1.0387, 0.6888, 0.3429]) return=  135363.21866666665\n",
      "probs of actions:  tensor([0.9988, 0.9987, 0.9984, 0.9989, 0.9992, 0.9992, 0.9989, 0.9989, 0.9989,\n",
      "        0.9990, 0.9988, 0.9990, 0.9992, 0.9991, 0.9991, 0.9990, 0.9993, 0.9990,\n",
      "        0.9997, 0.9995, 0.9996, 0.9997, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([1.6494, 1.6818, 1.6480, 1.5672, 1.4527, 1.3137, 1.1568, 0.9868, 0.8071,\n",
      "        0.6202, 0.4279, 0.2316])\n",
      "----------------------------------------\n",
      "iter  0  stage  12  ep  0   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0112, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.2297, 5.2297, 5.2297, 5.2297, 5.2297, 5.2297, 5.2297, 5.2297, 5.2297,\n",
      "        5.2297, 5.2297, 5.2297, 5.2297, 4.6551, 4.1467, 3.6853, 3.2577, 2.8545,\n",
      "        2.4693, 2.0971, 1.7347, 1.3794, 1.0295, 0.6836, 0.3407]) return=  135363.21866666665\n",
      "probs of actions:  tensor([0.9988, 0.9987, 0.9984, 0.9989, 0.9992, 0.9992, 0.9989, 0.9989, 0.9989,\n",
      "        0.9990, 0.9988, 0.9990, 0.9992, 0.9991, 0.9991, 0.9990, 0.9993, 0.9990,\n",
      "        0.9997, 0.9995, 0.9996, 0.9997, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([1.8509, 1.8833, 1.8495, 1.7687, 1.6542, 1.5152, 1.3583, 1.1883, 1.0085,\n",
      "        0.8216, 0.6293, 0.4331, 0.2339])\n",
      "----------------------------------------\n",
      "iter  0  stage  11  ep  0   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0138, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.5687, 5.5687, 5.5687, 5.5687, 5.5687, 5.5687, 5.5687, 5.5687, 5.5687,\n",
      "        5.5687, 5.5687, 5.5687, 4.9941, 4.4858, 4.0244, 3.5968, 3.1936, 2.8083,\n",
      "        2.4362, 2.0737, 1.7185, 1.3686, 1.0227, 0.6798, 0.3390]) return=  135363.21866666665\n",
      "probs of actions:  tensor([0.9988, 0.9987, 0.9984, 0.9989, 0.9992, 0.9992, 0.9989, 0.9989, 0.9989,\n",
      "        0.9990, 0.9988, 0.9990, 0.9992, 0.9991, 0.9991, 0.9990, 0.9993, 0.9990,\n",
      "        0.9997, 0.9995, 0.9996, 0.9997, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([2.0540, 2.0864, 2.0526, 1.9718, 1.8573, 1.7183, 1.5614, 1.3914, 1.2117,\n",
      "        1.0247, 0.8325, 0.6362, 0.4370, 0.2355])\n",
      "----------------------------------------\n",
      "iter  0  stage  10  ep  52   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0151, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.9065, 5.9065, 5.9065, 5.9065, 5.9065, 5.9065, 5.9065, 5.9065, 5.9065,\n",
      "        5.9065, 5.9065, 5.3319, 4.8236, 4.3622, 3.9346, 3.5314, 3.1461, 2.7740,\n",
      "        2.4115, 2.0563, 1.7064, 1.3605, 1.0176, 0.6768, 0.3378]) return=  135363.21866666665\n",
      "probs of actions:  tensor([0.9988, 0.9988, 0.9985, 0.9989, 0.9992, 0.9992, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990, 0.9990, 0.9992, 0.9993, 0.9992, 0.9992, 0.9991, 0.9994, 0.9991,\n",
      "        0.9997, 0.9995, 0.9996, 0.9997, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([2.2584, 2.2908, 2.2570, 2.1762, 2.0617, 1.9227, 1.7658, 1.5958, 1.4160,\n",
      "        1.2291, 1.0368, 0.8406, 0.6413, 0.4399, 0.2368])\n",
      "----------------------------------------\n",
      "iter  0  stage  9  ep  0   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0182, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.2434, 6.2434, 6.2434, 6.2434, 6.2434, 6.2434, 6.2434, 6.2434, 6.2434,\n",
      "        6.2434, 5.6688, 5.1604, 4.6991, 4.2714, 3.8683, 3.4830, 3.1108, 2.7484,\n",
      "        2.3932, 2.0433, 1.6974, 1.3544, 1.0137, 0.6747, 0.3369]) return=  135363.21866666665\n",
      "probs of actions:  tensor([0.9988, 0.9988, 0.9985, 0.9989, 0.9992, 0.9992, 0.9990, 0.9990, 0.9990,\n",
      "        0.9990, 0.9990, 0.9992, 0.9993, 0.9992, 0.9992, 0.9991, 0.9994, 0.9991,\n",
      "        0.9997, 0.9995, 0.9996, 0.9997, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([2.4637, 2.4961, 2.4623, 2.3815, 2.2669, 2.1279, 1.9711, 1.8010, 1.6213,\n",
      "        1.4344, 1.2421, 1.0459, 0.8466, 0.6452, 0.4421, 0.2377])\n",
      "----------------------------------------\n",
      "iter  0  stage  8  ep  6   adversary:  AdversaryModes.imitation_132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0212, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.5795, 6.5795, 6.5795, 6.5795, 6.5795, 6.5795, 6.5795, 6.5795, 6.5795,\n",
      "        6.0050, 5.4966, 5.0352, 4.6076, 4.2044, 3.8191, 3.4470, 3.0846, 2.7293,\n",
      "        2.3794, 2.0335, 1.6906, 1.3499, 1.0108, 0.6730, 0.3362]) return=  135363.21866666665\n",
      "probs of actions:  tensor([0.9988, 0.9988, 0.9985, 0.9990, 0.9993, 0.9992, 0.9990, 0.9990, 0.9990,\n",
      "        0.9991, 0.9990, 0.9992, 0.9993, 0.9992, 0.9993, 0.9991, 0.9994, 0.9991,\n",
      "        0.9997, 0.9995, 0.9996, 0.9997, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([2.6696, 2.7020, 2.6682, 2.5875, 2.4729, 2.3339, 2.1771, 2.0070, 1.8273,\n",
      "        1.6404, 1.4481, 1.2519, 1.0526, 0.8512, 0.6480, 0.4437, 0.2384])\n",
      "----------------------------------------\n",
      "iter  0  stage  7  ep  11   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0241, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.9152, 6.9152, 6.9152, 6.9152, 6.9152, 6.9152, 6.9152, 6.9152, 6.3406,\n",
      "        5.8323, 5.3709, 4.9432, 4.5401, 4.1548, 3.7826, 3.4202, 3.0650, 2.7151,\n",
      "        2.3692, 2.0262, 1.6855, 1.3465, 1.0087, 0.6718, 0.3356]) return=  135363.21866666665\n",
      "probs of actions:  tensor([0.9989, 0.9988, 0.9986, 0.9990, 0.9993, 0.9992, 0.9990, 0.9990, 0.9990,\n",
      "        0.9991, 0.9991, 0.9992, 0.9993, 0.9992, 0.9993, 0.9992, 0.9994, 0.9991,\n",
      "        0.9997, 0.9995, 0.9996, 0.9997, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([2.8762, 2.9086, 2.8748, 2.7940, 2.6794, 2.5405, 2.3836, 2.2136, 2.0338,\n",
      "        1.8469, 1.6546, 1.4584, 1.2591, 1.0577, 0.8546, 0.6502, 0.4449, 0.2389])\n",
      "----------------------------------------\n",
      "iter  0  stage  6  ep  0   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0279, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.2504, 7.2504, 7.2504, 7.2504, 7.2504, 7.2504, 7.2504, 6.6759, 6.1675,\n",
      "        5.7061, 5.2785, 4.8753, 4.4901, 4.1179, 3.7555, 3.4002, 3.0503, 2.7044,\n",
      "        2.3615, 2.0208, 1.6818, 1.3440, 1.0071, 0.6709, 0.3353]) return=  135363.21866666665\n",
      "probs of actions:  tensor([0.9989, 0.9988, 0.9986, 0.9990, 0.9993, 0.9992, 0.9990, 0.9990, 0.9990,\n",
      "        0.9991, 0.9991, 0.9992, 0.9993, 0.9992, 0.9993, 0.9992, 0.9994, 0.9991,\n",
      "        0.9997, 0.9995, 0.9996, 0.9997, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([3.0831, 3.1155, 3.0817, 3.0009, 2.8864, 2.7474, 2.5905, 2.4205, 2.2407,\n",
      "        2.0538, 1.8615, 1.6653, 1.4660, 1.2646, 1.0615, 0.8571, 0.6518, 0.4458,\n",
      "        0.2393])\n",
      "----------------------------------------\n",
      "iter  0  stage  5  ep  0   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0311, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.5855, 7.5855, 7.5855, 7.5855, 7.5855, 7.5855, 7.0108, 6.5025, 6.0411,\n",
      "        5.6135, 5.2103, 4.8250, 4.4529, 4.0904, 3.7352, 3.3853, 3.0394, 2.6965,\n",
      "        2.3558, 2.0167, 1.6789, 1.3420, 1.0059, 0.6702, 0.3350]) return=  135363.21866666665\n",
      "probs of actions:  tensor([0.9989, 0.9988, 0.9986, 0.9990, 0.9993, 0.9992, 0.9990, 0.9990, 0.9991,\n",
      "        0.9991, 0.9991, 0.9992, 0.9993, 0.9992, 0.9993, 0.9992, 0.9994, 0.9991,\n",
      "        0.9997, 0.9995, 0.9996, 0.9997, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([3.2903, 3.3227, 3.2889, 3.2081, 3.0936, 2.9546, 2.7977, 2.6277, 2.4479,\n",
      "        2.2610, 2.0687, 1.8725, 1.6732, 1.4718, 1.2687, 1.0643, 0.8590, 0.6530,\n",
      "        0.4465, 0.2396])\n",
      "----------------------------------------\n",
      "iter  0  stage  4  ep  0   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0343, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.9200, 7.9200, 7.9200, 7.9200, 7.9200, 7.3456, 6.8372, 6.3759, 5.9482,\n",
      "        5.5450, 5.1598, 4.7876, 4.4252, 4.0699, 3.7200, 3.3741, 3.0312, 2.6905,\n",
      "        2.3515, 2.0137, 1.6768, 1.3406, 1.0050, 0.6697, 0.3347]) return=  135363.21866666665\n",
      "probs of actions:  tensor([0.9989, 0.9988, 0.9986, 0.9990, 0.9993, 0.9992, 0.9990, 0.9990, 0.9991,\n",
      "        0.9991, 0.9991, 0.9992, 0.9993, 0.9992, 0.9993, 0.9992, 0.9994, 0.9991,\n",
      "        0.9997, 0.9995, 0.9996, 0.9997, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([3.4977, 3.5301, 3.4963, 3.4155, 3.3010, 3.1620, 3.0051, 2.8351, 2.6554,\n",
      "        2.4684, 2.2762, 2.0799, 1.8807, 1.6792, 1.4761, 1.2717, 1.0664, 0.8604,\n",
      "        0.6539, 0.4470, 0.2398])\n",
      "----------------------------------------\n",
      "iter  0  stage  3  ep  5   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0380, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.2556, 8.2556, 8.2556, 8.2556, 7.6800, 7.1719, 6.7104, 6.2828, 5.8796,\n",
      "        5.4943, 5.1222, 4.7598, 4.4045, 4.0546, 3.7087, 3.3658, 3.0251, 2.6860,\n",
      "        2.3482, 2.0114, 1.6752, 1.3395, 1.0043, 0.6693, 0.3346]) return=  135363.21866666665\n",
      "probs of actions:  tensor([0.9989, 0.9989, 0.9986, 0.9990, 0.9993, 0.9993, 0.9990, 0.9990, 0.9991,\n",
      "        0.9991, 0.9991, 0.9992, 0.9994, 0.9993, 0.9993, 0.9992, 0.9994, 0.9991,\n",
      "        0.9997, 0.9995, 0.9996, 0.9997, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([3.7053, 3.7377, 3.7039, 3.6231, 3.5086, 3.3696, 3.2127, 3.0427, 2.8629,\n",
      "        2.6760, 2.4837, 2.2875, 2.0882, 1.8868, 1.6837, 1.4793, 1.2740, 1.0680,\n",
      "        0.8615, 0.6546, 0.4474, 0.2400])\n",
      "----------------------------------------\n",
      "iter  0  stage  2  ep  223   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0335, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.5859, 8.5859, 8.5859, 8.0154, 7.5061, 7.0449, 6.6172, 6.2141, 5.8288,\n",
      "        5.4567, 5.0942, 4.7390, 4.3891, 4.0432, 3.7002, 3.3595, 3.0205, 2.6827,\n",
      "        2.3458, 2.0097, 1.6740, 1.3387, 1.0038, 0.6690, 0.3345]) return=  135363.21866666665\n",
      "probs of actions:  tensor([0.9990, 0.9990, 0.9990, 0.9993, 0.9995, 0.9995, 0.9992, 0.9992, 0.9993,\n",
      "        0.9994, 0.9993, 0.9994, 0.9995, 0.9994, 0.9994, 0.9994, 0.9996, 0.9993,\n",
      "        0.9998, 0.9996, 0.9997, 0.9997, 0.9999, 0.9999, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([3.9130, 3.9454, 3.9116, 3.8308, 3.7163, 3.5773, 3.4204, 3.2504, 3.0706,\n",
      "        2.8837, 2.6914, 2.4952, 2.2960, 2.0945, 1.8914, 1.6870, 1.4817, 1.2757,\n",
      "        1.0692, 0.8623, 0.6551, 0.4477, 0.2401])\n",
      "----------------------------------------\n",
      "iter  0  stage  1  ep  0   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0382, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.9369, 8.9369, 8.3459, 7.8414, 7.3791, 6.9517, 6.5484, 6.1632, 5.7910,\n",
      "        5.4286, 5.0733, 4.7234, 4.3775, 4.0346, 3.6939, 3.3549, 3.0171, 2.6802,\n",
      "        2.3440, 2.0084, 1.6731, 1.3381, 1.0034, 0.6688, 0.3344]) return=  135363.21866666665\n",
      "probs of actions:  tensor([0.9990, 0.9990, 0.9990, 0.9993, 0.9995, 0.9995, 0.9992, 0.9992, 0.9993,\n",
      "        0.9994, 0.9993, 0.9994, 0.9995, 0.9994, 0.9994, 0.9994, 0.9996, 0.9993,\n",
      "        0.9998, 0.9996, 0.9997, 0.9997, 0.9999, 0.9999, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([4.1206, 4.1530, 4.1195, 4.0386, 3.9241, 3.7851, 3.6282, 3.4582, 3.2784,\n",
      "        3.0915, 2.8992, 2.7030, 2.5038, 2.3023, 2.0992, 1.8948, 1.6895, 1.4835,\n",
      "        1.2770, 1.0701, 0.8629, 0.6555, 0.4479, 0.2402])\n",
      "----------------------------------------\n",
      "iter  0  stage  0  ep  0   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0431, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.2070, 8.6958, 8.1720, 7.7142, 7.2857, 6.8828, 6.4974, 6.1253, 5.7629,\n",
      "        5.4076, 5.0577, 4.7118, 4.3689, 4.0282, 3.6892, 3.3513, 3.0145, 2.6783,\n",
      "        2.3427, 2.0074, 1.6724, 1.3377, 1.0031, 0.6687, 0.3343]) return=  135363.21866666665\n",
      "probs of actions:  tensor([0.9990, 0.9990, 0.9990, 0.9993, 0.9995, 0.9995, 0.9992, 0.9992, 0.9993,\n",
      "        0.9994, 0.9993, 0.9994, 0.9995, 0.9994, 0.9994, 0.9994, 0.9996, 0.9993,\n",
      "        0.9998, 0.9996, 0.9997, 0.9997, 0.9999, 0.9999, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422,\n",
      "        0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5422, 0.5746])\n",
      "finalReturns:  tensor([4.3293, 4.3617, 4.3269, 4.2466, 4.1319, 3.9929, 3.8361, 3.6660, 3.4863,\n",
      "        3.2994, 3.1071, 2.9109, 2.7116, 2.5102, 2.3071, 2.1027, 1.8974, 1.6914,\n",
      "        1.4849, 1.2780, 1.0708, 0.8634, 0.6558, 0.4481, 0.2403])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1682701103 saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[767881, 'tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])', 135363.21866666665, 84740.85066666668, 0.043078526854515076, 1e-05, 1, 0, 'tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\\n        18, 18, 18, 18, 18, 18,  0])', '[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\\n 1.]', '0,[1e-05,1][1, 10000, 1, 1],1682701103', 25, 50, 164632.33345850307, 193521.60851536895, 78111.91688437786, 135363.21866666665, 132442.06666666665, 102604.52812037412, 102604.52812037412, 120699.79187222247, 120677.7693195532, 90621.14382460734, 102604.52812037412, 120699.79187222247]\n",
      "policy reset\n",
      "----------------------------------------\n",
      "iter  1  stage  24  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213,\n",
      "        0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213,\n",
      "        0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213]) return=  130240.65866666666\n",
      "probs of actions:  tensor([0.9077, 0.8732, 0.8904, 0.8995, 0.8972, 0.8745, 0.8788, 0.9012, 0.8913,\n",
      "        0.8767, 0.8811, 0.8843, 0.8777, 0.8816, 0.8965, 0.8712, 0.8952, 0.9004,\n",
      "        0.8862, 0.9074, 0.8904, 0.8827, 0.8910, 0.8883, 0.9826],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5238, 0.5207, 0.5214, 0.5212, 0.5213, 0.5213, 0.5213, 0.5213,\n",
      "        0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213,\n",
      "        0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  1  stage  23  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([ 0,  0, 13,  0, 13,  0, 13,  0, 20,  3, 18,  3,  0, 18,  0, 15, 20,  0,\n",
      "         0, 11,  0,  0, 17,  8,  0])\n",
      "loss=  tensor(0.1189, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0900, 1.0900, 1.0900, 1.0900, 1.0900, 1.0900, 1.0900, 1.0900, 1.0900,\n",
      "        1.0900, 1.0900, 1.0900, 1.0900, 1.0900, 1.0900, 1.0900, 1.0900, 1.0900,\n",
      "        1.0900, 1.0900, 1.0900, 1.0900, 1.0900, 1.0900, 0.5062]) return=  132730.16473578967\n",
      "probs of actions:  tensor([0.5746, 0.4005, 0.0869, 0.5135, 0.0746, 0.3050, 0.0707, 0.4547, 0.0140,\n",
      "        0.0303, 0.0911, 0.0262, 0.3726, 0.1150, 0.3055, 0.0646, 0.0140, 0.4273,\n",
      "        0.4826, 0.0104, 0.4164, 0.3292, 0.0586, 0.0051, 0.9925],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5238, 0.5038, 0.5694, 0.4927, 0.5723, 0.4920, 0.5725, 0.4688,\n",
      "        0.5984, 0.4809, 0.5895, 0.5154, 0.4904, 0.5879, 0.4828, 0.5411, 0.5805,\n",
      "        0.5070, 0.5128, 0.5608, 0.5116, 0.4948, 0.5774, 0.5351])\n",
      "finalReturns:  tensor([0.0225, 0.0289])\n",
      "----------------------------------------\n",
      "iter  1  stage  22  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 17, 18, 17, 17, 18, 18, 18, 23, 18, 17, 18,\n",
      "        18, 18, 10, 17, 18, 18,  0])\n",
      "loss=  tensor(0.1164, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.5474, 1.5474, 1.5474, 1.5474, 1.5474, 1.5474, 1.5474, 1.5474, 1.5474,\n",
      "        1.5474, 1.5474, 1.5474, 1.5474, 1.5474, 1.5474, 1.5474, 1.5474, 1.5474,\n",
      "        1.5474, 1.5474, 1.5474, 1.5474, 1.5474, 0.9691, 0.4616]) return=  135321.06489242485\n",
      "probs of actions:  tensor([0.6817, 0.6622, 0.7054, 0.6411, 0.6841, 0.6765, 0.6575, 0.1339, 0.6236,\n",
      "        0.1263, 0.1174, 0.6488, 0.6663, 0.7073, 0.0190, 0.6308, 0.1307, 0.6938,\n",
      "        0.6575, 0.6494, 0.0023, 0.1165, 0.6519, 0.6349, 0.9946],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5381, 0.5432, 0.5419, 0.5422, 0.5421, 0.5457, 0.5384,\n",
      "        0.5466, 0.5416, 0.5394, 0.5429, 0.5420, 0.5217, 0.5613, 0.5409, 0.5396,\n",
      "        0.5428, 0.5420, 0.5646, 0.5157, 0.5460, 0.5412, 0.5748])\n",
      "finalReturns:  tensor([0.1145, 0.1469, 0.1132])\n",
      "----------------------------------------\n",
      "iter  1  stage  21  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([17, 23, 15, 18, 20, 23, 18, 20, 18, 20, 17, 20, 18, 18, 20, 19, 25, 18,\n",
      "        23, 18, 20, 18, 20, 20,  0])\n",
      "loss=  tensor(0.6976, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.9726, 1.9726, 1.9726, 1.9726, 1.9726, 1.9726, 1.9726, 1.9726, 1.9726,\n",
      "        1.9726, 1.9726, 1.9726, 1.9726, 1.9726, 1.9726, 1.9726, 1.9726, 1.9726,\n",
      "        1.9726, 1.9726, 1.9726, 1.9726, 1.3892, 0.8829, 0.4210]) return=  135023.68474879788\n",
      "probs of actions:  tensor([0.0946, 0.0818, 0.0169, 0.4284, 0.2996, 0.0770, 0.3800, 0.2651, 0.3633,\n",
      "        0.2819, 0.1024, 0.2574, 0.4822, 0.4497, 0.3131, 0.0237, 0.0440, 0.4993,\n",
      "        0.0806, 0.4510, 0.2391, 0.3161, 0.4021, 0.4613, 0.9969],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4823, 0.5342, 0.5680, 0.5269, 0.5384, 0.5283, 0.5596, 0.5302, 0.5509,\n",
      "        0.5324, 0.5538, 0.5288, 0.5512, 0.5399, 0.5351, 0.5459, 0.5140, 0.5685,\n",
      "        0.5152, 0.5629, 0.5294, 0.5511, 0.5323, 0.5427, 0.5801])\n",
      "finalReturns:  tensor([0.2336, 0.2660, 0.2400, 0.1591])\n",
      "----------------------------------------\n",
      "iter  1  stage  20  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([25, 25, 23, 20, 23, 25, 18, 23, 18, 23, 18, 23, 23, 24, 17, 18, 23, 25,\n",
      "        25, 23, 19, 25, 25, 20,  0])\n",
      "loss=  tensor(3.1660, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.3248, 2.3248, 2.3248, 2.3248, 2.3248, 2.3248, 2.3248, 2.3248, 2.3248,\n",
      "        2.3248, 2.3248, 2.3248, 2.3248, 2.3248, 2.3248, 2.3248, 2.3248, 2.3248,\n",
      "        2.3248, 2.3248, 2.3248, 1.7364, 1.2313, 0.7725, 0.3692]) return=  134249.1143811909\n",
      "probs of actions:  tensor([0.1810, 0.2000, 0.2224, 0.3108, 0.2469, 0.2092, 0.1259, 0.2311, 0.1225,\n",
      "        0.2222, 0.2016, 0.2075, 0.2114, 0.0136, 0.0336, 0.1335, 0.2390, 0.2126,\n",
      "        0.2172, 0.2367, 0.0198, 0.2455, 0.2390, 0.4338, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5557, 0.5376, 0.5497, 0.5255, 0.5302, 0.5644, 0.5162, 0.5627,\n",
      "        0.5166, 0.5626, 0.5166, 0.5421, 0.5309, 0.5651, 0.5336, 0.5238, 0.5306,\n",
      "        0.5342, 0.5429, 0.5522, 0.5124, 0.5388, 0.5547, 0.5772])\n",
      "finalReturns:  tensor([0.4105, 0.4466, 0.4394, 0.3593, 0.2079])\n",
      "----------------------------------------\n",
      "iter  1  stage  19  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([25, 18, 25, 25, 25, 25, 25, 23, 25, 25, 25, 25, 26, 25, 25, 20, 25, 25,\n",
      "        25, 25, 25, 25, 25, 20,  0])\n",
      "loss=  tensor(1.5108, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.6317, 2.6317, 2.6317, 2.6317, 2.6317, 2.6317, 2.6317, 2.6317, 2.6317,\n",
      "        2.6317, 2.6317, 2.6317, 2.6317, 2.6317, 2.6317, 2.6317, 2.6317, 2.6317,\n",
      "        2.6317, 2.6317, 2.0354, 1.5321, 1.0930, 0.6993, 0.3379]) return=  133536.32186988593\n",
      "probs of actions:  tensor([0.7044, 0.0200, 0.7188, 0.7269, 0.7118, 0.7290, 0.7193, 0.1459, 0.7147,\n",
      "        0.7104, 0.7352, 0.7272, 0.0040, 0.7251, 0.7320, 0.0698, 0.7153, 0.7446,\n",
      "        0.7193, 0.7999, 0.8080, 0.7683, 0.7679, 0.1276, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5858, 0.5014, 0.5416, 0.5315, 0.5340, 0.5334, 0.5431, 0.5258,\n",
      "        0.5354, 0.5330, 0.5336, 0.5284, 0.5374, 0.5325, 0.5562, 0.5143, 0.5383,\n",
      "        0.5323, 0.5338, 0.5334, 0.5335, 0.5335, 0.5560, 0.5768])\n",
      "finalReturns:  tensor([0.6353, 0.6978, 0.6677, 0.5733, 0.4336, 0.2389])\n",
      "----------------------------------------\n",
      "iter  1  stage  18  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([25, 25, 23, 25, 25, 23, 25, 25, 25, 25, 25, 25, 25, 25, 25, 23, 23, 25,\n",
      "        25, 25, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(0.3647, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.9535, 2.9535, 2.9535, 2.9535, 2.9535, 2.9535, 2.9535, 2.9535, 2.9535,\n",
      "        2.9535, 2.9535, 2.9535, 2.9535, 2.9535, 2.9535, 2.9535, 2.9535, 2.9535,\n",
      "        2.9535, 2.3561, 1.8530, 1.4139, 1.0202, 0.6589, 0.3209]) return=  133464.3841915384\n",
      "probs of actions:  tensor([0.8935, 0.8888, 0.0682, 0.9030, 0.8959, 0.0677, 0.8989, 0.9129, 0.8946,\n",
      "        0.8929, 0.9044, 0.9014, 0.9122, 0.9016, 0.9045, 0.0701, 0.0710, 0.9099,\n",
      "        0.9199, 0.9389, 0.9494, 0.9240, 0.9188, 0.8524, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5557, 0.5376, 0.5272, 0.5351, 0.5427, 0.5259, 0.5354, 0.5330,\n",
      "        0.5336, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5431, 0.5354, 0.5277,\n",
      "        0.5349, 0.5331, 0.5336, 0.5335, 0.5335, 0.5335, 0.5960])\n",
      "finalReturns:  tensor([0.8445, 0.9070, 0.8770, 0.7825, 0.6428, 0.4706, 0.2751])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  17  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(0.1831, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.2608, 3.2608, 3.2608, 3.2608, 3.2608, 3.2608, 3.2608, 3.2608, 3.2608,\n",
      "        3.2608, 3.2608, 3.2608, 3.2608, 3.2608, 3.2608, 3.2608, 3.2608, 3.2608,\n",
      "        2.6648, 2.1614, 1.7223, 1.3286, 0.9673, 0.6294, 0.3084]) return=  133326.59199999998\n",
      "probs of actions:  tensor([0.9582, 0.9553, 0.9595, 0.9618, 0.9593, 0.9611, 0.9598, 0.9668, 0.9578,\n",
      "        0.9583, 0.9635, 0.9620, 0.9660, 0.9618, 0.9629, 0.9578, 0.9580, 0.9654,\n",
      "        0.9735, 0.9787, 0.9825, 0.9718, 0.9683, 0.9487, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5557, 0.5280, 0.5349, 0.5331, 0.5336, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5960])\n",
      "finalReturns:  tensor([1.0696, 1.1321, 1.1020, 1.0076, 0.8678, 0.6957, 0.5001, 0.2875])\n",
      "----------------------------------------\n",
      "iter  1  stage  16  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 23, 23, 25,\n",
      "        25, 25, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(4.6647, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.5711, 3.5711, 3.5711, 3.5711, 3.5711, 3.5711, 3.5711, 3.5711, 3.5711,\n",
      "        3.5711, 3.5711, 3.5711, 3.5711, 3.5711, 3.5711, 3.5711, 3.5711, 2.9828,\n",
      "        2.4776, 2.0323, 1.6354, 1.2714, 0.9317, 0.6094, 0.3000]) return=  133395.47270678813\n",
      "probs of actions:  tensor([0.9645, 0.9623, 0.9662, 0.9679, 0.9659, 0.9673, 0.9660, 0.9719, 0.9648,\n",
      "        0.9654, 0.9701, 0.9689, 0.9721, 0.9677, 0.9688, 0.0327, 0.0306, 0.9745,\n",
      "        0.9786, 0.9827, 0.9879, 0.9764, 0.9713, 0.9507, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5557, 0.5280, 0.5349, 0.5331, 0.5336, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5431, 0.5354, 0.5277,\n",
      "        0.5349, 0.5331, 0.5336, 0.5335, 0.5335, 0.5335, 0.5960])\n",
      "finalReturns:  tensor([1.2901, 1.3430, 1.3204, 1.2308, 1.0946, 0.9250, 0.7313, 0.5201, 0.2960])\n",
      "----------------------------------------\n",
      "iter  1  stage  15  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(0.2434, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.8525, 3.8525, 3.8525, 3.8525, 3.8525, 3.8525, 3.8525, 3.8525, 3.8525,\n",
      "        3.8525, 3.8525, 3.8525, 3.8525, 3.8525, 3.8525, 3.8525, 3.2565, 2.7531,\n",
      "        2.3140, 1.9203, 1.5590, 1.2210, 0.9001, 0.5917, 0.2924]) return=  133326.59199999998\n",
      "probs of actions:  tensor([0.9686, 0.9667, 0.9713, 0.9722, 0.9703, 0.9719, 0.9702, 0.9756, 0.9696,\n",
      "        0.9702, 0.9744, 0.9733, 0.9758, 0.9720, 0.9730, 0.9731, 0.9757, 0.9777,\n",
      "        0.9782, 0.9864, 0.9914, 0.9763, 0.9761, 0.9581, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5557, 0.5280, 0.5349, 0.5331, 0.5336, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5960])\n",
      "finalReturns:  tensor([1.5449, 1.6074, 1.5773, 1.4829, 1.3431, 1.1710, 0.9754, 0.7628, 0.5378,\n",
      "        0.3036])\n",
      "----------------------------------------\n",
      "iter  1  stage  14  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([23, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(0.1918, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.1398, 4.1398, 4.1398, 4.1398, 4.1398, 4.1398, 4.1398, 4.1398, 4.1398,\n",
      "        4.1398, 4.1398, 4.1398, 4.1398, 4.1398, 4.1398, 3.5438, 3.0405, 2.6014,\n",
      "        2.2077, 1.8463, 1.5084, 1.1875, 0.8791, 0.5798, 0.2874]) return=  133359.57866666667\n",
      "probs of actions:  tensor([0.0204, 0.9779, 0.9806, 0.9816, 0.9804, 0.9811, 0.9801, 0.9839, 0.9800,\n",
      "        0.9800, 0.9831, 0.9827, 0.9841, 0.9813, 0.9832, 0.9834, 0.9820, 0.9878,\n",
      "        0.9881, 0.9937, 0.9951, 0.9868, 0.9850, 0.9698, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4583, 0.5479, 0.5299, 0.5344, 0.5333, 0.5335, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5960])\n",
      "finalReturns:  tensor([1.7910, 1.8535, 1.8234, 1.7290, 1.5892, 1.4171, 1.2215, 1.0089, 0.7839,\n",
      "        0.5497, 0.3086])\n",
      "----------------------------------------\n",
      "iter  1  stage  13  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(0.1643, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.4235, 4.4235, 4.4235, 4.4235, 4.4235, 4.4235, 4.4235, 4.4235, 4.4235,\n",
      "        4.4235, 4.4235, 4.4235, 4.4235, 4.4235, 3.8275, 3.3241, 2.8850, 2.4913,\n",
      "        2.1300, 1.7920, 1.4711, 1.1627, 0.8634, 0.5710, 0.2836]) return=  133326.59199999998\n",
      "probs of actions:  tensor([0.9850, 0.9838, 0.9855, 0.9863, 0.9856, 0.9861, 0.9855, 0.9883, 0.9852,\n",
      "        0.9854, 0.9877, 0.9874, 0.9883, 0.9897, 0.9887, 0.9869, 0.9845, 0.9928,\n",
      "        0.9936, 0.9961, 0.9976, 0.9919, 0.9882, 0.9812, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5557, 0.5280, 0.5349, 0.5331, 0.5336, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5960])\n",
      "finalReturns:  tensor([2.0409, 2.1034, 2.0733, 1.9789, 1.8391, 1.6669, 1.4714, 1.2588, 1.0338,\n",
      "        0.7995, 0.5585, 0.3124])\n",
      "----------------------------------------\n",
      "iter  1  stage  12  ep  96178   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(0.0253, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.7043, 4.7043, 4.7043, 4.7043, 4.7043, 4.7043, 4.7043, 4.7043, 4.7043,\n",
      "        4.7043, 4.7043, 4.7043, 4.7043, 4.1083, 3.6049, 3.1658, 2.7721, 2.4108,\n",
      "        2.0728, 1.7519, 1.4435, 1.1442, 0.8518, 0.5644, 0.2808]) return=  133326.59199999998\n",
      "probs of actions:  tensor([0.9974, 0.9971, 0.9973, 0.9975, 0.9975, 0.9975, 0.9974, 0.9980, 0.9973,\n",
      "        0.9974, 0.9978, 0.9977, 0.9990, 0.9988, 0.9986, 0.9978, 0.9972, 0.9993,\n",
      "        0.9994, 0.9997, 1.0000, 0.9991, 0.9988, 0.9974, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5557, 0.5280, 0.5349, 0.5331, 0.5336, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5960])\n",
      "finalReturns:  tensor([2.2935, 2.3560, 2.3259, 2.2315, 2.0918, 1.9196, 1.7240, 1.5115, 1.2864,\n",
      "        1.0522, 0.8111, 0.5650, 0.3152])\n",
      "----------------------------------------\n",
      "iter  1  stage  11  ep  85709   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(0.0115, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.9830, 4.9830, 4.9830, 4.9830, 4.9830, 4.9830, 4.9830, 4.9830, 4.9830,\n",
      "        4.9830, 4.9830, 4.9830, 4.3870, 3.8836, 3.4445, 3.0508, 2.6895, 2.3516,\n",
      "        2.0306, 1.7222, 1.4230, 1.1305, 0.8432, 0.5595, 0.2787]) return=  133326.59199999998\n",
      "probs of actions:  tensor([0.9985, 0.9983, 0.9985, 0.9986, 0.9986, 0.9986, 0.9986, 0.9989, 0.9985,\n",
      "        0.9985, 0.9988, 0.9990, 0.9996, 0.9997, 0.9994, 0.9992, 0.9990, 1.0000,\n",
      "        0.9998, 1.0000, 1.0000, 0.9997, 0.9998, 0.9988, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5557, 0.5280, 0.5349, 0.5331, 0.5336, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5960])\n",
      "finalReturns:  tensor([2.5483, 2.6108, 2.5807, 2.4863, 2.3465, 2.1744, 1.9788, 1.7662, 1.5412,\n",
      "        1.3070, 1.0659, 0.8198, 0.5699, 0.3173])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  10  ep  132   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(0.0145, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.2601, 5.2601, 5.2601, 5.2601, 5.2601, 5.2601, 5.2601, 5.2601, 5.2601,\n",
      "        5.2601, 5.2601, 4.6642, 4.1608, 3.7217, 3.3280, 2.9667, 2.6287, 2.3078,\n",
      "        1.9994, 1.7001, 1.4077, 1.1203, 0.8367, 0.5559, 0.2772]) return=  133326.59199999998\n",
      "probs of actions:  tensor([0.9986, 0.9984, 0.9986, 0.9987, 0.9987, 0.9987, 0.9986, 0.9989, 0.9985,\n",
      "        0.9986, 0.9990, 0.9990, 0.9996, 0.9997, 0.9995, 0.9992, 0.9990, 1.0000,\n",
      "        0.9998, 1.0000, 1.0000, 0.9998, 0.9998, 0.9988, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5557, 0.5280, 0.5349, 0.5331, 0.5336, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5960])\n",
      "finalReturns:  tensor([2.8046, 2.8671, 2.8370, 2.7426, 2.6028, 2.4307, 2.2351, 2.0226, 1.7975,\n",
      "        1.5633, 1.3222, 1.0761, 0.8263, 0.5736, 0.3188])\n",
      "----------------------------------------\n",
      "iter  1  stage  9  ep  2917   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(0.0123, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.5361, 5.5361, 5.5361, 5.5361, 5.5361, 5.5361, 5.5361, 5.5361, 5.5361,\n",
      "        5.5361, 4.9401, 4.4367, 3.9977, 3.6040, 3.2426, 2.9047, 2.5838, 2.2753,\n",
      "        1.9761, 1.6837, 1.3963, 1.1127, 0.8319, 0.5531, 0.2760]) return=  133326.59199999998\n",
      "probs of actions:  tensor([0.9990, 0.9988, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9992, 0.9989,\n",
      "        0.9990, 0.9997, 0.9994, 0.9997, 0.9998, 0.9996, 0.9995, 0.9993, 1.0000,\n",
      "        0.9999, 1.0000, 1.0000, 0.9998, 0.9999, 0.9991, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5557, 0.5280, 0.5349, 0.5331, 0.5336, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5960])\n",
      "finalReturns:  tensor([3.0621, 3.1246, 3.0945, 3.0001, 2.8603, 2.6882, 2.4926, 2.2801, 2.0550,\n",
      "        1.8208, 1.5797, 1.3336, 1.0838, 0.8311, 0.5763, 0.3200])\n",
      "----------------------------------------\n",
      "iter  1  stage  8  ep  76   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(0.0164, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.8112, 5.8112, 5.8112, 5.8112, 5.8112, 5.8112, 5.8112, 5.8112, 5.8112,\n",
      "        5.2152, 4.7119, 4.2728, 3.8791, 3.5177, 3.1798, 2.8589, 2.5504, 2.2512,\n",
      "        1.9588, 1.6714, 1.3878, 1.1070, 0.8282, 0.5511, 0.2751]) return=  133326.59199999998\n",
      "probs of actions:  tensor([0.9990, 0.9988, 0.9989, 0.9990, 0.9990, 0.9990, 0.9990, 0.9992, 0.9990,\n",
      "        0.9990, 0.9997, 0.9994, 0.9997, 0.9998, 0.9996, 0.9995, 0.9993, 1.0000,\n",
      "        0.9999, 1.0000, 1.0000, 0.9998, 0.9999, 0.9991, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5557, 0.5280, 0.5349, 0.5331, 0.5336, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5960])\n",
      "finalReturns:  tensor([3.3205, 3.3830, 3.3529, 3.2585, 3.1187, 2.9466, 2.7510, 2.5385, 2.3134,\n",
      "        2.0792, 1.8381, 1.5920, 1.3421, 1.0895, 0.8347, 0.5784, 0.3209])\n",
      "----------------------------------------\n",
      "iter  1  stage  7  ep  0   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(0.0199, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.0857, 6.0857, 6.0857, 6.0857, 6.0857, 6.0857, 6.0857, 6.0857, 5.4897,\n",
      "        4.9863, 4.5472, 4.1535, 3.7922, 3.4543, 3.1333, 2.8249, 2.5257, 2.2332,\n",
      "        1.9458, 1.6622, 1.3814, 1.1027, 0.8255, 0.5496, 0.2744]) return=  133326.59199999998\n",
      "probs of actions:  tensor([0.9990, 0.9988, 0.9989, 0.9990, 0.9990, 0.9990, 0.9990, 0.9992, 0.9990,\n",
      "        0.9990, 0.9997, 0.9994, 0.9997, 0.9998, 0.9996, 0.9995, 0.9993, 1.0000,\n",
      "        0.9999, 1.0000, 1.0000, 0.9998, 0.9999, 0.9991, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5557, 0.5280, 0.5349, 0.5331, 0.5336, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5960])\n",
      "finalReturns:  tensor([3.5795, 3.6420, 3.6119, 3.5175, 3.3778, 3.2056, 3.0101, 2.7975, 2.5724,\n",
      "        2.3382, 2.0971, 1.8510, 1.6012, 1.3485, 1.0937, 0.8374, 0.5799, 0.3215])\n",
      "----------------------------------------\n",
      "iter  1  stage  6  ep  13   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(0.0244, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.3596, 6.3596, 6.3596, 6.3596, 6.3596, 6.3596, 6.3596, 5.7637, 5.2603,\n",
      "        4.8212, 4.4275, 4.0662, 3.7282, 3.4073, 3.0989, 2.7996, 2.5072, 2.2198,\n",
      "        1.9362, 1.6554, 1.3767, 1.0995, 0.8235, 0.5484, 0.2740]) return=  133326.59199999998\n",
      "probs of actions:  tensor([0.9990, 0.9988, 0.9989, 0.9990, 0.9990, 0.9990, 0.9990, 0.9992, 0.9990,\n",
      "        0.9990, 0.9997, 0.9994, 0.9997, 0.9998, 0.9996, 0.9995, 0.9993, 1.0000,\n",
      "        0.9999, 1.0000, 1.0000, 0.9998, 0.9999, 0.9991, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5557, 0.5280, 0.5349, 0.5331, 0.5336, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5960])\n",
      "finalReturns:  tensor([3.8391, 3.9016, 3.8715, 3.7771, 3.6373, 3.4651, 3.2696, 3.0570, 2.8320,\n",
      "        2.5977, 2.3567, 2.1106, 1.8607, 1.6080, 1.3533, 1.0969, 0.8394, 0.5811,\n",
      "        0.3220])\n",
      "----------------------------------------\n",
      "iter  1  stage  5  ep  7131   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(0.0227, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.6333, 6.6333, 6.6333, 6.6333, 6.6333, 6.6333, 6.0372, 5.5339, 5.0948,\n",
      "        4.7011, 4.3397, 4.0018, 3.6809, 3.3724, 3.0732, 2.7808, 2.4934, 2.2098,\n",
      "        1.9290, 1.6502, 1.3731, 1.0971, 0.8220, 0.5475, 0.2736]) return=  133326.59199999998\n",
      "probs of actions:  tensor([0.9990, 0.9989, 0.9990, 0.9990, 0.9991, 0.9990, 0.9997, 0.9994, 0.9993,\n",
      "        0.9991, 0.9998, 0.9994, 0.9998, 0.9999, 0.9996, 0.9997, 0.9993, 1.0000,\n",
      "        0.9999, 1.0000, 1.0000, 0.9999, 1.0000, 0.9992, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5557, 0.5280, 0.5349, 0.5331, 0.5336, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5960])\n",
      "finalReturns:  tensor([4.0990, 4.1615, 4.1314, 4.0369, 3.8972, 3.7250, 3.5295, 3.3169, 3.0919,\n",
      "        2.8576, 2.6166, 2.3705, 2.1206, 1.8679, 1.6132, 1.3568, 1.0993, 0.8410,\n",
      "        0.5819, 0.3224])\n",
      "----------------------------------------\n",
      "iter  1  stage  4  ep  0   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(0.0274, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.9063, 6.9063, 6.9063, 6.9063, 6.9063, 6.3106, 5.8071, 5.3681, 4.9744,\n",
      "        4.6131, 4.2751, 3.9542, 3.6458, 3.3465, 3.0541, 2.7667, 2.4831, 2.2023,\n",
      "        1.9236, 1.6464, 1.3704, 1.0953, 0.8209, 0.5469, 0.2733]) return=  133326.59199999998\n",
      "probs of actions:  tensor([0.9990, 0.9989, 0.9990, 0.9990, 0.9991, 0.9990, 0.9997, 0.9994, 0.9993,\n",
      "        0.9991, 0.9998, 0.9994, 0.9998, 0.9999, 0.9996, 0.9997, 0.9993, 1.0000,\n",
      "        0.9999, 1.0000, 1.0000, 0.9999, 1.0000, 0.9992, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5557, 0.5280, 0.5349, 0.5331, 0.5336, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5960])\n",
      "finalReturns:  tensor([4.3591, 4.4216, 4.3915, 4.2971, 4.1574, 3.9852, 3.7896, 3.5771, 3.3520,\n",
      "        3.1178, 2.8767, 2.6306, 2.3808, 2.1281, 1.8733, 1.6170, 1.3595, 1.1011,\n",
      "        0.8421, 0.5826, 0.3227])\n",
      "----------------------------------------\n",
      "iter  1  stage  3  ep  0   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(0.0327, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.1808, 7.1808, 7.1808, 7.1808, 6.5834, 6.0803, 5.6412, 5.2475, 4.8862,\n",
      "        4.5482, 4.2273, 3.9189, 3.6196, 3.3272, 3.0398, 2.7562, 2.4754, 2.1967,\n",
      "        1.9195, 1.6435, 1.3684, 1.0940, 0.8200, 0.5464, 0.2731]) return=  133326.59199999998\n",
      "probs of actions:  tensor([0.9990, 0.9989, 0.9990, 0.9990, 0.9991, 0.9990, 0.9997, 0.9994, 0.9993,\n",
      "        0.9991, 0.9998, 0.9994, 0.9998, 0.9999, 0.9996, 0.9997, 0.9993, 1.0000,\n",
      "        0.9999, 1.0000, 1.0000, 0.9999, 1.0000, 0.9992, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5557, 0.5280, 0.5349, 0.5331, 0.5336, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5960])\n",
      "finalReturns:  tensor([4.6195, 4.6820, 4.6519, 4.5575, 4.4177, 4.2456, 4.0500, 3.8375, 3.6124,\n",
      "        3.3782, 3.1371, 2.8910, 2.6411, 2.3885, 2.1337, 1.8774, 1.6199, 1.3615,\n",
      "        1.1025, 0.8429, 0.5831, 0.3229])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  2  ep  9   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(0.0382, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.4481, 7.4481, 7.4481, 6.8576, 6.3530, 5.9142, 5.5204, 5.1591, 4.8212,\n",
      "        4.5002, 4.1918, 3.8926, 3.6001, 3.3128, 3.0291, 2.7483, 2.4696, 2.1925,\n",
      "        1.9165, 1.6414, 1.3669, 1.0930, 0.8194, 0.5461, 0.2729]) return=  133326.59199999998\n",
      "probs of actions:  tensor([0.9990, 0.9989, 0.9990, 0.9991, 0.9991, 0.9990, 0.9997, 0.9994, 0.9993,\n",
      "        0.9991, 0.9998, 0.9994, 0.9998, 0.9999, 0.9996, 0.9997, 0.9993, 1.0000,\n",
      "        0.9999, 1.0000, 1.0000, 0.9999, 1.0000, 0.9992, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5557, 0.5280, 0.5349, 0.5331, 0.5336, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5960])\n",
      "finalReturns:  tensor([4.8801, 4.9426, 4.9124, 4.8180, 4.6783, 4.5061, 4.3106, 4.0980, 3.8729,\n",
      "        3.6387, 3.3977, 3.1515, 2.9017, 2.6490, 2.3942, 2.1379, 1.8804, 1.6220,\n",
      "        1.3630, 1.1035, 0.8436, 0.5834, 0.3230])\n",
      "----------------------------------------\n",
      "iter  1  stage  1  ep  70   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(0.0420, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.7435, 7.7435, 7.1254, 6.6270, 6.1868, 5.7933, 5.4319, 5.0940, 4.7731,\n",
      "        4.4646, 4.1654, 3.8730, 3.5856, 3.3020, 3.0212, 2.7424, 2.4653, 2.1893,\n",
      "        1.9142, 1.6397, 1.3658, 1.0922, 0.8189, 0.5458, 0.2728]) return=  133326.59199999998\n",
      "probs of actions:  tensor([0.9990, 0.9990, 0.9991, 0.9992, 0.9992, 0.9990, 0.9997, 0.9994, 0.9993,\n",
      "        0.9991, 0.9998, 0.9994, 0.9998, 0.9999, 0.9996, 0.9997, 0.9993, 1.0000,\n",
      "        0.9999, 1.0000, 1.0000, 0.9999, 1.0000, 0.9993, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5557, 0.5280, 0.5349, 0.5331, 0.5336, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5960])\n",
      "finalReturns:  tensor([5.1404, 5.2029, 5.1732, 5.0786, 4.9389, 4.7668, 4.5712, 4.3586, 4.1336,\n",
      "        3.8994, 3.6583, 3.4122, 3.1623, 2.9097, 2.6549, 2.3986, 2.1411, 1.8827,\n",
      "        1.6237, 1.3641, 1.1042, 0.8441, 0.5837, 0.3232])\n",
      "----------------------------------------\n",
      "iter  1  stage  0  ep  0   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(0.0482, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.9300, 7.4188, 6.8950, 6.4606, 6.0658, 5.7047, 5.3667, 5.0458, 4.7374,\n",
      "        4.4382, 4.1457, 3.8583, 3.5747, 3.2939, 3.0152, 2.7380, 2.4620, 2.1869,\n",
      "        1.9125, 1.6385, 1.3649, 1.0916, 0.8185, 0.5456, 0.2727]) return=  133326.59199999998\n",
      "probs of actions:  tensor([0.9990, 0.9990, 0.9991, 0.9992, 0.9992, 0.9990, 0.9997, 0.9994, 0.9993,\n",
      "        0.9991, 0.9998, 0.9994, 0.9998, 0.9999, 0.9996, 0.9997, 0.9993, 1.0000,\n",
      "        0.9999, 1.0000, 1.0000, 0.9999, 1.0000, 0.9993, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5557, 0.5280, 0.5349, 0.5331, 0.5336, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335,\n",
      "        0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5335, 0.5960])\n",
      "finalReturns:  tensor([5.4026, 5.4651, 5.4333, 5.3396, 5.1996, 5.0275, 4.8319, 4.6194, 4.3943,\n",
      "        4.1601, 3.9190, 3.6729, 3.4231, 3.1704, 2.9156, 2.6593, 2.4018, 2.1434,\n",
      "        1.8844, 1.6249, 1.3650, 1.1048, 0.8444, 0.5839, 0.3232])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1682740769 saved\n",
      "[1632248, 'tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])', 133326.59199999998, 77073.984, 0.0481695681810379, 1e-05, 1, 0, 'tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\\n        25, 25, 25, 25, 25, 25,  0])', '[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\\n 1.]', '0,[1e-05,1][1, 10000, 1, 1],1682740769', 25, 50, 171147.58350022632, 214073.728368001, 88670.91886744411, 133326.59199999998, 130350.63466666665, 79008.24733959991, 78991.42784922772, 95380.52812037412, 95380.52812037412, 102530.56714139864, 79008.24733959991, 95380.52812037412]\n",
      "policy reset\n",
      "----------------------------------------\n",
      "iter  2  stage  24  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213,\n",
      "        0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213,\n",
      "        0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213, 0.5213]) return=  130508.83063279034\n",
      "probs of actions:  tensor([0.8197, 0.8212, 0.8176, 0.8134, 0.8230, 0.8173, 0.8226, 0.1095, 0.8180,\n",
      "        0.1128, 0.8094, 0.1104, 0.8382, 0.8347, 0.8563, 0.8227, 0.0014, 0.8433,\n",
      "        0.7915, 0.8165, 0.8150, 0.8321, 0.8387, 0.7760, 0.9487],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5238, 0.5207, 0.5214, 0.5212, 0.5213, 0.5213, 0.5212, 0.5249,\n",
      "        0.5203, 0.5251, 0.5202, 0.5251, 0.5203, 0.5215, 0.5212, 0.5132, 0.5543,\n",
      "        0.5132, 0.5233, 0.5208, 0.5214, 0.5213, 0.5213, 0.5213])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  2  stage  23  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([ 0,  0, 17,  0,  0,  2,  0,  0,  0,  0, 12, 17, 16,  3,  0,  0, 16,  0,\n",
      "        17, 17,  0, 17, 17, 17,  0])\n",
      "loss=  tensor(0.0142, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0776, 1.0776, 1.0776, 1.0776, 1.0776, 1.0776, 1.0776, 1.0776, 1.0776,\n",
      "        1.0776, 1.0776, 1.0776, 1.0776, 1.0776, 1.0776, 1.0776, 1.0776, 1.0776,\n",
      "        1.0776, 1.0776, 1.0776, 1.0776, 1.0776, 1.0776, 0.5100]) return=  132640.40457803075\n",
      "probs of actions:  tensor([4.3731e-01, 4.2664e-01, 3.0690e-01, 4.7486e-01, 6.2047e-01, 1.8283e-02,\n",
      "        5.1649e-01, 4.6048e-01, 5.6530e-01, 5.5434e-01, 4.1340e-03, 2.0115e-01,\n",
      "        1.6684e-02, 3.7736e-04, 4.3648e-01, 4.8998e-01, 1.9829e-02, 5.7272e-01,\n",
      "        4.0393e-01, 3.1937e-01, 4.3911e-01, 3.3119e-01, 3.0537e-01, 6.6976e-01,\n",
      "        9.8906e-01], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5238, 0.4918, 0.5846, 0.5060, 0.5247, 0.5276, 0.5197, 0.5217,\n",
      "        0.5212, 0.5069, 0.5366, 0.5475, 0.5665, 0.5208, 0.5214, 0.4957, 0.5807,\n",
      "        0.4780, 0.5594, 0.5674, 0.4812, 0.5586, 0.5387, 0.5725])\n",
      "finalReturns:  tensor([0.0336, 0.0625])\n",
      "----------------------------------------\n",
      "iter  2  stage  22  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([17, 17, 17, 17, 17, 17, 17, 17, 17, 26, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "        17, 17, 17, 17, 17, 17,  0])\n",
      "loss=  tensor(0.0192, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.5452, 1.5452, 1.5452, 1.5452, 1.5452, 1.5452, 1.5452, 1.5452, 1.5452,\n",
      "        1.5452, 1.5452, 1.5452, 1.5452, 1.5452, 1.5452, 1.5452, 1.5452, 1.5452,\n",
      "        1.5452, 1.5452, 1.5452, 1.5452, 1.5452, 0.9737, 0.4646]) return=  135360.55464815182\n",
      "probs of actions:  tensor([0.9285, 0.9338, 0.9111, 0.9164, 0.8840, 0.9259, 0.8917, 0.9240, 0.9054,\n",
      "        0.0012, 0.9157, 0.9091, 0.9244, 0.9328, 0.9384, 0.9183, 0.9283, 0.9249,\n",
      "        0.9404, 0.9240, 0.9221, 0.9283, 0.9593, 0.9090, 0.9896],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4823, 0.5582, 0.5388, 0.5436, 0.5424, 0.5427, 0.5426, 0.5426, 0.5426,\n",
      "        0.5039, 0.5772, 0.5342, 0.5448, 0.5421, 0.5428, 0.5426, 0.5426, 0.5426,\n",
      "        0.5426, 0.5426, 0.5426, 0.5426, 0.5426, 0.5426, 0.5715])\n",
      "finalReturns:  tensor([0.1116, 0.1405, 0.1069])\n",
      "----------------------------------------\n",
      "iter  2  stage  21  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([17, 24, 17, 17, 17, 17, 17, 22, 17, 17, 17, 17, 17, 17, 17, 17, 17, 22,\n",
      "        17, 17, 22, 17, 17, 17,  0])\n",
      "loss=  tensor(0.1288, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.9930, 1.9930, 1.9930, 1.9930, 1.9930, 1.9930, 1.9930, 1.9930, 1.9930,\n",
      "        1.9930, 1.9930, 1.9930, 1.9930, 1.9930, 1.9930, 1.9930, 1.9930, 1.9930,\n",
      "        1.9930, 1.9930, 1.9930, 1.9930, 1.4027, 0.8980, 0.4323]) return=  135269.9521378386\n",
      "probs of actions:  tensor([0.8732, 0.0179, 0.8410, 0.8647, 0.8634, 0.8757, 0.8419, 0.0913, 0.8787,\n",
      "        0.9062, 0.8654, 0.8757, 0.8808, 0.9079, 0.8892, 0.8849, 0.8757, 0.0644,\n",
      "        0.8793, 0.8759, 0.0651, 0.8430, 0.8864, 0.7571, 0.9970],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4823, 0.5295, 0.5654, 0.5370, 0.5440, 0.5423, 0.5427, 0.5231, 0.5617,\n",
      "        0.5379, 0.5438, 0.5423, 0.5427, 0.5426, 0.5426, 0.5426, 0.5426, 0.5231,\n",
      "        0.5617, 0.5379, 0.5243, 0.5614, 0.5380, 0.5438, 0.5712])\n",
      "finalReturns:  tensor([0.2214, 0.2503, 0.2170, 0.1389])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  2  stage  20  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([18, 22, 22, 22, 16, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 17, 24,  0])\n",
      "loss=  tensor(2.2023, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.3434, 2.3434, 2.3434, 2.3434, 2.3434, 2.3434, 2.3434, 2.3434, 2.3434,\n",
      "        2.3434, 2.3434, 2.3434, 2.3434, 2.3434, 2.3434, 2.3434, 2.3434, 2.3434,\n",
      "        2.3434, 2.3434, 2.3434, 1.7567, 1.2512, 0.8026, 0.3945]) return=  134596.2288854169\n",
      "probs of actions:  tensor([0.0027, 0.8000, 0.8295, 0.7982, 0.0137, 0.7866, 0.8077, 0.8311, 0.7757,\n",
      "        0.7676, 0.8022, 0.8268, 0.8164, 0.7579, 0.8175, 0.7962, 0.8125, 0.8182,\n",
      "        0.8061, 0.8040, 0.8673, 0.8985, 0.0647, 0.0441, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5426, 0.5373, 0.5386, 0.5611, 0.5156, 0.5441, 0.5369, 0.5387,\n",
      "        0.5383, 0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5384,\n",
      "        0.5384, 0.5384, 0.5384, 0.5384, 0.5579, 0.5102, 0.5993])\n",
      "finalReturns:  tensor([0.4006, 0.4490, 0.4161, 0.3068, 0.2047])\n",
      "----------------------------------------\n",
      "iter  2  stage  19  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 17, 22, 22, 22, 22, 22, 24, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.1875, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.6857, 2.6857, 2.6857, 2.6857, 2.6857, 2.6857, 2.6857, 2.6857, 2.6857,\n",
      "        2.6857, 2.6857, 2.6857, 2.6857, 2.6857, 2.6857, 2.6857, 2.6857, 2.6857,\n",
      "        2.6857, 2.6857, 2.0990, 1.5935, 1.1449, 0.7368, 0.3579]) return=  134488.70432871333\n",
      "probs of actions:  tensor([0.9050, 0.9012, 0.9150, 0.8960, 0.8927, 0.0392, 0.9032, 0.9113, 0.8899,\n",
      "        0.8879, 0.9010, 0.0538, 0.9051, 0.8822, 0.9117, 0.8996, 0.9057, 0.9102,\n",
      "        0.9060, 0.9025, 0.9461, 0.9436, 0.9424, 0.9373, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5581, 0.5335, 0.5396, 0.5381, 0.5579, 0.5193, 0.5432, 0.5372,\n",
      "        0.5387, 0.5383, 0.5292, 0.5460, 0.5364, 0.5388, 0.5382, 0.5384, 0.5383,\n",
      "        0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5868])\n",
      "finalReturns:  tensor([0.5928, 0.6412, 0.6084, 0.5186, 0.3883, 0.2289])\n",
      "----------------------------------------\n",
      "iter  2  stage  18  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 17, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.2667, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.0284, 3.0284, 3.0284, 3.0284, 3.0284, 3.0284, 3.0284, 3.0284, 3.0284,\n",
      "        3.0284, 3.0284, 3.0284, 3.0284, 3.0284, 3.0284, 3.0284, 3.0284, 3.0284,\n",
      "        3.0284, 2.4413, 1.9359, 1.4873, 1.0792, 0.7003, 0.3424]) return=  134519.1548127569\n",
      "probs of actions:  tensor([0.9210, 0.9158, 0.9274, 0.9103, 0.9067, 0.9078, 0.9168, 0.9221, 0.9075,\n",
      "        0.9069, 0.9156, 0.9256, 0.9163, 0.9014, 0.0181, 0.9141, 0.9187, 0.9227,\n",
      "        0.9245, 0.9104, 0.9560, 0.9496, 0.9478, 0.9500, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5581, 0.5335, 0.5396, 0.5381, 0.5384, 0.5383, 0.5384, 0.5384,\n",
      "        0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5579, 0.5194, 0.5432, 0.5372,\n",
      "        0.5387, 0.5383, 0.5384, 0.5384, 0.5384, 0.5384, 0.5868])\n",
      "finalReturns:  tensor([0.7887, 0.8371, 0.8043, 0.7145, 0.5842, 0.4249, 0.2443])\n",
      "----------------------------------------\n",
      "iter  2  stage  17  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([22, 24, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.2357, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.3592, 3.3592, 3.3592, 3.3592, 3.3592, 3.3592, 3.3592, 3.3592, 3.3592,\n",
      "        3.3592, 3.3592, 3.3592, 3.3592, 3.3592, 3.3592, 3.3592, 3.3592, 3.3592,\n",
      "        2.7724, 2.2669, 1.8184, 1.4103, 1.0313, 0.6735, 0.3310]) return=  134444.89466666666\n",
      "probs of actions:  tensor([0.9528, 0.0494, 0.9549, 0.9443, 0.9409, 0.9427, 0.9482, 0.9500, 0.9457,\n",
      "        0.9458, 0.9480, 0.9526, 0.9470, 0.9422, 0.9535, 0.9476, 0.9500, 0.9530,\n",
      "        0.9664, 0.9401, 0.9671, 0.9680, 0.9685, 0.9721, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5489, 0.5411, 0.5377, 0.5385, 0.5383, 0.5384, 0.5384, 0.5384,\n",
      "        0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5384,\n",
      "        0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5868])\n",
      "finalReturns:  tensor([0.9960, 1.0444, 1.0116, 0.9218, 0.7915, 0.6322, 0.4517, 0.2557])\n",
      "----------------------------------------\n",
      "iter  2  stage  16  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 24, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.2379, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.6803, 3.6803, 3.6803, 3.6803, 3.6803, 3.6803, 3.6803, 3.6803, 3.6803,\n",
      "        3.6803, 3.6803, 3.6803, 3.6803, 3.6803, 3.6803, 3.6803, 3.6803, 3.0955,\n",
      "        2.5895, 2.1410, 1.7329, 1.3540, 0.9961, 0.6537, 0.3227]) return=  134445.23460823056\n",
      "probs of actions:  tensor([0.9648, 0.9603, 0.9664, 0.9577, 0.9548, 0.9563, 0.9609, 0.9619, 0.9596,\n",
      "        0.9595, 0.9610, 0.9640, 0.9599, 0.9569, 0.0341, 0.9608, 0.9626, 0.9690,\n",
      "        0.9761, 0.9543, 0.9755, 0.9771, 0.9764, 0.9821, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5581, 0.5335, 0.5396, 0.5381, 0.5384, 0.5383, 0.5384, 0.5384,\n",
      "        0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5292, 0.5460, 0.5364, 0.5388,\n",
      "        0.5382, 0.5384, 0.5383, 0.5384, 0.5384, 0.5384, 0.5868])\n",
      "finalReturns:  tensor([1.2118, 1.2602, 1.2273, 1.1375, 1.0072, 0.8479, 0.6674, 0.4714, 0.2641])\n",
      "----------------------------------------\n",
      "iter  2  stage  15  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.2841, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.9983, 3.9983, 3.9983, 3.9983, 3.9983, 3.9983, 3.9983, 3.9983, 3.9983,\n",
      "        3.9983, 3.9983, 3.9983, 3.9983, 3.9983, 3.9983, 3.9983, 3.4115, 2.9060,\n",
      "        2.4574, 2.0494, 1.6704, 1.3125, 0.9701, 0.6391, 0.3164]) return=  134475.688\n",
      "probs of actions:  tensor([0.9686, 0.9641, 0.9696, 0.9614, 0.9594, 0.9605, 0.9648, 0.9655, 0.9639,\n",
      "        0.9637, 0.9650, 0.9671, 0.9636, 0.9615, 0.9688, 0.9677, 0.9654, 0.9739,\n",
      "        0.9781, 0.9543, 0.9812, 0.9795, 0.9813, 0.9855, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5581, 0.5335, 0.5396, 0.5381, 0.5384, 0.5383, 0.5384, 0.5384,\n",
      "        0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5384,\n",
      "        0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5868])\n",
      "finalReturns:  tensor([1.4337, 1.4821, 1.4492, 1.3595, 1.2292, 1.0698, 0.8893, 0.6933, 0.4860,\n",
      "        0.2703])\n",
      "----------------------------------------\n",
      "iter  2  stage  14  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([22, 22, 22, 24, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 22, 22,  0])\n",
      "loss=  tensor(0.3424, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.3101, 4.3101, 4.3101, 4.3101, 4.3101, 4.3101, 4.3101, 4.3101, 4.3101,\n",
      "        4.3101, 4.3101, 4.3101, 4.3101, 4.3101, 4.3101, 3.7233, 3.2178, 2.7692,\n",
      "        2.3612, 1.9822, 1.6243, 1.2819, 0.9509, 0.6282, 0.3118]) return=  134445.21341666667\n",
      "probs of actions:  tensor([0.9685, 0.9639, 0.9698, 0.0383, 0.9595, 0.9606, 0.9645, 0.9653, 0.9641,\n",
      "        0.9640, 0.9651, 0.9672, 0.9636, 0.9619, 0.9746, 0.9696, 0.9669, 0.9763,\n",
      "        0.9773, 0.9544, 0.9801, 0.9832, 0.9823, 0.9843, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5581, 0.5335, 0.5304, 0.5457, 0.5365, 0.5388, 0.5382, 0.5384,\n",
      "        0.5383, 0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5384,\n",
      "        0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5384, 0.5868])\n",
      "finalReturns:  tensor([1.6602, 1.7086, 1.6758, 1.5860, 1.4557, 1.2964, 1.1158, 0.9199, 0.7126,\n",
      "        0.4969, 0.2750])\n",
      "----------------------------------------\n",
      "iter  2  stage  13  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([22, 22, 22, 22, 22, 24, 22, 24, 24, 22, 22, 22, 24, 22, 22, 22, 22, 22,\n",
      "        24, 22, 24, 22, 22, 22,  0])\n",
      "loss=  tensor(8.3404, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.5998, 4.5998, 4.5998, 4.5998, 4.5998, 4.5998, 4.5998, 4.5998, 4.5998,\n",
      "        4.5998, 4.5998, 4.5998, 4.5998, 4.5998, 4.0054, 3.5016, 3.0526, 2.6447,\n",
      "        2.2657, 1.9078, 1.5654, 1.2401, 0.9203, 0.6119, 0.3046]) return=  134292.63893419717\n",
      "probs of actions:  tensor([0.9018, 0.8871, 0.9064, 0.8867, 0.8751, 0.1197, 0.8925, 0.1104, 0.1097,\n",
      "        0.8910, 0.8925, 0.8960, 0.1160, 0.8909, 0.9092, 0.9009, 0.8661, 0.9170,\n",
      "        0.0809, 0.8616, 0.0710, 0.9437, 0.9367, 0.9406, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5581, 0.5335, 0.5396, 0.5381, 0.5292, 0.5460, 0.5272, 0.5373,\n",
      "        0.5440, 0.5370, 0.5387, 0.5291, 0.5461, 0.5364, 0.5388, 0.5382, 0.5384,\n",
      "        0.5291, 0.5460, 0.5272, 0.5465, 0.5363, 0.5389, 0.5866])\n",
      "finalReturns:  tensor([1.9089, 1.9573, 1.9246, 1.8347, 1.7045, 1.5451, 1.3738, 1.1702, 0.9682,\n",
      "        0.7415, 0.5136, 0.2820])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  2  stage  12  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([22, 24, 24, 22, 22, 22, 24, 22, 22, 22, 22, 24, 24, 22, 22, 22, 22, 22,\n",
      "        22, 22, 24, 24, 22, 24,  0])\n",
      "loss=  tensor(12.3434, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.8923, 4.8923, 4.8923, 4.8923, 4.8923, 4.8923, 4.8923, 4.8923, 4.8923,\n",
      "        4.8923, 4.8923, 4.8923, 4.8923, 4.2979, 3.7942, 3.3519, 2.9471, 2.5707,\n",
      "        2.2148, 1.8738, 1.5437, 1.2218, 0.9060, 0.6001, 0.3004]) return=  134247.45269681388\n",
      "probs of actions:  tensor([0.6391, 0.4036, 0.3466, 0.6153, 0.5722, 0.5878, 0.3735, 0.5996, 0.6154,\n",
      "        0.6103, 0.6166, 0.3860, 0.4416, 0.5771, 0.5627, 0.6402, 0.5170, 0.6724,\n",
      "        0.6441, 0.5184, 0.3052, 0.2713, 0.6895, 0.3034, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5489, 0.5319, 0.5453, 0.5366, 0.5388, 0.5290, 0.5461, 0.5364,\n",
      "        0.5388, 0.5382, 0.5292, 0.5368, 0.5441, 0.5369, 0.5387, 0.5383, 0.5384,\n",
      "        0.5384, 0.5384, 0.5292, 0.5368, 0.5441, 0.5277, 0.5948])\n",
      "finalReturns:  tensor([2.1502, 2.2078, 2.1675, 2.0728, 1.9389, 1.7770, 1.5946, 1.3972, 1.1889,\n",
      "        0.9816, 0.7607, 0.5224, 0.2944])\n",
      "----------------------------------------\n",
      "iter  2  stage  11  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([24, 24, 24, 24, 24, 22, 24, 22, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 22, 24, 24, 24,  0])\n",
      "loss=  tensor(5.1378, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.0743, 5.0743, 5.0743, 5.0743, 5.0743, 5.0743, 5.0743, 5.0743, 5.0743,\n",
      "        5.0743, 5.0743, 5.0743, 4.4812, 3.9772, 3.5349, 3.1365, 2.7693, 2.4248,\n",
      "        2.0968, 1.7809, 1.4739, 1.1736, 0.8782, 0.5810, 0.2893]) return=  133847.94214272863\n",
      "probs of actions:  tensor([0.8439, 0.8704, 0.8356, 0.8477, 0.8817, 0.1337, 0.8461, 0.1280, 0.8540,\n",
      "        0.8596, 0.8526, 0.8777, 0.9175, 0.8723, 0.9118, 0.8651, 0.9045, 0.8369,\n",
      "        0.8812, 0.9076, 0.1471, 0.8375, 0.8724, 0.8691, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4536, 0.5567, 0.5300, 0.5366, 0.5350, 0.5446, 0.5276, 0.5464, 0.5271,\n",
      "        0.5373, 0.5348, 0.5354, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353,\n",
      "        0.5353, 0.5353, 0.5445, 0.5276, 0.5372, 0.5348, 0.5930])\n",
      "finalReturns:  tensor([2.4807, 2.5383, 2.5071, 2.4141, 2.2772, 2.1091, 1.9183, 1.7110, 1.4916,\n",
      "        1.2632, 1.0191, 0.7869, 0.5468, 0.3037])\n",
      "----------------------------------------\n",
      "iter  2  stage  10  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([24, 24, 24, 24, 22, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24,  0])\n",
      "loss=  tensor(0.4986, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.3491, 5.3491, 5.3491, 5.3491, 5.3491, 5.3491, 5.3491, 5.3491, 5.3491,\n",
      "        5.3491, 5.3491, 4.7562, 4.2521, 3.8099, 3.4114, 3.0443, 2.6998, 2.3718,\n",
      "        2.0559, 1.7489, 1.4486, 1.1532, 0.8614, 0.5724, 0.2855]) return=  133786.32760416673\n",
      "probs of actions:  tensor([0.9711, 0.9771, 0.9691, 0.9704, 0.0206, 0.9748, 0.9702, 0.9777, 0.9723,\n",
      "        0.9741, 0.9751, 0.9822, 0.9866, 0.9817, 0.9886, 0.9788, 0.9858, 0.9735,\n",
      "        0.9832, 0.9838, 0.9789, 0.9763, 0.9846, 0.9826, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4536, 0.5567, 0.5300, 0.5366, 0.5442, 0.5277, 0.5372, 0.5348, 0.5354,\n",
      "        0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353,\n",
      "        0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5929])\n",
      "finalReturns:  tensor([2.7380, 2.7956, 2.7644, 2.6713, 2.5345, 2.3663, 2.1755, 1.9682, 1.7488,\n",
      "        1.5205, 1.2855, 1.0456, 0.8021, 0.5558, 0.3074])\n",
      "----------------------------------------\n",
      "iter  2  stage  9  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24,  0])\n",
      "loss=  tensor(0.2965, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.6334, 5.6334, 5.6334, 5.6334, 5.6334, 5.6334, 5.6334, 5.6334, 5.6334,\n",
      "        5.6334, 5.0405, 4.5364, 4.0942, 3.6957, 3.3286, 2.9841, 2.6561, 2.3402,\n",
      "        2.0332, 1.7329, 1.4375, 1.1458, 0.8567, 0.5698, 0.2843]) return=  133755.66666666666\n",
      "probs of actions:  tensor([0.9837, 0.9872, 0.9825, 0.9829, 0.9885, 0.9855, 0.9831, 0.9875, 0.9842,\n",
      "        0.9881, 0.9885, 0.9909, 0.9935, 0.9897, 0.9942, 0.9897, 0.9917, 0.9867,\n",
      "        0.9917, 0.9918, 0.9900, 0.9879, 0.9922, 0.9919, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4536, 0.5567, 0.5300, 0.5366, 0.5350, 0.5354, 0.5353, 0.5353, 0.5353,\n",
      "        0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353,\n",
      "        0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5929])\n",
      "finalReturns:  tensor([2.9890, 3.0466, 3.0154, 2.9223, 2.7855, 2.6173, 2.4265, 2.2192, 1.9998,\n",
      "        1.7715, 1.5365, 1.2966, 1.0530, 0.8068, 0.5584, 0.3086])\n",
      "----------------------------------------\n",
      "iter  2  stage  8  ep  99999   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24,  0])\n",
      "loss=  tensor(0.0961, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.9169, 5.9169, 5.9169, 5.9169, 5.9169, 5.9169, 5.9169, 5.9169, 5.9169,\n",
      "        5.3240, 4.8199, 4.3777, 3.9792, 3.6120, 3.2675, 2.9395, 2.6236, 2.3167,\n",
      "        2.0164, 1.7210, 1.4292, 1.1402, 0.8532, 0.5678, 0.2835]) return=  133755.66666666666\n",
      "probs of actions:  tensor([0.9944, 0.9956, 0.9939, 0.9940, 0.9961, 0.9949, 0.9941, 0.9957, 0.9959,\n",
      "        0.9963, 0.9971, 0.9972, 0.9983, 0.9974, 0.9986, 0.9978, 0.9971, 0.9966,\n",
      "        0.9973, 0.9980, 0.9976, 0.9971, 0.9983, 0.9984, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4536, 0.5567, 0.5300, 0.5366, 0.5350, 0.5354, 0.5353, 0.5353, 0.5353,\n",
      "        0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353,\n",
      "        0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5929])\n",
      "finalReturns:  tensor([3.2408, 3.2984, 3.2672, 3.1741, 3.0373, 2.8692, 2.6784, 2.4711, 2.2517,\n",
      "        2.0233, 1.7883, 1.5484, 1.3049, 1.0586, 0.8103, 0.5604, 0.3094])\n",
      "----------------------------------------\n",
      "iter  2  stage  7  ep  73532   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24,  0])\n",
      "loss=  tensor(0.0306, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.1997, 6.1997, 6.1997, 6.1997, 6.1997, 6.1997, 6.1997, 6.1997, 5.6068,\n",
      "        5.1027, 4.6605, 4.2620, 3.8949, 3.5503, 3.2223, 2.9065, 2.5995, 2.2992,\n",
      "        2.0038, 1.7120, 1.4230, 1.1360, 0.8506, 0.5663, 0.2828]) return=  133755.66666666666\n",
      "probs of actions:  tensor([0.9977, 0.9982, 0.9975, 0.9975, 0.9984, 0.9978, 0.9975, 0.9990, 0.9988,\n",
      "        0.9988, 0.9991, 0.9994, 0.9995, 0.9995, 0.9999, 0.9995, 0.9991, 0.9989,\n",
      "        0.9993, 0.9994, 0.9993, 0.9992, 0.9997, 0.9996, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4536, 0.5567, 0.5300, 0.5366, 0.5350, 0.5354, 0.5353, 0.5353, 0.5353,\n",
      "        0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353,\n",
      "        0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5929])\n",
      "finalReturns:  tensor([3.4933, 3.5509, 3.5197, 3.4266, 3.2898, 3.1216, 2.9309, 2.7236, 2.5041,\n",
      "        2.2758, 2.0408, 1.8009, 1.5574, 1.3111, 1.0628, 0.8129, 0.5619, 0.3101])\n",
      "----------------------------------------\n",
      "iter  2  stage  6  ep  44880   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24,  0])\n",
      "loss=  tensor(0.0163, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.4820, 6.4820, 6.4820, 6.4820, 6.4820, 6.4820, 6.4820, 5.8892, 5.3851,\n",
      "        4.9428, 4.5444, 4.1772, 3.8327, 3.5047, 3.1888, 2.8818, 2.5815, 2.2861,\n",
      "        1.9944, 1.7053, 1.4184, 1.1329, 0.8486, 0.5652, 0.2823]) return=  133755.66666666666\n",
      "probs of actions:  tensor([0.9986, 0.9989, 0.9985, 0.9985, 0.9991, 0.9987, 0.9990, 0.9997, 0.9994,\n",
      "        0.9994, 0.9995, 0.9997, 1.0000, 0.9998, 1.0000, 0.9999, 0.9996, 0.9995,\n",
      "        0.9997, 0.9998, 0.9997, 0.9997, 1.0000, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4536, 0.5567, 0.5300, 0.5366, 0.5350, 0.5354, 0.5353, 0.5353, 0.5353,\n",
      "        0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353,\n",
      "        0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5929])\n",
      "finalReturns:  tensor([3.7462, 3.8038, 3.7726, 3.6796, 3.5428, 3.3746, 3.1838, 2.9765, 2.7571,\n",
      "        2.5288, 2.2938, 2.0539, 1.8103, 1.5641, 1.3157, 1.0659, 0.8149, 0.5630,\n",
      "        0.3106])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  2  stage  5  ep  5209   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24,  0])\n",
      "loss=  tensor(0.0176, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.7641, 6.7641, 6.7641, 6.7641, 6.7641, 6.7641, 6.1711, 5.6670, 5.2248,\n",
      "        4.8263, 4.4592, 4.1147, 3.7867, 3.4708, 3.1638, 2.8635, 2.5681, 2.2763,\n",
      "        1.9873, 1.7003, 1.4149, 1.1306, 0.8471, 0.5643, 0.2820]) return=  133755.66666666666\n",
      "probs of actions:  tensor([0.9988, 0.9991, 0.9987, 0.9987, 0.9992, 0.9990, 0.9991, 0.9998, 0.9995,\n",
      "        0.9995, 0.9996, 0.9998, 1.0000, 0.9999, 1.0000, 0.9999, 0.9997, 0.9995,\n",
      "        0.9998, 0.9998, 0.9998, 0.9998, 1.0000, 0.9999, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4536, 0.5567, 0.5300, 0.5366, 0.5350, 0.5354, 0.5353, 0.5353, 0.5353,\n",
      "        0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353,\n",
      "        0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5929])\n",
      "finalReturns:  tensor([3.9996, 4.0572, 4.0260, 3.9329, 3.7961, 3.6279, 3.4371, 3.2299, 3.0104,\n",
      "        2.7821, 2.5471, 2.3072, 2.0637, 1.8174, 1.5691, 1.3192, 1.0682, 0.8164,\n",
      "        0.5639, 0.3109])\n",
      "----------------------------------------\n",
      "iter  2  stage  4  ep  0   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24,  0])\n",
      "loss=  tensor(0.0217, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.0455, 7.0455, 7.0455, 7.0455, 7.0455, 6.4529, 5.9487, 5.5065, 5.1080,\n",
      "        4.7409, 4.3964, 4.0684, 3.7525, 3.4455, 3.1452, 2.8498, 2.5580, 2.2690,\n",
      "        1.9821, 1.6966, 1.4123, 1.1288, 0.8460, 0.5637, 0.2817]) return=  133755.66666666666\n",
      "probs of actions:  tensor([0.9988, 0.9991, 0.9987, 0.9987, 0.9992, 0.9990, 0.9991, 0.9998, 0.9995,\n",
      "        0.9995, 0.9996, 0.9998, 1.0000, 0.9999, 1.0000, 0.9999, 0.9997, 0.9995,\n",
      "        0.9998, 0.9998, 0.9998, 0.9998, 1.0000, 0.9999, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4536, 0.5567, 0.5300, 0.5366, 0.5350, 0.5354, 0.5353, 0.5353, 0.5353,\n",
      "        0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353,\n",
      "        0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5929])\n",
      "finalReturns:  tensor([4.2532, 4.3108, 4.2796, 4.1865, 4.0497, 3.8815, 3.6907, 3.4834, 3.2640,\n",
      "        3.0357, 2.8007, 2.5608, 2.3173, 2.0710, 1.8226, 1.5728, 1.3218, 1.0700,\n",
      "        0.8175, 0.5645, 0.3112])\n",
      "----------------------------------------\n",
      "iter  2  stage  3  ep  357   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24,  0])\n",
      "loss=  tensor(0.0256, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.3283, 7.3283, 7.3283, 7.3283, 6.7341, 6.2303, 5.7880, 5.3895, 5.0224,\n",
      "        4.6779, 4.3499, 4.0340, 3.7270, 3.4267, 3.1313, 2.8396, 2.5505, 2.2636,\n",
      "        1.9781, 1.6938, 1.4103, 1.1275, 0.8452, 0.5632, 0.2815]) return=  133755.66666666666\n",
      "probs of actions:  tensor([0.9988, 0.9991, 0.9987, 0.9990, 0.9994, 0.9990, 0.9991, 0.9998, 0.9995,\n",
      "        0.9995, 0.9996, 0.9998, 1.0000, 0.9999, 1.0000, 0.9999, 0.9997, 0.9996,\n",
      "        0.9998, 0.9999, 0.9998, 0.9998, 1.0000, 0.9999, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4536, 0.5567, 0.5300, 0.5366, 0.5350, 0.5354, 0.5353, 0.5353, 0.5353,\n",
      "        0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353,\n",
      "        0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5929])\n",
      "finalReturns:  tensor([4.5069, 4.5645, 4.5334, 4.4403, 4.3035, 4.1353, 3.9445, 3.7372, 3.5178,\n",
      "        3.2895, 3.0545, 2.8146, 2.5711, 2.3248, 2.0764, 1.8266, 1.5756, 1.3238,\n",
      "        1.0713, 0.8183, 0.5650, 0.3114])\n",
      "----------------------------------------\n",
      "iter  2  stage  2  ep  216   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24,  0])\n",
      "loss=  tensor(0.0289, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.6043, 7.6043, 7.6043, 7.0167, 6.5114, 6.0694, 5.6709, 5.3037, 4.9592,\n",
      "        4.6312, 4.3153, 4.0084, 3.7081, 3.4127, 3.1209, 2.8319, 2.5449, 2.2595,\n",
      "        1.9752, 1.6917, 1.4089, 1.1265, 0.8446, 0.5629, 0.2814]) return=  133755.66666666666\n",
      "probs of actions:  tensor([0.9989, 0.9991, 0.9990, 0.9991, 0.9995, 0.9991, 0.9992, 0.9998, 0.9995,\n",
      "        0.9996, 0.9997, 0.9999, 1.0000, 0.9999, 1.0000, 1.0000, 0.9997, 0.9996,\n",
      "        0.9998, 0.9999, 0.9998, 0.9998, 1.0000, 0.9999, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4536, 0.5567, 0.5300, 0.5366, 0.5350, 0.5354, 0.5353, 0.5353, 0.5353,\n",
      "        0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353,\n",
      "        0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5929])\n",
      "finalReturns:  tensor([4.7610, 4.8186, 4.7873, 4.6942, 4.5574, 4.3893, 4.1985, 3.9912, 3.7718,\n",
      "        3.5434, 3.3084, 3.0685, 2.8250, 2.5787, 2.3304, 2.0805, 1.8295, 1.5777,\n",
      "        1.3252, 1.0723, 0.8189, 0.5653, 0.3115])\n",
      "----------------------------------------\n",
      "iter  2  stage  1  ep  0   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24,  0])\n",
      "loss=  tensor(0.0342, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.9073, 7.9073, 7.2930, 6.7938, 6.3504, 5.9522, 5.5850, 5.2405, 4.9125,\n",
      "        4.5966, 4.2896, 3.9893, 3.6939, 3.4021, 3.1131, 2.8262, 2.5407, 2.2564,\n",
      "        1.9729, 1.6901, 1.4078, 1.1258, 0.8441, 0.5626, 0.2812]) return=  133755.66666666666\n",
      "probs of actions:  tensor([0.9989, 0.9991, 0.9990, 0.9991, 0.9995, 0.9991, 0.9992, 0.9998, 0.9995,\n",
      "        0.9996, 0.9997, 0.9999, 1.0000, 0.9999, 1.0000, 1.0000, 0.9997, 0.9996,\n",
      "        0.9998, 0.9999, 0.9998, 0.9998, 1.0000, 0.9999, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4536, 0.5567, 0.5300, 0.5366, 0.5350, 0.5354, 0.5353, 0.5353, 0.5353,\n",
      "        0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353,\n",
      "        0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5929])\n",
      "finalReturns:  tensor([5.0147, 5.0723, 5.0415, 4.9482, 4.8115, 4.6433, 4.4525, 4.2452, 4.0258,\n",
      "        3.7975, 3.5625, 3.3226, 3.0791, 2.8328, 2.5844, 2.3346, 2.0836, 1.8318,\n",
      "        1.5793, 1.3263, 1.0730, 0.8194, 0.5656, 0.3117])\n",
      "----------------------------------------\n",
      "iter  2  stage  0  ep  64   adversary:  AdversaryModes.imitation_132\n",
      "  actions:  tensor([24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24,  0])\n",
      "loss=  tensor(0.0387, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.1053, 7.5941, 7.0703, 6.6326, 6.2331, 5.8662, 5.5216, 5.1936, 4.8777,\n",
      "        4.5708, 4.2705, 3.9750, 3.6833, 3.3943, 3.1073, 2.8219, 2.5375, 2.2541,\n",
      "        1.9713, 1.6889, 1.4070, 1.1253, 0.8437, 0.5624, 0.2812]) return=  133755.66666666666\n",
      "probs of actions:  tensor([0.9990, 0.9992, 0.9991, 0.9992, 0.9995, 0.9991, 0.9992, 0.9998, 0.9996,\n",
      "        0.9996, 0.9997, 0.9999, 1.0000, 0.9999, 1.0000, 1.0000, 0.9997, 0.9996,\n",
      "        0.9998, 0.9999, 0.9998, 0.9998, 1.0000, 0.9999, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4536, 0.5567, 0.5300, 0.5366, 0.5350, 0.5354, 0.5353, 0.5353, 0.5353,\n",
      "        0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353,\n",
      "        0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5353, 0.5929])\n",
      "finalReturns:  tensor([5.2702, 5.3278, 5.2950, 5.2026, 5.0656, 4.8975, 4.7067, 4.4994, 4.2800,\n",
      "        4.0516, 3.8166, 3.5768, 3.3332, 3.0869, 2.8386, 2.5887, 2.3378, 2.0859,\n",
      "        1.8334, 1.5805, 1.3271, 1.0735, 0.8198, 0.5658, 0.3117])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1682794213 saved\n",
      "[2164266, 'tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])', 133755.66666666666, 78214.06666666668, 0.038705166429281235, 1e-05, 1, 0, 'tensor([24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\\n        24, 24, 24, 24, 24, 24,  0])', '[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\\n 1.]', '0,[1e-05,1][1, 10000, 1, 1],1682794213', 25, 50, 170327.33349408704, 211165.96084616287, 87190.73961268678, 133755.6666666667, 130787.53866666667, 82417.65584066519, 82417.65584066519, 99036.07704587854, 99036.07704587854, 100843.49835063974, 82417.65584066519, 99036.07704587854]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x648 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy reset\n",
      "----------------------------------------\n",
      "iter  0  stage  24  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([0, 0, 0, 0, 0, 7, 0, 0, 0, 5, 0, 4, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5134, 0.5134, 0.5134, 0.5134, 0.5134, 0.5134, 0.5134, 0.5134, 0.5134,\n",
      "        0.5134, 0.5134, 0.5134, 0.5134, 0.5134, 0.5134, 0.5134, 0.5134, 0.5134,\n",
      "        0.5134, 0.5134, 0.5134, 0.5134, 0.5134, 0.5134, 0.5134]) return=  127976.82117179644\n",
      "probs of actions:  tensor([0.8485, 0.8295, 0.8628, 0.8631, 0.7988, 0.0024, 0.8453, 0.8518, 0.8468,\n",
      "        0.0049, 0.8436, 0.0070, 0.0926, 0.8624, 0.8310, 0.0958, 0.8208, 0.8386,\n",
      "        0.0177, 0.8306, 0.8285, 0.8579, 0.8012, 0.0884, 0.9670],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5094, 0.5099, 0.5098, 0.5098, 0.5049, 0.5351, 0.5036, 0.5114,\n",
      "        0.5069, 0.5279, 0.5037, 0.5252, 0.5095, 0.5099, 0.5097, 0.5134, 0.5089,\n",
      "        0.5096, 0.5169, 0.5080, 0.5102, 0.5097, 0.5097, 0.5134])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  0  stage  23  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([16,  0, 16, 15, 14, 15, 11,  3,  0,  0, 20, 15, 15,  1, 11,  8,  8, 16,\n",
      "         1,  1, 15, 16,  0, 15,  0])\n",
      "loss=  tensor(0.0781, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0114, 1.0114, 1.0114, 1.0114, 1.0114, 1.0114, 1.0114, 1.0114, 1.0114,\n",
      "        1.0114, 1.0114, 1.0114, 1.0114, 1.0114, 1.0114, 1.0114, 1.0114, 1.0114,\n",
      "        1.0114, 1.0114, 1.0114, 1.0114, 1.0114, 1.0114, 0.5126]) return=  131016.7267856707\n",
      "probs of actions:  tensor([0.0725, 0.2308, 0.0930, 0.1020, 0.0359, 0.0781, 0.0610, 0.0218, 0.3880,\n",
      "        0.3613, 0.0086, 0.1072, 0.0867, 0.0801, 0.0709, 0.0041, 0.0056, 0.0781,\n",
      "        0.0742, 0.0698, 0.0922, 0.0806, 0.1969, 0.0934, 0.9848],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4856, 0.5681, 0.4701, 0.5498, 0.5293, 0.5285, 0.5421, 0.5377, 0.5134,\n",
      "        0.5089, 0.4700, 0.5611, 0.5236, 0.5553, 0.4901, 0.5454, 0.5218, 0.5085,\n",
      "        0.5621, 0.5005, 0.4932, 0.5376, 0.5548, 0.4763, 0.5677])\n",
      "finalReturns:  tensor([0.0326, 0.0551])\n",
      "----------------------------------------\n",
      "iter  0  stage  22  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([22, 22, 18, 15, 20, 18, 12, 12, 27, 19, 15, 18, 15, 12,  1, 22,  0, 18,\n",
      "        15, 14, 18, 18, 21, 22,  0])\n",
      "loss=  tensor(0.8808, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.4995, 1.4995, 1.4995, 1.4995, 1.4995, 1.4995, 1.4995, 1.4995, 1.4995,\n",
      "        1.4995, 1.4995, 1.4995, 1.4995, 1.4995, 1.4995, 1.4995, 1.4995, 1.4995,\n",
      "        1.4995, 1.4995, 1.4995, 1.4995, 1.4995, 0.9377, 0.4405]) return=  131827.89335068475\n",
      "probs of actions:  tensor([0.0964, 0.0982, 0.3525, 0.0528, 0.0570, 0.3105, 0.1858, 0.0708, 0.0098,\n",
      "        0.0618, 0.0573, 0.3612, 0.0493, 0.1349, 0.0565, 0.1031, 0.0742, 0.2706,\n",
      "        0.0473, 0.0258, 0.3585, 0.2952, 0.0171, 0.0871, 0.9850],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5426, 0.5381, 0.5380, 0.5118, 0.5403, 0.5456, 0.5264, 0.4727,\n",
      "        0.5651, 0.5343, 0.5203, 0.5425, 0.5363, 0.5430, 0.4568, 0.5926, 0.4577,\n",
      "        0.5589, 0.5271, 0.5191, 0.5329, 0.5177, 0.5256, 0.5747])\n",
      "finalReturns:  tensor([0.1185, 0.1626, 0.1342])\n",
      "----------------------------------------\n",
      "iter  0  stage  21  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([22, 22, 22,  0, 22, 22, 18, 22, 18, 22, 23, 22, 17, 22, 14, 22, 23, 22,\n",
      "        22, 16, 22, 22, 16, 22,  0])\n",
      "loss=  tensor(1.5077, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.9284, 1.9284, 1.9284, 1.9284, 1.9284, 1.9284, 1.9284, 1.9284, 1.9284,\n",
      "        1.9284, 1.9284, 1.9284, 1.9284, 1.9284, 1.9284, 1.9284, 1.9284, 1.9284,\n",
      "        1.9284, 1.9284, 1.9284, 1.9284, 1.3481, 0.8552, 0.4170]) return=  131625.96944387257\n",
      "probs of actions:  tensor([0.5168, 0.4888, 0.5555, 0.0028, 0.5101, 0.5398, 0.1094, 0.5222, 0.1204,\n",
      "        0.5231, 0.1283, 0.5068, 0.0124, 0.5243, 0.0104, 0.5004, 0.1249, 0.4907,\n",
      "        0.5293, 0.0305, 0.5330, 0.6392, 0.0121, 0.5230, 0.9884],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4628, 0.5426, 0.5221, 0.5756, 0.4456, 0.5473, 0.5369, 0.5124, 0.5456,\n",
      "        0.5102, 0.5257, 0.5290, 0.5450, 0.5075, 0.5597, 0.4951, 0.5296, 0.5280,\n",
      "        0.5257, 0.5491, 0.5036, 0.5319, 0.5475, 0.5040, 0.5802])\n",
      "finalReturns:  tensor([0.2352, 0.2836, 0.2289, 0.1632])\n",
      "----------------------------------------\n",
      "iter  0  stage  20  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([23, 22, 22, 22, 22, 22, 22, 23, 19, 22, 27, 22, 22, 22, 23, 23, 23, 18,\n",
      "        22, 22, 22, 23, 20, 22,  0])\n",
      "loss=  tensor(2.8372, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.2744, 2.2744, 2.2744, 2.2744, 2.2744, 2.2744, 2.2744, 2.2744, 2.2744,\n",
      "        2.2744, 2.2744, 2.2744, 2.2744, 2.2744, 2.2744, 2.2744, 2.2744, 2.2744,\n",
      "        2.2744, 2.2744, 2.2744, 1.7008, 1.2064, 0.7685, 0.3738]) return=  131444.76673770836\n",
      "probs of actions:  tensor([0.1846, 0.6751, 0.7233, 0.6721, 0.6664, 0.6970, 0.6251, 0.1851, 0.0142,\n",
      "        0.6894, 0.0127, 0.6822, 0.6845, 0.6798, 0.1922, 0.1971, 0.1872, 0.0422,\n",
      "        0.6683, 0.6213, 0.7433, 0.1346, 0.0178, 0.7598, 0.9964],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4583, 0.5464, 0.5212, 0.5274, 0.5259, 0.5262, 0.5261, 0.5217, 0.5423,\n",
      "        0.5139, 0.5047, 0.5445, 0.5216, 0.5273, 0.5214, 0.5255, 0.5245, 0.5453,\n",
      "        0.5103, 0.5302, 0.5252, 0.5219, 0.5383, 0.5177, 0.5767])\n",
      "finalReturns:  tensor([0.4053, 0.4537, 0.4263, 0.3258, 0.2029])\n",
      "----------------------------------------\n",
      "iter  0  stage  19  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([23, 22, 23, 22, 23, 22, 23, 22, 22, 22, 23, 18, 23, 23, 22, 22, 23, 23,\n",
      "        22, 22, 22, 22, 22, 19,  0])\n",
      "loss=  tensor(2.8711, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.6216, 2.6216, 2.6216, 2.6216, 2.6216, 2.6216, 2.6216, 2.6216, 2.6216,\n",
      "        2.6216, 2.6216, 2.6216, 2.6216, 2.6216, 2.6216, 2.6216, 2.6216, 2.6216,\n",
      "        2.6216, 2.6216, 2.0477, 1.5533, 1.1155, 0.7175, 0.3483]) return=  131442.16007456576\n",
      "probs of actions:  tensor([0.2345, 0.7010, 0.2029, 0.6672, 0.2341, 0.7003, 0.2802, 0.7011, 0.6640,\n",
      "        0.7034, 0.2696, 0.0222, 0.2170, 0.2349, 0.6298, 0.6698, 0.2356, 0.2339,\n",
      "        0.6703, 0.6671, 0.7788, 0.8038, 0.7787, 0.0031, 0.9995],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4583, 0.5464, 0.5167, 0.5312, 0.5204, 0.5303, 0.5206, 0.5302, 0.5252,\n",
      "        0.5264, 0.5216, 0.5460, 0.5057, 0.5295, 0.5280, 0.5257, 0.5218, 0.5254,\n",
      "        0.5290, 0.5255, 0.5263, 0.5261, 0.5262, 0.5385, 0.5633])\n",
      "finalReturns:  tensor([0.5842, 0.6326, 0.6007, 0.5124, 0.3842, 0.2149])\n",
      "----------------------------------------\n",
      "iter  0  stage  18  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([23, 23, 22, 23, 22, 22, 23, 22, 22, 23, 22, 22, 22, 22, 22, 23, 22, 22,\n",
      "        27, 23, 23, 22, 22, 22,  0])\n",
      "loss=  tensor(5.6294, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.9041, 2.9041, 2.9041, 2.9041, 2.9041, 2.9041, 2.9041, 2.9041, 2.9041,\n",
      "        2.9041, 2.9041, 2.9041, 2.9041, 2.9041, 2.9041, 2.9041, 2.9041, 2.9041,\n",
      "        2.9041, 2.3293, 1.8351, 1.4136, 1.0266, 0.6685, 0.3275]) return=  131314.67447750745\n",
      "probs of actions:  tensor([0.3571, 0.3441, 0.6513, 0.3953, 0.5974, 0.6013, 0.4182, 0.6063, 0.5660,\n",
      "        0.3563, 0.5332, 0.6063, 0.6232, 0.6000, 0.5383, 0.3729, 0.6065, 0.6052,\n",
      "        0.0218, 0.4118, 0.2705, 0.7124, 0.6872, 0.7186, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4583, 0.5419, 0.5249, 0.5220, 0.5299, 0.5252, 0.5219, 0.5299, 0.5252,\n",
      "        0.5219, 0.5299, 0.5252, 0.5264, 0.5261, 0.5262, 0.5217, 0.5300, 0.5252,\n",
      "        0.5019, 0.5407, 0.5207, 0.5302, 0.5252, 0.5264, 0.5745])\n",
      "finalReturns:  tensor([0.8156, 0.8885, 0.8419, 0.7427, 0.5995, 0.4324, 0.2470])\n",
      "----------------------------------------\n",
      "iter  0  stage  17  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([23, 23, 23, 23, 23, 23, 23, 22, 23, 23, 22, 23, 22, 20, 23, 22, 23, 23,\n",
      "        23, 23, 23, 23, 22, 23,  0])\n",
      "loss=  tensor(2.5241, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.2482, 3.2482, 3.2482, 3.2482, 3.2482, 3.2482, 3.2482, 3.2482, 3.2482,\n",
      "        3.2482, 3.2482, 3.2482, 3.2482, 3.2482, 3.2482, 3.2482, 3.2482, 3.2482,\n",
      "        2.6695, 2.1762, 1.7414, 1.3482, 0.9849, 0.6432, 0.3172]) return=  131291.9557196191\n",
      "probs of actions:  tensor([0.7267, 0.7023, 0.7028, 0.7538, 0.6919, 0.7213, 0.7498, 0.2223, 0.7321,\n",
      "        0.7212, 0.1687, 0.7095, 0.2447, 0.0093, 0.6804, 0.2080, 0.7106, 0.7318,\n",
      "        0.7188, 0.7547, 0.6970, 0.6964, 0.2844, 0.7467, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4583, 0.5419, 0.5204, 0.5258, 0.5244, 0.5248, 0.5247, 0.5292, 0.5209,\n",
      "        0.5257, 0.5290, 0.5210, 0.5301, 0.5336, 0.5144, 0.5318, 0.5203, 0.5258,\n",
      "        0.5244, 0.5248, 0.5247, 0.5247, 0.5292, 0.5209, 0.5786])\n",
      "finalReturns:  tensor([1.0049, 1.0578, 1.0266, 0.9367, 0.8051, 0.6438, 0.4563, 0.2614])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  0  stage  16  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([23, 23, 23, 23, 23, 23, 27, 23, 23, 23, 23, 27, 22, 23, 23, 23, 22, 23,\n",
      "        23, 23, 23, 23, 23, 23,  0])\n",
      "loss=  tensor(4.1050, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.5591, 3.5591, 3.5591, 3.5591, 3.5591, 3.5591, 3.5591, 3.5591, 3.5591,\n",
      "        3.5591, 3.5591, 3.5591, 3.5591, 3.5591, 3.5591, 3.5591, 3.5591, 2.9813,\n",
      "        2.4879, 2.0498, 1.6550, 1.2904, 0.9477, 0.6211, 0.3062]) return=  131053.19988043512\n",
      "probs of actions:  tensor([0.7668, 0.7477, 0.7517, 0.7799, 0.7302, 0.7619, 0.0950, 0.7659, 0.7621,\n",
      "        0.7609, 0.7681, 0.0614, 0.1864, 0.7542, 0.6877, 0.7610, 0.1592, 0.7650,\n",
      "        0.7330, 0.7773, 0.7630, 0.7196, 0.7487, 0.8173, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4583, 0.5419, 0.5204, 0.5258, 0.5244, 0.5248, 0.5047, 0.5400, 0.5209,\n",
      "        0.5257, 0.5245, 0.5048, 0.5445, 0.5171, 0.5266, 0.5242, 0.5293, 0.5209,\n",
      "        0.5257, 0.5245, 0.5248, 0.5247, 0.5247, 0.5247, 0.5776])\n",
      "finalReturns:  tensor([1.2177, 1.2661, 1.2387, 1.1511, 1.0214, 0.8613, 0.6793, 0.4812, 0.2714])\n",
      "----------------------------------------\n",
      "iter  0  stage  15  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([23, 23, 23, 27, 27, 27, 23, 23, 27, 23, 27, 23, 23, 27, 27, 23, 27, 27,\n",
      "        27, 27, 27, 23, 23, 22,  0])\n",
      "loss=  tensor(9.6563, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.7219, 3.7219, 3.7219, 3.7219, 3.7219, 3.7219, 3.7219, 3.7219, 3.7219,\n",
      "        3.7219, 3.7219, 3.7219, 3.7219, 3.7219, 3.7219, 3.7219, 3.1328, 2.6420,\n",
      "        2.2066, 1.8260, 1.4805, 1.1610, 0.8602, 0.5731, 0.2854]) return=  130253.95806091337\n",
      "probs of actions:  tensor([0.5601, 0.5729, 0.5629, 0.4018, 0.3564, 0.3282, 0.4954, 0.5843, 0.4051,\n",
      "        0.5535, 0.4352, 0.5639, 0.5251, 0.3924, 0.5742, 0.5398, 0.3543, 0.4443,\n",
      "        0.5178, 0.4116, 0.2818, 0.5405, 0.6194, 0.1406, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4583, 0.5419, 0.5204, 0.5058, 0.5197, 0.5162, 0.5371, 0.5216, 0.5055,\n",
      "        0.5398, 0.5010, 0.5410, 0.5207, 0.5057, 0.5197, 0.5362, 0.5018, 0.5207,\n",
      "        0.5160, 0.5172, 0.5169, 0.5369, 0.5217, 0.5300, 0.5736])\n",
      "finalReturns:  tensor([1.5490, 1.6019, 1.5909, 1.5056, 1.3702, 1.1986, 1.0012, 0.7650, 0.5304,\n",
      "        0.2882])\n",
      "----------------------------------------\n",
      "iter  0  stage  14  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 23, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 22, 27, 23,  0])\n",
      "loss=  tensor(5.8675, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.9442, 3.9442, 3.9442, 3.9442, 3.9442, 3.9442, 3.9442, 3.9442, 3.9442,\n",
      "        3.9442, 3.9442, 3.9442, 3.9442, 3.9442, 3.9442, 3.3544, 2.8637, 2.4413,\n",
      "        2.0669, 1.7265, 1.4106, 1.1124, 0.8272, 0.5516, 0.2698]) return=  129554.93117643229\n",
      "probs of actions:  tensor([0.8762, 0.8405, 0.1034, 0.9194, 0.8752, 0.8740, 0.9195, 0.8534, 0.9118,\n",
      "        0.8799, 0.9213, 0.8607, 0.8698, 0.9070, 0.9539, 0.9097, 0.9026, 0.9266,\n",
      "        0.9496, 0.9206, 0.8964, 0.0236, 0.8098, 0.2168, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5318, 0.5029, 0.5205, 0.5160, 0.5171, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5414, 0.4979, 0.5417, 0.5734])\n",
      "finalReturns:  tensor([1.8287, 1.9016, 1.8754, 1.7808, 1.6383, 1.4618, 1.2608, 1.0420, 0.7858,\n",
      "        0.5636, 0.3035])\n",
      "----------------------------------------\n",
      "iter  0  stage  13  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.3084, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.1841, 4.1841, 4.1841, 4.1841, 4.1841, 4.1841, 4.1841, 4.1841, 4.1841,\n",
      "        4.1841, 4.1841, 4.1841, 4.1841, 4.1841, 3.5943, 3.1036, 2.6813, 2.3068,\n",
      "        1.9664, 1.6505, 1.3523, 1.0671, 0.7915, 0.5229, 0.2595]) return=  129338.69866666665\n",
      "probs of actions:  tensor([0.9714, 0.9610, 0.9711, 0.9835, 0.9692, 0.9713, 0.9829, 0.9650, 0.9811,\n",
      "        0.9735, 0.9837, 0.9670, 0.9692, 0.9851, 0.9930, 0.9845, 0.9812, 0.9859,\n",
      "        0.9918, 0.9852, 0.9800, 0.9791, 0.9566, 0.9344, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5166, 0.5170, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5898])\n",
      "finalReturns:  tensor([2.0919, 2.1648, 2.1385, 2.0440, 1.9015, 1.7249, 1.5239, 1.3052, 1.0735,\n",
      "        0.8322, 0.5839, 0.3303])\n",
      "----------------------------------------\n",
      "iter  0  stage  12  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.0437, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.4407, 4.4407, 4.4407, 4.4407, 4.4407, 4.4407, 4.4407, 4.4407, 4.4407,\n",
      "        4.4407, 4.4407, 4.4407, 4.4407, 3.8509, 3.3602, 2.9378, 2.5634, 2.2230,\n",
      "        1.9071, 1.6089, 1.3237, 1.0481, 0.7795, 0.5161, 0.2566]) return=  129338.69866666665\n",
      "probs of actions:  tensor([0.9954, 0.9937, 0.9958, 0.9977, 0.9947, 0.9956, 0.9974, 0.9943, 0.9972,\n",
      "        0.9960, 0.9976, 0.9947, 0.9961, 0.9984, 0.9995, 0.9984, 0.9975, 0.9986,\n",
      "        0.9997, 0.9986, 0.9983, 0.9978, 0.9940, 0.9915, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5166, 0.5170, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5898])\n",
      "finalReturns:  tensor([2.3522, 2.4251, 2.3989, 2.3043, 2.1618, 1.9853, 1.7843, 1.5655, 1.3338,\n",
      "        1.0925, 0.8442, 0.5907, 0.3332])\n",
      "----------------------------------------\n",
      "iter  0  stage  11  ep  67016   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.0133, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.6951, 4.6951, 4.6951, 4.6951, 4.6951, 4.6951, 4.6951, 4.6951, 4.6951,\n",
      "        4.6951, 4.6951, 4.6951, 4.1053, 3.6146, 3.1923, 2.8179, 2.4775, 2.1615,\n",
      "        1.8634, 1.5782, 1.3025, 1.0339, 0.7705, 0.5110, 0.2544]) return=  129338.69866666665\n",
      "probs of actions:  tensor([0.9983, 0.9978, 0.9985, 0.9992, 0.9981, 0.9985, 0.9991, 0.9980, 0.9990,\n",
      "        0.9986, 0.9991, 0.9990, 0.9989, 0.9997, 1.0000, 0.9998, 0.9992, 0.9999,\n",
      "        1.0000, 1.0000, 0.9999, 0.9993, 0.9979, 0.9973, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5166, 0.5170, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5898])\n",
      "finalReturns:  tensor([2.6147, 2.6876, 2.6614, 2.5668, 2.4243, 2.2478, 2.0468, 1.8280, 1.5963,\n",
      "        1.3550, 1.1067, 0.8532, 0.5957, 0.3354])\n",
      "----------------------------------------\n",
      "iter  0  stage  10  ep  0   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.0165, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.9480, 4.9480, 4.9480, 4.9480, 4.9480, 4.9480, 4.9480, 4.9480, 4.9480,\n",
      "        4.9480, 4.9480, 4.3581, 3.8674, 3.4451, 3.0707, 2.7303, 2.4144, 2.1162,\n",
      "        1.8310, 1.5553, 1.2867, 1.0233, 0.7638, 0.5073, 0.2528]) return=  129338.69866666665\n",
      "probs of actions:  tensor([0.9983, 0.9978, 0.9985, 0.9992, 0.9981, 0.9985, 0.9991, 0.9980, 0.9990,\n",
      "        0.9986, 0.9991, 0.9990, 0.9989, 0.9997, 1.0000, 0.9998, 0.9992, 0.9999,\n",
      "        1.0000, 1.0000, 0.9999, 0.9993, 0.9979, 0.9973, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5166, 0.5170, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5898])\n",
      "finalReturns:  tensor([2.8788, 2.9517, 2.9255, 2.8309, 2.6884, 2.5119, 2.3109, 2.0921, 1.8604,\n",
      "        1.6191, 1.3708, 1.1173, 0.8598, 0.5995, 0.3370])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  0  stage  9  ep  11308   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.0184, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.1996, 5.1996, 5.1996, 5.1996, 5.1996, 5.1996, 5.1996, 5.1996, 5.1996,\n",
      "        5.1996, 4.6098, 4.1191, 3.6967, 3.3223, 2.9819, 2.6660, 2.3678, 2.0826,\n",
      "        1.8069, 1.5383, 1.2750, 1.0155, 0.7589, 0.5044, 0.2516]) return=  129338.69866666665\n",
      "probs of actions:  tensor([0.9984, 0.9979, 0.9986, 0.9992, 0.9982, 0.9985, 0.9991, 0.9981, 0.9990,\n",
      "        0.9990, 0.9993, 0.9991, 0.9990, 0.9997, 1.0000, 0.9998, 0.9992, 1.0000,\n",
      "        1.0000, 1.0000, 0.9999, 0.9994, 0.9980, 0.9975, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5166, 0.5170, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5898])\n",
      "finalReturns:  tensor([3.1441, 3.2170, 3.1908, 3.0962, 2.9537, 2.7772, 2.5762, 2.3574, 2.1257,\n",
      "        1.8844, 1.6361, 1.3826, 1.1251, 0.8648, 0.6023, 0.3382])\n",
      "----------------------------------------\n",
      "iter  0  stage  8  ep  0   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.0226, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.4503, 5.4503, 5.4503, 5.4503, 5.4503, 5.4503, 5.4503, 5.4503, 5.4503,\n",
      "        4.8605, 4.3698, 3.9474, 3.5730, 3.2326, 2.9167, 2.6185, 2.3333, 2.0576,\n",
      "        1.7890, 1.5257, 1.2662, 1.0096, 0.7551, 0.5023, 0.2507]) return=  129338.69866666665\n",
      "probs of actions:  tensor([0.9984, 0.9979, 0.9986, 0.9992, 0.9982, 0.9985, 0.9991, 0.9981, 0.9990,\n",
      "        0.9990, 0.9993, 0.9991, 0.9990, 0.9997, 1.0000, 0.9998, 0.9992, 1.0000,\n",
      "        1.0000, 1.0000, 0.9999, 0.9994, 0.9980, 0.9975, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5166, 0.5170, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5898])\n",
      "finalReturns:  tensor([3.4103, 3.4832, 3.4570, 3.3624, 3.2199, 3.0434, 2.8424, 2.6236, 2.3919,\n",
      "        2.1506, 1.9023, 1.6488, 1.3914, 1.1310, 0.8685, 0.6044, 0.3391])\n",
      "----------------------------------------\n",
      "iter  0  stage  7  ep  23219   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.0149, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.7003, 5.7003, 5.7003, 5.7003, 5.7003, 5.7003, 5.7003, 5.7003, 5.1105,\n",
      "        4.6198, 4.1974, 3.8230, 3.4826, 3.1667, 2.8685, 2.5833, 2.3077, 2.0391,\n",
      "        1.7757, 1.5162, 1.2596, 1.0052, 0.7523, 0.5007, 0.2500]) return=  129338.69866666665\n",
      "probs of actions:  tensor([0.9988, 0.9983, 0.9990, 0.9994, 0.9986, 0.9989, 0.9993, 0.9990, 1.0000,\n",
      "        0.9993, 0.9995, 0.9994, 0.9998, 0.9998, 1.0000, 1.0000, 0.9996, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9996, 0.9986, 0.9984, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5166, 0.5170, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5898])\n",
      "finalReturns:  tensor([3.6772, 3.7501, 3.7239, 3.6293, 3.4868, 3.3103, 3.1093, 2.8905, 2.6588,\n",
      "        2.4175, 2.1692, 1.9157, 1.6582, 1.3979, 1.1354, 0.8713, 0.6060, 0.3398])\n",
      "----------------------------------------\n",
      "iter  0  stage  6  ep  0   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.0181, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.9498, 5.9498, 5.9498, 5.9498, 5.9498, 5.9498, 5.9498, 5.3600, 4.8693,\n",
      "        4.4470, 4.0725, 3.7322, 3.4162, 3.1181, 2.8329, 2.5572, 2.2886, 2.0252,\n",
      "        1.7657, 1.5091, 1.2547, 1.0019, 0.7503, 0.4996, 0.2495]) return=  129338.69866666665\n",
      "probs of actions:  tensor([0.9988, 0.9983, 0.9990, 0.9994, 0.9986, 0.9989, 0.9993, 0.9990, 1.0000,\n",
      "        0.9993, 0.9995, 0.9994, 0.9998, 0.9998, 1.0000, 1.0000, 0.9996, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9996, 0.9986, 0.9984, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5166, 0.5170, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5898])\n",
      "finalReturns:  tensor([3.9446, 4.0175, 3.9913, 3.8967, 3.7542, 3.5777, 3.3767, 3.1579, 2.9262,\n",
      "        2.6849, 2.4366, 2.1831, 1.9257, 1.6653, 1.4028, 1.1387, 0.8734, 0.6072,\n",
      "        0.3403])\n",
      "----------------------------------------\n",
      "iter  0  stage  5  ep  63   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.0223, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.1990, 6.1990, 6.1990, 6.1990, 6.1990, 6.1990, 5.6091, 5.1185, 4.6961,\n",
      "        4.3217, 3.9813, 3.6654, 3.3672, 3.0820, 2.8063, 2.5377, 2.2744, 2.0149,\n",
      "        1.7583, 1.5038, 1.2510, 0.9994, 0.7487, 0.4987, 0.2491]) return=  129338.69866666665\n",
      "probs of actions:  tensor([0.9988, 0.9984, 0.9990, 0.9994, 0.9986, 0.9990, 0.9994, 0.9990, 1.0000,\n",
      "        0.9994, 0.9995, 0.9995, 0.9998, 0.9998, 1.0000, 1.0000, 0.9996, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9996, 0.9986, 0.9985, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5166, 0.5170, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5898])\n",
      "finalReturns:  tensor([4.2124, 4.2853, 4.2591, 4.1645, 4.0220, 3.8455, 3.6445, 3.4257, 3.1940,\n",
      "        2.9527, 2.7044, 2.4509, 2.1934, 1.9331, 1.6706, 1.4065, 1.1412, 0.8750,\n",
      "        0.6081, 0.3407])\n",
      "----------------------------------------\n",
      "iter  0  stage  4  ep  250   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.0241, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.4476, 6.4476, 6.4476, 6.4476, 6.4476, 5.8581, 5.3673, 4.9450, 4.5705,\n",
      "        4.2302, 3.9142, 3.6161, 3.3309, 3.0552, 2.7866, 2.5232, 2.2637, 2.0071,\n",
      "        1.7527, 1.4999, 1.2483, 0.9976, 0.7475, 0.4980, 0.2489]) return=  129338.69866666665\n",
      "probs of actions:  tensor([0.9989, 0.9985, 0.9991, 0.9995, 0.9990, 0.9993, 0.9996, 0.9991, 1.0000,\n",
      "        0.9994, 0.9995, 0.9995, 0.9998, 0.9998, 1.0000, 1.0000, 0.9997, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9997, 0.9987, 0.9986, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5166, 0.5170, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5898])\n",
      "finalReturns:  tensor([4.4805, 4.5534, 4.5271, 4.4326, 4.2901, 4.1135, 3.9125, 3.6938, 3.4621,\n",
      "        3.2208, 2.9725, 2.7189, 2.4615, 2.2012, 1.9387, 1.6746, 1.4093, 1.1430,\n",
      "        0.8761, 0.6087, 0.3410])\n",
      "----------------------------------------\n",
      "iter  0  stage  3  ep  0   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.0272, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.6975, 6.6975, 6.6975, 6.6975, 6.1064, 5.6160, 5.1936, 4.8192, 4.4788,\n",
      "        4.1629, 3.8647, 3.5795, 3.3038, 3.0352, 2.7719, 2.5124, 2.2558, 2.0013,\n",
      "        1.7485, 1.4969, 1.2462, 0.9962, 0.7466, 0.4975, 0.2486]) return=  129338.69866666665\n",
      "probs of actions:  tensor([0.9989, 0.9985, 0.9991, 0.9995, 0.9990, 0.9993, 0.9996, 0.9991, 1.0000,\n",
      "        0.9994, 0.9995, 0.9995, 0.9998, 0.9998, 1.0000, 1.0000, 0.9997, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9997, 0.9987, 0.9986, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5166, 0.5170, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5898])\n",
      "finalReturns:  tensor([4.7487, 4.8216, 4.7954, 4.7008, 4.5583, 4.3818, 4.1808, 3.9621, 3.7303,\n",
      "        3.4891, 3.2408, 2.9872, 2.7298, 2.4694, 2.2070, 1.9429, 1.6775, 1.4113,\n",
      "        1.1444, 0.8770, 0.6092, 0.3412])\n",
      "----------------------------------------\n",
      "iter  0  stage  2  ep  0   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.0326, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.9408, 6.9408, 6.9408, 6.3561, 5.8642, 5.4422, 5.0677, 4.7273, 4.4114,\n",
      "        4.1132, 3.8280, 3.5523, 3.2837, 3.0203, 2.7608, 2.5042, 2.2498, 1.9970,\n",
      "        1.7454, 1.4947, 1.2447, 0.9951, 0.7460, 0.4971, 0.2485]) return=  129338.69866666665\n",
      "probs of actions:  tensor([0.9989, 0.9985, 0.9991, 0.9995, 0.9990, 0.9993, 0.9996, 0.9991, 1.0000,\n",
      "        0.9994, 0.9995, 0.9995, 0.9998, 0.9998, 1.0000, 1.0000, 0.9997, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9997, 0.9987, 0.9986, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5166, 0.5170, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5898])\n",
      "finalReturns:  tensor([5.0173, 5.0902, 5.0638, 4.9693, 4.8268, 4.6503, 4.4493, 4.2305, 3.9988,\n",
      "        3.7575, 3.5092, 3.2557, 2.9982, 2.7379, 2.4754, 2.2113, 1.9460, 1.6798,\n",
      "        1.4129, 1.1455, 0.8777, 0.6096, 0.3413])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  0  stage  1  ep  594   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.0351, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.2101, 7.2101, 6.5997, 6.1137, 5.6903, 5.3161, 4.9756, 4.6597, 4.3615,\n",
      "        4.0763, 3.8007, 3.5321, 3.2687, 3.0092, 2.7526, 2.4982, 2.2454, 1.9938,\n",
      "        1.7430, 1.4930, 1.2435, 0.9943, 0.7455, 0.4968, 0.2484]) return=  129338.69866666665\n",
      "probs of actions:  tensor([0.9989, 0.9990, 0.9994, 0.9997, 0.9990, 0.9994, 0.9996, 0.9991, 1.0000,\n",
      "        0.9994, 0.9995, 0.9995, 0.9998, 0.9998, 1.0000, 1.0000, 0.9997, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9997, 0.9987, 0.9986, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5166, 0.5170, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5898])\n",
      "finalReturns:  tensor([5.2854, 5.3583, 5.3326, 5.2378, 5.0954, 4.9188, 4.7178, 4.4991, 4.2673,\n",
      "        4.0261, 3.7778, 3.5242, 3.2668, 3.0064, 2.7440, 2.4799, 2.2145, 1.9483,\n",
      "        1.6814, 1.4140, 1.1462, 0.8782, 0.6099, 0.3415])\n",
      "----------------------------------------\n",
      "iter  0  stage  0  ep  69   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.0403, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.3783, 6.8670, 6.3576, 5.9396, 5.5641, 5.2240, 4.9080, 4.6098, 4.3246,\n",
      "        4.0490, 3.7804, 3.5170, 3.2575, 3.0009, 2.7465, 2.4936, 2.2420, 1.9913,\n",
      "        1.7413, 1.4918, 1.2426, 0.9938, 0.7451, 0.4966, 0.2483]) return=  129338.69866666665\n",
      "probs of actions:  tensor([0.9990, 0.9990, 0.9994, 0.9997, 0.9991, 0.9994, 0.9997, 0.9991, 1.0000,\n",
      "        0.9994, 0.9995, 0.9995, 0.9998, 0.9998, 1.0000, 1.0000, 0.9997, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9997, 0.9987, 0.9986, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5166, 0.5170, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5898])\n",
      "finalReturns:  tensor([5.5556, 5.6285, 5.6005, 5.5067, 5.3639, 5.1875, 4.9865, 4.7677, 4.5360,\n",
      "        4.2947, 4.0464, 3.7929, 3.5354, 3.2751, 3.0126, 2.7485, 2.4832, 2.2170,\n",
      "        1.9501, 1.6827, 1.4149, 1.1468, 0.8786, 0.6101, 0.3416])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1682835632 saved\n",
      "[1662531, 'tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])', 129338.69866666665, 75332.17066666666, 0.04029531031847, 1e-05, 1, 0, 'tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\\n        27, 27, 27, 27, 27, 27,  0])', '[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\\n 1.]', '0,[1e-05,1][1, 10000, 1, 1],1682835632', 25, 50, 172677.5835126837, 219861.0138117109, 91603.02777699233, 132330.31466666664, 129338.69866666666, 72150.91909080048, 72150.91909080048, 88030.91902269641, 88030.91902269641, 105848.51071772553, 72121.02259730823, 87961.7660416823]\n",
      "policy reset\n",
      "----------------------------------------\n",
      "iter  1  stage  24  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 4, 0, 0,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5107, 0.5107, 0.5107, 0.5107, 0.5107, 0.5107, 0.5107, 0.5107, 0.5107,\n",
      "        0.5107, 0.5107, 0.5107, 0.5107, 0.5107, 0.5107, 0.5107, 0.5107, 0.5107,\n",
      "        0.5107, 0.5107, 0.5107, 0.5107, 0.5107, 0.5107, 0.5107]) return=  127695.93011642047\n",
      "probs of actions:  tensor([0.0087, 0.9502, 0.0086, 0.9312, 0.9305, 0.9451, 0.9223, 0.9450, 0.9302,\n",
      "        0.9339, 0.9603, 0.9469, 0.9478, 0.9461, 0.9511, 0.9430, 0.0276, 0.9222,\n",
      "        0.9427, 0.9465, 0.9359, 0.0034, 0.9471, 0.9232, 0.9943],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5108, 0.5166, 0.5077, 0.5174, 0.5079, 0.5103, 0.5097, 0.5098, 0.5098,\n",
      "        0.5098, 0.5098, 0.5098, 0.5098, 0.5098, 0.5098, 0.5098, 0.5097, 0.5134,\n",
      "        0.5089, 0.5100, 0.5097, 0.5082, 0.5242, 0.5062, 0.5107])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  1  stage  23  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18,  0, 18,  6,  0,  0,  1,  0,  0,  0,  0, 18,  0,  0,  0, 18,\n",
      "        18,  0,  0, 18,  0, 18,  0])\n",
      "loss=  tensor(0.0075, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0078, 1.0078, 1.0078, 1.0078, 1.0078, 1.0078, 1.0078, 1.0078, 1.0078,\n",
      "        1.0078, 1.0078, 1.0078, 1.0078, 1.0078, 1.0078, 1.0078, 1.0078, 1.0078,\n",
      "        1.0078, 1.0078, 1.0078, 1.0078, 1.0078, 1.0078, 0.5138]) return=  129636.67739657205\n",
      "probs of actions:  tensor([0.3905, 0.2322, 0.2606, 0.3254, 0.3457, 0.0249, 0.3994, 0.4527, 0.0195,\n",
      "        0.5938, 0.7716, 0.4525, 0.6561, 0.3951, 0.5880, 0.6364, 0.4128, 0.3614,\n",
      "        0.4197, 0.4487, 0.4621, 0.3929, 0.5632, 0.8044, 0.9996],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5633, 0.4644, 0.5760, 0.5143, 0.5087, 0.5100,\n",
      "        0.5133, 0.5089, 0.5100, 0.5097, 0.4774, 0.5761, 0.4939, 0.5138, 0.4764,\n",
      "        0.5439, 0.5591, 0.4978, 0.4804, 0.5753, 0.4616, 0.5803])\n",
      "finalReturns:  tensor([0.0341, 0.0665])\n",
      "----------------------------------------\n",
      "iter  1  stage  22  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        16, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0044, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.5102, 1.5102, 1.5102, 1.5102, 1.5102, 1.5102, 1.5102, 1.5102, 1.5102,\n",
      "        1.5102, 1.5102, 1.5102, 1.5102, 1.5102, 1.5102, 1.5102, 1.5102, 1.5102,\n",
      "        1.5102, 1.5102, 1.5102, 1.5102, 1.5102, 0.9476, 0.4506]) return=  132450.34798175492\n",
      "probs of actions:  tensor([0.9696, 0.9712, 0.9630, 0.9755, 0.9730, 0.9697, 0.9775, 0.9711, 0.9708,\n",
      "        0.9457, 0.9814, 0.9735, 0.9778, 0.9767, 0.9721, 0.9678, 0.9664, 0.9676,\n",
      "        0.0071, 0.9770, 0.9657, 0.9756, 0.9838, 0.9828, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5369, 0.5226, 0.5320, 0.5296, 0.5302, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([0.1126, 0.1450, 0.1119])\n",
      "----------------------------------------\n",
      "iter  1  stage  21  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0026, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.9273, 1.9273, 1.9273, 1.9273, 1.9273, 1.9273, 1.9273, 1.9273, 1.9273,\n",
      "        1.9273, 1.9273, 1.9273, 1.9273, 1.9273, 1.9273, 1.9273, 1.9273, 1.9273,\n",
      "        1.9273, 1.9273, 1.9273, 1.9273, 1.3648, 0.8678, 0.4172]) return=  132442.06666666665\n",
      "probs of actions:  tensor([0.9932, 0.9936, 0.9917, 0.9944, 0.9938, 0.9933, 0.9947, 0.9937, 0.9934,\n",
      "        0.9874, 0.9960, 0.9942, 0.9954, 0.9948, 0.9939, 0.9928, 0.9918, 0.9923,\n",
      "        0.9933, 0.9947, 0.9919, 0.9962, 0.9969, 0.9958, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([0.2255, 0.2579, 0.2248, 0.1453])\n",
      "----------------------------------------\n",
      "iter  1  stage  20  ep  81988   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0010, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.3204, 2.3204, 2.3204, 2.3204, 2.3204, 2.3204, 2.3204, 2.3204, 2.3204,\n",
      "        2.3204, 2.3204, 2.3204, 2.3204, 2.3204, 2.3204, 2.3204, 2.3204, 2.3204,\n",
      "        2.3204, 2.3204, 2.3204, 1.7579, 1.2609, 0.8103, 0.3931]) return=  132442.06666666665\n",
      "probs of actions:  tensor([0.9987, 0.9987, 0.9984, 0.9988, 0.9987, 0.9986, 0.9989, 0.9988, 0.9986,\n",
      "        0.9973, 0.9993, 0.9988, 0.9991, 0.9989, 0.9988, 0.9986, 0.9981, 0.9983,\n",
      "        0.9986, 0.9989, 0.9990, 0.9992, 0.9999, 0.9990, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([0.3625, 0.3949, 0.3618, 0.2823, 0.1694])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  19  ep  56   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0018, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.6958, 2.6958, 2.6958, 2.6958, 2.6958, 2.6958, 2.6958, 2.6958, 2.6958,\n",
      "        2.6958, 2.6958, 2.6958, 2.6958, 2.6958, 2.6958, 2.6958, 2.6958, 2.6958,\n",
      "        2.6958, 2.6958, 2.1333, 1.6363, 1.1857, 0.7685, 0.3754]) return=  132442.06666666665\n",
      "probs of actions:  tensor([0.9986, 0.9987, 0.9983, 0.9988, 0.9986, 0.9985, 0.9988, 0.9987, 0.9986,\n",
      "        0.9973, 0.9992, 0.9988, 0.9991, 0.9989, 0.9988, 0.9986, 0.9981, 0.9983,\n",
      "        0.9985, 0.9990, 0.9990, 0.9992, 0.9999, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([0.5172, 0.5496, 0.5165, 0.4370, 0.3241, 0.1871])\n",
      "----------------------------------------\n",
      "iter  1  stage  18  ep  344   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0027, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.0583, 3.0583, 3.0583, 3.0583, 3.0583, 3.0583, 3.0583, 3.0583, 3.0583,\n",
      "        3.0583, 3.0583, 3.0583, 3.0583, 3.0583, 3.0583, 3.0583, 3.0583, 3.0583,\n",
      "        3.0583, 2.4958, 1.9987, 1.5482, 1.1309, 0.7379, 0.3624]) return=  132442.06666666665\n",
      "probs of actions:  tensor([0.9987, 0.9987, 0.9984, 0.9988, 0.9987, 0.9986, 0.9989, 0.9988, 0.9986,\n",
      "        0.9974, 0.9993, 0.9988, 0.9992, 0.9989, 0.9988, 0.9987, 0.9982, 0.9984,\n",
      "        0.9990, 0.9993, 0.9990, 0.9993, 0.9999, 0.9990, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([0.6848, 0.7172, 0.6842, 0.6046, 0.4918, 0.3547, 0.2001])\n",
      "----------------------------------------\n",
      "iter  1  stage  17  ep  653   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0039, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.4111, 3.4111, 3.4111, 3.4111, 3.4111, 3.4111, 3.4111, 3.4111, 3.4111,\n",
      "        3.4111, 3.4111, 3.4111, 3.4111, 3.4111, 3.4111, 3.4111, 3.4111, 3.4111,\n",
      "        2.8486, 2.3516, 1.9010, 1.4838, 1.0907, 0.7153, 0.3529]) return=  132442.06666666665\n",
      "probs of actions:  tensor([0.9987, 0.9988, 0.9985, 0.9988, 0.9987, 0.9986, 0.9989, 0.9988, 0.9986,\n",
      "        0.9974, 0.9993, 0.9988, 0.9992, 0.9989, 0.9988, 0.9987, 0.9982, 0.9990,\n",
      "        0.9991, 0.9994, 0.9990, 0.9993, 0.9999, 0.9990, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([0.8621, 0.8945, 0.8614, 0.7819, 0.6690, 0.5320, 0.3773, 0.2096])\n",
      "----------------------------------------\n",
      "iter  1  stage  16  ep  23347   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0031, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.7569, 3.7569, 3.7569, 3.7569, 3.7569, 3.7569, 3.7569, 3.7569, 3.7569,\n",
      "        3.7569, 3.7569, 3.7569, 3.7569, 3.7569, 3.7569, 3.7569, 3.7569, 3.1944,\n",
      "        2.6974, 2.2468, 1.8296, 1.4365, 1.0611, 0.6986, 0.3458]) return=  132442.06666666665\n",
      "probs of actions:  tensor([0.9992, 0.9993, 0.9991, 0.9993, 0.9992, 0.9991, 0.9993, 0.9993, 0.9992,\n",
      "        0.9985, 0.9996, 0.9993, 0.9995, 0.9994, 0.9993, 0.9992, 0.9990, 0.9996,\n",
      "        0.9995, 0.9997, 0.9995, 0.9998, 1.0000, 0.9996, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([1.0464, 1.0788, 1.0457, 0.9662, 0.8533, 0.7163, 0.5616, 0.3940, 0.2167])\n",
      "----------------------------------------\n",
      "iter  1  stage  15  ep  0   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0044, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.0974, 4.0974, 4.0974, 4.0974, 4.0974, 4.0974, 4.0974, 4.0974, 4.0974,\n",
      "        4.0974, 4.0974, 4.0974, 4.0974, 4.0974, 4.0974, 4.0974, 3.5349, 3.0379,\n",
      "        2.5873, 2.1701, 1.7770, 1.4016, 1.0391, 0.6863, 0.3405]) return=  132442.06666666665\n",
      "probs of actions:  tensor([0.9992, 0.9993, 0.9991, 0.9993, 0.9992, 0.9991, 0.9993, 0.9993, 0.9992,\n",
      "        0.9985, 0.9996, 0.9993, 0.9995, 0.9994, 0.9993, 0.9992, 0.9990, 0.9996,\n",
      "        0.9995, 0.9997, 0.9995, 0.9998, 1.0000, 0.9996, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([1.2360, 1.2684, 1.2353, 1.1558, 1.0429, 0.9059, 0.7512, 0.5836, 0.4063,\n",
      "        0.2220])\n",
      "----------------------------------------\n",
      "iter  1  stage  14  ep  0   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0059, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.4340, 4.4340, 4.4340, 4.4340, 4.4340, 4.4340, 4.4340, 4.4340, 4.4340,\n",
      "        4.4340, 4.4340, 4.4340, 4.4340, 4.4340, 4.4340, 3.8715, 3.3744, 2.9239,\n",
      "        2.5066, 2.1136, 1.7381, 1.3757, 1.0228, 0.6770, 0.3366]) return=  132442.06666666665\n",
      "probs of actions:  tensor([0.9992, 0.9993, 0.9991, 0.9993, 0.9992, 0.9991, 0.9993, 0.9993, 0.9992,\n",
      "        0.9985, 0.9996, 0.9993, 0.9995, 0.9994, 0.9993, 0.9992, 0.9990, 0.9996,\n",
      "        0.9995, 0.9997, 0.9995, 0.9998, 1.0000, 0.9996, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([1.4295, 1.4619, 1.4289, 1.3493, 1.2365, 1.0994, 0.9448, 0.7771, 0.5999,\n",
      "        0.4156, 0.2259])\n",
      "----------------------------------------\n",
      "iter  1  stage  13  ep  0   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0074, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.7676, 4.7676, 4.7676, 4.7676, 4.7676, 4.7676, 4.7676, 4.7676, 4.7676,\n",
      "        4.7676, 4.7676, 4.7676, 4.7676, 4.7676, 4.2051, 3.7081, 3.2575, 2.8402,\n",
      "        2.4472, 2.0718, 1.7093, 1.3564, 1.0107, 0.6702, 0.3336]) return=  132442.06666666665\n",
      "probs of actions:  tensor([0.9992, 0.9993, 0.9991, 0.9993, 0.9992, 0.9991, 0.9993, 0.9993, 0.9992,\n",
      "        0.9985, 0.9996, 0.9993, 0.9995, 0.9994, 0.9993, 0.9992, 0.9990, 0.9996,\n",
      "        0.9995, 0.9997, 0.9995, 0.9998, 1.0000, 0.9996, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([1.6260, 1.6584, 1.6253, 1.5458, 1.4330, 1.2959, 1.1412, 0.9736, 0.7964,\n",
      "        0.6120, 0.4224, 0.2289])\n",
      "----------------------------------------\n",
      "iter  1  stage  12  ep  0   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0088, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.0990, 5.0990, 5.0990, 5.0990, 5.0990, 5.0990, 5.0990, 5.0990, 5.0990,\n",
      "        5.0990, 5.0990, 5.0990, 5.0990, 4.5365, 4.0395, 3.5889, 3.1717, 2.7786,\n",
      "        2.4032, 2.0407, 1.6879, 1.3421, 1.0016, 0.6651, 0.3314]) return=  132442.06666666665\n",
      "probs of actions:  tensor([0.9992, 0.9993, 0.9991, 0.9993, 0.9992, 0.9991, 0.9993, 0.9993, 0.9992,\n",
      "        0.9985, 0.9996, 0.9993, 0.9995, 0.9994, 0.9993, 0.9992, 0.9990, 0.9996,\n",
      "        0.9995, 0.9997, 0.9995, 0.9998, 1.0000, 0.9996, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([1.8247, 1.8571, 1.8240, 1.7445, 1.6316, 1.4946, 1.3399, 1.1723, 0.9950,\n",
      "        0.8107, 0.6211, 0.4275, 0.2311])\n",
      "----------------------------------------\n",
      "iter  1  stage  11  ep  0   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0107, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.4288, 5.4288, 5.4288, 5.4288, 5.4288, 5.4288, 5.4288, 5.4288, 5.4288,\n",
      "        5.4288, 5.4288, 5.4288, 4.8663, 4.3693, 3.9187, 3.5015, 3.1084, 2.7330,\n",
      "        2.3705, 2.0177, 1.6719, 1.3314, 0.9949, 0.6612, 0.3298]) return=  132442.06666666665\n",
      "probs of actions:  tensor([0.9992, 0.9993, 0.9991, 0.9993, 0.9992, 0.9991, 0.9993, 0.9993, 0.9992,\n",
      "        0.9985, 0.9996, 0.9993, 0.9995, 0.9994, 0.9993, 0.9992, 0.9990, 0.9996,\n",
      "        0.9995, 0.9997, 0.9995, 0.9998, 1.0000, 0.9996, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([2.0250, 2.0574, 2.0243, 1.9448, 1.8319, 1.6949, 1.5402, 1.3726, 1.1953,\n",
      "        1.0110, 0.8214, 0.6278, 0.4314, 0.2327])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  10  ep  0   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0121, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.7574, 5.7574, 5.7574, 5.7574, 5.7574, 5.7574, 5.7574, 5.7574, 5.7574,\n",
      "        5.7574, 5.7574, 5.1949, 4.6979, 4.2473, 3.8300, 3.4370, 3.0615, 2.6991,\n",
      "        2.3462, 2.0005, 1.6600, 1.3234, 0.9898, 0.6584, 0.3286]) return=  132442.06666666665\n",
      "probs of actions:  tensor([0.9992, 0.9993, 0.9991, 0.9993, 0.9992, 0.9991, 0.9993, 0.9993, 0.9992,\n",
      "        0.9985, 0.9996, 0.9993, 0.9995, 0.9994, 0.9993, 0.9992, 0.9990, 0.9996,\n",
      "        0.9995, 0.9997, 0.9995, 0.9998, 1.0000, 0.9996, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([2.2265, 2.2589, 2.2258, 2.1463, 2.0335, 1.8964, 1.7418, 1.5741, 1.3969,\n",
      "        1.2125, 1.0229, 0.8294, 0.6329, 0.4342, 0.2339])\n",
      "----------------------------------------\n",
      "iter  1  stage  9  ep  207   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0116, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.0850, 6.0850, 6.0850, 6.0850, 6.0850, 6.0850, 6.0850, 6.0850, 6.0850,\n",
      "        6.0850, 5.5225, 5.0255, 4.5749, 4.1577, 3.7646, 3.3892, 3.0267, 2.6739,\n",
      "        2.3281, 1.9876, 1.6511, 1.3174, 0.9860, 0.6562, 0.3276]) return=  132442.06666666665\n",
      "probs of actions:  tensor([0.9993, 0.9994, 0.9992, 0.9994, 0.9993, 0.9993, 0.9994, 0.9994, 0.9993,\n",
      "        0.9990, 0.9997, 0.9995, 0.9997, 0.9996, 0.9995, 0.9995, 0.9991, 0.9996,\n",
      "        0.9996, 0.9997, 0.9996, 0.9998, 1.0000, 0.9997, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([2.4290, 2.4614, 2.4283, 2.3488, 2.2359, 2.0989, 1.9442, 1.7766, 1.5993,\n",
      "        1.4150, 1.2254, 1.0318, 0.8354, 0.6367, 0.4364, 0.2349])\n",
      "----------------------------------------\n",
      "iter  1  stage  8  ep  0   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0139, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.4120, 6.4120, 6.4120, 6.4120, 6.4120, 6.4120, 6.4120, 6.4120, 6.4120,\n",
      "        5.8495, 5.3525, 4.9019, 4.4846, 4.0916, 3.7162, 3.3537, 3.0008, 2.6551,\n",
      "        2.3146, 1.9780, 1.6444, 1.3130, 0.9832, 0.6546, 0.3270]) return=  132442.06666666665\n",
      "probs of actions:  tensor([0.9993, 0.9994, 0.9992, 0.9994, 0.9993, 0.9993, 0.9994, 0.9994, 0.9993,\n",
      "        0.9990, 0.9997, 0.9995, 0.9997, 0.9996, 0.9995, 0.9995, 0.9991, 0.9996,\n",
      "        0.9996, 0.9997, 0.9996, 0.9998, 1.0000, 0.9997, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([2.6321, 2.6645, 2.6314, 2.5519, 2.4391, 2.3020, 2.1473, 1.9797, 1.8025,\n",
      "        1.6181, 1.4285, 1.2350, 1.0385, 0.8398, 0.6395, 0.4380, 0.2355])\n",
      "----------------------------------------\n",
      "iter  1  stage  7  ep  0   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0161, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.7384, 6.7384, 6.7384, 6.7384, 6.7384, 6.7384, 6.7384, 6.7384, 6.1759,\n",
      "        5.6789, 5.2283, 4.8111, 4.4180, 4.0426, 3.6802, 3.3273, 2.9815, 2.6410,\n",
      "        2.3045, 1.9708, 1.6394, 1.3096, 0.9811, 0.6534, 0.3264]) return=  132442.06666666665\n",
      "probs of actions:  tensor([0.9993, 0.9994, 0.9992, 0.9994, 0.9993, 0.9993, 0.9994, 0.9994, 0.9993,\n",
      "        0.9990, 0.9997, 0.9995, 0.9997, 0.9996, 0.9995, 0.9995, 0.9991, 0.9996,\n",
      "        0.9996, 0.9997, 0.9996, 0.9998, 1.0000, 0.9997, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([2.8358, 2.8682, 2.8351, 2.7556, 2.6427, 2.5057, 2.3510, 2.1833, 2.0061,\n",
      "        1.8218, 1.6322, 1.4386, 1.2422, 1.0435, 0.8432, 0.6416, 0.4392, 0.2361])\n",
      "----------------------------------------\n",
      "iter  1  stage  6  ep  0   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0183, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.0645, 7.0645, 7.0645, 7.0645, 7.0645, 7.0645, 7.0645, 6.5020, 6.0050,\n",
      "        5.5544, 5.1371, 4.7441, 4.3687, 4.0062, 3.6533, 3.3076, 2.9671, 2.6305,\n",
      "        2.2969, 1.9655, 1.6357, 1.3071, 0.9795, 0.6525, 0.3261]) return=  132442.06666666665\n",
      "probs of actions:  tensor([0.9993, 0.9994, 0.9992, 0.9994, 0.9993, 0.9993, 0.9994, 0.9994, 0.9993,\n",
      "        0.9990, 0.9997, 0.9995, 0.9997, 0.9996, 0.9995, 0.9995, 0.9991, 0.9996,\n",
      "        0.9996, 0.9997, 0.9996, 0.9998, 1.0000, 0.9997, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([3.0398, 3.0722, 3.0391, 2.9596, 2.8467, 2.7097, 2.5550, 2.3874, 2.2102,\n",
      "        2.0258, 1.8362, 1.6427, 1.4462, 1.2475, 1.0472, 0.8457, 0.6432, 0.4401,\n",
      "        0.2364])\n",
      "----------------------------------------\n",
      "iter  1  stage  5  ep  0   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0212, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.3903, 7.3903, 7.3903, 7.3903, 7.3903, 7.3903, 6.8277, 6.3307, 5.8802,\n",
      "        5.4629, 5.0698, 4.6944, 4.3320, 3.9791, 3.6333, 3.2929, 2.9563, 2.6227,\n",
      "        2.2912, 1.9614, 1.6329, 1.3052, 0.9783, 0.6518, 0.3258]) return=  132442.06666666665\n",
      "probs of actions:  tensor([0.9993, 0.9994, 0.9992, 0.9994, 0.9993, 0.9993, 0.9994, 0.9994, 0.9993,\n",
      "        0.9990, 0.9997, 0.9995, 0.9997, 0.9996, 0.9995, 0.9995, 0.9991, 0.9996,\n",
      "        0.9996, 0.9997, 0.9996, 0.9998, 1.0000, 0.9997, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([3.2441, 3.2765, 3.2435, 3.1639, 3.0511, 2.9141, 2.7594, 2.5917, 2.4145,\n",
      "        2.2302, 2.0405, 1.8470, 1.6505, 1.4519, 1.2516, 1.0500, 0.8476, 0.6444,\n",
      "        0.4408, 0.2367])\n",
      "----------------------------------------\n",
      "iter  1  stage  4  ep  0   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0241, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.7156, 7.7156, 7.7156, 7.7156, 7.7156, 7.1533, 6.6563, 6.2057, 5.7885,\n",
      "        5.3954, 5.0200, 4.6575, 4.3047, 3.9589, 3.6184, 3.2818, 2.9482, 2.6168,\n",
      "        2.2870, 1.9584, 1.6308, 1.3038, 0.9774, 0.6513, 0.3256]) return=  132442.06666666665\n",
      "probs of actions:  tensor([0.9993, 0.9994, 0.9992, 0.9994, 0.9993, 0.9993, 0.9994, 0.9994, 0.9993,\n",
      "        0.9990, 0.9997, 0.9995, 0.9997, 0.9996, 0.9995, 0.9995, 0.9991, 0.9996,\n",
      "        0.9996, 0.9997, 0.9996, 0.9998, 1.0000, 0.9997, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([3.4487, 3.4811, 3.4480, 3.3685, 3.2556, 3.1186, 2.9639, 2.7963, 2.6190,\n",
      "        2.4347, 2.2451, 2.0516, 1.8551, 1.6564, 1.4561, 1.2546, 1.0521, 0.8490,\n",
      "        0.6453, 0.4413, 0.2369])\n",
      "----------------------------------------\n",
      "iter  1  stage  3  ep  0   adversary:  AdversaryModes.imitation_128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0268, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.0419, 8.0419, 8.0419, 8.0419, 7.4785, 6.9817, 6.5311, 6.1139, 5.7208,\n",
      "        5.3454, 4.9829, 4.6301, 4.2843, 3.9438, 3.6072, 3.2736, 2.9422, 2.6124,\n",
      "        2.2838, 1.9562, 1.6292, 1.3028, 0.9767, 0.6509, 0.3254]) return=  132442.06666666665\n",
      "probs of actions:  tensor([0.9993, 0.9994, 0.9992, 0.9994, 0.9993, 0.9993, 0.9994, 0.9994, 0.9993,\n",
      "        0.9990, 0.9997, 0.9995, 0.9997, 0.9996, 0.9995, 0.9995, 0.9991, 0.9996,\n",
      "        0.9996, 0.9997, 0.9996, 0.9998, 1.0000, 0.9997, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([3.6534, 3.6858, 3.6527, 3.5732, 3.4603, 3.3233, 3.1686, 3.0010, 2.8237,\n",
      "        2.6394, 2.4498, 2.2563, 2.0598, 1.8611, 1.6608, 1.4593, 1.2568, 1.0537,\n",
      "        0.8500, 0.6460, 0.4417, 0.2371])\n",
      "----------------------------------------\n",
      "iter  1  stage  2  ep  0   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0304, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.3638, 8.3638, 8.3638, 7.8046, 7.3068, 6.8564, 6.4391, 6.0461, 5.6706,\n",
      "        5.3082, 4.9553, 4.6095, 4.2691, 3.9325, 3.5989, 3.2674, 2.9377, 2.6091,\n",
      "        2.2814, 1.9545, 1.6280, 1.3020, 0.9762, 0.6507, 0.3253]) return=  132442.06666666665\n",
      "probs of actions:  tensor([0.9993, 0.9994, 0.9992, 0.9994, 0.9993, 0.9993, 0.9994, 0.9994, 0.9993,\n",
      "        0.9990, 0.9997, 0.9995, 0.9997, 0.9996, 0.9995, 0.9995, 0.9991, 0.9996,\n",
      "        0.9996, 0.9997, 0.9996, 0.9998, 1.0000, 0.9997, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([3.8583, 3.8907, 3.8575, 3.7780, 3.6652, 3.5281, 3.3735, 3.2058, 3.0286,\n",
      "        2.8442, 2.6546, 2.4611, 2.2646, 2.0660, 1.8656, 1.6641, 1.4617, 1.2585,\n",
      "        1.0549, 0.8508, 0.6465, 0.4419, 0.2372])\n",
      "----------------------------------------\n",
      "iter  1  stage  1  ep  0   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0335, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.7023, 8.7023, 8.1266, 7.6327, 7.1814, 6.7643, 6.3712, 5.9958, 5.6334,\n",
      "        5.2805, 4.9347, 4.5942, 4.2577, 3.9241, 3.5926, 3.2628, 2.9343, 2.6066,\n",
      "        2.2797, 1.9532, 1.6271, 1.3014, 0.9758, 0.6504, 0.3252]) return=  132442.06666666665\n",
      "probs of actions:  tensor([0.9993, 0.9994, 0.9992, 0.9994, 0.9993, 0.9993, 0.9994, 0.9994, 0.9993,\n",
      "        0.9990, 0.9997, 0.9995, 0.9997, 0.9996, 0.9995, 0.9995, 0.9991, 0.9996,\n",
      "        0.9996, 0.9997, 0.9996, 0.9998, 1.0000, 0.9997, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([4.0630, 4.0954, 4.0626, 3.9829, 3.8701, 3.7331, 3.5784, 3.4107, 3.2335,\n",
      "        3.0492, 2.8596, 2.6660, 2.4695, 2.2709, 2.0706, 1.8690, 1.6666, 1.4634,\n",
      "        1.2598, 1.0558, 0.8514, 0.6469, 0.4422, 0.2373])\n",
      "----------------------------------------\n",
      "iter  1  stage  0  ep  0   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0369, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.9755, 8.4643, 7.9549, 7.5072, 7.0893, 6.6964, 6.3209, 5.9585, 5.6056,\n",
      "        5.2598, 4.9193, 4.5828, 4.2492, 3.9177, 3.5879, 3.2594, 2.9317, 2.6048,\n",
      "        2.2783, 1.9523, 1.6265, 1.3009, 0.9755, 0.6503, 0.3251]) return=  132442.06666666665\n",
      "probs of actions:  tensor([0.9993, 0.9994, 0.9992, 0.9994, 0.9993, 0.9993, 0.9994, 0.9994, 0.9993,\n",
      "        0.9990, 0.9997, 0.9996, 0.9997, 0.9996, 0.9996, 0.9995, 0.9991, 0.9996,\n",
      "        0.9996, 0.9997, 0.9996, 0.9998, 1.0000, 0.9997, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5433, 0.5268, 0.5309, 0.5299, 0.5302, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301,\n",
      "        0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5301, 0.5625])\n",
      "finalReturns:  tensor([4.2687, 4.3011, 4.2672, 4.1880, 4.0751, 3.9381, 3.7834, 3.6157, 3.4385,\n",
      "        3.2542, 3.0646, 2.8710, 2.6745, 2.4759, 2.2756, 2.0740, 1.8716, 1.6684,\n",
      "        1.4648, 1.2607, 1.0564, 0.8519, 0.6472, 0.4423, 0.2374])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1682849652 saved\n",
      "[586616, 'tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])', 132442.06666666665, 85788.46666666665, 0.036905523389577866, 1e-05, 1, 0, 'tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\\n        18, 18, 18, 18, 18, 18,  0])', '[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\\n 1.]', '0,[1e-05,1][1, 10000, 1, 1],1682849652', 25, 50, 164645.8985006173, 193521.60851536895, 78111.91688437786, 135363.21866666665, 132442.06666666665, 102604.52812037412, 102604.52812037412, 120699.79187222247, 120699.79187222247, 90646.446578478, 102630.27987904308, 120699.79187222247]\n",
      "policy reset\n",
      "----------------------------------------\n",
      "iter  2  stage  24  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5098, 0.5098, 0.5098, 0.5098, 0.5098, 0.5098, 0.5098, 0.5098, 0.5098,\n",
      "        0.5098, 0.5098, 0.5098, 0.5098, 0.5098, 0.5098, 0.5098, 0.5098, 0.5098,\n",
      "        0.5098, 0.5098, 0.5098, 0.5098, 0.5098, 0.5098, 0.5098]) return=  127541.45169466147\n",
      "probs of actions:  tensor([0.8878, 0.8998, 0.9017, 0.0455, 0.8904, 0.8897, 0.8844, 0.9010, 0.9006,\n",
      "        0.9017, 0.8732, 0.9087, 0.8745, 0.8932, 0.8736, 0.9308, 0.8922, 0.0170,\n",
      "        0.8716, 0.8762, 0.8754, 0.8779, 0.8982, 0.9095, 0.9904],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5094, 0.5099, 0.5097, 0.5134, 0.5089, 0.5100, 0.5097, 0.5098,\n",
      "        0.5098, 0.5098, 0.5098, 0.5098, 0.5098, 0.5098, 0.5098, 0.5098, 0.5094,\n",
      "        0.5170, 0.5080, 0.5102, 0.5097, 0.5098, 0.5098, 0.5098])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  2  stage  23  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([ 0, 16,  0, 10,  3, 17,  1, 19,  1,  0,  1, 19,  1,  0,  0,  0,  1,  1,\n",
      "         0, 19,  0, 19,  0, 25,  0])\n",
      "loss=  tensor(0.1094, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0062, 1.0062, 1.0062, 1.0062, 1.0062, 1.0062, 1.0062, 1.0062, 1.0062,\n",
      "        1.0062, 1.0062, 1.0062, 1.0062, 1.0062, 1.0062, 1.0062, 1.0062, 1.0062,\n",
      "        1.0062, 1.0062, 1.0062, 1.0062, 1.0062, 1.0062, 0.5143]) return=  129474.82480479611\n",
      "probs of actions:  tensor([0.3228, 0.0171, 0.2714, 0.0419, 0.0283, 0.0064, 0.0947, 0.2192, 0.0974,\n",
      "        0.4085, 0.0975, 0.1958, 0.0921, 0.3377, 0.2460, 0.5109, 0.0855, 0.0916,\n",
      "        0.2574, 0.2246, 0.2617, 0.2131, 0.3345, 0.0302, 0.9922],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.4838, 0.5686, 0.4856, 0.5489, 0.4818, 0.5720, 0.4622, 0.5829,\n",
      "        0.4958, 0.5132, 0.4764, 0.5791, 0.4967, 0.5131, 0.5090, 0.5099, 0.5132,\n",
      "        0.5125, 0.4730, 0.5801, 0.4568, 0.5844, 0.4294, 0.6079])\n",
      "finalReturns:  tensor([0.0311, 0.0936])\n",
      "----------------------------------------\n",
      "iter  2  stage  22  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([19, 19, 19, 19, 19, 20, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "        19, 19, 19, 19, 19, 19,  0])\n",
      "loss=  tensor(0.0571, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.5092, 1.5092, 1.5092, 1.5092, 1.5092, 1.5092, 1.5092, 1.5092, 1.5092,\n",
      "        1.5092, 1.5092, 1.5092, 1.5092, 1.5092, 1.5092, 1.5092, 1.5092, 1.5092,\n",
      "        1.5092, 1.5092, 1.5092, 1.5092, 1.5092, 0.9437, 0.4474]) return=  132272.56485156258\n",
      "probs of actions:  tensor([0.8868, 0.8613, 0.8450, 0.8907, 0.8717, 0.0441, 0.8733, 0.8735, 0.9006,\n",
      "        0.8514, 0.8717, 0.8657, 0.8498, 0.8412, 0.8213, 0.8708, 0.8853, 0.8783,\n",
      "        0.8333, 0.8497, 0.8522, 0.8566, 0.9069, 0.7395, 0.9979],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4751, 0.5434, 0.5259, 0.5303, 0.5292, 0.5256, 0.5332, 0.5285, 0.5296,\n",
      "        0.5293, 0.5294, 0.5294, 0.5294, 0.5294, 0.5294, 0.5294, 0.5294, 0.5294,\n",
      "        0.5294, 0.5294, 0.5294, 0.5294, 0.5294, 0.5294, 0.5655])\n",
      "finalReturns:  tensor([0.1151, 0.1512, 0.1181])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  2  stage  21  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "        19, 19, 19, 19, 19, 20,  0])\n",
      "loss=  tensor(0.5379, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.9216, 1.9216, 1.9216, 1.9216, 1.9216, 1.9216, 1.9216, 1.9216, 1.9216,\n",
      "        1.9216, 1.9216, 1.9216, 1.9216, 1.9216, 1.9216, 1.9216, 1.9216, 1.9216,\n",
      "        1.9216, 1.9216, 1.9216, 1.9216, 1.3561, 0.8598, 0.4124]) return=  132280.08116666667\n",
      "probs of actions:  tensor([0.9403, 0.9196, 0.9068, 0.9415, 0.9396, 0.9242, 0.9294, 0.9377, 0.9528,\n",
      "        0.9275, 0.9298, 0.9316, 0.9182, 0.9129, 0.8965, 0.9457, 0.9382, 0.9369,\n",
      "        0.9005, 0.9167, 0.9171, 0.9177, 0.9507, 0.1169, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4751, 0.5434, 0.5259, 0.5303, 0.5292, 0.5295, 0.5294, 0.5294, 0.5294,\n",
      "        0.5294, 0.5294, 0.5294, 0.5294, 0.5294, 0.5294, 0.5294, 0.5294, 0.5294,\n",
      "        0.5294, 0.5294, 0.5294, 0.5294, 0.5294, 0.5255, 0.5693])\n",
      "finalReturns:  tensor([0.2320, 0.2681, 0.2350, 0.1569])\n",
      "----------------------------------------\n",
      "iter  2  stage  20  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([19, 20, 19, 19, 19, 19, 19, 20, 19, 19, 19, 19, 19, 19, 19, 19, 20, 19,\n",
      "        19, 19, 19, 20, 19, 19,  0])\n",
      "loss=  tensor(1.0797, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.3038, 2.3038, 2.3038, 2.3038, 2.3038, 2.3038, 2.3038, 2.3038, 2.3038,\n",
      "        2.3038, 2.3038, 2.3038, 2.3038, 2.3038, 2.3038, 2.3038, 2.3038, 2.3038,\n",
      "        2.3038, 2.3038, 2.3038, 1.7384, 1.2420, 0.7946, 0.3855]) return=  132246.351396498\n",
      "probs of actions:  tensor([0.8866, 0.1261, 0.8280, 0.8925, 0.8865, 0.8621, 0.8719, 0.0946, 0.9101,\n",
      "        0.8700, 0.8727, 0.8724, 0.8557, 0.8425, 0.8268, 0.8876, 0.0950, 0.8847,\n",
      "        0.8229, 0.8516, 0.8295, 0.1281, 0.9126, 0.6660, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4751, 0.5395, 0.5297, 0.5293, 0.5294, 0.5294, 0.5294, 0.5255, 0.5332,\n",
      "        0.5285, 0.5296, 0.5293, 0.5294, 0.5294, 0.5294, 0.5294, 0.5255, 0.5332,\n",
      "        0.5285, 0.5296, 0.5293, 0.5255, 0.5332, 0.5285, 0.5657])\n",
      "finalReturns:  tensor([0.3784, 0.4145, 0.3853, 0.2996, 0.1803])\n",
      "----------------------------------------\n",
      "iter  2  stage  19  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([19, 19, 19, 25, 20, 19, 20, 19, 19, 19, 19, 19, 23, 19, 20, 19, 20, 20,\n",
      "        19, 20, 20, 19, 19, 19,  0])\n",
      "loss=  tensor(2.0426, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.6633, 2.6633, 2.6633, 2.6633, 2.6633, 2.6633, 2.6633, 2.6633, 2.6633,\n",
      "        2.6633, 2.6633, 2.6633, 2.6633, 2.6633, 2.6633, 2.6633, 2.6633, 2.6633,\n",
      "        2.6633, 2.6633, 2.0985, 1.6020, 1.1580, 0.7504, 0.3663]) return=  132091.35601931607\n",
      "probs of actions:  tensor([0.7048, 0.6305, 0.5994, 0.0062, 0.2655, 0.6701, 0.2830, 0.6955, 0.7536,\n",
      "        0.6773, 0.6796, 0.6751, 0.0043, 0.6330, 0.3343, 0.6932, 0.2750, 0.2573,\n",
      "        0.5974, 0.3659, 0.3144, 0.5819, 0.7250, 0.3477, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4751, 0.5434, 0.5259, 0.5039, 0.5481, 0.5276, 0.5260, 0.5331, 0.5285,\n",
      "        0.5296, 0.5293, 0.5294, 0.5126, 0.5445, 0.5218, 0.5341, 0.5243, 0.5296,\n",
      "        0.5322, 0.5248, 0.5294, 0.5322, 0.5287, 0.5296, 0.5655])\n",
      "finalReturns:  tensor([0.5469, 0.5869, 0.5539, 0.4658, 0.3446, 0.1992])\n",
      "----------------------------------------\n",
      "iter  2  stage  18  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([20, 20, 20, 20, 19, 20, 20, 20, 20, 20, 20, 20, 19, 19, 20, 20, 19, 19,\n",
      "        20, 20, 20, 20, 20, 20,  0])\n",
      "loss=  tensor(0.9034, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.0040, 3.0040, 3.0040, 3.0040, 3.0040, 3.0040, 3.0040, 3.0040, 3.0040,\n",
      "        3.0040, 3.0040, 3.0040, 3.0040, 3.0040, 3.0040, 3.0040, 3.0040, 3.0040,\n",
      "        3.0040, 2.4383, 1.9420, 1.4980, 1.0904, 0.7094, 0.3476]) return=  132119.19007904181\n",
      "probs of actions:  tensor([0.6944, 0.7625, 0.7787, 0.6775, 0.2659, 0.7191, 0.7076, 0.7213, 0.6555,\n",
      "        0.7170, 0.7109, 0.7317, 0.2381, 0.2190, 0.7402, 0.7420, 0.2519, 0.2741,\n",
      "        0.8336, 0.8034, 0.7274, 0.7529, 0.7599, 0.9213, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5433, 0.5248, 0.5294, 0.5322, 0.5248, 0.5294, 0.5283, 0.5286,\n",
      "        0.5285, 0.5285, 0.5285, 0.5324, 0.5287, 0.5257, 0.5292, 0.5322, 0.5287,\n",
      "        0.5257, 0.5292, 0.5283, 0.5286, 0.5285, 0.5285, 0.5685])\n",
      "finalReturns:  tensor([0.7334, 0.7734, 0.7404, 0.6561, 0.5351, 0.3877, 0.2209])\n",
      "----------------------------------------\n",
      "iter  2  stage  17  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([20, 20, 20, 20, 20, 21, 20, 19, 25, 19, 19, 19, 19, 21, 20, 20, 20, 19,\n",
      "        20, 20, 19, 20, 19, 20,  0])\n",
      "loss=  tensor(4.4889, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.3606, 3.3606, 3.3606, 3.3606, 3.3606, 3.3606, 3.3606, 3.3606, 3.3606,\n",
      "        3.3606, 3.3606, 3.3606, 3.3606, 3.3606, 3.3606, 3.3606, 3.3606, 3.3606,\n",
      "        2.7921, 2.2965, 1.8489, 1.4398, 1.0574, 0.6917, 0.3419]) return=  132054.13451253538\n",
      "probs of actions:  tensor([0.6917, 0.7543, 0.7701, 0.6788, 0.7036, 0.0511, 0.7061, 0.2013, 0.0152,\n",
      "        0.2061, 0.2140, 0.1935, 0.1996, 0.0562, 0.7250, 0.7468, 0.7122, 0.2247,\n",
      "        0.7869, 0.8139, 0.1817, 0.7292, 0.1659, 0.9044, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5433, 0.5248, 0.5294, 0.5283, 0.5245, 0.5323, 0.5315, 0.5025,\n",
      "        0.5523, 0.5237, 0.5308, 0.5290, 0.5215, 0.5330, 0.5274, 0.5288, 0.5323,\n",
      "        0.5248, 0.5295, 0.5322, 0.5248, 0.5333, 0.5245, 0.5695])\n",
      "finalReturns:  tensor([0.9104, 0.9465, 0.9173, 0.8354, 0.7124, 0.5700, 0.4023, 0.2276])\n",
      "----------------------------------------\n",
      "iter  2  stage  16  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([19, 20, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 23, 20, 25, 20, 20, 21,\n",
      "        20, 20, 27, 21, 20, 20,  0])\n",
      "loss=  tensor(10.3548, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.6175, 3.6175, 3.6175, 3.6175, 3.6175, 3.6175, 3.6175, 3.6175, 3.6175,\n",
      "        3.6175, 3.6175, 3.6175, 3.6175, 3.6175, 3.6175, 3.6175, 3.6175, 3.0539,\n",
      "        2.5571, 2.1132, 1.7088, 1.3293, 0.9688, 0.6424, 0.3189]) return=  131844.90001474044\n",
      "probs of actions:  tensor([0.1382, 0.7796, 0.0968, 0.7182, 0.7469, 0.7440, 0.7399, 0.7577, 0.7211,\n",
      "        0.7426, 0.7502, 0.7602, 0.0066, 0.7539, 0.0349, 0.7833, 0.7792, 0.0772,\n",
      "        0.7695, 0.8051, 0.0124, 0.0893, 0.8103, 0.9007, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4751, 0.5395, 0.5297, 0.5254, 0.5293, 0.5283, 0.5286, 0.5285, 0.5285,\n",
      "        0.5285, 0.5285, 0.5285, 0.5156, 0.5399, 0.5032, 0.5482, 0.5236, 0.5256,\n",
      "        0.5320, 0.5277, 0.4958, 0.5511, 0.5257, 0.5292, 0.5683])\n",
      "finalReturns:  tensor([1.1615, 1.2015, 1.1727, 1.0846, 0.9614, 0.8450, 0.6544, 0.4551, 0.2495])\n",
      "----------------------------------------\n",
      "iter  2  stage  15  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([20, 20, 20, 20, 27, 20, 20, 21, 19, 20, 20, 20, 21, 20, 19, 25, 20, 20,\n",
      "        20, 20, 20, 20, 25, 20,  0])\n",
      "loss=  tensor(7.6280, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.9346, 3.9346, 3.9346, 3.9346, 3.9346, 3.9346, 3.9346, 3.9346, 3.9346,\n",
      "        3.9346, 3.9346, 3.9346, 3.9346, 3.9346, 3.9346, 3.9346, 3.3696, 2.8732,\n",
      "        2.4457, 2.0460, 1.6717, 1.3147, 0.9705, 0.6359, 0.3083]) return=  131802.06019900876\n",
      "probs of actions:  tensor([0.7385, 0.7852, 0.7893, 0.7300, 0.0182, 0.7516, 0.7516, 0.0807, 0.0850,\n",
      "        0.7540, 0.7621, 0.7685, 0.0843, 0.7577, 0.0585, 0.0655, 0.7731, 0.7445,\n",
      "        0.7731, 0.7853, 0.7374, 0.7530, 0.0523, 0.8993, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5433, 0.5248, 0.5294, 0.4954, 0.5553, 0.5219, 0.5261, 0.5358,\n",
      "        0.5239, 0.5297, 0.5282, 0.5245, 0.5323, 0.5315, 0.5025, 0.5484, 0.5236,\n",
      "        0.5298, 0.5282, 0.5286, 0.5285, 0.5060, 0.5475, 0.5638])\n",
      "finalReturns:  tensor([1.3723, 1.4348, 1.3828, 1.2867, 1.1566, 1.0027, 0.8312, 0.6468, 0.4754,\n",
      "        0.2555])\n",
      "----------------------------------------\n",
      "iter  2  stage  14  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([20, 25, 25, 20, 19, 25, 21, 27, 21, 20, 21, 25, 25, 20, 19, 19, 19, 20,\n",
      "        25, 25, 23, 25, 25, 25,  0])\n",
      "loss=  tensor(24.3833, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.2216, 4.2216, 4.2216, 4.2216, 4.2216, 4.2216, 4.2216, 4.2216, 4.2216,\n",
      "        4.2216, 4.2216, 4.2216, 4.2216, 4.2216, 4.2216, 3.6567, 3.1602, 2.7129,\n",
      "        2.3005, 1.9135, 1.5480, 1.2121, 0.8942, 0.5849, 0.2885]) return=  131224.90587325243\n",
      "probs of actions:  tensor([0.3167, 0.4112, 0.4304, 0.3126, 0.0764, 0.4228, 0.0819, 0.0535, 0.0741,\n",
      "        0.3408, 0.0841, 0.4277, 0.4054, 0.3495, 0.0591, 0.0609, 0.0489, 0.3048,\n",
      "        0.4070, 0.4643, 0.0152, 0.4746, 0.4893, 0.3663, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5208, 0.5213, 0.5437, 0.5287, 0.5032, 0.5441, 0.4945, 0.5514,\n",
      "        0.5256, 0.5251, 0.5096, 0.5241, 0.5430, 0.5288, 0.5295, 0.5294, 0.5255,\n",
      "        0.5068, 0.5248, 0.5299, 0.5138, 0.5231, 0.5207, 0.5838])\n",
      "finalReturns:  tensor([1.5945, 1.6306, 1.5975, 1.5155, 1.4024, 1.2826, 1.1233, 0.9293, 0.7334,\n",
      "        0.5197, 0.2953])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  2  stage  13  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([25, 25, 25, 25, 25, 25, 20, 25, 20, 25, 25, 20, 25, 25, 25, 25, 25, 25,\n",
      "        20, 25, 25, 25, 27, 21,  0])\n",
      "loss=  tensor(11.5393, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.3408, 4.3408, 4.3408, 4.3408, 4.3408, 4.3408, 4.3408, 4.3408, 4.3408,\n",
      "        4.3408, 4.3408, 4.3408, 4.3408, 4.3408, 3.7524, 3.2614, 2.8326, 2.4489,\n",
      "        2.0972, 1.7685, 1.4565, 1.1431, 0.8458, 0.5561, 0.2730]) return=  130632.29857987192\n",
      "probs of actions:  tensor([0.8027, 0.7853, 0.8031, 0.8005, 0.7618, 0.7843, 0.0751, 0.7990, 0.0598,\n",
      "        0.7952, 0.7745, 0.0666, 0.7593, 0.8055, 0.8090, 0.8378, 0.8045, 0.7969,\n",
      "        0.0747, 0.8099, 0.7747, 0.8152, 0.0467, 0.0235, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5401, 0.5165, 0.5224, 0.5209, 0.5213, 0.5437, 0.5023, 0.5485,\n",
      "        0.5011, 0.5263, 0.5424, 0.5026, 0.5259, 0.5200, 0.5215, 0.5211, 0.5212,\n",
      "        0.5437, 0.5023, 0.5260, 0.5200, 0.5111, 0.5472, 0.5667])\n",
      "finalReturns:  tensor([1.9858, 2.0483, 2.0193, 1.9266, 1.7891, 1.6197, 1.4047, 1.2144, 1.0019,\n",
      "        0.7792, 0.5578, 0.2936])\n",
      "----------------------------------------\n",
      "iter  2  stage  12  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 27, 25, 27, 25, 27,\n",
      "        25, 25, 25, 25, 25, 25,  0])\n",
      "loss=  tensor(16.6462, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.5332, 4.5332, 4.5332, 4.5332, 4.5332, 4.5332, 4.5332, 4.5332, 4.5332,\n",
      "        4.5332, 4.5332, 4.5332, 4.5332, 3.9495, 3.4574, 3.0288, 2.6513, 2.3024,\n",
      "        1.9819, 1.6745, 1.3840, 1.0988, 0.8192, 0.5435, 0.2707]) return=  130222.86739483997\n",
      "probs of actions:  tensor([0.8502, 0.8415, 0.8612, 0.8445, 0.8176, 0.8402, 0.8292, 0.8532, 0.8535,\n",
      "        0.8500, 0.8340, 0.8461, 0.8389, 0.1052, 0.8543, 0.0961, 0.8421, 0.1346,\n",
      "        0.8162, 0.8575, 0.8188, 0.8667, 0.9187, 0.9012, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5401, 0.5165, 0.5224, 0.5209, 0.5213, 0.5212, 0.5212, 0.5212,\n",
      "        0.5212, 0.5212, 0.5212, 0.5212, 0.5108, 0.5289, 0.5089, 0.5293, 0.5088,\n",
      "        0.5294, 0.5192, 0.5217, 0.5211, 0.5212, 0.5212, 0.5837])\n",
      "finalReturns:  tensor([2.2921, 2.3546, 2.3359, 2.2356, 2.1043, 1.9237, 1.7355, 1.5135, 1.2849,\n",
      "        1.0484, 0.8069, 0.5614, 0.3130])\n",
      "----------------------------------------\n",
      "iter  2  stage  11  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([25, 25, 25, 25, 27, 25, 27, 25, 27, 25, 25, 25, 25, 27, 27, 27, 25, 25,\n",
      "        27, 25, 25, 27, 25, 27,  0])\n",
      "loss=  tensor(23.1371, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.7819, 4.7819, 4.7819, 4.7819, 4.7819, 4.7819, 4.7819, 4.7819, 4.7819,\n",
      "        4.7819, 4.7819, 4.7819, 4.1977, 3.7057, 3.2771, 2.8934, 2.5475, 2.2274,\n",
      "        1.9262, 1.6334, 1.3481, 1.0734, 0.8000, 0.5293, 0.2653]) return=  129983.17438300277\n",
      "probs of actions:  tensor([0.8312, 0.8252, 0.8517, 0.8265, 0.1764, 0.8258, 0.1594, 0.8394, 0.1417,\n",
      "        0.8382, 0.8193, 0.8525, 0.8306, 0.1470, 0.1561, 0.1345, 0.8147, 0.7903,\n",
      "        0.1904, 0.8504, 0.8082, 0.1412, 0.9162, 0.0991, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5401, 0.5165, 0.5224, 0.5105, 0.5289, 0.5089, 0.5293, 0.5088,\n",
      "        0.5294, 0.5192, 0.5217, 0.5211, 0.5108, 0.5185, 0.5165, 0.5274, 0.5196,\n",
      "        0.5112, 0.5288, 0.5193, 0.5113, 0.5287, 0.5089, 0.5918])\n",
      "finalReturns:  tensor([2.5538, 2.6163, 2.5872, 2.5050, 2.3702, 2.1996, 1.9923, 1.7738, 1.5555,\n",
      "        1.3120, 1.0674, 0.8294, 0.5714, 0.3265])\n",
      "----------------------------------------\n",
      "iter  2  stage  10  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([25, 27, 25, 25, 27, 25, 27, 27, 25, 25, 27, 27, 25, 25, 25, 25, 25, 27,\n",
      "        27, 25, 25, 27, 25, 27,  0])\n",
      "loss=  tensor(19.3476, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.0490, 5.0490, 5.0490, 5.0490, 5.0490, 5.0490, 5.0490, 5.0490, 5.0490,\n",
      "        5.0490, 5.0490, 4.4650, 3.9730, 3.5509, 3.1764, 2.8302, 2.5058, 2.1969,\n",
      "        1.8996, 1.6107, 1.3334, 1.0633, 0.7937, 0.5257, 0.2638]) return=  129940.02348244\n",
      "probs of actions:  tensor([0.6411, 0.3478, 0.6864, 0.6324, 0.3795, 0.6402, 0.3471, 0.3273, 0.6574,\n",
      "        0.6662, 0.3993, 0.2574, 0.6499, 0.6282, 0.6171, 0.6509, 0.5852, 0.4329,\n",
      "        0.4035, 0.6756, 0.6048, 0.3174, 0.7760, 0.2519, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5297, 0.5242, 0.5205, 0.5110, 0.5288, 0.5089, 0.5189, 0.5268,\n",
      "        0.5198, 0.5111, 0.5184, 0.5270, 0.5198, 0.5216, 0.5211, 0.5212, 0.5108,\n",
      "        0.5185, 0.5269, 0.5198, 0.5112, 0.5288, 0.5089, 0.5918])\n",
      "finalReturns:  tensor([2.8077, 2.8806, 2.8543, 2.7493, 2.6041, 2.4288, 2.2321, 2.0197, 1.8063,\n",
      "        1.5767, 1.3270, 1.0774, 0.8358, 0.5750, 0.3281])\n",
      "----------------------------------------\n",
      "iter  2  stage  9  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 25, 25, 27, 25, 25, 27, 27, 25, 25, 25, 27, 25, 27,\n",
      "        27, 25, 25, 27, 25, 27,  0])\n",
      "loss=  tensor(23.0461, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.3010, 5.3010, 5.3010, 5.3010, 5.3010, 5.3010, 5.3010, 5.3010, 5.3010,\n",
      "        5.3010, 4.7192, 4.2266, 3.7982, 3.4206, 3.0777, 2.7543, 2.4466, 2.1500,\n",
      "        1.8670, 1.5875, 1.3174, 1.0525, 0.7869, 0.5219, 0.2621]) return=  129813.23890870859\n",
      "probs of actions:  tensor([0.5159, 0.5150, 0.4584, 0.5240, 0.5492, 0.4800, 0.4801, 0.4968, 0.4940,\n",
      "        0.4976, 0.5986, 0.4028, 0.4706, 0.4470, 0.4710, 0.5364, 0.4000, 0.5993,\n",
      "        0.5524, 0.4923, 0.4229, 0.4827, 0.6139, 0.3995, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5166, 0.5274, 0.5196, 0.5112, 0.5288,\n",
      "        0.5193, 0.5113, 0.5183, 0.5270, 0.5198, 0.5216, 0.5107, 0.5289, 0.5089,\n",
      "        0.5189, 0.5268, 0.5198, 0.5111, 0.5288, 0.5089, 0.5918])\n",
      "finalReturns:  tensor([3.0709, 3.1334, 3.1147, 3.0248, 2.8754, 2.6986, 2.5004, 2.2974, 2.0651,\n",
      "        1.8392, 1.5998, 1.3431, 1.0881, 0.8426, 0.5789, 0.3297])\n",
      "----------------------------------------\n",
      "iter  2  stage  8  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([25, 27, 27, 27, 27, 27, 27, 25, 25, 27, 25, 25, 27, 25, 25, 27, 25, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(31.1249, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.5427, 5.5427, 5.5427, 5.5427, 5.5427, 5.5427, 5.5427, 5.5427, 5.5427,\n",
      "        4.9605, 4.4680, 4.0396, 3.6620, 3.3132, 2.9870, 2.6824, 2.3868, 2.0995,\n",
      "        1.8233, 1.5489, 1.2825, 1.0205, 0.7620, 0.5062, 0.2524]) return=  129679.56150960914\n",
      "probs of actions:  tensor([0.2493, 0.7420, 0.6959, 0.7517, 0.7673, 0.7372, 0.7310, 0.2662, 0.2334,\n",
      "        0.7474, 0.1887, 0.3415, 0.7633, 0.2293, 0.2571, 0.7751, 0.1964, 0.8103,\n",
      "        0.7621, 0.7381, 0.8030, 0.7262, 0.6998, 0.6726, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4487, 0.5297, 0.5138, 0.5177, 0.5167, 0.5170, 0.5169, 0.5273, 0.5197,\n",
      "        0.5112, 0.5288, 0.5193, 0.5113, 0.5287, 0.5193, 0.5113, 0.5287, 0.5089,\n",
      "        0.5189, 0.5164, 0.5170, 0.5169, 0.5169, 0.5169, 0.5898])\n",
      "finalReturns:  tensor([3.3375, 3.4000, 3.3813, 3.2810, 3.1392, 2.9768, 2.7742, 2.5595, 2.3439,\n",
      "        2.1024, 1.8696, 1.6252, 1.3751, 1.1201, 0.8616, 0.6005, 0.3374])\n",
      "----------------------------------------\n",
      "iter  2  stage  7  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 25, 27, 27, 27, 27, 27, 27, 25, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 25, 27, 27, 27,  0])\n",
      "loss=  tensor(15.3898, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.7261, 5.7261, 5.7261, 5.7261, 5.7261, 5.7261, 5.7261, 5.7261, 5.1368,\n",
      "        4.6459, 4.2236, 3.8492, 3.5088, 3.1929, 2.8892, 2.6013, 2.3234, 2.0532,\n",
      "        1.7886, 1.5282, 1.2709, 1.0159, 0.7627, 0.5058, 0.2524]) return=  129467.4131065994\n",
      "probs of actions:  tensor([0.9444, 0.9402, 0.9242, 0.9438, 0.0534, 0.9387, 0.9331, 0.9445, 0.9529,\n",
      "        0.9536, 0.9645, 0.0761, 0.9519, 0.9444, 0.9565, 0.9595, 0.9607, 0.9553,\n",
      "        0.9569, 0.9400, 0.0355, 0.9364, 0.9489, 0.9312, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5270, 0.5093, 0.5188, 0.5164, 0.5170,\n",
      "        0.5169, 0.5169, 0.5273, 0.5093, 0.5188, 0.5164, 0.5170, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5273, 0.5093, 0.5188, 0.5164, 0.5899])\n",
      "finalReturns:  tensor([3.6596, 3.7325, 3.7063, 3.6117, 3.4692, 3.2823, 3.0890, 2.8738, 2.6452,\n",
      "        2.4061, 2.1594, 1.9071, 1.6506, 1.3909, 1.1186, 0.8625, 0.6006, 0.3376])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  2  stage  6  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.5009, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.9498, 5.9498, 5.9498, 5.9498, 5.9498, 5.9498, 5.9498, 5.3600, 4.8693,\n",
      "        4.4470, 4.0725, 3.7322, 3.4162, 3.1181, 2.8329, 2.5572, 2.2886, 2.0252,\n",
      "        1.7657, 1.5091, 1.2547, 1.0019, 0.7503, 0.4996, 0.2495]) return=  129338.69866666665\n",
      "probs of actions:  tensor([0.9869, 0.9858, 0.9812, 0.9867, 0.9870, 0.9850, 0.9836, 0.9897, 0.9896,\n",
      "        0.9908, 0.9930, 0.9842, 0.9890, 0.9900, 0.9919, 0.9932, 0.9919, 0.9909,\n",
      "        0.9907, 0.9878, 0.9923, 0.9858, 0.9907, 0.9860, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5166, 0.5170, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5898])\n",
      "finalReturns:  tensor([3.9446, 4.0175, 3.9913, 3.8967, 3.7542, 3.5777, 3.3767, 3.1579, 2.9262,\n",
      "        2.6849, 2.4366, 2.1831, 1.9257, 1.6653, 1.4028, 1.1387, 0.8734, 0.6072,\n",
      "        0.3403])\n",
      "----------------------------------------\n",
      "iter  2  stage  5  ep  99999   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.1744, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.1990, 6.1990, 6.1990, 6.1990, 6.1990, 6.1990, 5.6091, 5.1185, 4.6961,\n",
      "        4.3217, 3.9813, 3.6654, 3.3672, 3.0820, 2.8063, 2.5377, 2.2744, 2.0149,\n",
      "        1.7583, 1.5038, 1.2510, 0.9994, 0.7487, 0.4987, 0.2491]) return=  129338.69866666665\n",
      "probs of actions:  tensor([0.9952, 0.9947, 0.9929, 0.9951, 0.9951, 0.9953, 0.9948, 0.9969, 0.9974,\n",
      "        0.9973, 0.9979, 0.9956, 0.9958, 0.9967, 0.9979, 0.9983, 0.9980, 0.9967,\n",
      "        0.9975, 0.9965, 0.9977, 0.9943, 0.9977, 0.9957, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5166, 0.5170, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5898])\n",
      "finalReturns:  tensor([4.2124, 4.2853, 4.2591, 4.1645, 4.0220, 3.8455, 3.6445, 3.4257, 3.1940,\n",
      "        2.9527, 2.7044, 2.4509, 2.1934, 1.9331, 1.6706, 1.4065, 1.1412, 0.8750,\n",
      "        0.6081, 0.3407])\n",
      "----------------------------------------\n",
      "iter  2  stage  4  ep  66024   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.0306, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.4476, 6.4476, 6.4476, 6.4476, 6.4476, 5.8581, 5.3673, 4.9450, 4.5705,\n",
      "        4.2302, 3.9142, 3.6161, 3.3309, 3.0552, 2.7866, 2.5232, 2.2637, 2.0071,\n",
      "        1.7527, 1.4999, 1.2483, 0.9976, 0.7475, 0.4980, 0.2489]) return=  129338.69866666665\n",
      "probs of actions:  tensor([0.9990, 0.9988, 0.9984, 0.9989, 0.9990, 0.9993, 0.9992, 0.9996, 0.9996,\n",
      "        0.9996, 0.9998, 0.9994, 0.9993, 0.9995, 0.9997, 0.9999, 0.9997, 0.9995,\n",
      "        0.9997, 0.9994, 0.9998, 0.9989, 0.9998, 0.9994, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5166, 0.5170, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5898])\n",
      "finalReturns:  tensor([4.4805, 4.5534, 4.5271, 4.4326, 4.2901, 4.1135, 3.9125, 3.6938, 3.4621,\n",
      "        3.2208, 2.9725, 2.7189, 2.4615, 2.2012, 1.9387, 1.6746, 1.4093, 1.1430,\n",
      "        0.8761, 0.6087, 0.3410])\n",
      "----------------------------------------\n",
      "iter  2  stage  3  ep  370   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.0339, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.6975, 6.6975, 6.6975, 6.6975, 6.1064, 5.6160, 5.1936, 4.8192, 4.4788,\n",
      "        4.1629, 3.8647, 3.5795, 3.3038, 3.0352, 2.7719, 2.5124, 2.2558, 2.0013,\n",
      "        1.7485, 1.4969, 1.2462, 0.9962, 0.7466, 0.4975, 0.2486]) return=  129338.69866666665\n",
      "probs of actions:  tensor([0.9990, 0.9989, 0.9985, 0.9990, 0.9991, 0.9993, 0.9993, 0.9996, 0.9996,\n",
      "        0.9996, 0.9998, 0.9994, 0.9993, 0.9995, 0.9997, 0.9999, 0.9997, 0.9996,\n",
      "        0.9998, 0.9994, 0.9998, 0.9990, 0.9998, 0.9994, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5166, 0.5170, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5898])\n",
      "finalReturns:  tensor([4.7487, 4.8216, 4.7954, 4.7008, 4.5583, 4.3818, 4.1808, 3.9621, 3.7303,\n",
      "        3.4891, 3.2408, 2.9872, 2.7298, 2.4694, 2.2070, 1.9429, 1.6775, 1.4113,\n",
      "        1.1444, 0.8770, 0.6092, 0.3412])\n",
      "----------------------------------------\n",
      "iter  2  stage  2  ep  4830   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.0356, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.9408, 6.9408, 6.9408, 6.3561, 5.8642, 5.4422, 5.0677, 4.7273, 4.4114,\n",
      "        4.1132, 3.8280, 3.5523, 3.2837, 3.0203, 2.7608, 2.5042, 2.2498, 1.9970,\n",
      "        1.7454, 1.4947, 1.2447, 0.9951, 0.7460, 0.4971, 0.2485]) return=  129338.69866666665\n",
      "probs of actions:  tensor([0.9991, 0.9990, 0.9990, 0.9990, 0.9991, 0.9995, 0.9994, 0.9997, 0.9997,\n",
      "        0.9997, 0.9999, 0.9995, 0.9994, 0.9996, 0.9998, 0.9999, 0.9997, 0.9996,\n",
      "        0.9998, 0.9994, 0.9998, 0.9990, 0.9998, 0.9995, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5166, 0.5170, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5898])\n",
      "finalReturns:  tensor([5.0173, 5.0902, 5.0638, 4.9693, 4.8268, 4.6503, 4.4493, 4.2305, 3.9988,\n",
      "        3.7575, 3.5092, 3.2557, 2.9982, 2.7379, 2.4754, 2.2113, 1.9460, 1.6798,\n",
      "        1.4129, 1.1455, 0.8777, 0.6096, 0.3413])\n",
      "----------------------------------------\n",
      "iter  2  stage  1  ep  10   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.0417, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.2101, 7.2101, 6.5997, 6.1137, 5.6903, 5.3161, 4.9756, 4.6597, 4.3615,\n",
      "        4.0763, 3.8007, 3.5321, 3.2687, 3.0092, 2.7526, 2.4982, 2.2454, 1.9938,\n",
      "        1.7430, 1.4930, 1.2435, 0.9943, 0.7455, 0.4968, 0.2484]) return=  129338.69866666665\n",
      "probs of actions:  tensor([0.9991, 0.9990, 0.9990, 0.9990, 0.9991, 0.9995, 0.9994, 0.9997, 0.9997,\n",
      "        0.9997, 0.9999, 0.9995, 0.9994, 0.9996, 0.9998, 0.9999, 0.9997, 0.9996,\n",
      "        0.9998, 0.9994, 0.9998, 0.9990, 0.9998, 0.9995, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5166, 0.5170, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5898])\n",
      "finalReturns:  tensor([5.2854, 5.3583, 5.3326, 5.2378, 5.0954, 4.9188, 4.7178, 4.4991, 4.2673,\n",
      "        4.0261, 3.7778, 3.5242, 3.2668, 3.0064, 2.7440, 2.4799, 2.2145, 1.9483,\n",
      "        1.6814, 1.4140, 1.1462, 0.8782, 0.6099, 0.3415])\n",
      "----------------------------------------\n",
      "iter  2  stage  0  ep  0   adversary:  AdversaryModes.imitation_128\n",
      "  actions:  tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27,  0])\n",
      "loss=  tensor(0.0479, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.3783, 6.8670, 6.3576, 5.9396, 5.5641, 5.2240, 4.9080, 4.6098, 4.3246,\n",
      "        4.0490, 3.7804, 3.5170, 3.2575, 3.0009, 2.7465, 2.4936, 2.2420, 1.9913,\n",
      "        1.7413, 1.4918, 1.2426, 0.9938, 0.7451, 0.4966, 0.2483]) return=  129338.69866666665\n",
      "probs of actions:  tensor([0.9991, 0.9990, 0.9990, 0.9990, 0.9991, 0.9995, 0.9994, 0.9997, 0.9997,\n",
      "        0.9997, 0.9999, 0.9995, 0.9994, 0.9996, 0.9998, 0.9999, 0.9997, 0.9996,\n",
      "        0.9998, 0.9994, 0.9998, 0.9990, 0.9998, 0.9995, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4383, 0.5375, 0.5118, 0.5182, 0.5166, 0.5170, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169,\n",
      "        0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5169, 0.5898])\n",
      "finalReturns:  tensor([5.5556, 5.6285, 5.6005, 5.5067, 5.3639, 5.1875, 4.9865, 4.7677, 4.5360,\n",
      "        4.2947, 4.0464, 3.7929, 3.5354, 3.2751, 3.0126, 2.7485, 2.4832, 2.2170,\n",
      "        1.9501, 1.6827, 1.4149, 1.1468, 0.8786, 0.6101, 0.3416])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1682912335 saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2471239, 'tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])', 129338.69866666665, 75332.17066666666, 0.047861263155937195, 1e-05, 1, 0, 'tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\\n        27, 27, 27, 27, 27, 27,  0])', '[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\\n 1.]', '0,[1e-05,1][1, 10000, 1, 1],1682912335', 25, 50, 172677.5835126837, 219861.0138117109, 91586.68399118823, 132334.4363268229, 129338.69866666666, 72150.91909080048, 72150.91909080048, 88030.91902269641, 88030.91902269641, 105848.51071772553, 72150.91909080048, 88030.91902269641]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x648 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy reset\n",
      "----------------------------------------\n",
      "iter  0  stage  24  ep  99999   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0, 11,  0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.6045, 0.6045, 0.6045, 0.6045, 0.6045, 0.6045, 0.6045, 0.6045, 0.6045,\n",
      "        0.6045, 0.6045, 0.6045, 0.6045, 0.6045, 0.6045, 0.6045, 0.6045, 0.6045,\n",
      "        0.6045, 0.6045, 0.6045, 0.6045, 0.6045, 0.6045, 0.6045]) return=  138853.62351012303\n",
      "probs of actions:  tensor([0.8900, 0.8782, 0.9039, 0.8629, 0.8870, 0.8853, 0.8782, 0.8892, 0.8739,\n",
      "        0.9083, 0.9194, 0.8914, 0.9129, 0.9059, 0.8999, 0.8765, 0.8980, 0.8966,\n",
      "        0.8917, 0.8911, 0.8851, 0.8940, 0.9132, 0.0012, 0.9840],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5238, 0.5334, 0.5406, 0.5460, 0.5501, 0.5532, 0.5555, 0.5573,\n",
      "        0.5586, 0.5595, 0.5603, 0.5608, 0.5613, 0.5616, 0.5618, 0.5620, 0.5621,\n",
      "        0.5622, 0.5623, 0.5623, 0.5624, 0.5624, 0.5503, 0.6045])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  0  stage  23  ep  99999   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([8, 9, 8, 8, 9, 9, 8, 1, 8, 8, 8, 9, 8, 9, 8, 0, 0, 0, 5, 0, 8, 9, 8, 5,\n",
      "        0])\n",
      "loss=  tensor(0.0605, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.8075, 0.8075, 0.8075, 0.8075, 0.8075, 0.8075, 0.8075, 0.8075, 0.8075,\n",
      "        0.8075, 0.8075, 0.8075, 0.8075, 0.8075, 0.8075, 0.8075, 0.8075, 0.8075,\n",
      "        0.8075, 0.8075, 0.8075, 0.8075, 0.8075, 0.8075, 0.3942]) return=  111874.68821578428\n",
      "probs of actions:  tensor([0.3709, 0.2137, 0.2994, 0.3707, 0.2204, 0.1786, 0.3582, 0.0190, 0.2927,\n",
      "        0.3360, 0.3653, 0.2350, 0.3832, 0.1780, 0.3652, 0.2011, 0.1929, 0.2182,\n",
      "        0.0257, 0.2460, 0.3083, 0.1934, 0.2999, 0.0109, 0.9932],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5048, 0.5451, 0.5831, 0.5434, 0.5128, 0.4952, 0.4839, 0.4771, 0.4374,\n",
      "        0.4362, 0.4353, 0.4329, 0.4374, 0.4345, 0.4386, 0.4435, 0.4161, 0.3962,\n",
      "        0.3790, 0.3861, 0.3677, 0.3817, 0.3986, 0.4109, 0.4100])\n",
      "finalReturns:  tensor([0.0134, 0.0159])\n",
      "----------------------------------------\n",
      "iter  0  stage  22  ep  99999   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([ 9, 11, 15, 15, 15, 15, 13, 11, 11, 11,  8, 11,  9, 16, 10, 11, 11, 14,\n",
      "        11,  8,  0,  9, 15, 15,  0])\n",
      "loss=  tensor(0.2034, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2501, 1.2501, 1.2501, 1.2501, 1.2501, 1.2501, 1.2501, 1.2501, 1.2501,\n",
      "        1.2501, 1.2501, 1.2501, 1.2501, 1.2501, 1.2501, 1.2501, 1.2501, 1.2501,\n",
      "        1.2501, 1.2501, 1.2501, 1.2501, 1.2501, 0.8086, 0.3944]) return=  123413.1590088095\n",
      "probs of actions:  tensor([0.1262, 0.2931, 0.2773, 0.2741, 0.2788, 0.2340, 0.0820, 0.3281, 0.2858,\n",
      "        0.3215, 0.0544, 0.3065, 0.1199, 0.0377, 0.0284, 0.3071, 0.3022, 0.0157,\n",
      "        0.3336, 0.0777, 0.0206, 0.1411, 0.4354, 0.3270, 0.9987],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.5448, 0.5776, 0.5443, 0.5482, 0.5390, 0.5378, 0.5301, 0.5136,\n",
      "        0.5014, 0.4980, 0.4750, 0.4766, 0.4504, 0.4838, 0.4741, 0.4720, 0.4629,\n",
      "        0.4796, 0.4818, 0.4751, 0.4305, 0.4190, 0.4414, 0.4811])\n",
      "finalReturns:  tensor([0.0914, 0.1139, 0.0867])\n",
      "----------------------------------------\n",
      "iter  0  stage  21  ep  99999   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([15, 15, 15, 15,  8, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0145, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.5548, 1.5548, 1.5548, 1.5548, 1.5548, 1.5548, 1.5548, 1.5548, 1.5548,\n",
      "        1.5548, 1.5548, 1.5548, 1.5548, 1.5548, 1.5548, 1.5548, 1.5548, 1.5548,\n",
      "        1.5548, 1.5548, 1.5548, 1.5548, 1.1058, 0.7056, 0.3403]) return=  111734.54075265459\n",
      "probs of actions:  tensor([0.9539, 0.9634, 0.9569, 0.9437, 0.0011, 0.9408, 0.9508, 0.9561, 0.9347,\n",
      "        0.9630, 0.9612, 0.9563, 0.9605, 0.9461, 0.9509, 0.9305, 0.9621, 0.9472,\n",
      "        0.9524, 0.9441, 0.9524, 0.9738, 0.9814, 0.9642, 0.9999],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5570, 0.5228, 0.4978, 0.4956, 0.4418, 0.4379, 0.4350, 0.4329,\n",
      "        0.4312, 0.4300, 0.4291, 0.4284, 0.4279, 0.4275, 0.4273, 0.4270, 0.4269,\n",
      "        0.4268, 0.4267, 0.4266, 0.4266, 0.4265, 0.4265, 0.4490])\n",
      "finalReturns:  tensor([0.1737, 0.1962, 0.1698, 0.1087])\n",
      "----------------------------------------\n",
      "iter  0  stage  20  ep  99999   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0103, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.8779, 1.8779, 1.8779, 1.8779, 1.8779, 1.8779, 1.8779, 1.8779, 1.8779,\n",
      "        1.8779, 1.8779, 1.8779, 1.8779, 1.8779, 1.8779, 1.8779, 1.8779, 1.8779,\n",
      "        1.8779, 1.8779, 1.8779, 1.4285, 1.0281, 0.6626, 0.3222]) return=  112524.66365021843\n",
      "probs of actions:  tensor([0.9843, 0.9883, 0.9852, 0.9815, 0.9834, 0.9801, 0.9822, 0.9850, 0.9790,\n",
      "        0.9890, 0.9879, 0.9852, 0.9864, 0.9812, 0.9827, 0.9752, 0.9875, 0.9837,\n",
      "        0.9836, 0.9815, 0.9887, 0.9899, 0.9930, 0.9900, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5570, 0.5228, 0.4978, 0.4795, 0.4659, 0.4559, 0.4484, 0.4429,\n",
      "        0.4387, 0.4356, 0.4333, 0.4316, 0.4303, 0.4293, 0.4286, 0.4280, 0.4276,\n",
      "        0.4273, 0.4271, 0.4269, 0.4268, 0.4267, 0.4266, 0.4491])\n",
      "finalReturns:  tensor([0.2781, 0.3006, 0.2743, 0.2131, 0.1269])\n",
      "----------------------------------------\n",
      "iter  0  stage  19  ep  82452   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0011, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.1872, 2.1872, 2.1872, 2.1872, 2.1872, 2.1872, 2.1872, 2.1872, 2.1872,\n",
      "        2.1872, 2.1872, 2.1872, 2.1872, 2.1872, 2.1872, 2.1872, 2.1872, 2.1872,\n",
      "        2.1872, 2.1872, 1.7377, 1.3371, 0.9715, 0.6310, 0.3088]) return=  112524.66365021843\n",
      "probs of actions:  tensor([0.9987, 0.9991, 0.9988, 0.9984, 0.9986, 0.9982, 0.9984, 0.9987, 0.9981,\n",
      "        0.9991, 0.9991, 0.9987, 0.9989, 0.9983, 0.9985, 0.9978, 0.9990, 0.9986,\n",
      "        0.9985, 0.9990, 0.9993, 0.9994, 0.9998, 0.9996, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5570, 0.5228, 0.4978, 0.4795, 0.4659, 0.4559, 0.4484, 0.4429,\n",
      "        0.4387, 0.4356, 0.4333, 0.4316, 0.4303, 0.4293, 0.4286, 0.4280, 0.4276,\n",
      "        0.4273, 0.4271, 0.4269, 0.4268, 0.4267, 0.4266, 0.4491])\n",
      "finalReturns:  tensor([0.3959, 0.4184, 0.3921, 0.3309, 0.2447, 0.1402])\n",
      "----------------------------------------\n",
      "iter  0  stage  18  ep  358   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0017, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.4870, 2.4870, 2.4870, 2.4870, 2.4870, 2.4870, 2.4870, 2.4870, 2.4870,\n",
      "        2.4870, 2.4870, 2.4870, 2.4870, 2.4870, 2.4870, 2.4870, 2.4870, 2.4870,\n",
      "        2.4870, 2.0371, 1.6364, 1.2707, 0.9301, 0.6079, 0.2990]) return=  112524.66365021843\n",
      "probs of actions:  tensor([0.9989, 0.9992, 0.9989, 0.9986, 0.9988, 0.9984, 0.9986, 0.9989, 0.9984,\n",
      "        0.9992, 0.9992, 0.9989, 0.9990, 0.9985, 0.9986, 0.9981, 0.9991, 0.9988,\n",
      "        0.9990, 0.9991, 0.9994, 0.9994, 0.9998, 0.9996, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5570, 0.5228, 0.4978, 0.4795, 0.4659, 0.4559, 0.4484, 0.4429,\n",
      "        0.4387, 0.4356, 0.4333, 0.4316, 0.4303, 0.4293, 0.4286, 0.4280, 0.4276,\n",
      "        0.4273, 0.4271, 0.4269, 0.4268, 0.4267, 0.4266, 0.4491])\n",
      "finalReturns:  tensor([0.5235, 0.5460, 0.5197, 0.4585, 0.3722, 0.2678, 0.1501])\n",
      "----------------------------------------\n",
      "iter  0  stage  17  ep  94   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0026, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.7797, 2.7797, 2.7797, 2.7797, 2.7797, 2.7797, 2.7797, 2.7797, 2.7797,\n",
      "        2.7797, 2.7797, 2.7797, 2.7797, 2.7797, 2.7797, 2.7797, 2.7797, 2.7797,\n",
      "        2.3296, 1.9286, 1.5627, 1.2221, 0.8997, 0.5908, 0.2918]) return=  112524.66365021843\n",
      "probs of actions:  tensor([0.9989, 0.9992, 0.9990, 0.9987, 0.9988, 0.9985, 0.9987, 0.9989, 0.9984,\n",
      "        0.9993, 0.9992, 0.9990, 0.9991, 0.9986, 0.9987, 0.9982, 0.9992, 0.9990,\n",
      "        0.9991, 0.9992, 0.9994, 0.9995, 0.9998, 0.9996, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5570, 0.5228, 0.4978, 0.4795, 0.4659, 0.4559, 0.4484, 0.4429,\n",
      "        0.4387, 0.4356, 0.4333, 0.4316, 0.4303, 0.4293, 0.4286, 0.4280, 0.4276,\n",
      "        0.4273, 0.4271, 0.4269, 0.4268, 0.4267, 0.4266, 0.4491])\n",
      "finalReturns:  tensor([0.6584, 0.6809, 0.6545, 0.5933, 0.5071, 0.4026, 0.2849, 0.1573])\n",
      "----------------------------------------\n",
      "iter  0  stage  16  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0036, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.0674, 3.0674, 3.0674, 3.0674, 3.0674, 3.0674, 3.0674, 3.0674, 3.0674,\n",
      "        3.0674, 3.0674, 3.0674, 3.0674, 3.0674, 3.0674, 3.0674, 3.0674, 2.6169,\n",
      "        2.2157, 1.8496, 1.5087, 1.1863, 0.8773, 0.5782, 0.2864]) return=  112524.66365021843\n",
      "probs of actions:  tensor([0.9989, 0.9992, 0.9990, 0.9987, 0.9988, 0.9985, 0.9987, 0.9989, 0.9984,\n",
      "        0.9993, 0.9992, 0.9990, 0.9991, 0.9986, 0.9987, 0.9982, 0.9992, 0.9990,\n",
      "        0.9991, 0.9992, 0.9994, 0.9995, 0.9998, 0.9996, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5570, 0.5228, 0.4978, 0.4795, 0.4659, 0.4559, 0.4484, 0.4429,\n",
      "        0.4387, 0.4356, 0.4333, 0.4316, 0.4303, 0.4293, 0.4286, 0.4280, 0.4276,\n",
      "        0.4273, 0.4271, 0.4269, 0.4268, 0.4267, 0.4266, 0.4491])\n",
      "finalReturns:  tensor([0.7987, 0.8212, 0.7948, 0.7336, 0.6473, 0.5429, 0.4251, 0.2975, 0.1627])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  0  stage  15  ep  305   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0035, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.3516, 3.3516, 3.3516, 3.3516, 3.3516, 3.3516, 3.3516, 3.3516, 3.3516,\n",
      "        3.3516, 3.3516, 3.3516, 3.3516, 3.3516, 3.3516, 3.3516, 2.9005, 2.4989,\n",
      "        2.1326, 1.7916, 1.4690, 1.1598, 0.8606, 0.5688, 0.2824]) return=  112524.66365021843\n",
      "probs of actions:  tensor([0.9992, 0.9994, 0.9993, 0.9990, 0.9991, 0.9989, 0.9990, 0.9992, 0.9988,\n",
      "        0.9995, 0.9994, 0.9992, 0.9993, 0.9990, 0.9991, 0.9990, 0.9995, 0.9994,\n",
      "        0.9994, 0.9994, 0.9996, 0.9996, 0.9999, 0.9997, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5570, 0.5228, 0.4978, 0.4795, 0.4659, 0.4559, 0.4484, 0.4429,\n",
      "        0.4387, 0.4356, 0.4333, 0.4316, 0.4303, 0.4293, 0.4286, 0.4280, 0.4276,\n",
      "        0.4273, 0.4271, 0.4269, 0.4268, 0.4267, 0.4266, 0.4491])\n",
      "finalReturns:  tensor([0.9431, 0.9656, 0.9392, 0.8779, 0.7916, 0.6871, 0.5693, 0.4417, 0.3069,\n",
      "        0.1667])\n",
      "----------------------------------------\n",
      "iter  0  stage  14  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0049, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.6335, 3.6335, 3.6335, 3.6335, 3.6335, 3.6335, 3.6335, 3.6335, 3.6335,\n",
      "        3.6335, 3.6335, 3.6335, 3.6335, 3.6335, 3.6335, 3.1817, 2.7796, 2.4128,\n",
      "        2.0716, 1.7488, 1.4395, 1.1402, 0.8483, 0.5618, 0.2794]) return=  112524.66365021843\n",
      "probs of actions:  tensor([0.9992, 0.9994, 0.9993, 0.9990, 0.9992, 0.9989, 0.9990, 0.9992, 0.9988,\n",
      "        0.9995, 0.9994, 0.9992, 0.9993, 0.9990, 0.9991, 0.9990, 0.9995, 0.9994,\n",
      "        0.9994, 0.9994, 0.9996, 0.9996, 0.9999, 0.9997, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5570, 0.5228, 0.4978, 0.4795, 0.4659, 0.4559, 0.4484, 0.4429,\n",
      "        0.4387, 0.4356, 0.4333, 0.4316, 0.4303, 0.4293, 0.4286, 0.4280, 0.4276,\n",
      "        0.4273, 0.4271, 0.4269, 0.4268, 0.4267, 0.4266, 0.4491])\n",
      "finalReturns:  tensor([1.0905, 1.1130, 1.0866, 1.0253, 0.9389, 0.8344, 0.7166, 0.5889, 0.4541,\n",
      "        0.3139, 0.1697])\n",
      "----------------------------------------\n",
      "iter  0  stage  13  ep  9   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0065, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.9141, 3.9141, 3.9141, 3.9141, 3.9141, 3.9141, 3.9141, 3.9141, 3.9141,\n",
      "        3.9141, 3.9141, 3.9141, 3.9141, 3.9141, 3.4613, 3.0585, 2.6912, 2.3496,\n",
      "        2.0265, 1.7171, 1.4177, 1.1256, 0.8391, 0.5566, 0.2772]) return=  112524.66365021843\n",
      "probs of actions:  tensor([0.9992, 0.9994, 0.9993, 0.9991, 0.9992, 0.9989, 0.9990, 0.9992, 0.9988,\n",
      "        0.9995, 0.9995, 0.9992, 0.9993, 0.9990, 0.9991, 0.9990, 0.9995, 0.9994,\n",
      "        0.9994, 0.9994, 0.9996, 0.9996, 0.9999, 0.9997, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5570, 0.5228, 0.4978, 0.4795, 0.4659, 0.4559, 0.4484, 0.4429,\n",
      "        0.4387, 0.4356, 0.4333, 0.4316, 0.4303, 0.4293, 0.4286, 0.4280, 0.4276,\n",
      "        0.4273, 0.4271, 0.4269, 0.4268, 0.4267, 0.4266, 0.4491])\n",
      "finalReturns:  tensor([1.2403, 1.2628, 1.2363, 1.1749, 1.0885, 0.9839, 0.8661, 0.7384, 0.6036,\n",
      "        0.4633, 0.3191, 0.1719])\n",
      "----------------------------------------\n",
      "iter  0  stage  12  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0079, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.1941, 4.1941, 4.1941, 4.1941, 4.1941, 4.1941, 4.1941, 4.1941, 4.1941,\n",
      "        4.1941, 4.1941, 4.1941, 4.1941, 3.7400, 3.3363, 2.9684, 2.6263, 2.3029,\n",
      "        1.9932, 1.6936, 1.4014, 1.1147, 0.8322, 0.5527, 0.2755]) return=  112524.66365021843\n",
      "probs of actions:  tensor([0.9992, 0.9994, 0.9993, 0.9991, 0.9992, 0.9989, 0.9990, 0.9992, 0.9988,\n",
      "        0.9995, 0.9995, 0.9992, 0.9993, 0.9990, 0.9991, 0.9990, 0.9995, 0.9994,\n",
      "        0.9994, 0.9994, 0.9996, 0.9996, 0.9999, 0.9997, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5570, 0.5228, 0.4978, 0.4795, 0.4659, 0.4559, 0.4484, 0.4429,\n",
      "        0.4387, 0.4356, 0.4333, 0.4316, 0.4303, 0.4293, 0.4286, 0.4280, 0.4276,\n",
      "        0.4273, 0.4271, 0.4269, 0.4268, 0.4267, 0.4266, 0.4491])\n",
      "finalReturns:  tensor([1.3918, 1.4143, 1.3877, 1.3263, 1.2399, 1.1352, 1.0173, 0.8896, 0.7547,\n",
      "        0.6144, 0.4702, 0.3230, 0.1736])\n",
      "----------------------------------------\n",
      "iter  0  stage  11  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0095, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.4745, 4.4745, 4.4745, 4.4745, 4.4745, 4.4745, 4.4745, 4.4745, 4.4745,\n",
      "        4.4745, 4.4745, 4.4745, 4.0187, 3.6137, 3.2449, 2.9022, 2.5783, 2.2683,\n",
      "        1.9684, 1.6760, 1.3893, 1.1066, 0.8270, 0.5498, 0.2742]) return=  112524.66365021843\n",
      "probs of actions:  tensor([0.9992, 0.9994, 0.9993, 0.9991, 0.9992, 0.9989, 0.9990, 0.9992, 0.9988,\n",
      "        0.9995, 0.9995, 0.9992, 0.9993, 0.9990, 0.9991, 0.9990, 0.9996, 0.9994,\n",
      "        0.9994, 0.9994, 0.9996, 0.9996, 0.9999, 0.9997, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5570, 0.5228, 0.4978, 0.4795, 0.4659, 0.4559, 0.4484, 0.4429,\n",
      "        0.4387, 0.4356, 0.4333, 0.4316, 0.4303, 0.4293, 0.4286, 0.4280, 0.4276,\n",
      "        0.4273, 0.4271, 0.4269, 0.4268, 0.4267, 0.4266, 0.4491])\n",
      "finalReturns:  tensor([1.5447, 1.5672, 1.5406, 1.4791, 1.3925, 1.2878, 1.1698, 1.0421, 0.9071,\n",
      "        0.7668, 0.6226, 0.4753, 0.3259, 0.1748])\n",
      "----------------------------------------\n",
      "iter  0  stage  10  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0108, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.7560, 4.7560, 4.7560, 4.7560, 4.7560, 4.7560, 4.7560, 4.7560, 4.7560,\n",
      "        4.7560, 4.7560, 4.2979, 3.8913, 3.5214, 3.1778, 2.8533, 2.5428, 2.2426,\n",
      "        1.9500, 1.6630, 1.3802, 1.1005, 0.8232, 0.5476, 0.2733]) return=  112524.66365021843\n",
      "probs of actions:  tensor([0.9992, 0.9994, 0.9993, 0.9991, 0.9992, 0.9989, 0.9990, 0.9992, 0.9988,\n",
      "        0.9995, 0.9995, 0.9992, 0.9993, 0.9990, 0.9991, 0.9990, 0.9996, 0.9994,\n",
      "        0.9994, 0.9994, 0.9996, 0.9996, 0.9999, 0.9997, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5570, 0.5228, 0.4978, 0.4795, 0.4659, 0.4559, 0.4484, 0.4429,\n",
      "        0.4387, 0.4356, 0.4333, 0.4316, 0.4303, 0.4293, 0.4286, 0.4280, 0.4276,\n",
      "        0.4273, 0.4271, 0.4269, 0.4268, 0.4267, 0.4266, 0.4491])\n",
      "finalReturns:  tensor([1.6988, 1.7213, 1.6946, 1.6330, 1.5463, 1.4415, 1.3234, 1.1955, 1.0605,\n",
      "        0.9202, 0.7759, 0.6286, 0.4792, 0.3281, 0.1757])\n",
      "----------------------------------------\n",
      "iter  0  stage  9  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0122, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.0396, 5.0396, 5.0396, 5.0396, 5.0396, 5.0396, 5.0396, 5.0396, 5.0396,\n",
      "        5.0396, 4.5784, 4.1697, 3.7981, 3.4534, 3.1281, 2.8170, 2.5164, 2.2234,\n",
      "        1.9362, 1.6532, 1.3734, 1.0960, 0.8203, 0.5460, 0.2726]) return=  112524.66365021843\n",
      "probs of actions:  tensor([0.9992, 0.9994, 0.9993, 0.9991, 0.9992, 0.9989, 0.9990, 0.9992, 0.9989,\n",
      "        0.9995, 0.9995, 0.9993, 0.9993, 0.9990, 0.9991, 0.9990, 0.9996, 0.9994,\n",
      "        0.9994, 0.9994, 0.9996, 0.9996, 0.9999, 0.9997, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5570, 0.5228, 0.4978, 0.4795, 0.4659, 0.4559, 0.4484, 0.4429,\n",
      "        0.4387, 0.4356, 0.4333, 0.4316, 0.4303, 0.4293, 0.4286, 0.4280, 0.4276,\n",
      "        0.4273, 0.4271, 0.4269, 0.4268, 0.4267, 0.4266, 0.4491])\n",
      "finalReturns:  tensor([1.8539, 1.8764, 1.8496, 1.7878, 1.7009, 1.5959, 1.4777, 1.3498, 1.2147,\n",
      "        1.0743, 0.9299, 0.7827, 0.6332, 0.4821, 0.3297, 0.1764])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  0  stage  8  ep  31   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0130, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.3265, 5.3265, 5.3265, 5.3265, 5.3265, 5.3265, 5.3265, 5.3265, 5.3265,\n",
      "        4.8611, 4.4494, 4.0759, 3.7296, 3.4032, 3.0913, 2.7901, 2.4967, 2.2091,\n",
      "        1.9259, 1.6459, 1.3684, 1.0926, 0.8182, 0.5448, 0.2721]) return=  112524.66365021843\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9991, 0.9992, 0.9990, 0.9991, 0.9993, 0.9990,\n",
      "        0.9995, 0.9995, 0.9994, 0.9994, 0.9992, 0.9992, 0.9991, 0.9996, 0.9995,\n",
      "        0.9995, 0.9995, 0.9996, 0.9997, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5570, 0.5228, 0.4978, 0.4795, 0.4659, 0.4559, 0.4484, 0.4429,\n",
      "        0.4387, 0.4356, 0.4333, 0.4316, 0.4303, 0.4293, 0.4286, 0.4280, 0.4276,\n",
      "        0.4273, 0.4271, 0.4269, 0.4268, 0.4267, 0.4266, 0.4491])\n",
      "finalReturns:  tensor([2.0099, 2.0324, 2.0054, 1.9434, 1.8563, 1.7511, 1.6327, 1.5047, 1.3695,\n",
      "        1.2290, 1.0846, 0.9372, 0.7877, 0.6366, 0.4842, 0.3309, 0.1770])\n",
      "----------------------------------------\n",
      "iter  0  stage  7  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0149, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.6180, 5.6180, 5.6180, 5.6180, 5.6180, 5.6180, 5.6180, 5.6180, 5.1471,\n",
      "        4.7315, 4.3551, 4.0068, 3.6790, 3.3660, 3.0639, 2.7700, 2.4820, 2.1984,\n",
      "        1.9182, 1.6405, 1.3646, 1.0900, 0.8166, 0.5438, 0.2717]) return=  112524.66365021843\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9991, 0.9992, 0.9990, 0.9991, 0.9993, 0.9990,\n",
      "        0.9995, 0.9995, 0.9994, 0.9994, 0.9992, 0.9992, 0.9991, 0.9996, 0.9995,\n",
      "        0.9995, 0.9995, 0.9996, 0.9997, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5570, 0.5228, 0.4978, 0.4795, 0.4659, 0.4559, 0.4484, 0.4429,\n",
      "        0.4387, 0.4356, 0.4333, 0.4316, 0.4303, 0.4293, 0.4286, 0.4280, 0.4276,\n",
      "        0.4273, 0.4271, 0.4269, 0.4268, 0.4267, 0.4266, 0.4491])\n",
      "finalReturns:  tensor([2.1668, 2.1893, 2.1621, 2.0997, 2.0124, 1.9069, 1.7883, 1.6601, 1.5248,\n",
      "        1.3842, 1.2397, 1.0923, 0.9427, 0.7915, 0.6391, 0.4858, 0.3318, 0.1774])\n",
      "----------------------------------------\n",
      "iter  0  stage  6  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0173, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.9160, 5.9160, 5.9160, 5.9160, 5.9160, 5.9160, 5.9160, 5.4376, 5.0168,\n",
      "        4.6366, 4.2857, 3.9558, 3.6414, 3.3383, 3.0435, 2.7550, 2.4710, 2.1905,\n",
      "        1.9125, 1.6364, 1.3617, 1.0881, 0.8153, 0.5432, 0.2714]) return=  112524.66365021843\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9991, 0.9992, 0.9990, 0.9991, 0.9993, 0.9990,\n",
      "        0.9996, 0.9995, 0.9994, 0.9994, 0.9992, 0.9992, 0.9991, 0.9996, 0.9995,\n",
      "        0.9995, 0.9995, 0.9996, 0.9997, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5570, 0.5228, 0.4978, 0.4795, 0.4659, 0.4559, 0.4484, 0.4429,\n",
      "        0.4387, 0.4356, 0.4333, 0.4316, 0.4303, 0.4293, 0.4286, 0.4280, 0.4276,\n",
      "        0.4273, 0.4271, 0.4269, 0.4268, 0.4267, 0.4266, 0.4491])\n",
      "finalReturns:  tensor([2.3247, 2.3472, 2.3197, 2.2569, 2.1692, 2.0634, 1.9445, 1.8160, 1.6805,\n",
      "        1.5398, 1.3951, 1.2477, 1.0980, 0.9468, 0.7944, 0.6410, 0.4870, 0.3325,\n",
      "        0.1777])\n",
      "----------------------------------------\n",
      "iter  0  stage  5  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0201, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.2229, 6.2229, 6.2229, 6.2229, 6.2229, 6.2229, 5.7345, 5.3065, 4.9214,\n",
      "        4.5668, 4.2343, 3.9180, 3.6135, 3.3177, 3.0283, 2.7438, 2.4628, 2.1845,\n",
      "        1.9082, 1.6333, 1.3596, 1.0867, 0.8144, 0.5426, 0.2712]) return=  112524.66365021843\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9991, 0.9993, 0.9990, 0.9991, 0.9993, 0.9990,\n",
      "        0.9996, 0.9995, 0.9994, 0.9994, 0.9992, 0.9992, 0.9991, 0.9996, 0.9995,\n",
      "        0.9995, 0.9995, 0.9996, 0.9997, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5570, 0.5228, 0.4978, 0.4795, 0.4659, 0.4559, 0.4484, 0.4429,\n",
      "        0.4387, 0.4356, 0.4333, 0.4316, 0.4303, 0.4293, 0.4286, 0.4280, 0.4276,\n",
      "        0.4273, 0.4271, 0.4269, 0.4268, 0.4267, 0.4266, 0.4491])\n",
      "finalReturns:  tensor([2.4838, 2.5063, 2.4783, 2.4151, 2.3268, 2.2205, 2.1012, 1.9724, 1.8367,\n",
      "        1.6957, 1.5510, 1.4034, 1.2536, 1.1023, 0.9498, 0.7965, 0.6425, 0.4879,\n",
      "        0.3330, 0.1779])\n",
      "----------------------------------------\n",
      "iter  0  stage  4  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0225, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.5419, 6.5419, 6.5419, 6.5419, 6.5419, 6.0399, 5.6025, 5.2106, 4.8512,\n",
      "        4.5152, 4.1963, 3.8899, 3.5927, 3.3023, 3.0169, 2.7354, 2.4567, 2.1800,\n",
      "        1.9049, 1.6310, 1.3580, 1.0856, 0.8138, 0.5423, 0.2710]) return=  112524.66365021843\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9992, 0.9993, 0.9990, 0.9991, 0.9993, 0.9990,\n",
      "        0.9996, 0.9995, 0.9994, 0.9994, 0.9992, 0.9992, 0.9992, 0.9996, 0.9995,\n",
      "        0.9995, 0.9995, 0.9996, 0.9997, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5570, 0.5228, 0.4978, 0.4795, 0.4659, 0.4559, 0.4484, 0.4429,\n",
      "        0.4387, 0.4356, 0.4333, 0.4316, 0.4303, 0.4293, 0.4286, 0.4280, 0.4276,\n",
      "        0.4273, 0.4271, 0.4269, 0.4268, 0.4267, 0.4266, 0.4491])\n",
      "finalReturns:  tensor([2.6442, 2.6667, 2.6382, 2.5743, 2.4853, 2.3784, 2.2586, 2.1294, 1.9932,\n",
      "        1.8521, 1.7071, 1.5593, 1.4095, 1.2581, 1.1056, 0.9521, 0.7981, 0.6435,\n",
      "        0.4886, 0.3334, 0.1780])\n",
      "----------------------------------------\n",
      "iter  0  stage  3  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0253, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.8775, 6.8775, 6.8775, 6.8775, 6.3572, 5.9069, 5.5059, 5.1400, 4.7993,\n",
      "        4.4769, 4.1680, 3.8689, 3.5771, 3.2908, 3.0084, 2.7291, 2.4521, 2.1767,\n",
      "        1.9025, 1.6293, 1.3568, 1.0848, 0.8132, 0.5420, 0.2709]) return=  112524.66365021843\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9992, 0.9993, 0.9990, 0.9991, 0.9993, 0.9990,\n",
      "        0.9996, 0.9995, 0.9994, 0.9995, 0.9992, 0.9992, 0.9992, 0.9996, 0.9995,\n",
      "        0.9995, 0.9995, 0.9996, 0.9997, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5570, 0.5228, 0.4978, 0.4795, 0.4659, 0.4559, 0.4484, 0.4429,\n",
      "        0.4387, 0.4356, 0.4333, 0.4316, 0.4303, 0.4293, 0.4286, 0.4280, 0.4276,\n",
      "        0.4273, 0.4271, 0.4269, 0.4268, 0.4267, 0.4266, 0.4491])\n",
      "finalReturns:  tensor([2.8065, 2.8290, 2.7997, 2.7348, 2.6449, 2.5372, 2.4167, 2.2869, 2.1503,\n",
      "        2.0088, 1.8636, 1.7156, 1.5656, 1.4141, 1.2615, 1.1080, 0.9539, 0.7993,\n",
      "        0.6443, 0.4891, 0.3337, 0.1782])\n",
      "----------------------------------------\n",
      "iter  0  stage  2  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0276, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.2357, 7.2357, 7.2357, 6.6905, 6.2228, 5.8096, 5.4348, 5.0878, 4.7608,\n",
      "        4.4484, 4.1469, 3.8532, 3.5655, 3.2822, 3.0021, 2.7244, 2.4486, 2.1741,\n",
      "        1.9007, 1.6280, 1.3559, 1.0842, 0.8129, 0.5417, 0.2708]) return=  112524.66365021843\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9992, 0.9993, 0.9990, 0.9991, 0.9993, 0.9990,\n",
      "        0.9996, 0.9995, 0.9994, 0.9995, 0.9992, 0.9992, 0.9992, 0.9996, 0.9995,\n",
      "        0.9995, 0.9995, 0.9996, 0.9997, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5570, 0.5228, 0.4978, 0.4795, 0.4659, 0.4559, 0.4484, 0.4429,\n",
      "        0.4387, 0.4356, 0.4333, 0.4316, 0.4303, 0.4293, 0.4286, 0.4280, 0.4276,\n",
      "        0.4273, 0.4271, 0.4269, 0.4268, 0.4267, 0.4266, 0.4491])\n",
      "finalReturns:  tensor([2.9710, 2.9935, 2.9633, 2.8971, 2.8059, 2.6971, 2.5757, 2.4451, 2.3080,\n",
      "        2.1660, 2.0204, 1.8722, 1.7220, 1.5703, 1.4175, 1.2640, 1.1098, 0.9551,\n",
      "        0.8002, 0.6449, 0.4895, 0.3339, 0.1783])\n",
      "----------------------------------------\n",
      "iter  0  stage  1  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0295, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.6252, 7.6252, 7.0457, 6.5544, 6.1245, 5.7379, 5.3822, 5.0490, 4.7321,\n",
      "        4.4272, 4.1311, 3.8415, 3.5568, 3.2757, 2.9973, 2.7209, 2.4460, 2.1723,\n",
      "        1.8993, 1.6271, 1.3552, 1.0838, 0.8126, 0.5416, 0.2707]) return=  112524.66365021843\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9992, 0.9993, 0.9990, 0.9991, 0.9993, 0.9990,\n",
      "        0.9996, 0.9996, 0.9994, 0.9995, 0.9992, 0.9993, 0.9992, 0.9996, 0.9995,\n",
      "        0.9995, 0.9995, 0.9996, 0.9997, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5570, 0.5228, 0.4978, 0.4795, 0.4659, 0.4559, 0.4484, 0.4429,\n",
      "        0.4387, 0.4356, 0.4333, 0.4316, 0.4303, 0.4293, 0.4286, 0.4280, 0.4276,\n",
      "        0.4273, 0.4271, 0.4269, 0.4268, 0.4267, 0.4266, 0.4491])\n",
      "finalReturns:  tensor([3.1385, 3.1610, 3.1295, 3.0617, 2.9688, 2.8585, 2.7358, 2.6043, 2.4664,\n",
      "        2.3237, 2.1777, 2.0291, 1.8786, 1.7267, 1.5738, 1.4201, 1.2658, 1.1111,\n",
      "        0.9561, 0.8008, 0.6454, 0.4898, 0.3341, 0.1783])\n",
      "----------------------------------------\n",
      "iter  0  stage  0  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0320, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.9425, 7.4312, 6.9074, 6.4547, 6.0520, 5.6848, 5.3432, 5.0202, 4.7108,\n",
      "        4.4114, 4.1193, 3.8328, 3.5503, 3.2709, 2.9937, 2.7183, 2.4441, 2.1709,\n",
      "        1.8983, 1.6263, 1.3547, 1.0834, 0.8124, 0.5415, 0.2707]) return=  112524.66365021843\n",
      "probs of actions:  tensor([0.9993, 0.9995, 0.9994, 0.9992, 0.9993, 0.9991, 0.9992, 0.9993, 0.9990,\n",
      "        0.9996, 0.9996, 0.9994, 0.9995, 0.9992, 0.9993, 0.9992, 0.9996, 0.9995,\n",
      "        0.9995, 0.9995, 0.9996, 0.9997, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5570, 0.5228, 0.4978, 0.4795, 0.4659, 0.4559, 0.4484, 0.4429,\n",
      "        0.4387, 0.4356, 0.4333, 0.4316, 0.4303, 0.4293, 0.4286, 0.4280, 0.4276,\n",
      "        0.4273, 0.4271, 0.4269, 0.4268, 0.4267, 0.4266, 0.4491])\n",
      "finalReturns:  tensor([3.3100, 3.3325, 3.2993, 3.2292, 3.1341, 3.0218, 2.8975, 2.7647, 2.6257,\n",
      "        2.4822, 2.3355, 2.1864, 2.0356, 1.8834, 1.7303, 1.5764, 1.4221, 1.2673,\n",
      "        1.1122, 0.9568, 0.8013, 0.6457, 0.4900, 0.3342, 0.1784])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1682929682 saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[683269, 'tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])', 112524.66365021843, 93537.02666495038, 0.03203679621219635, 1e-05, 1, 0, 'tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\\n        15, 15, 15, 15, 15, 15,  0])', '[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\\n 1.]', '0,[1e-05,1][1, 10000, 1, 1],1682929682', 25, 50, 161287.58344151574, 184572.3091501231, 73453.3647159636, 135545.41866666666, 132647.75466666667, 112524.66365021843, 112524.66365021843, 117051.59338141435, 117058.92268888396, 85375.85726043607, 112524.66365021843, 117061.61105796597]\n",
      "policy reset\n",
      "----------------------------------------\n",
      "iter  1  stage  24  ep  99999   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.4723, 0.4723, 0.4723, 0.4723, 0.4723, 0.4723, 0.4723, 0.4723, 0.4723,\n",
      "        0.4723, 0.4723, 0.4723, 0.4723, 0.4723, 0.4723, 0.4723, 0.4723, 0.4723,\n",
      "        0.4723, 0.4723, 0.4723, 0.4723, 0.4723, 0.4723, 0.4723]) return=  133459.1809763887\n",
      "probs of actions:  tensor([0.9112, 0.0541, 0.9293, 0.9027, 0.9259, 0.9068, 0.9231, 0.9408, 0.9248,\n",
      "        0.9224, 0.9068, 0.9271, 0.9228, 0.9337, 0.9150, 0.0406, 0.9252, 0.9250,\n",
      "        0.8971, 0.9140, 0.9118, 0.9137, 0.9213, 0.9058, 0.9889],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5237, 0.5370, 0.5433, 0.5481, 0.5517, 0.5544, 0.5564, 0.5579,\n",
      "        0.5591, 0.5599, 0.5606, 0.5610, 0.5614, 0.5617, 0.5618, 0.5658, 0.5390,\n",
      "        0.5193, 0.5047, 0.4940, 0.4860, 0.4800, 0.4756, 0.4723])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  1  stage  23  ep  99999   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([8, 8, 0, 7, 7, 6, 1, 6, 1, 0, 7, 0, 7, 7, 0, 0, 9, 7, 8, 7, 0, 7, 7, 7,\n",
      "        0])\n",
      "loss=  tensor(0.0095, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.8497, 0.8497, 0.8497, 0.8497, 0.8497, 0.8497, 0.8497, 0.8497, 0.8497,\n",
      "        0.8497, 0.8497, 0.8497, 0.8497, 0.8497, 0.8497, 0.8497, 0.8497, 0.8497,\n",
      "        0.8497, 0.8497, 0.8497, 0.8497, 0.8497, 0.8497, 0.4157]) return=  112339.1163673332\n",
      "probs of actions:  tensor([0.0652, 0.0703, 0.3638, 0.4614, 0.4650, 0.0731, 0.0180, 0.0588, 0.0144,\n",
      "        0.3030, 0.4144, 0.2663, 0.4258, 0.3902, 0.3342, 0.2061, 0.0289, 0.4026,\n",
      "        0.0797, 0.4148, 0.0888, 0.5708, 0.4182, 0.5905, 0.9986],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5048, 0.5468, 0.5857, 0.5202, 0.5014, 0.4889, 0.4787, 0.4481, 0.4484,\n",
      "        0.4296, 0.4075, 0.4222, 0.4021, 0.4132, 0.4265, 0.4101, 0.3900, 0.4128,\n",
      "        0.4198, 0.4311, 0.4400, 0.4152, 0.4231, 0.4291, 0.4385])\n",
      "finalReturns:  tensor([0.0180, 0.0229])\n",
      "----------------------------------------\n",
      "iter  1  stage  22  ep  99999   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 18, 18, 18, 16, 18, 18, 14, 18, 18, 21, 15, 18, 18, 18, 11, 18, 19,\n",
      "        18, 18, 11, 18, 18, 11,  0])\n",
      "loss=  tensor(0.2019, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.3261, 1.3261, 1.3261, 1.3261, 1.3261, 1.3261, 1.3261, 1.3261, 1.3261,\n",
      "        1.3261, 1.3261, 1.3261, 1.3261, 1.3261, 1.3261, 1.3261, 1.3261, 1.3261,\n",
      "        1.3261, 1.3261, 1.3261, 1.3261, 1.3261, 0.8335, 0.3968]) return=  122146.33270204287\n",
      "probs of actions:  tensor([0.1114, 0.5005, 0.4775, 0.4623, 0.0738, 0.4914, 0.4920, 0.0551, 0.5121,\n",
      "        0.4809, 0.0076, 0.0585, 0.4930, 0.4906, 0.5019, 0.1138, 0.4775, 0.0571,\n",
      "        0.4833, 0.4679, 0.1263, 0.5121, 0.6630, 0.2893, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5320, 0.5428, 0.5274, 0.5228, 0.5002, 0.4957, 0.5052, 0.4755,\n",
      "        0.4772, 0.4668, 0.5002, 0.4775, 0.4788, 0.4797, 0.5006, 0.4561, 0.4589,\n",
      "        0.4711, 0.4739, 0.4963, 0.4529, 0.4602, 0.4860, 0.4778])\n",
      "finalReturns:  tensor([0.0979, 0.1303, 0.0809])\n",
      "----------------------------------------\n",
      "iter  1  stage  21  ep  99999   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([18, 18, 18, 18, 19, 18, 17, 18, 18, 18, 16, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 16, 16, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0733, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.3379, 1.3379, 1.3379, 1.3379, 1.3379, 1.3379, 1.3379, 1.3379, 1.3379,\n",
      "        1.3379, 1.3379, 1.3379, 1.3379, 1.3379, 1.3379, 1.3379, 1.3379, 1.3379,\n",
      "        1.3379, 1.3379, 1.3379, 1.3379, 0.9393, 0.5929, 0.2833]) return=  102188.08265443008\n",
      "probs of actions:  tensor([0.8722, 0.8706, 0.8676, 0.8419, 0.0511, 0.8602, 0.0068, 0.8830, 0.8689,\n",
      "        0.8490, 0.0319, 0.8320, 0.8612, 0.8626, 0.8607, 0.8587, 0.8471, 0.8518,\n",
      "        0.8542, 0.0333, 0.0318, 0.9187, 0.9270, 0.7905, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5101, 0.4751, 0.4460, 0.4344, 0.4233, 0.4056, 0.3984,\n",
      "        0.3931, 0.3959, 0.3796, 0.3790, 0.3786, 0.3782, 0.3780, 0.3778, 0.3776,\n",
      "        0.3775, 0.3842, 0.3778, 0.3662, 0.3689, 0.3710, 0.4049])\n",
      "finalReturns:  tensor([0.1732, 0.2056, 0.1830, 0.1216])\n",
      "----------------------------------------\n",
      "iter  1  stage  20  ep  99999   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0276, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.6350, 1.6350, 1.6350, 1.6350, 1.6350, 1.6350, 1.6350, 1.6350, 1.6350,\n",
      "        1.6350, 1.6350, 1.6350, 1.6350, 1.6350, 1.6350, 1.6350, 1.6350, 1.6350,\n",
      "        1.6350, 1.6350, 1.6350, 1.2248, 0.8702, 0.5549, 0.2674]) return=  102604.52812037413\n",
      "probs of actions:  tensor([0.9710, 0.9696, 0.9702, 0.9607, 0.9690, 0.9665, 0.9702, 0.9737, 0.9688,\n",
      "        0.9612, 0.9591, 0.9589, 0.9674, 0.9678, 0.9658, 0.9667, 0.9641, 0.9647,\n",
      "        0.9651, 0.9602, 0.9635, 0.9843, 0.9838, 0.9696, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5101, 0.4751, 0.4497, 0.4310, 0.4172, 0.4071, 0.3995,\n",
      "        0.3939, 0.3897, 0.3865, 0.3842, 0.3824, 0.3811, 0.3801, 0.3794, 0.3789,\n",
      "        0.3784, 0.3781, 0.3779, 0.3777, 0.3776, 0.3775, 0.4098])\n",
      "finalReturns:  tensor([0.2855, 0.3179, 0.2947, 0.2324, 0.1424])\n",
      "----------------------------------------\n",
      "iter  1  stage  19  ep  99999   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        16, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0543, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.8695, 1.8695, 1.8695, 1.8695, 1.8695, 1.8695, 1.8695, 1.8695, 1.8695,\n",
      "        1.8695, 1.8695, 1.8695, 1.8695, 1.8695, 1.8695, 1.8695, 1.8695, 1.8695,\n",
      "        1.8695, 1.8695, 1.4654, 1.1152, 0.8028, 0.5175, 0.2517]) return=  102462.48329383403\n",
      "probs of actions:  tensor([0.9692, 0.9680, 0.9683, 0.9593, 0.9679, 0.9651, 0.9692, 0.9716, 0.9667,\n",
      "        0.9584, 0.9585, 0.9576, 0.9667, 0.9662, 0.9652, 0.9653, 0.9625, 0.9646,\n",
      "        0.0057, 0.9604, 0.9625, 0.9790, 0.9841, 0.9763, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5101, 0.4751, 0.4497, 0.4310, 0.4172, 0.4071, 0.3995,\n",
      "        0.3939, 0.3897, 0.3865, 0.3842, 0.3824, 0.3811, 0.3801, 0.3794, 0.3789,\n",
      "        0.3852, 0.3717, 0.3731, 0.3741, 0.3749, 0.3755, 0.4083])\n",
      "finalReturns:  tensor([0.4081, 0.4405, 0.4176, 0.3558, 0.2662, 0.1566])\n",
      "----------------------------------------\n",
      "iter  1  stage  18  ep  99999   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 19, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(2.0158, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.1318, 2.1318, 2.1318, 2.1318, 2.1318, 2.1318, 2.1318, 2.1318, 2.1318,\n",
      "        2.1318, 2.1318, 2.1318, 2.1318, 2.1318, 2.1318, 2.1318, 2.1318, 2.1318,\n",
      "        2.1318, 1.7209, 1.3661, 1.0504, 0.7628, 0.4952, 0.2423]) return=  102665.3466452767\n",
      "probs of actions:  tensor([0.9743, 0.9729, 0.9736, 0.9664, 0.9736, 0.9710, 0.9747, 0.9763, 0.9718,\n",
      "        0.9644, 0.9661, 0.9645, 0.9721, 0.9718, 0.9708, 0.9715, 0.9687, 0.9713,\n",
      "        0.9729, 0.0345, 0.9690, 0.9847, 0.9871, 0.9855, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5101, 0.4751, 0.4497, 0.4310, 0.4172, 0.4071, 0.3995,\n",
      "        0.3939, 0.3897, 0.3865, 0.3842, 0.3824, 0.3811, 0.3801, 0.3794, 0.3789,\n",
      "        0.3784, 0.3744, 0.3811, 0.3801, 0.3794, 0.3788, 0.4108])\n",
      "finalReturns:  tensor([0.5514, 0.5838, 0.5642, 0.4988, 0.4063, 0.2945, 0.1686])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  17  ep  99999   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.2928, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.3675, 2.3675, 2.3675, 2.3675, 2.3675, 2.3675, 2.3675, 2.3675, 2.3675,\n",
      "        2.3675, 2.3675, 2.3675, 2.3675, 2.3675, 2.3675, 2.3675, 2.3675, 2.3675,\n",
      "        1.9563, 1.6011, 1.2853, 0.9975, 0.7298, 0.4768, 0.2344]) return=  102604.52812037413\n",
      "probs of actions:  tensor([0.9312, 0.9290, 0.9287, 0.9162, 0.9312, 0.9262, 0.9345, 0.9344, 0.9254,\n",
      "        0.9113, 0.9177, 0.9099, 0.9296, 0.9272, 0.9277, 0.9261, 0.9193, 0.9308,\n",
      "        0.9225, 0.9046, 0.9087, 0.9552, 0.9603, 0.9625, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5586, 0.5101, 0.4751, 0.4497, 0.4310, 0.4172, 0.4071, 0.3995,\n",
      "        0.3939, 0.3897, 0.3865, 0.3842, 0.3824, 0.3811, 0.3801, 0.3794, 0.3789,\n",
      "        0.3784, 0.3781, 0.3779, 0.3777, 0.3776, 0.3775, 0.4098])\n",
      "finalReturns:  tensor([0.6884, 0.7208, 0.6976, 0.6352, 0.5452, 0.4351, 0.3105, 0.1754])\n",
      "----------------------------------------\n",
      "iter  1  stage  16  ep  99999   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([19, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 19,\n",
      "        18, 19, 18, 19, 19, 18,  0])\n",
      "loss=  tensor(4.2699, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.4183, 2.4183, 2.4183, 2.4183, 2.4183, 2.4183, 2.4183, 2.4183, 2.4183,\n",
      "        2.4183, 2.4183, 2.4183, 2.4183, 2.4183, 2.4183, 2.4183, 2.4183, 2.0290,\n",
      "        1.6949, 1.3995, 1.1316, 0.8834, 0.6494, 0.4260, 0.2101]) return=  97928.81160926864\n",
      "probs of actions:  tensor([0.3446, 0.6577, 0.6444, 0.3681, 0.6642, 0.6525, 0.6800, 0.6510, 0.6417,\n",
      "        0.6183, 0.6431, 0.6039, 0.6727, 0.6523, 0.3263, 0.6423, 0.6035, 0.3472,\n",
      "        0.6241, 0.3860, 0.4948, 0.3028, 0.2832, 0.7703, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4751, 0.5624, 0.5055, 0.4610, 0.4386, 0.4161, 0.3996, 0.3875, 0.3785,\n",
      "        0.3718, 0.3668, 0.3631, 0.3603, 0.3582, 0.3529, 0.3586, 0.3569, 0.3520,\n",
      "        0.3579, 0.3527, 0.3584, 0.3531, 0.3550, 0.3602, 0.3905])\n",
      "finalReturns:  tensor([0.8185, 0.8509, 0.8330, 0.7705, 0.6857, 0.5755, 0.4563, 0.3247, 0.1804])\n",
      "----------------------------------------\n",
      "iter  1  stage  15  ep  99999   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([18, 19, 18, 19, 18, 18, 19, 18, 19, 19, 19, 18, 19, 18, 19, 18, 19, 19,\n",
      "        18, 19, 19, 19, 19, 18,  0])\n",
      "loss=  tensor(4.4872, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.8513, 2.8513, 2.8513, 2.8513, 2.8513, 2.8513, 2.8513, 2.8513, 2.8513,\n",
      "        2.8513, 2.8513, 2.8513, 2.8513, 2.8513, 2.8513, 2.8513, 2.4309, 2.0693,\n",
      "        1.7489, 1.4578, 1.1878, 0.9331, 0.6895, 0.4541, 0.2248]) return=  103665.53870452796\n",
      "probs of actions:  tensor([0.2510, 0.7391, 0.2437, 0.7508, 0.2622, 0.2638, 0.7149, 0.2397, 0.7548,\n",
      "        0.7597, 0.7351, 0.2216, 0.7160, 0.2554, 0.7117, 0.2246, 0.7894, 0.7670,\n",
      "        0.2307, 0.7989, 0.8463, 0.7543, 0.7680, 0.3619, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5549, 0.5138, 0.4741, 0.4551, 0.4350, 0.4165, 0.4126, 0.3999,\n",
      "        0.3965, 0.3940, 0.3958, 0.3874, 0.3908, 0.3837, 0.3881, 0.3816, 0.3828,\n",
      "        0.3874, 0.3812, 0.3825, 0.3834, 0.3842, 0.3884, 0.4180])\n",
      "finalReturns:  tensor([1.0263, 1.0587, 1.0387, 0.9762, 0.8799, 0.7687, 0.6410, 0.5011, 0.3523,\n",
      "        0.1932])\n",
      "----------------------------------------\n",
      "iter  1  stage  14  ep  99999   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 19, 19,\n",
      "        19, 19, 19, 19, 19, 19,  0])\n",
      "loss=  tensor(3.7333, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.0920, 3.0920, 3.0920, 3.0920, 3.0920, 3.0920, 3.0920, 3.0920, 3.0920,\n",
      "        3.0920, 3.0920, 3.0920, 3.0920, 3.0920, 3.0920, 2.6659, 2.3003, 1.9772,\n",
      "        1.6841, 1.4127, 1.1569, 0.9125, 0.6766, 0.4468, 0.2217]) return=  104293.09788509886\n",
      "probs of actions:  tensor([0.0991, 0.8924, 0.9044, 0.8958, 0.8924, 0.8902, 0.8814, 0.9084, 0.9020,\n",
      "        0.9004, 0.8872, 0.9107, 0.8797, 0.8967, 0.8710, 0.0876, 0.9185, 0.9141,\n",
      "        0.9163, 0.9272, 0.9501, 0.9235, 0.9188, 0.8713, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5549, 0.5101, 0.4777, 0.4540, 0.4367, 0.4238, 0.4143, 0.4073,\n",
      "        0.4020, 0.3981, 0.3951, 0.3929, 0.3913, 0.3901, 0.3929, 0.3852, 0.3855,\n",
      "        0.3857, 0.3859, 0.3860, 0.3861, 0.3862, 0.3862, 0.4224])\n",
      "finalReturns:  tensor([1.2002, 1.2363, 1.2090, 1.1469, 1.0545, 0.9402, 0.8101, 0.6684, 0.5182,\n",
      "        0.3618, 0.2007])\n",
      "----------------------------------------\n",
      "iter  1  stage  13  ep  99999   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "        19, 19, 19, 19, 19, 19,  0])\n",
      "loss=  tensor(0.1330, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.0724, 3.0724, 3.0724, 3.0724, 3.0724, 3.0724, 3.0724, 3.0724, 3.0724,\n",
      "        3.0724, 3.0724, 3.0724, 3.0724, 3.0724, 2.6699, 2.3266, 2.0248, 1.7523,\n",
      "        1.5007, 1.2644, 1.0391, 0.8220, 0.6109, 0.4043, 0.2009]) return=  99272.14211264676\n",
      "probs of actions:  tensor([0.9851, 0.9828, 0.9857, 0.9825, 0.9828, 0.9817, 0.9803, 0.9868, 0.9846,\n",
      "        0.9830, 0.9800, 0.9857, 0.9797, 0.9868, 0.9807, 0.9862, 0.9914, 0.9871,\n",
      "        0.9861, 0.9902, 0.9942, 0.9929, 0.9898, 0.9844, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4751, 0.5587, 0.5055, 0.4672, 0.4394, 0.4191, 0.4042, 0.3931, 0.3849,\n",
      "        0.3788, 0.3743, 0.3709, 0.3684, 0.3665, 0.3650, 0.3640, 0.3632, 0.3626,\n",
      "        0.3621, 0.3618, 0.3616, 0.3614, 0.3612, 0.3611, 0.3971])\n",
      "finalReturns:  tensor([1.3152, 1.3513, 1.3294, 1.2673, 1.1766, 1.0656, 0.9398, 0.8033, 0.6588,\n",
      "        0.5086, 0.3540, 0.1962])\n",
      "----------------------------------------\n",
      "iter  1  stage  12  ep  99999   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "        19, 19, 19, 19, 19, 19,  0])\n",
      "loss=  tensor(0.0689, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.2779, 3.2779, 3.2779, 3.2779, 3.2779, 3.2779, 3.2779, 3.2779, 3.2779,\n",
      "        3.2779, 3.2779, 3.2779, 3.2779, 2.8735, 2.5289, 2.2262, 1.9530, 1.7010,\n",
      "        1.4643, 1.2388, 1.0215, 0.8103, 0.6035, 0.4001, 0.1991]) return=  99272.14211264676\n",
      "probs of actions:  tensor([0.9934, 0.9920, 0.9937, 0.9918, 0.9921, 0.9914, 0.9908, 0.9942, 0.9929,\n",
      "        0.9919, 0.9907, 0.9933, 0.9896, 0.9951, 0.9918, 0.9950, 0.9964, 0.9951,\n",
      "        0.9949, 0.9964, 0.9983, 0.9980, 0.9968, 0.9942, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4751, 0.5587, 0.5055, 0.4672, 0.4394, 0.4191, 0.4042, 0.3931, 0.3849,\n",
      "        0.3788, 0.3743, 0.3709, 0.3684, 0.3665, 0.3650, 0.3640, 0.3632, 0.3626,\n",
      "        0.3621, 0.3618, 0.3616, 0.3614, 0.3612, 0.3611, 0.3971])\n",
      "finalReturns:  tensor([1.4780, 1.5141, 1.4922, 1.4299, 1.3391, 1.2279, 1.1020, 0.9654, 0.8209,\n",
      "        0.6706, 0.5160, 0.3582, 0.1980])\n",
      "----------------------------------------\n",
      "iter  1  stage  11  ep  99999   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "        19, 19, 19, 19, 19, 19,  0])\n",
      "loss=  tensor(0.0430, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.4843, 3.4843, 3.4843, 3.4843, 3.4843, 3.4843, 3.4843, 3.4843, 3.4843,\n",
      "        3.4843, 3.4843, 3.4843, 3.0773, 2.7310, 2.4271, 2.1530, 1.9004, 1.6632,\n",
      "        1.4374, 1.2198, 1.0084, 0.8015, 0.5979, 0.3969, 0.1978]) return=  99272.14211264676\n",
      "probs of actions:  tensor([0.9958, 0.9948, 0.9961, 0.9946, 0.9950, 0.9944, 0.9941, 0.9963, 0.9954,\n",
      "        0.9947, 0.9939, 0.9959, 0.9941, 0.9979, 0.9955, 0.9974, 0.9982, 0.9979,\n",
      "        0.9976, 0.9987, 0.9994, 0.9993, 0.9986, 0.9973, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4751, 0.5587, 0.5055, 0.4672, 0.4394, 0.4191, 0.4042, 0.3931, 0.3849,\n",
      "        0.3788, 0.3743, 0.3709, 0.3684, 0.3665, 0.3650, 0.3640, 0.3632, 0.3626,\n",
      "        0.3621, 0.3618, 0.3616, 0.3614, 0.3612, 0.3611, 0.3971])\n",
      "finalReturns:  tensor([1.6425, 1.6786, 1.6565, 1.5941, 1.5031, 1.3918, 1.2657, 1.1290, 0.9844,\n",
      "        0.8340, 0.6794, 0.5215, 0.3613, 0.1994])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  10  ep  96876   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "        19, 19, 19, 19, 19, 19,  0])\n",
      "loss=  tensor(0.0057, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.9867, 3.9867, 3.9867, 3.9867, 3.9867, 3.9867, 3.9867, 3.9867, 3.9867,\n",
      "        3.9867, 3.9867, 3.5525, 3.1814, 2.8543, 2.5585, 2.2850, 2.0277, 1.7823,\n",
      "        1.5456, 1.3153, 1.0897, 0.8677, 0.6483, 0.4308, 0.2149]) return=  104376.37516292125\n",
      "probs of actions:  tensor([8.0187e-04, 9.9891e-01, 9.9926e-01, 9.9889e-01, 9.9902e-01, 9.9886e-01,\n",
      "        9.9884e-01, 9.9934e-01, 9.9911e-01, 9.9894e-01, 9.9903e-01, 9.9962e-01,\n",
      "        9.9943e-01, 1.0000e+00, 9.9945e-01, 9.9960e-01, 9.9995e-01, 9.9999e-01,\n",
      "        9.9977e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9992e-01,\n",
      "        1.0000e+00], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5549, 0.5101, 0.4777, 0.4540, 0.4367, 0.4238, 0.4143, 0.4073,\n",
      "        0.4020, 0.3981, 0.3951, 0.3929, 0.3913, 0.3901, 0.3892, 0.3885, 0.3879,\n",
      "        0.3876, 0.3873, 0.3871, 0.3869, 0.3868, 0.3867, 0.4227])\n",
      "finalReturns:  tensor([1.8914, 1.9275, 1.9034, 1.8376, 1.7421, 1.6255, 1.4936, 1.3505, 1.1993,\n",
      "        1.0421, 0.8804, 0.7154, 0.5479, 0.3785, 0.2078])\n",
      "----------------------------------------\n",
      "iter  1  stage  9  ep  64   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "        19, 19, 19, 19, 19, 19,  0])\n",
      "loss=  tensor(0.0079, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.9042, 3.9042, 3.9042, 3.9042, 3.9042, 3.9042, 3.9042, 3.9042, 3.9042,\n",
      "        3.9042, 3.4893, 3.1375, 2.8297, 2.5529, 2.2983, 2.0597, 1.8328, 1.6145,\n",
      "        1.4025, 1.1952, 0.9913, 0.7900, 0.5907, 0.3928, 0.1960]) return=  99272.14211264676\n",
      "probs of actions:  tensor([0.9992, 0.9989, 0.9993, 0.9989, 0.9990, 0.9989, 0.9988, 0.9993, 0.9991,\n",
      "        0.9990, 0.9990, 0.9996, 0.9994, 1.0000, 0.9994, 0.9996, 0.9999, 1.0000,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4751, 0.5587, 0.5055, 0.4672, 0.4394, 0.4191, 0.4042, 0.3931, 0.3849,\n",
      "        0.3788, 0.3743, 0.3709, 0.3684, 0.3665, 0.3650, 0.3640, 0.3632, 0.3626,\n",
      "        0.3621, 0.3618, 0.3616, 0.3614, 0.3612, 0.3611, 0.3971])\n",
      "finalReturns:  tensor([1.9757, 2.0118, 1.9893, 1.9262, 1.8347, 1.7229, 1.5964, 1.4593, 1.3144,\n",
      "        1.1639, 1.0090, 0.8511, 0.6908, 0.5288, 0.3654, 0.2011])\n",
      "----------------------------------------\n",
      "iter  1  stage  8  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "        19, 19, 19, 19, 19, 19,  0])\n",
      "loss=  tensor(0.0104, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.1205, 4.1205, 4.1205, 4.1205, 4.1205, 4.1205, 4.1205, 4.1205, 4.1205,\n",
      "        3.6995, 3.3435, 3.0328, 2.7539, 2.4977, 2.2581, 2.0304, 1.8115, 1.5990,\n",
      "        1.3914, 1.1873, 0.9859, 0.7864, 0.5884, 0.3915, 0.1954]) return=  99272.14211264676\n",
      "probs of actions:  tensor([0.9992, 0.9989, 0.9993, 0.9989, 0.9990, 0.9989, 0.9988, 0.9993, 0.9991,\n",
      "        0.9990, 0.9990, 0.9996, 0.9994, 1.0000, 0.9994, 0.9996, 0.9999, 1.0000,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4751, 0.5587, 0.5055, 0.4672, 0.4394, 0.4191, 0.4042, 0.3931, 0.3849,\n",
      "        0.3788, 0.3743, 0.3709, 0.3684, 0.3665, 0.3650, 0.3640, 0.3632, 0.3626,\n",
      "        0.3621, 0.3618, 0.3616, 0.3614, 0.3612, 0.3611, 0.3971])\n",
      "finalReturns:  tensor([2.1443, 2.1804, 2.1575, 2.0941, 2.0021, 1.8898, 1.7631, 1.6257, 1.4806,\n",
      "        1.3299, 1.1750, 1.0169, 0.8565, 0.6945, 0.5311, 0.3667, 0.2017])\n",
      "----------------------------------------\n",
      "iter  1  stage  7  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "        19, 19, 19, 19, 19, 19,  0])\n",
      "loss=  tensor(0.0124, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.3436, 4.3436, 4.3436, 4.3436, 4.3436, 4.3436, 4.3436, 4.3436, 3.9144,\n",
      "        3.5528, 3.2380, 2.9563, 2.6982, 2.4571, 2.2283, 2.0086, 1.7956, 1.5875,\n",
      "        1.3831, 1.1814, 0.9818, 0.7837, 0.5867, 0.3905, 0.1950]) return=  99272.14211264676\n",
      "probs of actions:  tensor([0.9992, 0.9989, 0.9993, 0.9989, 0.9990, 0.9989, 0.9988, 0.9993, 0.9991,\n",
      "        0.9990, 0.9990, 0.9996, 0.9994, 1.0000, 0.9994, 0.9996, 0.9999, 1.0000,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4751, 0.5587, 0.5055, 0.4672, 0.4394, 0.4191, 0.4042, 0.3931, 0.3849,\n",
      "        0.3788, 0.3743, 0.3709, 0.3684, 0.3665, 0.3650, 0.3640, 0.3632, 0.3626,\n",
      "        0.3621, 0.3618, 0.3616, 0.3614, 0.3612, 0.3611, 0.3971])\n",
      "finalReturns:  tensor([2.3143, 2.3504, 2.3271, 2.2631, 2.1705, 2.0577, 1.9305, 1.7928, 1.6475,\n",
      "        1.4965, 1.3414, 1.1832, 1.0228, 0.8606, 0.6972, 0.5328, 0.3677, 0.2021])\n",
      "----------------------------------------\n",
      "iter  1  stage  6  ep  98   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "        19, 19, 19, 19, 19, 19,  0])\n",
      "loss=  tensor(0.0142, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.5761, 4.5761, 4.5761, 4.5761, 4.5761, 4.5761, 4.5761, 4.1358, 3.7666,\n",
      "        3.4465, 3.1610, 2.9002, 2.6571, 2.4269, 2.2062, 1.9924, 1.7837, 1.5789,\n",
      "        1.3769, 1.1770, 0.9787, 0.7816, 0.5854, 0.3898, 0.1947]) return=  99272.14211264676\n",
      "probs of actions:  tensor([0.9993, 0.9990, 0.9993, 0.9990, 0.9991, 0.9989, 0.9990, 0.9994, 0.9992,\n",
      "        0.9991, 0.9991, 0.9996, 0.9994, 1.0000, 0.9995, 0.9996, 0.9999, 1.0000,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4751, 0.5587, 0.5055, 0.4672, 0.4394, 0.4191, 0.4042, 0.3931, 0.3849,\n",
      "        0.3788, 0.3743, 0.3709, 0.3684, 0.3665, 0.3650, 0.3640, 0.3632, 0.3626,\n",
      "        0.3621, 0.3618, 0.3616, 0.3614, 0.3612, 0.3611, 0.3971])\n",
      "finalReturns:  tensor([2.4860, 2.5221, 2.4983, 2.4334, 2.3400, 2.2266, 2.0988, 1.9607, 1.8149,\n",
      "        1.6637, 1.5084, 1.3500, 1.1894, 1.0272, 0.8637, 0.6992, 0.5341, 0.3685,\n",
      "        0.2024])\n",
      "----------------------------------------\n",
      "iter  1  stage  5  ep  43   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "        19, 19, 19, 19, 19, 19,  0])\n",
      "loss=  tensor(0.0167, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.8214, 4.8214, 4.8214, 4.8214, 4.8214, 4.8214, 4.3662, 3.9867, 3.6595,\n",
      "        3.3690, 3.1045, 2.8588, 2.6267, 2.4045, 2.1897, 1.9803, 1.7749, 1.5725,\n",
      "        1.3723, 1.1737, 0.9764, 0.7801, 0.5844, 0.3892, 0.1945]) return=  99272.14211264676\n",
      "probs of actions:  tensor([0.9993, 0.9990, 0.9993, 0.9990, 0.9991, 0.9990, 0.9991, 0.9995, 0.9993,\n",
      "        0.9992, 0.9991, 0.9996, 0.9995, 1.0000, 0.9995, 0.9996, 0.9999, 1.0000,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4751, 0.5587, 0.5055, 0.4672, 0.4394, 0.4191, 0.4042, 0.3931, 0.3849,\n",
      "        0.3788, 0.3743, 0.3709, 0.3684, 0.3665, 0.3650, 0.3640, 0.3632, 0.3626,\n",
      "        0.3621, 0.3618, 0.3616, 0.3614, 0.3612, 0.3611, 0.3971])\n",
      "finalReturns:  tensor([2.6598, 2.6959, 2.6712, 2.6053, 2.5109, 2.3966, 2.2680, 2.1292, 1.9830,\n",
      "        1.8314, 1.6758, 1.5172, 1.3565, 1.1941, 1.0305, 0.8660, 0.7008, 0.5351,\n",
      "        0.3690, 0.2027])\n",
      "----------------------------------------\n",
      "iter  1  stage  4  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "        19, 19, 19, 19, 19, 19,  0])\n",
      "loss=  tensor(0.0199, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.0845, 5.0845, 5.0845, 5.0845, 5.0845, 4.6089, 4.2155, 3.8787, 3.5813,\n",
      "        3.3120, 3.0628, 2.8281, 2.6041, 2.3879, 2.1774, 1.9713, 1.7683, 1.5677,\n",
      "        1.3688, 1.1713, 0.9747, 0.7789, 0.5837, 0.3888, 0.1943]) return=  99272.14211264676\n",
      "probs of actions:  tensor([0.9993, 0.9990, 0.9993, 0.9990, 0.9991, 0.9990, 0.9991, 0.9995, 0.9993,\n",
      "        0.9992, 0.9991, 0.9996, 0.9995, 1.0000, 0.9995, 0.9996, 0.9999, 1.0000,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4751, 0.5587, 0.5055, 0.4672, 0.4394, 0.4191, 0.4042, 0.3931, 0.3849,\n",
      "        0.3788, 0.3743, 0.3709, 0.3684, 0.3665, 0.3650, 0.3640, 0.3632, 0.3626,\n",
      "        0.3621, 0.3618, 0.3616, 0.3614, 0.3612, 0.3611, 0.3971])\n",
      "finalReturns:  tensor([2.8362, 2.8723, 2.8465, 2.7792, 2.6835, 2.5679, 2.4383, 2.2987, 2.1518,\n",
      "        1.9997, 1.8437, 1.6848, 1.5238, 1.3613, 1.1975, 1.0329, 0.8677, 0.7019,\n",
      "        0.5358, 0.3694, 0.2028])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  3  ep  10   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "        19, 19, 19, 19, 19, 19,  0])\n",
      "loss=  tensor(0.0232, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.3719, 5.3719, 5.3719, 5.3719, 4.8686, 4.4563, 4.1064, 3.7998, 3.5239,\n",
      "        3.2700, 3.0319, 2.8053, 2.5873, 2.3754, 2.1683, 1.9645, 1.7633, 1.5641,\n",
      "        1.3662, 1.1694, 0.9734, 0.7781, 0.5831, 0.3885, 0.1942]) return=  99272.14211264676\n",
      "probs of actions:  tensor([0.9993, 0.9990, 0.9993, 0.9990, 0.9991, 0.9990, 0.9991, 0.9995, 0.9993,\n",
      "        0.9992, 0.9991, 0.9996, 0.9995, 1.0000, 0.9995, 0.9996, 0.9999, 1.0000,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4751, 0.5587, 0.5055, 0.4672, 0.4394, 0.4191, 0.4042, 0.3931, 0.3849,\n",
      "        0.3788, 0.3743, 0.3709, 0.3684, 0.3665, 0.3650, 0.3640, 0.3632, 0.3626,\n",
      "        0.3621, 0.3618, 0.3616, 0.3614, 0.3612, 0.3611, 0.3971])\n",
      "finalReturns:  tensor([3.0159, 3.0520, 3.0249, 2.9557, 2.8581, 2.7409, 2.6099, 2.4692, 2.3215,\n",
      "        2.1687, 2.0121, 1.8529, 1.6916, 1.5288, 1.3649, 1.2001, 1.0348, 0.8690,\n",
      "        0.7028, 0.5364, 0.3697, 0.2030])\n",
      "----------------------------------------\n",
      "iter  1  stage  2  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "        19, 19, 19, 19, 19, 19,  0])\n",
      "loss=  tensor(0.0260, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.6933, 5.6933, 5.6933, 5.1517, 4.7135, 4.3457, 4.0266, 3.7419, 3.4815,\n",
      "        3.2388, 3.0089, 2.7884, 2.5747, 2.3662, 2.1614, 1.9595, 1.7596, 1.5614,\n",
      "        1.3643, 1.1680, 0.9725, 0.7774, 0.5827, 0.3883, 0.1941]) return=  99272.14211264676\n",
      "probs of actions:  tensor([0.9993, 0.9990, 0.9993, 0.9990, 0.9991, 0.9990, 0.9991, 0.9995, 0.9993,\n",
      "        0.9992, 0.9991, 0.9996, 0.9995, 1.0000, 0.9995, 0.9996, 0.9999, 1.0000,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4751, 0.5587, 0.5055, 0.4672, 0.4394, 0.4191, 0.4042, 0.3931, 0.3849,\n",
      "        0.3788, 0.3743, 0.3709, 0.3684, 0.3665, 0.3650, 0.3640, 0.3632, 0.3626,\n",
      "        0.3621, 0.3618, 0.3616, 0.3614, 0.3612, 0.3611, 0.3971])\n",
      "finalReturns:  tensor([3.2001, 3.2362, 3.2071, 3.1355, 3.0354, 2.9161, 2.7833, 2.6411, 2.4922,\n",
      "        2.3385, 2.1812, 2.0214, 1.8597, 1.6966, 1.5325, 1.3676, 1.2021, 1.0362,\n",
      "        0.8699, 0.7034, 0.5368, 0.3700, 0.2031])\n",
      "----------------------------------------\n",
      "iter  1  stage  1  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "        19, 19, 19, 19, 19, 19,  0])\n",
      "loss=  tensor(0.0299, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.0620, 6.0620, 5.4672, 4.9933, 4.6010, 4.2649, 3.9681, 3.6991, 3.4501,\n",
      "        3.2157, 2.9918, 2.7757, 2.5653, 2.3592, 2.1563, 1.9557, 1.7569, 1.5593,\n",
      "        1.3628, 1.1670, 0.9718, 0.7769, 0.5824, 0.3881, 0.1940]) return=  99272.14211264676\n",
      "probs of actions:  tensor([0.9993, 0.9990, 0.9993, 0.9990, 0.9991, 0.9990, 0.9991, 0.9995, 0.9993,\n",
      "        0.9992, 0.9991, 0.9996, 0.9995, 1.0000, 0.9995, 0.9996, 0.9999, 1.0000,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4751, 0.5587, 0.5055, 0.4672, 0.4394, 0.4191, 0.4042, 0.3931, 0.3849,\n",
      "        0.3788, 0.3743, 0.3709, 0.3684, 0.3665, 0.3650, 0.3640, 0.3632, 0.3626,\n",
      "        0.3621, 0.3618, 0.3616, 0.3614, 0.3612, 0.3611, 0.3971])\n",
      "finalReturns:  tensor([3.3901, 3.4262, 3.3946, 3.3197, 3.2163, 3.0940, 2.9588, 2.8147, 2.6642,\n",
      "        2.5093, 2.3511, 2.1906, 2.0284, 1.8649, 1.7004, 1.5353, 1.3696, 1.2035,\n",
      "        1.0372, 0.8706, 0.7039, 0.5371, 0.3701, 0.2031])\n",
      "----------------------------------------\n",
      "iter  1  stage  0  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "        19, 19, 19, 19, 19, 19,  0])\n",
      "loss=  tensor(0.0331, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.3395, 5.8282, 5.3044, 4.8783, 4.5187, 4.2054, 3.9248, 3.6674, 3.4268,\n",
      "        3.1984, 2.9790, 2.7662, 2.5583, 2.3540, 2.1524, 1.9529, 1.7548, 1.5578,\n",
      "        1.3617, 1.1662, 0.9712, 0.7766, 0.5822, 0.3880, 0.1939]) return=  99272.14211264676\n",
      "probs of actions:  tensor([0.9993, 0.9990, 0.9993, 0.9990, 0.9991, 0.9990, 0.9991, 0.9995, 0.9993,\n",
      "        0.9992, 0.9991, 0.9996, 0.9995, 1.0000, 0.9995, 0.9996, 0.9999, 1.0000,\n",
      "        0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4751, 0.5587, 0.5055, 0.4672, 0.4394, 0.4191, 0.4042, 0.3931, 0.3849,\n",
      "        0.3788, 0.3743, 0.3709, 0.3684, 0.3665, 0.3650, 0.3640, 0.3632, 0.3626,\n",
      "        0.3621, 0.3618, 0.3616, 0.3614, 0.3612, 0.3611, 0.3971])\n",
      "finalReturns:  tensor([3.5878, 3.6239, 3.5889, 3.5096, 3.4019, 3.2758, 3.1373, 2.9905, 2.8380,\n",
      "        2.6815, 2.5221, 2.3606, 2.1976, 2.0336, 1.8687, 1.7032, 1.5373, 1.3711,\n",
      "        1.2046, 1.0380, 0.8712, 0.7043, 0.5373, 0.3703, 0.2032])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1682974149 saved\n",
      "[1777102, 'tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])', 99272.14211264676, 86128.77669412928, 0.033126410096883774, 1e-05, 1, 0, 'tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\\n        19, 19, 19, 19, 19, 19,  0])', '[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\\n 1.]', '0,[1e-05,1][1, 10000, 1, 1],1682974149', 25, 50, 165673.58346428472, 196485.87523713993, 79648.59533906801, 135210.39999999997, 132281.41866666666, 99272.14211264675, 99263.9372312988, 117110.65916440394, 117121.26544005591, 92350.73961268678, 99272.14211264675, 117121.26544005591]\n",
      "policy reset\n",
      "----------------------------------------\n",
      "iter  2  stage  24  ep  99999   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5624, 0.5624, 0.5624, 0.5624, 0.5624, 0.5624, 0.5624, 0.5624, 0.5624,\n",
      "        0.5624, 0.5624, 0.5624, 0.5624, 0.5624, 0.5624, 0.5624, 0.5624, 0.5624,\n",
      "        0.5624, 0.5624, 0.5624, 0.5624, 0.5624, 0.5624, 0.5624]) return=  138554.5803254051\n",
      "probs of actions:  tensor([0.8737, 0.8892, 0.8547, 0.8885, 0.9043, 0.8971, 0.8719, 0.9021, 0.9021,\n",
      "        0.8920, 0.8935, 0.8782, 0.8894, 0.8990, 0.8990, 0.9052, 0.8961, 0.8807,\n",
      "        0.8858, 0.8928, 0.8808, 0.9072, 0.9093, 0.9040, 0.9838],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5238, 0.5334, 0.5406, 0.5460, 0.5501, 0.5532, 0.5555, 0.5573,\n",
      "        0.5586, 0.5595, 0.5603, 0.5608, 0.5613, 0.5616, 0.5618, 0.5620, 0.5621,\n",
      "        0.5622, 0.5623, 0.5623, 0.5624, 0.5624, 0.5624, 0.5624])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  2  stage  23  ep  99999   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 13, 11,  0,  0,  0,  7, 11, 16,  7, 11,  7,  1,  8,  5,  7,\n",
      "         1,  5,  6, 11, 13, 11,  0])\n",
      "loss=  tensor(0.0061, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.9062, 0.9062, 0.9062, 0.9062, 0.9062, 0.9062, 0.9062, 0.9062, 0.9062,\n",
      "        0.9062, 0.9062, 0.9062, 0.9062, 0.9062, 0.9062, 0.9062, 0.9062, 0.9062,\n",
      "        0.9062, 0.9062, 0.9062, 0.9062, 0.9062, 0.9062, 0.4390]) return=  116362.73893427021\n",
      "probs of actions:  tensor([4.2262e-01, 4.3894e-01, 4.4018e-01, 3.8976e-03, 4.5545e-01, 1.8125e-01,\n",
      "        1.2230e-01, 1.1571e-01, 4.2787e-02, 4.4436e-01, 9.1077e-04, 5.0764e-02,\n",
      "        5.1019e-01, 4.6578e-02, 3.3308e-02, 1.2511e-01, 3.2285e-02, 4.3355e-02,\n",
      "        2.2989e-02, 2.9944e-02, 5.5487e-02, 4.7207e-01, 2.5546e-03, 7.9477e-01,\n",
      "        9.9147e-01], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5207, 0.5243, 0.5282, 0.4832, 0.4507, 0.4222,\n",
      "        0.4204, 0.4243, 0.4754, 0.4603, 0.4753, 0.4722, 0.4397, 0.4477, 0.4383,\n",
      "        0.4445, 0.4234, 0.4214, 0.4156, 0.4293, 0.4550, 0.4762])\n",
      "finalReturns:  tensor([0.0251, 0.0372])\n",
      "----------------------------------------\n",
      "iter  2  stage  22  ep  89767   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0003, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.4083, 1.4083, 1.4083, 1.4083, 1.4083, 1.4083, 1.4083, 1.4083, 1.4083,\n",
      "        1.4083, 1.4083, 1.4083, 1.4083, 1.4083, 1.4083, 1.4083, 1.4083, 1.4083,\n",
      "        1.4083, 1.4083, 1.4083, 1.4083, 1.4083, 0.9041, 0.4382]) return=  125571.79187222246\n",
      "probs of actions:  tensor([0.9818, 0.9901, 0.9821, 0.9903, 0.9905, 0.9868, 0.9907, 0.9941, 0.9928,\n",
      "        0.9893, 0.9932, 0.9848, 0.9921, 0.9868, 0.9691, 0.9914, 0.9843, 0.9874,\n",
      "        0.9920, 0.9875, 0.9880, 0.9906, 0.9990, 0.9993, 0.9979],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([0.0801, 0.0922, 0.0660])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  2  stage  21  ep  39489   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0004, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.8263, 1.8263, 1.8263, 1.8263, 1.8263, 1.8263, 1.8263, 1.8263, 1.8263,\n",
      "        1.8263, 1.8263, 1.8263, 1.8263, 1.8263, 1.8263, 1.8263, 1.8263, 1.8263,\n",
      "        1.8263, 1.8263, 1.8263, 1.8263, 1.3220, 0.8561, 0.4179]) return=  125571.79187222246\n",
      "probs of actions:  tensor([0.9967, 0.9983, 0.9966, 0.9982, 0.9983, 0.9977, 0.9982, 0.9989, 0.9987,\n",
      "        0.9981, 0.9987, 0.9971, 0.9985, 0.9973, 0.9934, 0.9985, 0.9974, 0.9975,\n",
      "        0.9984, 0.9977, 0.9977, 0.9990, 0.9999, 0.9999, 0.9970],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([0.1543, 0.1664, 0.1402, 0.0863])\n",
      "----------------------------------------\n",
      "iter  2  stage  20  ep  15903   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0008, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.2295, 2.2295, 2.2295, 2.2295, 2.2295, 2.2295, 2.2295, 2.2295, 2.2295,\n",
      "        2.2295, 2.2295, 2.2295, 2.2295, 2.2295, 2.2295, 2.2295, 2.2295, 2.2295,\n",
      "        2.2295, 2.2295, 2.2295, 1.7252, 1.2592, 0.8210, 0.4030]) return=  125571.79187222246\n",
      "probs of actions:  tensor([0.9983, 0.9991, 0.9982, 0.9991, 0.9991, 0.9988, 0.9990, 0.9994, 0.9993,\n",
      "        0.9990, 0.9993, 0.9984, 0.9992, 0.9985, 0.9964, 0.9992, 0.9986, 0.9987,\n",
      "        0.9991, 0.9987, 0.9990, 0.9995, 0.9999, 1.0000, 0.9962],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([0.2433, 0.2554, 0.2292, 0.1753, 0.1011])\n",
      "----------------------------------------\n",
      "iter  2  stage  19  ep  94   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0013, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.6218, 2.6218, 2.6218, 2.6218, 2.6218, 2.6218, 2.6218, 2.6218, 2.6218,\n",
      "        2.6218, 2.6218, 2.6218, 2.6218, 2.6218, 2.6218, 2.6218, 2.6218, 2.6218,\n",
      "        2.6218, 2.6218, 2.1174, 1.6514, 1.2131, 0.7951, 0.3921]) return=  125571.79187222246\n",
      "probs of actions:  tensor([0.9983, 0.9991, 0.9982, 0.9991, 0.9991, 0.9988, 0.9991, 0.9995, 0.9993,\n",
      "        0.9990, 0.9993, 0.9985, 0.9992, 0.9986, 0.9965, 0.9992, 0.9987, 0.9987,\n",
      "        0.9992, 0.9990, 0.9990, 0.9995, 0.9999, 1.0000, 0.9961],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([0.3433, 0.3554, 0.3292, 0.2753, 0.2011, 0.1121])\n",
      "----------------------------------------\n",
      "iter  2  stage  18  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0019, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.0061, 3.0061, 3.0061, 3.0061, 3.0061, 3.0061, 3.0061, 3.0061, 3.0061,\n",
      "        3.0061, 3.0061, 3.0061, 3.0061, 3.0061, 3.0061, 3.0061, 3.0061, 3.0061,\n",
      "        3.0061, 2.5016, 2.0355, 1.5972, 1.1791, 0.7760, 0.3839]) return=  125571.79187222246\n",
      "probs of actions:  tensor([0.9983, 0.9991, 0.9982, 0.9991, 0.9991, 0.9988, 0.9991, 0.9995, 0.9993,\n",
      "        0.9990, 0.9993, 0.9985, 0.9992, 0.9986, 0.9965, 0.9992, 0.9987, 0.9987,\n",
      "        0.9992, 0.9990, 0.9990, 0.9995, 0.9999, 1.0000, 0.9961],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([0.4515, 0.4636, 0.4374, 0.3835, 0.3093, 0.2202, 0.1202])\n",
      "----------------------------------------\n",
      "iter  2  stage  17  ep  155   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0026, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.3845, 3.3845, 3.3845, 3.3845, 3.3845, 3.3845, 3.3845, 3.3845, 3.3845,\n",
      "        3.3845, 3.3845, 3.3845, 3.3845, 3.3845, 3.3845, 3.3845, 3.3845, 3.3845,\n",
      "        2.8798, 2.4136, 1.9752, 1.5571, 1.1540, 0.7619, 0.3779]) return=  125571.79187222246\n",
      "probs of actions:  tensor([0.9983, 0.9992, 0.9983, 0.9991, 0.9992, 0.9988, 0.9991, 0.9995, 0.9993,\n",
      "        0.9991, 0.9994, 0.9985, 0.9992, 0.9986, 0.9965, 0.9992, 0.9987, 0.9990,\n",
      "        0.9994, 0.9990, 0.9991, 0.9996, 0.9999, 1.0000, 0.9964],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([0.5657, 0.5778, 0.5516, 0.4976, 0.4235, 0.3344, 0.2344, 0.1263])\n",
      "----------------------------------------\n",
      "iter  2  stage  16  ep  95   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0034, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.7586, 3.7586, 3.7586, 3.7586, 3.7586, 3.7586, 3.7586, 3.7586, 3.7586,\n",
      "        3.7586, 3.7586, 3.7586, 3.7586, 3.7586, 3.7586, 3.7586, 3.7586, 3.2537,\n",
      "        2.7873, 2.3488, 1.9307, 1.5275, 1.1353, 0.7513, 0.3734]) return=  125571.79187222246\n",
      "probs of actions:  tensor([0.9984, 0.9992, 0.9983, 0.9992, 0.9992, 0.9989, 0.9992, 0.9995, 0.9994,\n",
      "        0.9991, 0.9994, 0.9986, 0.9993, 0.9987, 0.9966, 0.9993, 0.9990, 0.9991,\n",
      "        0.9994, 0.9991, 0.9991, 0.9996, 0.9999, 1.0000, 0.9963],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([0.6844, 0.6965, 0.6703, 0.6163, 0.5422, 0.4531, 0.3531, 0.2450, 0.1308])\n",
      "----------------------------------------\n",
      "iter  2  stage  15  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0043, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.1295, 4.1295, 4.1295, 4.1295, 4.1295, 4.1295, 4.1295, 4.1295, 4.1295,\n",
      "        4.1295, 4.1295, 4.1295, 4.1295, 4.1295, 4.1295, 4.1295, 3.6244, 3.1578,\n",
      "        2.7192, 2.3010, 1.8977, 1.5055, 1.1214, 0.7435, 0.3700]) return=  125571.79187222246\n",
      "probs of actions:  tensor([0.9984, 0.9992, 0.9983, 0.9992, 0.9992, 0.9989, 0.9992, 0.9995, 0.9994,\n",
      "        0.9991, 0.9994, 0.9986, 0.9993, 0.9987, 0.9966, 0.9993, 0.9990, 0.9991,\n",
      "        0.9994, 0.9991, 0.9991, 0.9996, 0.9999, 1.0000, 0.9963],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([0.8065, 0.8186, 0.7924, 0.7384, 0.6642, 0.5752, 0.4751, 0.3670, 0.2528,\n",
      "        0.1341])\n",
      "----------------------------------------\n",
      "iter  2  stage  14  ep  11090   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0038, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.4983, 4.4983, 4.4983, 4.4983, 4.4983, 4.4983, 4.4983, 4.4983, 4.4983,\n",
      "        4.4983, 4.4983, 4.4983, 4.4983, 4.4983, 4.4983, 3.9928, 3.5260, 3.0872,\n",
      "        2.6688, 2.2654, 1.8732, 1.4891, 1.1111, 0.7376, 0.3675]) return=  125571.79187222246\n",
      "probs of actions:  tensor([0.9992, 0.9996, 0.9991, 0.9995, 0.9995, 0.9994, 0.9995, 0.9997, 0.9996,\n",
      "        0.9995, 0.9996, 0.9992, 0.9996, 0.9993, 0.9990, 0.9997, 0.9998, 0.9997,\n",
      "        0.9997, 0.9996, 0.9995, 0.9998, 1.0000, 1.0000, 0.9916],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([0.9311, 0.9432, 0.9170, 0.8630, 0.7888, 0.6997, 0.5997, 0.4915, 0.3774,\n",
      "        0.2587, 0.1366])\n",
      "----------------------------------------\n",
      "iter  2  stage  13  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0048, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.8656, 4.8656, 4.8656, 4.8656, 4.8656, 4.8656, 4.8656, 4.8656, 4.8656,\n",
      "        4.8656, 4.8656, 4.8656, 4.8656, 4.8656, 4.3597, 3.8925, 3.4535, 3.0349,\n",
      "        2.6314, 2.2390, 1.8549, 1.4768, 1.1033, 0.7332, 0.3657]) return=  125571.79187222246\n",
      "probs of actions:  tensor([0.9992, 0.9996, 0.9991, 0.9995, 0.9995, 0.9994, 0.9995, 0.9997, 0.9996,\n",
      "        0.9995, 0.9996, 0.9992, 0.9996, 0.9993, 0.9990, 0.9997, 0.9998, 0.9997,\n",
      "        0.9997, 0.9996, 0.9995, 0.9998, 1.0000, 1.0000, 0.9916],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([1.0577, 1.0698, 1.0435, 0.9895, 0.9153, 0.8262, 0.7262, 0.6180, 0.5038,\n",
      "        0.3851, 0.2631, 0.1385])\n",
      "----------------------------------------\n",
      "iter  2  stage  12  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11, 11])\n",
      "loss=  tensor(0.6158, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.2321, 5.2321, 5.2321, 5.2321, 5.2321, 5.2321, 5.2321, 5.2321, 5.2321,\n",
      "        5.2321, 5.2321, 5.2321, 5.2321, 4.7256, 4.2580, 3.8186, 3.3998, 2.9961,\n",
      "        2.6036, 2.2193, 1.8412, 1.4677, 1.0975, 0.7299, 0.3643]) return=  125450.79187222246\n",
      "probs of actions:  tensor([0.9992, 0.9996, 0.9991, 0.9995, 0.9995, 0.9994, 0.9995, 0.9997, 0.9996,\n",
      "        0.9995, 0.9996, 0.9992, 0.9996, 0.9993, 0.9990, 0.9997, 0.9998, 0.9997,\n",
      "        0.9997, 0.9996, 0.9995, 0.9998, 1.0000, 1.0000, 0.0084],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.4921])\n",
      "finalReturns:  tensor([1.1735, 1.1856, 1.1594, 1.1053, 1.0311, 0.9420, 0.8419, 0.7337, 0.6195,\n",
      "        0.5008, 0.3788, 0.2542, 0.1278])\n",
      "----------------------------------------\n",
      "iter  2  stage  11  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0069, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.5984, 5.5984, 5.5984, 5.5984, 5.5984, 5.5984, 5.5984, 5.5984, 5.5984,\n",
      "        5.5984, 5.5984, 5.5984, 5.0910, 4.6228, 4.1830, 3.7639, 3.3600, 2.9673,\n",
      "        2.5829, 2.2046, 1.8310, 1.4608, 1.0932, 0.7275, 0.3632]) return=  125571.79187222246\n",
      "probs of actions:  tensor([0.9992, 0.9996, 0.9991, 0.9995, 0.9995, 0.9994, 0.9995, 0.9997, 0.9996,\n",
      "        0.9995, 0.9996, 0.9992, 0.9996, 0.9993, 0.9990, 0.9997, 0.9998, 0.9997,\n",
      "        0.9997, 0.9996, 0.9995, 0.9998, 1.0000, 1.0000, 0.9916],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([1.3147, 1.3268, 1.3005, 1.2465, 1.1722, 1.0830, 0.9830, 0.8748, 0.7605,\n",
      "        0.6418, 0.5198, 0.3952, 0.2688, 0.1410])\n",
      "----------------------------------------\n",
      "iter  2  stage  10  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0077, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.9648, 5.9648, 5.9648, 5.9648, 5.9648, 5.9648, 5.9648, 5.9648, 5.9648,\n",
      "        5.9648, 5.9648, 5.4563, 4.9874, 4.5470, 4.1274, 3.7232, 3.3303, 2.9457,\n",
      "        2.5674, 2.1937, 1.8234, 1.4557, 1.0900, 0.7257, 0.3624]) return=  125571.79187222246\n",
      "probs of actions:  tensor([0.9992, 0.9996, 0.9991, 0.9995, 0.9995, 0.9994, 0.9995, 0.9997, 0.9996,\n",
      "        0.9995, 0.9996, 0.9992, 0.9996, 0.9993, 0.9990, 0.9997, 0.9998, 0.9997,\n",
      "        0.9997, 0.9996, 0.9995, 0.9998, 1.0000, 1.0000, 0.9916],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([1.4446, 1.4567, 1.4304, 1.3763, 1.3020, 1.2128, 1.1127, 1.0045, 0.8902,\n",
      "        0.7715, 0.6495, 0.5249, 0.3985, 0.2706, 0.1418])\n",
      "----------------------------------------\n",
      "iter  2  stage  9  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0088, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.3321, 6.3321, 6.3321, 6.3321, 6.3321, 6.3321, 6.3321, 6.3321, 6.3321,\n",
      "        6.3321, 5.8221, 5.3521, 4.9109, 4.4908, 4.0862, 3.6930, 3.3082, 2.9296,\n",
      "        2.5558, 2.1855, 1.8177, 1.4519, 1.0875, 0.7243, 0.3618]) return=  125571.79187222246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs of actions:  tensor([0.9992, 0.9996, 0.9991, 0.9995, 0.9995, 0.9994, 0.9995, 0.9997, 0.9996,\n",
      "        0.9995, 0.9996, 0.9992, 0.9996, 0.9993, 0.9990, 0.9997, 0.9998, 0.9997,\n",
      "        0.9997, 0.9996, 0.9995, 0.9998, 1.0000, 1.0000, 0.9916],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([1.5753, 1.5874, 1.5610, 1.5068, 1.4325, 1.3433, 1.2431, 1.1348, 1.0206,\n",
      "        0.9018, 0.7797, 0.6552, 0.5287, 0.4009, 0.2720, 0.1423])\n",
      "----------------------------------------\n",
      "iter  2  stage  8  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0097, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.7008, 6.7008, 6.7008, 6.7008, 6.7008, 6.7008, 6.7008, 6.7008, 6.7008,\n",
      "        6.1888, 5.7173, 5.2752, 4.8543, 4.4491, 4.0555, 3.6704, 3.2916, 2.9176,\n",
      "        2.5471, 2.1793, 1.8134, 1.4490, 1.0857, 0.7232, 0.3614]) return=  125571.79187222246\n",
      "probs of actions:  tensor([0.9992, 0.9996, 0.9991, 0.9995, 0.9995, 0.9994, 0.9995, 0.9997, 0.9996,\n",
      "        0.9995, 0.9996, 0.9992, 0.9996, 0.9993, 0.9990, 0.9997, 0.9998, 0.9997,\n",
      "        0.9997, 0.9996, 0.9995, 0.9998, 1.0000, 1.0000, 0.9916],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([1.7065, 1.7186, 1.6921, 1.6379, 1.5635, 1.4742, 1.3740, 1.2657, 1.1514,\n",
      "        1.0326, 0.9105, 0.7859, 0.6594, 0.5316, 0.4027, 0.2730, 0.1428])\n",
      "----------------------------------------\n",
      "iter  2  stage  7  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0105, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.0715, 7.0715, 7.0715, 7.0715, 7.0715, 7.0715, 7.0715, 7.0715, 6.5570,\n",
      "        6.0836, 5.6400, 5.2182, 4.8123, 4.4181, 4.0326, 3.6535, 3.2793, 2.9086,\n",
      "        2.5407, 2.1747, 1.8102, 1.4469, 1.0844, 0.7225, 0.3611]) return=  125571.79187222246\n",
      "probs of actions:  tensor([0.9992, 0.9996, 0.9991, 0.9995, 0.9995, 0.9994, 0.9995, 0.9997, 0.9996,\n",
      "        0.9995, 0.9996, 0.9992, 0.9996, 0.9993, 0.9990, 0.9997, 0.9998, 0.9997,\n",
      "        0.9997, 0.9996, 0.9995, 0.9998, 1.0000, 1.0000, 0.9915],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([1.8382, 1.8503, 1.8238, 1.7694, 1.6949, 1.6055, 1.5052, 1.3969, 1.2825,\n",
      "        1.1637, 1.0416, 0.9170, 0.7905, 0.6626, 0.5337, 0.4041, 0.2738, 0.1431])\n",
      "----------------------------------------\n",
      "iter  2  stage  6  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0117, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.4454, 7.4454, 7.4454, 7.4454, 7.4454, 7.4454, 7.4454, 6.9273, 6.4514,\n",
      "        6.0060, 5.5828, 5.1759, 4.7809, 4.3949, 4.0155, 3.6409, 3.2701, 2.9019,\n",
      "        2.5358, 2.1712, 1.8078, 1.4453, 1.0833, 0.7219, 0.3608]) return=  125571.79187222246\n",
      "probs of actions:  tensor([0.9992, 0.9996, 0.9991, 0.9995, 0.9995, 0.9994, 0.9995, 0.9997, 0.9996,\n",
      "        0.9995, 0.9996, 0.9992, 0.9996, 0.9993, 0.9990, 0.9997, 0.9998, 0.9997,\n",
      "        0.9997, 0.9996, 0.9995, 0.9998, 1.0000, 1.0000, 0.9915],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([1.9703, 1.9824, 1.9558, 1.9014, 1.8267, 1.7372, 1.6368, 1.5284, 1.4140,\n",
      "        1.2951, 1.1729, 1.0483, 0.9218, 0.7939, 0.6650, 0.5354, 0.4051, 0.2744,\n",
      "        0.1434])\n",
      "----------------------------------------\n",
      "iter  2  stage  5  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0132, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.8234, 7.8234, 7.8234, 7.8234, 7.8234, 7.8234, 7.3006, 6.8213, 6.3735,\n",
      "        5.9485, 5.5402, 5.1443, 4.7576, 4.3776, 4.0027, 3.6315, 3.2631, 2.8969,\n",
      "        2.5322, 2.1687, 1.8060, 1.4440, 1.0826, 0.7215, 0.3606]) return=  125571.79187222246\n",
      "probs of actions:  tensor([0.9992, 0.9996, 0.9991, 0.9995, 0.9995, 0.9994, 0.9995, 0.9997, 0.9996,\n",
      "        0.9995, 0.9996, 0.9992, 0.9996, 0.9993, 0.9990, 0.9997, 0.9998, 0.9997,\n",
      "        0.9997, 0.9996, 0.9995, 0.9998, 1.0000, 1.0000, 0.9915],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([2.1030, 2.1151, 2.0884, 2.0337, 1.9589, 1.8692, 1.7687, 1.6602, 1.5457,\n",
      "        1.4268, 1.3045, 1.1799, 1.0533, 0.9254, 0.7965, 0.6668, 0.5366, 0.4059,\n",
      "        0.2748, 0.1436])\n",
      "----------------------------------------\n",
      "iter  2  stage  4  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0145, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.2072, 8.2072, 8.2072, 8.2072, 8.2072, 7.6781, 7.1943, 6.7431, 6.3158,\n",
      "        5.9058, 5.5086, 5.1208, 4.7401, 4.3647, 3.9931, 3.6244, 3.2580, 2.8931,\n",
      "        2.5295, 2.1667, 1.8047, 1.4431, 1.0820, 0.7211, 0.3605]) return=  125571.79187222246\n",
      "probs of actions:  tensor([0.9992, 0.9996, 0.9991, 0.9995, 0.9995, 0.9994, 0.9995, 0.9997, 0.9996,\n",
      "        0.9995, 0.9996, 0.9992, 0.9996, 0.9993, 0.9990, 0.9997, 0.9998, 0.9997,\n",
      "        0.9997, 0.9996, 0.9995, 0.9998, 1.0000, 1.0000, 0.9915],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([2.2362, 2.2483, 2.2214, 2.1666, 2.0915, 2.0016, 1.9009, 1.7922, 1.6776,\n",
      "        1.5586, 1.4363, 1.3116, 1.1850, 1.0571, 0.9282, 0.7985, 0.6682, 0.5375,\n",
      "        0.4064, 0.2752, 0.1437])\n",
      "----------------------------------------\n",
      "iter  2  stage  3  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0158, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.5988, 8.5988, 8.5988, 8.5988, 8.0612, 7.5713, 7.1158, 6.6852, 6.2729,\n",
      "        5.8739, 5.4849, 5.1033, 4.7271, 4.3550, 3.9859, 3.6192, 3.2541, 2.8903,\n",
      "        2.5274, 2.1653, 1.8037, 1.4425, 1.0816, 0.7209, 0.3604]) return=  125571.79187222246\n",
      "probs of actions:  tensor([0.9992, 0.9996, 0.9991, 0.9995, 0.9995, 0.9994, 0.9995, 0.9997, 0.9996,\n",
      "        0.9995, 0.9996, 0.9992, 0.9996, 0.9993, 0.9990, 0.9997, 0.9998, 0.9997,\n",
      "        0.9997, 0.9996, 0.9995, 0.9998, 1.0000, 1.0000, 0.9915],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([2.3701, 2.3822, 2.3550, 2.2999, 2.2245, 2.1343, 2.0334, 1.9246, 1.8098,\n",
      "        1.6907, 1.5683, 1.4435, 1.3169, 1.1889, 1.0599, 0.9302, 0.7999, 0.6692,\n",
      "        0.5381, 0.4069, 0.2754, 0.1438])\n",
      "----------------------------------------\n",
      "iter  2  stage  2  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0183, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.0010, 9.0010, 9.0010, 8.4520, 7.9540, 7.4926, 7.0577, 6.6422, 6.2409,\n",
      "        5.8502, 5.4673, 5.0901, 4.7173, 4.3477, 3.9806, 3.6152, 3.2512, 2.8881,\n",
      "        2.5259, 2.1642, 1.8029, 1.4420, 1.0812, 0.7207, 0.3603]) return=  125571.79187222246\n",
      "probs of actions:  tensor([0.9992, 0.9996, 0.9991, 0.9995, 0.9995, 0.9994, 0.9995, 0.9997, 0.9996,\n",
      "        0.9995, 0.9996, 0.9992, 0.9996, 0.9993, 0.9990, 0.9997, 0.9998, 0.9997,\n",
      "        0.9997, 0.9996, 0.9995, 0.9998, 1.0000, 1.0000, 0.9915],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([2.5048, 2.5169, 2.4894, 2.4338, 2.3580, 2.2675, 2.1663, 2.0572, 1.9422,\n",
      "        1.8229, 1.7004, 1.5755, 1.4489, 1.3208, 1.1918, 1.0621, 0.9317, 0.8010,\n",
      "        0.6699, 0.5387, 0.4072, 0.2756, 0.1439])\n",
      "----------------------------------------\n",
      "iter  2  stage  1  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0198, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.4176, 9.4176, 8.8532, 8.3442, 7.8749, 7.4342, 7.0145, 6.6101, 6.2171,\n",
      "        5.8325, 5.4541, 5.0803, 4.7100, 4.3423, 3.9765, 3.6122, 3.2490, 2.8866,\n",
      "        2.5247, 2.1634, 1.8023, 1.4416, 1.0810, 0.7206, 0.3602]) return=  125571.79187222246\n",
      "probs of actions:  tensor([0.9992, 0.9996, 0.9991, 0.9995, 0.9995, 0.9994, 0.9995, 0.9997, 0.9996,\n",
      "        0.9995, 0.9996, 0.9992, 0.9996, 0.9993, 0.9990, 0.9997, 0.9998, 0.9997,\n",
      "        0.9997, 0.9996, 0.9995, 0.9998, 1.0000, 1.0000, 0.9915],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([2.6405, 2.6526, 2.6247, 2.5685, 2.4922, 2.4012, 2.2995, 2.1901, 2.0749,\n",
      "        1.9554, 1.8328, 1.7077, 1.5810, 1.4529, 1.3238, 1.1940, 1.0637, 0.9329,\n",
      "        0.8018, 0.6705, 0.5390, 0.4074, 0.2757, 0.1439])\n",
      "----------------------------------------\n",
      "iter  2  stage  0  ep  0   adversary:  AdversaryModes.fight_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0224, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.7796, 9.2684, 8.7446, 8.2646, 7.8162, 7.3909, 6.9823, 6.5862, 6.1993,\n",
      "        5.8192, 5.4442, 5.0730, 4.7046, 4.3383, 3.9735, 3.6100, 3.2473, 2.8854,\n",
      "        2.5239, 2.1628, 1.8019, 1.4413, 1.0808, 0.7205, 0.3602]) return=  125571.79187222246\n",
      "probs of actions:  tensor([0.9992, 0.9996, 0.9991, 0.9995, 0.9996, 0.9994, 0.9995, 0.9997, 0.9996,\n",
      "        0.9995, 0.9996, 0.9992, 0.9996, 0.9993, 0.9990, 0.9997, 0.9998, 0.9997,\n",
      "        0.9997, 0.9996, 0.9995, 0.9998, 1.0000, 1.0000, 0.9915],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([2.7775, 2.7896, 2.7612, 2.7043, 2.6272, 2.5355, 2.4333, 2.3235, 2.2079,\n",
      "        2.0881, 1.9653, 1.8401, 1.7132, 1.5850, 1.4559, 1.3260, 1.1956, 1.0649,\n",
      "        0.9338, 0.8024, 0.6709, 0.5393, 0.4076, 0.2758, 0.1440])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1682983428 saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[396616, 'tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])', 125571.79187222246, 95728.31346963784, 0.022367581725120544, 1e-05, 1, 0, 'tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\\n        11, 11, 11, 11, 11, 11,  0])', '[1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\\n 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   0.99]', '0,[1e-05,1][1, 10000, 1, 1],1682983428', 25, 50, 156312.25008636713, 172508.0785299521, 67091.50476852678, 135143.75466666667, 132277.408, 125571.79187222247, 125571.79187222247, 128010.77984270274, 128010.77984270274, 78103.35371185312, 125571.79187222247, 128010.77984270274]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x648 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy reset\n",
      "----------------------------------------\n",
      "iter  0  stage  24  ep  99999   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5624, 0.5624, 0.5624, 0.5624, 0.5624, 0.5624, 0.5624, 0.5624, 0.5624,\n",
      "        0.5624, 0.5624, 0.5624, 0.5624, 0.5624, 0.5624, 0.5624, 0.5624, 0.5624,\n",
      "        0.5624, 0.5624, 0.5624, 0.5624, 0.5624, 0.5624, 0.5624]) return=  138554.5803254051\n",
      "probs of actions:  tensor([0.8580, 0.8369, 0.8854, 0.8691, 0.8611, 0.8791, 0.8596, 0.8337, 0.8636,\n",
      "        0.8863, 0.8837, 0.8661, 0.8775, 0.8598, 0.8728, 0.8699, 0.8710, 0.8759,\n",
      "        0.8779, 0.8717, 0.8976, 0.8724, 0.8831, 0.8544, 0.9742],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5238, 0.5334, 0.5406, 0.5460, 0.5501, 0.5532, 0.5555, 0.5573,\n",
      "        0.5586, 0.5595, 0.5603, 0.5608, 0.5613, 0.5616, 0.5618, 0.5620, 0.5621,\n",
      "        0.5622, 0.5623, 0.5623, 0.5624, 0.5624, 0.5624, 0.5624])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  0  stage  23  ep  99999   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([13, 12, 11,  0,  0,  9, 11, 12, 17,  8, 13,  0,  9,  9, 10,  0, 10, 10,\n",
      "        20, 10,  0,  0, 16, 12,  0])\n",
      "loss=  tensor(0.0255, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.8069, 0.8069, 0.8069, 0.8069, 0.8069, 0.8069, 0.8069, 0.8069, 0.8069,\n",
      "        0.8069, 0.8069, 0.8069, 0.8069, 0.8069, 0.8069, 0.8069, 0.8069, 0.8069,\n",
      "        0.8069, 0.8069, 0.8069, 0.8069, 0.8069, 0.8069, 0.3898]) return=  108454.93368961285\n",
      "probs of actions:  tensor([0.1475, 0.0568, 0.0506, 0.2173, 0.2489, 0.1672, 0.0495, 0.0950, 0.0052,\n",
      "        0.0350, 0.1080, 0.1715, 0.1418, 0.1597, 0.0460, 0.3593, 0.0511, 0.0653,\n",
      "        0.0013, 0.0516, 0.3377, 0.2581, 0.0042, 0.3484, 0.9947],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4943, 0.5575, 0.5313, 0.5190, 0.4628, 0.4147, 0.4106, 0.4148, 0.4085,\n",
      "        0.4540, 0.4305, 0.4545, 0.4087, 0.4101, 0.4093, 0.4233, 0.3844, 0.3945,\n",
      "        0.3722, 0.4409, 0.4471, 0.4115, 0.3602, 0.4027, 0.4282])\n",
      "finalReturns:  tensor([0.0240, 0.0384])\n",
      "----------------------------------------\n",
      "iter  0  stage  22  ep  99999   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([ 7, 13, 13, 13, 13, 13, 13, 13, 12,  0, 13, 13, 13, 17, 13, 13, 13, 12,\n",
      "        13, 13, 13, 13, 13, 12,  0])\n",
      "loss=  tensor(0.0636, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.1083, 1.1083, 1.1083, 1.1083, 1.1083, 1.1083, 1.1083, 1.1083, 1.1083,\n",
      "        1.1083, 1.1083, 1.1083, 1.1083, 1.1083, 1.1083, 1.1083, 1.1083, 1.1083,\n",
      "        1.1083, 1.1083, 1.1083, 1.1083, 1.1083, 0.7025, 0.3368]) return=  106345.43462592686\n",
      "probs of actions:  tensor([0.0019, 0.8371, 0.8408, 0.8583, 0.8412, 0.8676, 0.8239, 0.7916, 0.1241,\n",
      "        0.0167, 0.8616, 0.8110, 0.8468, 0.0015, 0.8177, 0.8115, 0.8583, 0.1668,\n",
      "        0.7993, 0.8224, 0.8249, 0.8218, 0.9099, 0.5416, 0.9995],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5063, 0.5326, 0.5852, 0.5327, 0.4949, 0.4674, 0.4473, 0.4325, 0.4240,\n",
      "        0.4270, 0.3638, 0.3701, 0.3749, 0.3665, 0.3940, 0.3929, 0.3920, 0.3939,\n",
      "        0.3877, 0.3882, 0.3885, 0.3888, 0.3889, 0.3916, 0.4029])\n",
      "finalReturns:  tensor([0.0751, 0.0920, 0.0661])\n",
      "----------------------------------------\n",
      "iter  0  stage  21  ep  99999   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([13, 13, 13, 13, 13, 13, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        12, 13, 13, 14, 13, 12,  0])\n",
      "loss=  tensor(1.2667, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.6831, 1.6831, 1.6831, 1.6831, 1.6831, 1.6831, 1.6831, 1.6831, 1.6831,\n",
      "        1.6831, 1.6831, 1.6831, 1.6831, 1.6831, 1.6831, 1.6831, 1.6831, 1.6831,\n",
      "        1.6831, 1.6831, 1.6831, 1.6831, 1.2086, 0.7775, 0.3774]) return=  119140.59757259364\n",
      "probs of actions:  tensor([0.9729, 0.9534, 0.9539, 0.9617, 0.9595, 0.9673, 0.0031, 0.9340, 0.9457,\n",
      "        0.9719, 0.9712, 0.9486, 0.9622, 0.9698, 0.9563, 0.9580, 0.9652, 0.9294,\n",
      "        0.0450, 0.9563, 0.9583, 0.0017, 0.9820, 0.3256, 0.9998],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4943, 0.5550, 0.5302, 0.5120, 0.4986, 0.4886, 0.4784, 0.4791, 0.4741,\n",
      "        0.4704, 0.4676, 0.4655, 0.4639, 0.4627, 0.4618, 0.4612, 0.4607, 0.4603,\n",
      "        0.4625, 0.4564, 0.4571, 0.4549, 0.4615, 0.4634, 0.4739])\n",
      "finalReturns:  tensor([0.1705, 0.1901, 0.1599, 0.0965])\n",
      "----------------------------------------\n",
      "iter  0  stage  20  ep  99999   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 12, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13,  0])\n",
      "loss=  tensor(0.0540, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.0488, 2.0488, 2.0488, 2.0488, 2.0488, 2.0488, 2.0488, 2.0488, 2.0488,\n",
      "        2.0488, 2.0488, 2.0488, 2.0488, 2.0488, 2.0488, 2.0488, 2.0488, 2.0488,\n",
      "        2.0488, 2.0488, 2.0488, 1.5726, 1.1403, 0.7394, 0.3614]) return=  118964.00360478487\n",
      "probs of actions:  tensor([0.9881, 0.9788, 0.9789, 0.9827, 0.9821, 0.9856, 0.9772, 0.9685, 0.9748,\n",
      "        0.9878, 0.9878, 0.0196, 0.9830, 0.9867, 0.9804, 0.9813, 0.9843, 0.9665,\n",
      "        0.9737, 0.9803, 0.9839, 0.9912, 0.9935, 0.7924, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4943, 0.5550, 0.5302, 0.5120, 0.4986, 0.4886, 0.4811, 0.4756, 0.4715,\n",
      "        0.4684, 0.4661, 0.4669, 0.4596, 0.4595, 0.4594, 0.4594, 0.4593, 0.4593,\n",
      "        0.4593, 0.4593, 0.4592, 0.4592, 0.4592, 0.4592, 0.4761])\n",
      "finalReturns:  tensor([0.2643, 0.2812, 0.2543, 0.1959, 0.1147])\n",
      "----------------------------------------\n",
      "iter  0  stage  19  ep  58190   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13,  0])\n",
      "loss=  tensor(0.0039, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.3993, 2.3993, 2.3993, 2.3993, 2.3993, 2.3993, 2.3993, 2.3993, 2.3993,\n",
      "        2.3993, 2.3993, 2.3993, 2.3993, 2.3993, 2.3993, 2.3993, 2.3993, 2.3993,\n",
      "        2.3993, 2.3993, 1.9227, 1.4901, 1.0889, 0.7107, 0.3492]) return=  119073.90192566637\n",
      "probs of actions:  tensor([0.9991, 0.9983, 0.9984, 0.9987, 0.9986, 0.9990, 0.9981, 0.9974, 0.9981,\n",
      "        0.9992, 0.9991, 0.9981, 0.9987, 0.9989, 0.9984, 0.9985, 0.9988, 0.9975,\n",
      "        0.9980, 0.9990, 0.9994, 0.9997, 0.9998, 0.9864, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4943, 0.5550, 0.5302, 0.5120, 0.4986, 0.4886, 0.4811, 0.4756, 0.4715,\n",
      "        0.4684, 0.4661, 0.4644, 0.4631, 0.4621, 0.4614, 0.4608, 0.4604, 0.4601,\n",
      "        0.4599, 0.4597, 0.4596, 0.4595, 0.4594, 0.4594, 0.4762])\n",
      "finalReturns:  tensor([0.3745, 0.3914, 0.3644, 0.3061, 0.2249, 0.1270])\n",
      "----------------------------------------\n",
      "iter  0  stage  18  ep  1091   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13,  0])\n",
      "loss=  tensor(0.0049, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.7400, 2.7400, 2.7400, 2.7400, 2.7400, 2.7400, 2.7400, 2.7400, 2.7400,\n",
      "        2.7400, 2.7400, 2.7400, 2.7400, 2.7400, 2.7400, 2.7400, 2.7400, 2.7400,\n",
      "        2.7400, 2.2632, 1.8304, 1.4292, 1.0509, 0.6894, 0.3402]) return=  119073.90192566637\n",
      "probs of actions:  tensor([0.9991, 0.9983, 0.9984, 0.9987, 0.9986, 0.9990, 0.9982, 0.9974, 0.9981,\n",
      "        0.9992, 0.9992, 0.9981, 0.9987, 0.9990, 0.9984, 0.9985, 0.9988, 0.9975,\n",
      "        0.9990, 0.9990, 0.9994, 0.9998, 0.9998, 0.9863, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4943, 0.5550, 0.5302, 0.5120, 0.4986, 0.4886, 0.4811, 0.4756, 0.4715,\n",
      "        0.4684, 0.4661, 0.4644, 0.4631, 0.4621, 0.4614, 0.4608, 0.4604, 0.4601,\n",
      "        0.4599, 0.4597, 0.4596, 0.4595, 0.4594, 0.4594, 0.4762])\n",
      "finalReturns:  tensor([0.4937, 0.5106, 0.4836, 0.4253, 0.3441, 0.2462, 0.1361])\n",
      "----------------------------------------\n",
      "iter  0  stage  17  ep  11404   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([11, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13,  0])\n",
      "loss=  tensor(0.0041, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.4808, 3.4808, 3.4808, 3.4808, 3.4808, 3.4808, 3.4808, 3.4808, 3.4808,\n",
      "        3.4808, 3.4808, 3.4808, 3.4808, 3.4808, 3.4808, 3.4808, 3.4808, 3.4808,\n",
      "        2.9476, 2.4609, 2.0076, 1.5787, 1.1675, 0.7695, 0.3812]) return=  130250.32620960443\n",
      "probs of actions:  tensor([1.3511e-04, 9.9879e-01, 9.9887e-01, 9.9912e-01, 9.9907e-01, 9.9933e-01,\n",
      "        9.9877e-01, 9.9826e-01, 9.9875e-01, 9.9946e-01, 9.9946e-01, 9.9876e-01,\n",
      "        9.9916e-01, 9.9934e-01, 9.9897e-01, 9.9902e-01, 9.9923e-01, 9.9906e-01,\n",
      "        9.9938e-01, 9.9941e-01, 9.9971e-01, 9.9991e-01, 9.9989e-01, 9.9131e-01,\n",
      "        1.0000e+00], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5475, 0.5395, 0.5336, 0.5292, 0.5259, 0.5234, 0.5215, 0.5201,\n",
      "        0.5191, 0.5183, 0.5177, 0.5173, 0.5170, 0.5167, 0.5166, 0.5164, 0.5163,\n",
      "        0.5162, 0.5162, 0.5161, 0.5161, 0.5161, 0.5161, 0.5329])\n",
      "finalReturns:  tensor([0.6652, 0.6821, 0.6526, 0.5897, 0.5025, 0.3975, 0.2795, 0.1518])\n",
      "----------------------------------------\n",
      "iter  0  stage  16  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13,  0])\n",
      "loss=  tensor(0.0054, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.4037, 3.4037, 3.4037, 3.4037, 3.4037, 3.4037, 3.4037, 3.4037, 3.4037,\n",
      "        3.4037, 3.4037, 3.4037, 3.4037, 3.4037, 3.4037, 3.4037, 3.4037, 2.9264,\n",
      "        2.4932, 2.0917, 1.7132, 1.3515, 1.0022, 0.6619, 0.3284]) return=  119073.90192566637\n",
      "probs of actions:  tensor([0.9993, 0.9988, 0.9989, 0.9991, 0.9990, 0.9993, 0.9987, 0.9981, 0.9986,\n",
      "        0.9994, 0.9994, 0.9986, 0.9991, 0.9993, 0.9988, 0.9989, 0.9991, 0.9989,\n",
      "        0.9993, 0.9993, 0.9997, 0.9999, 0.9999, 0.9904, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4943, 0.5550, 0.5302, 0.5120, 0.4986, 0.4886, 0.4811, 0.4756, 0.4715,\n",
      "        0.4684, 0.4661, 0.4644, 0.4631, 0.4621, 0.4614, 0.4608, 0.4604, 0.4601,\n",
      "        0.4599, 0.4597, 0.4596, 0.4595, 0.4594, 0.4594, 0.4762])\n",
      "finalReturns:  tensor([0.7505, 0.7674, 0.7405, 0.6821, 0.6009, 0.5030, 0.3928, 0.2737, 0.1478])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  0  stage  15  ep  38   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13,  0])\n",
      "loss=  tensor(0.0065, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.7298, 3.7298, 3.7298, 3.7298, 3.7298, 3.7298, 3.7298, 3.7298, 3.7298,\n",
      "        3.7298, 3.7298, 3.7298, 3.7298, 3.7298, 3.7298, 3.7298, 3.2521, 2.8186,\n",
      "        2.4169, 2.0383, 1.6765, 1.3270, 0.9867, 0.6532, 0.3247]) return=  119073.90192566637\n",
      "probs of actions:  tensor([0.9994, 0.9988, 0.9989, 0.9991, 0.9990, 0.9993, 0.9987, 0.9981, 0.9986,\n",
      "        0.9994, 0.9994, 0.9986, 0.9991, 0.9993, 0.9989, 0.9990, 0.9992, 0.9990,\n",
      "        0.9993, 0.9993, 0.9997, 0.9999, 0.9999, 0.9905, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4943, 0.5550, 0.5302, 0.5120, 0.4986, 0.4886, 0.4811, 0.4756, 0.4715,\n",
      "        0.4684, 0.4661, 0.4644, 0.4631, 0.4621, 0.4614, 0.4608, 0.4604, 0.4601,\n",
      "        0.4599, 0.4597, 0.4596, 0.4595, 0.4594, 0.4594, 0.4762])\n",
      "finalReturns:  tensor([0.8853, 0.9022, 0.8752, 0.8168, 0.7355, 0.6376, 0.5275, 0.4083, 0.2824,\n",
      "        0.1515])\n",
      "----------------------------------------\n",
      "iter  0  stage  14  ep  64   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13,  0])\n",
      "loss=  tensor(0.0078, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.0537, 4.0537, 4.0537, 4.0537, 4.0537, 4.0537, 4.0537, 4.0537, 4.0537,\n",
      "        4.0537, 4.0537, 4.0537, 4.0537, 4.0537, 4.0537, 3.5754, 3.1415, 2.7396,\n",
      "        2.3607, 1.9987, 1.6492, 1.3088, 0.9752, 0.6467, 0.3220]) return=  119073.90192566637\n",
      "probs of actions:  tensor([0.9994, 0.9988, 0.9989, 0.9991, 0.9990, 0.9993, 0.9987, 0.9981, 0.9987,\n",
      "        0.9994, 0.9994, 0.9987, 0.9991, 0.9993, 0.9990, 0.9991, 0.9993, 0.9990,\n",
      "        0.9993, 0.9994, 0.9997, 0.9999, 0.9999, 0.9906, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4943, 0.5550, 0.5302, 0.5120, 0.4986, 0.4886, 0.4811, 0.4756, 0.4715,\n",
      "        0.4684, 0.4661, 0.4644, 0.4631, 0.4621, 0.4614, 0.4608, 0.4604, 0.4601,\n",
      "        0.4599, 0.4597, 0.4596, 0.4595, 0.4594, 0.4594, 0.4762])\n",
      "finalReturns:  tensor([1.0228, 1.0397, 1.0127, 0.9542, 0.8730, 0.7751, 0.6649, 0.5457, 0.4198,\n",
      "        0.2889, 0.1543])\n",
      "----------------------------------------\n",
      "iter  0  stage  13  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13,  0])\n",
      "loss=  tensor(0.0091, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.3761, 4.3761, 4.3761, 4.3761, 4.3761, 4.3761, 4.3761, 4.3761, 4.3761,\n",
      "        4.3761, 4.3761, 4.3761, 4.3761, 4.3761, 3.8971, 3.4627, 3.0604, 2.6813,\n",
      "        2.3191, 1.9694, 1.6289, 1.2953, 0.9667, 0.6419, 0.3199]) return=  119073.90192566637\n",
      "probs of actions:  tensor([0.9994, 0.9988, 0.9989, 0.9991, 0.9990, 0.9993, 0.9987, 0.9982, 0.9987,\n",
      "        0.9994, 0.9994, 0.9987, 0.9991, 0.9993, 0.9990, 0.9991, 0.9993, 0.9990,\n",
      "        0.9993, 0.9994, 0.9997, 0.9999, 0.9999, 0.9906, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4943, 0.5550, 0.5302, 0.5120, 0.4986, 0.4886, 0.4811, 0.4756, 0.4715,\n",
      "        0.4684, 0.4661, 0.4644, 0.4631, 0.4621, 0.4614, 0.4608, 0.4604, 0.4601,\n",
      "        0.4599, 0.4597, 0.4596, 0.4595, 0.4594, 0.4594, 0.4762])\n",
      "finalReturns:  tensor([1.1624, 1.1793, 1.1523, 1.0938, 1.0125, 0.9146, 0.8044, 0.6852, 0.5592,\n",
      "        0.4283, 0.2937, 0.1563])\n",
      "----------------------------------------\n",
      "iter  0  stage  12  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13,  0])\n",
      "loss=  tensor(0.0108, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.6979, 4.6979, 4.6979, 4.6979, 4.6979, 4.6979, 4.6979, 4.6979, 4.6979,\n",
      "        4.6979, 4.6979, 4.6979, 4.6979, 4.2179, 3.7828, 3.3800, 3.0005, 2.6381,\n",
      "        2.2882, 1.9476, 1.6138, 1.2851, 0.9603, 0.6382, 0.3183]) return=  119073.90192566637\n",
      "probs of actions:  tensor([0.9994, 0.9988, 0.9989, 0.9991, 0.9990, 0.9993, 0.9987, 0.9982, 0.9987,\n",
      "        0.9994, 0.9994, 0.9987, 0.9991, 0.9993, 0.9990, 0.9991, 0.9993, 0.9990,\n",
      "        0.9993, 0.9994, 0.9997, 0.9999, 0.9999, 0.9906, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4943, 0.5550, 0.5302, 0.5120, 0.4986, 0.4886, 0.4811, 0.4756, 0.4715,\n",
      "        0.4684, 0.4661, 0.4644, 0.4631, 0.4621, 0.4614, 0.4608, 0.4604, 0.4601,\n",
      "        0.4599, 0.4597, 0.4596, 0.4595, 0.4594, 0.4594, 0.4762])\n",
      "finalReturns:  tensor([1.3037, 1.3206, 1.2936, 1.2350, 1.1537, 1.0557, 0.9455, 0.8262, 0.7003,\n",
      "        0.5694, 0.4347, 0.2973, 0.1579])\n",
      "----------------------------------------\n",
      "iter  0  stage  11  ep  197   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13,  0])\n",
      "loss=  tensor(0.0113, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.0197, 5.0197, 5.0197, 5.0197, 5.0197, 5.0197, 5.0197, 5.0197, 5.0197,\n",
      "        5.0197, 5.0197, 5.0197, 4.5384, 4.1024, 3.6989, 3.3190, 2.9562, 2.6060,\n",
      "        2.2652, 1.9313, 1.6025, 1.2776, 0.9555, 0.6355, 0.3172]) return=  119073.90192566637\n",
      "probs of actions:  tensor([0.9994, 0.9989, 0.9989, 0.9992, 0.9991, 0.9993, 0.9988, 0.9982, 0.9987,\n",
      "        0.9994, 0.9994, 0.9990, 0.9993, 0.9995, 0.9992, 0.9992, 0.9994, 0.9990,\n",
      "        0.9994, 0.9994, 0.9997, 0.9999, 0.9999, 0.9910, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4943, 0.5550, 0.5302, 0.5120, 0.4986, 0.4886, 0.4811, 0.4756, 0.4715,\n",
      "        0.4684, 0.4661, 0.4644, 0.4631, 0.4621, 0.4614, 0.4608, 0.4604, 0.4601,\n",
      "        0.4599, 0.4597, 0.4596, 0.4595, 0.4594, 0.4594, 0.4762])\n",
      "finalReturns:  tensor([1.4463, 1.4632, 1.4361, 1.3775, 1.2961, 1.1980, 1.0878, 0.9685, 0.8425,\n",
      "        0.7116, 0.5769, 0.4395, 0.3001, 0.1591])\n",
      "----------------------------------------\n",
      "iter  0  stage  10  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13,  0])\n",
      "loss=  tensor(0.0126, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.3422, 5.3422, 5.3422, 5.3422, 5.3422, 5.3422, 5.3422, 5.3422, 5.3422,\n",
      "        5.3422, 5.3422, 4.8592, 4.4220, 4.0176, 3.6370, 3.2738, 2.9233, 2.5822,\n",
      "        2.2481, 1.9191, 1.5941, 1.2719, 0.9519, 0.6335, 0.3163]) return=  119073.90192566637\n",
      "probs of actions:  tensor([0.9994, 0.9989, 0.9989, 0.9992, 0.9991, 0.9993, 0.9988, 0.9982, 0.9987,\n",
      "        0.9994, 0.9994, 0.9990, 0.9993, 0.9995, 0.9992, 0.9992, 0.9994, 0.9990,\n",
      "        0.9994, 0.9994, 0.9997, 0.9999, 0.9999, 0.9910, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4943, 0.5550, 0.5302, 0.5120, 0.4986, 0.4886, 0.4811, 0.4756, 0.4715,\n",
      "        0.4684, 0.4661, 0.4644, 0.4631, 0.4621, 0.4614, 0.4608, 0.4604, 0.4601,\n",
      "        0.4599, 0.4597, 0.4596, 0.4595, 0.4594, 0.4594, 0.4762])\n",
      "finalReturns:  tensor([1.5898, 1.6067, 1.5796, 1.5209, 1.4394, 1.3413, 1.2310, 1.1116, 0.9856,\n",
      "        0.8546, 0.7200, 0.5826, 0.4431, 0.3021, 0.1599])\n",
      "----------------------------------------\n",
      "iter  0  stage  9  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13,  0])\n",
      "loss=  tensor(0.0140, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.6662, 5.6662, 5.6662, 5.6662, 5.6662, 5.6662, 5.6662, 5.6662, 5.6662,\n",
      "        5.6662, 5.1809, 4.7421, 4.3365, 3.9550, 3.5911, 3.2402, 2.8987, 2.5644,\n",
      "        2.2353, 1.9101, 1.5878, 1.2677, 0.9492, 0.6320, 0.3156]) return=  119073.90192566637\n",
      "probs of actions:  tensor([0.9994, 0.9989, 0.9989, 0.9992, 0.9991, 0.9993, 0.9988, 0.9982, 0.9987,\n",
      "        0.9994, 0.9994, 0.9990, 0.9993, 0.9995, 0.9992, 0.9992, 0.9994, 0.9990,\n",
      "        0.9994, 0.9994, 0.9997, 0.9999, 0.9999, 0.9910, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4943, 0.5550, 0.5302, 0.5120, 0.4986, 0.4886, 0.4811, 0.4756, 0.4715,\n",
      "        0.4684, 0.4661, 0.4644, 0.4631, 0.4621, 0.4614, 0.4608, 0.4604, 0.4601,\n",
      "        0.4599, 0.4597, 0.4596, 0.4595, 0.4594, 0.4594, 0.4762])\n",
      "finalReturns:  tensor([1.7342, 1.7511, 1.7239, 1.6651, 1.5835, 1.4853, 1.3749, 1.2555, 1.1294,\n",
      "        0.9984, 0.8637, 0.7263, 0.5868, 0.4458, 0.3036, 0.1606])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  0  stage  8  ep  109   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13,  0])\n",
      "loss=  tensor(0.0145, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.9926, 5.9926, 5.9926, 5.9926, 5.9926, 5.9926, 5.9926, 5.9926, 5.9926,\n",
      "        5.5042, 5.0631, 4.6560, 4.2733, 3.9086, 3.5570, 3.2151, 2.8804, 2.5511,\n",
      "        2.2257, 1.9033, 1.5831, 1.2645, 0.9472, 0.6308, 0.3152]) return=  119073.90192566637\n",
      "probs of actions:  tensor([0.9994, 0.9989, 0.9990, 0.9992, 0.9992, 0.9994, 0.9988, 0.9984, 0.9990,\n",
      "        0.9996, 0.9996, 0.9991, 0.9994, 0.9995, 0.9993, 0.9992, 0.9995, 0.9991,\n",
      "        0.9994, 0.9994, 0.9997, 0.9999, 0.9999, 0.9917, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4943, 0.5550, 0.5302, 0.5120, 0.4986, 0.4886, 0.4811, 0.4756, 0.4715,\n",
      "        0.4684, 0.4661, 0.4644, 0.4631, 0.4621, 0.4614, 0.4608, 0.4604, 0.4601,\n",
      "        0.4599, 0.4597, 0.4596, 0.4595, 0.4594, 0.4594, 0.4762])\n",
      "finalReturns:  tensor([1.8793, 1.8962, 1.8689, 1.8100, 1.7283, 1.6299, 1.5194, 1.3999, 1.2738,\n",
      "        1.1427, 1.0080, 0.8705, 0.7310, 0.5900, 0.4478, 0.3047, 0.1611])\n",
      "----------------------------------------\n",
      "iter  0  stage  7  ep  3450   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13,  0])\n",
      "loss=  tensor(0.0113, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.3224, 6.3224, 6.3224, 6.3224, 6.3224, 6.3224, 6.3224, 6.3224, 5.8299,\n",
      "        5.3859, 4.9766, 4.5924, 4.2265, 3.8741, 3.5316, 3.1965, 2.8668, 2.5411,\n",
      "        2.2185, 1.8982, 1.5795, 1.2622, 0.9457, 0.6300, 0.3148]) return=  119073.90192566637\n",
      "probs of actions:  tensor([0.9995, 0.9991, 0.9992, 0.9993, 0.9993, 0.9995, 0.9990, 0.9990, 0.9994,\n",
      "        0.9998, 0.9996, 0.9996, 0.9998, 0.9999, 0.9994, 0.9993, 0.9998, 0.9994,\n",
      "        0.9995, 0.9996, 0.9999, 1.0000, 0.9999, 0.9931, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4943, 0.5550, 0.5302, 0.5120, 0.4986, 0.4886, 0.4811, 0.4756, 0.4715,\n",
      "        0.4684, 0.4661, 0.4644, 0.4631, 0.4621, 0.4614, 0.4608, 0.4604, 0.4601,\n",
      "        0.4599, 0.4597, 0.4596, 0.4595, 0.4594, 0.4594, 0.4762])\n",
      "finalReturns:  tensor([2.0251, 2.0420, 2.0146, 1.9555, 1.8736, 1.7750, 1.6644, 1.5448, 1.4186,\n",
      "        1.2874, 1.1527, 1.0152, 0.8756, 0.7345, 0.5923, 0.4493, 0.3056, 0.1614])\n",
      "----------------------------------------\n",
      "iter  0  stage  6  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13,  0])\n",
      "loss=  tensor(0.0138, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.6570, 6.6570, 6.6570, 6.6570, 6.6570, 6.6570, 6.6570, 6.1590, 5.7110,\n",
      "        5.2989, 4.9127, 4.5453, 4.1918, 3.8484, 3.5127, 3.1825, 2.8566, 2.5337,\n",
      "        2.2132, 1.8944, 1.5769, 1.2604, 0.9446, 0.6293, 0.3145]) return=  119073.90192566637\n",
      "probs of actions:  tensor([0.9995, 0.9991, 0.9992, 0.9993, 0.9993, 0.9995, 0.9990, 0.9990, 0.9994,\n",
      "        0.9998, 0.9996, 0.9996, 0.9998, 0.9999, 0.9994, 0.9993, 0.9998, 0.9994,\n",
      "        0.9995, 0.9996, 0.9999, 1.0000, 0.9999, 0.9931, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4943, 0.5550, 0.5302, 0.5120, 0.4986, 0.4886, 0.4811, 0.4756, 0.4715,\n",
      "        0.4684, 0.4661, 0.4644, 0.4631, 0.4621, 0.4614, 0.4608, 0.4604, 0.4601,\n",
      "        0.4599, 0.4597, 0.4596, 0.4595, 0.4594, 0.4594, 0.4762])\n",
      "finalReturns:  tensor([2.1717, 2.1886, 2.1609, 2.1016, 2.0194, 1.9207, 1.8098, 1.6901, 1.5637,\n",
      "        1.4325, 1.2977, 1.1601, 1.0205, 0.8794, 0.7372, 0.5941, 0.4504, 0.3062,\n",
      "        0.1617])\n",
      "----------------------------------------\n",
      "iter  0  stage  5  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13,  0])\n",
      "loss=  tensor(0.0154, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.9983, 6.9983, 6.9983, 6.9983, 6.9983, 6.9983, 6.4928, 6.0396, 5.6236,\n",
      "        5.2347, 4.8653, 4.5103, 4.1659, 3.8293, 3.4985, 3.1721, 2.8489, 2.5281,\n",
      "        2.2092, 1.8915, 1.5749, 1.2590, 0.9437, 0.6289, 0.3143]) return=  119073.90192566637\n",
      "probs of actions:  tensor([0.9995, 0.9991, 0.9992, 0.9993, 0.9993, 0.9995, 0.9990, 0.9990, 0.9994,\n",
      "        0.9998, 0.9996, 0.9996, 0.9998, 0.9999, 0.9994, 0.9993, 0.9998, 0.9994,\n",
      "        0.9995, 0.9996, 0.9999, 1.0000, 0.9999, 0.9931, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4943, 0.5550, 0.5302, 0.5120, 0.4986, 0.4886, 0.4811, 0.4756, 0.4715,\n",
      "        0.4684, 0.4661, 0.4644, 0.4631, 0.4621, 0.4614, 0.4608, 0.4604, 0.4601,\n",
      "        0.4599, 0.4597, 0.4596, 0.4595, 0.4594, 0.4594, 0.4762])\n",
      "finalReturns:  tensor([2.3190, 2.3359, 2.3080, 2.2483, 2.1658, 2.0668, 1.9557, 1.8357, 1.7092,\n",
      "        1.5779, 1.4429, 1.3053, 1.1657, 1.0245, 0.8823, 0.7392, 0.5955, 0.4513,\n",
      "        0.3067, 0.1619])\n",
      "----------------------------------------\n",
      "iter  0  stage  4  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13,  0])\n",
      "loss=  tensor(0.0175, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.3486, 7.3486, 7.3486, 7.3486, 7.3486, 6.8331, 6.3728, 5.9518, 5.5591,\n",
      "        5.1870, 4.8301, 4.4842, 4.1465, 3.8150, 3.4879, 3.1643, 2.8432, 2.5240,\n",
      "        2.2061, 1.8894, 1.5734, 1.2580, 0.9431, 0.6285, 0.3142]) return=  119073.90192566637\n",
      "probs of actions:  tensor([0.9995, 0.9991, 0.9992, 0.9993, 0.9993, 0.9995, 0.9990, 0.9990, 0.9994,\n",
      "        0.9998, 0.9996, 0.9996, 0.9998, 0.9999, 0.9994, 0.9993, 0.9998, 0.9994,\n",
      "        0.9995, 0.9996, 0.9999, 1.0000, 0.9999, 0.9931, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4943, 0.5550, 0.5302, 0.5120, 0.4986, 0.4886, 0.4811, 0.4756, 0.4715,\n",
      "        0.4684, 0.4661, 0.4644, 0.4631, 0.4621, 0.4614, 0.4608, 0.4604, 0.4601,\n",
      "        0.4599, 0.4597, 0.4596, 0.4595, 0.4594, 0.4594, 0.4762])\n",
      "finalReturns:  tensor([2.4672, 2.4841, 2.4559, 2.3957, 2.3128, 2.2134, 2.1020, 1.9818, 1.8551,\n",
      "        1.7236, 1.5885, 1.4507, 1.3110, 1.1698, 1.0275, 0.8844, 0.7407, 0.5965,\n",
      "        0.4519, 0.3071, 0.1621])\n",
      "----------------------------------------\n",
      "iter  0  stage  3  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13,  0])\n",
      "loss=  tensor(0.0196, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.7112, 7.7112, 7.7112, 7.7112, 7.1822, 6.7124, 6.2846, 5.8870, 5.5113,\n",
      "        5.1517, 4.8038, 4.4647, 4.1321, 3.8043, 3.4800, 3.1585, 2.8389, 2.5208,\n",
      "        2.2039, 1.8878, 1.5723, 1.2573, 0.9426, 0.6282, 0.3140]) return=  119073.90192566637\n",
      "probs of actions:  tensor([0.9995, 0.9991, 0.9992, 0.9993, 0.9993, 0.9995, 0.9990, 0.9990, 0.9994,\n",
      "        0.9998, 0.9996, 0.9996, 0.9998, 0.9999, 0.9994, 0.9993, 0.9998, 0.9994,\n",
      "        0.9995, 0.9996, 0.9999, 1.0000, 0.9999, 0.9931, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4943, 0.5550, 0.5302, 0.5120, 0.4986, 0.4886, 0.4811, 0.4756, 0.4715,\n",
      "        0.4684, 0.4661, 0.4644, 0.4631, 0.4621, 0.4614, 0.4608, 0.4604, 0.4601,\n",
      "        0.4599, 0.4597, 0.4596, 0.4595, 0.4594, 0.4594, 0.4762])\n",
      "finalReturns:  tensor([2.6167, 2.6336, 2.6048, 2.5441, 2.4606, 2.3606, 2.2488, 2.1282, 2.0012,\n",
      "        1.8695, 1.7343, 1.5964, 1.4566, 1.3153, 1.1730, 1.0298, 0.8860, 0.7418,\n",
      "        0.5972, 0.4524, 0.3073, 0.1622])\n",
      "----------------------------------------\n",
      "iter  0  stage  2  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13,  0])\n",
      "loss=  tensor(0.0223, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.0905, 8.0905, 8.0905, 7.5434, 7.0607, 6.6237, 6.2194, 5.8389, 5.4757,\n",
      "        5.1253, 4.7843, 4.4502, 4.1213, 3.7963, 3.4741, 3.1541, 2.8357, 2.5185,\n",
      "        2.2022, 1.8866, 1.5715, 1.2567, 0.9423, 0.6280, 0.3140]) return=  119073.90192566637\n",
      "probs of actions:  tensor([0.9995, 0.9991, 0.9992, 0.9993, 0.9993, 0.9995, 0.9990, 0.9990, 0.9994,\n",
      "        0.9998, 0.9996, 0.9996, 0.9998, 0.9999, 0.9994, 0.9993, 0.9998, 0.9994,\n",
      "        0.9995, 0.9996, 0.9999, 1.0000, 0.9999, 0.9931, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4943, 0.5550, 0.5302, 0.5120, 0.4986, 0.4886, 0.4811, 0.4756, 0.4715,\n",
      "        0.4684, 0.4661, 0.4644, 0.4631, 0.4621, 0.4614, 0.4608, 0.4604, 0.4601,\n",
      "        0.4599, 0.4597, 0.4596, 0.4595, 0.4594, 0.4594, 0.4762])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalReturns:  tensor([2.7675, 2.7844, 2.7551, 2.6936, 2.6093, 2.5086, 2.3962, 2.2752, 2.1478,\n",
      "        2.0158, 1.8803, 1.7423, 1.6023, 1.4610, 1.3185, 1.1753, 1.0315, 0.8872,\n",
      "        0.7426, 0.5978, 0.4527, 0.3075, 0.1623])\n",
      "----------------------------------------\n",
      "iter  0  stage  1  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13,  0])\n",
      "loss=  tensor(0.0253, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.4928, 8.4928, 7.9208, 7.4207, 6.9713, 6.5581, 6.1711, 5.8032, 5.4492,\n",
      "        5.1056, 4.7696, 4.4393, 4.1132, 3.7903, 3.4697, 3.1508, 2.8333, 2.5167,\n",
      "        2.2009, 1.8857, 1.5708, 1.2563, 0.9420, 0.6279, 0.3139]) return=  119073.90192566637\n",
      "probs of actions:  tensor([0.9995, 0.9991, 0.9992, 0.9993, 0.9993, 0.9995, 0.9990, 0.9990, 0.9995,\n",
      "        0.9998, 0.9996, 0.9996, 0.9998, 0.9999, 0.9994, 0.9993, 0.9998, 0.9994,\n",
      "        0.9995, 0.9996, 0.9999, 1.0000, 0.9999, 0.9931, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4943, 0.5550, 0.5302, 0.5120, 0.4986, 0.4886, 0.4811, 0.4756, 0.4715,\n",
      "        0.4684, 0.4661, 0.4644, 0.4631, 0.4621, 0.4614, 0.4608, 0.4604, 0.4601,\n",
      "        0.4599, 0.4597, 0.4596, 0.4595, 0.4594, 0.4594, 0.4762])\n",
      "finalReturns:  tensor([2.9203, 2.9372, 2.9071, 2.8445, 2.7591, 2.6576, 2.5444, 2.4227, 2.2948,\n",
      "        2.1624, 2.0267, 1.8884, 1.7483, 1.6068, 1.4642, 1.3209, 1.1771, 1.0328,\n",
      "        0.8881, 0.7432, 0.5982, 0.4530, 0.3077, 0.1623])\n",
      "----------------------------------------\n",
      "iter  0  stage  0  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13,  0])\n",
      "loss=  tensor(0.0272, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.8318, 8.3206, 7.7967, 7.3305, 6.9053, 6.5095, 6.1352, 5.7766, 5.4295,\n",
      "        5.0909, 4.7587, 4.4312, 4.1072, 3.7858, 3.4663, 3.1484, 2.8315, 2.5154,\n",
      "        2.2000, 1.8850, 1.5704, 1.2560, 0.9418, 0.6278, 0.3138]) return=  119073.90192566637\n",
      "probs of actions:  tensor([0.9995, 0.9991, 0.9992, 0.9993, 0.9993, 0.9995, 0.9990, 0.9990, 0.9995,\n",
      "        0.9998, 0.9996, 0.9996, 0.9998, 0.9999, 0.9994, 0.9993, 0.9998, 0.9994,\n",
      "        0.9995, 0.9996, 0.9999, 1.0000, 0.9999, 0.9931, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4943, 0.5550, 0.5302, 0.5120, 0.4986, 0.4886, 0.4811, 0.4756, 0.4715,\n",
      "        0.4684, 0.4661, 0.4644, 0.4631, 0.4621, 0.4614, 0.4608, 0.4604, 0.4601,\n",
      "        0.4599, 0.4597, 0.4596, 0.4595, 0.4594, 0.4594, 0.4762])\n",
      "finalReturns:  tensor([3.0756, 3.0925, 3.0613, 2.9973, 2.9105, 2.8077, 2.6935, 2.5710, 2.4424,\n",
      "        2.3095, 2.1734, 2.0348, 1.8944, 1.7528, 1.6101, 1.4667, 1.3227, 1.1784,\n",
      "        1.0337, 0.8888, 0.7437, 0.5985, 0.4532, 0.3078, 0.1624])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1682999908 saved\n",
      "[674563, 'tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])', 119073.90192566637, 95284.79046306085, 0.027228858321905136, 1e-05, 1, 0, 'tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\\n        13, 13, 13, 13, 13, 13,  0])', '[1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\\n 1.   1.   1.   1.   1.   1.   1.   1.   1.   0.99 1.  ]', '0,[1e-05,1][1, 10000, 1, 1],1682999908', 25, 50, 158873.58343048888, 178559.0269066819, 70287.27661109495, 135436.67200000002, 132554.6666666667, 119073.90192566635, 119073.90192566635, 125827.28267117329, 125827.28267117329, 81831.91688437786, 119073.90192566635, 125827.28267117329]\n",
      "policy reset\n",
      "----------------------------------------\n",
      "iter  1  stage  24  ep  99999   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.4718, 0.4718, 0.4718, 0.4718, 0.4718, 0.4718, 0.4718, 0.4718, 0.4718,\n",
      "        0.4718, 0.4718, 0.4718, 0.4718, 0.4718, 0.4718, 0.4718, 0.4718, 0.4718,\n",
      "        0.4718, 0.4718, 0.4718, 0.4718, 0.4718, 0.4718, 0.4718]) return=  131596.28685975395\n",
      "probs of actions:  tensor([0.9367, 0.9428, 0.9187, 0.9468, 0.9307, 0.9152, 0.9421, 0.9478, 0.9220,\n",
      "        0.9468, 0.9338, 0.9398, 0.9479, 0.0224, 0.9485, 0.9489, 0.9547, 0.9500,\n",
      "        0.9286, 0.9317, 0.9214, 0.0105, 0.9444, 0.9322, 0.9943],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5238, 0.5334, 0.5406, 0.5460, 0.5501, 0.5532, 0.5555, 0.5573,\n",
      "        0.5586, 0.5595, 0.5603, 0.5608, 0.5612, 0.5653, 0.5386, 0.5190, 0.5046,\n",
      "        0.4938, 0.4859, 0.4800, 0.4751, 0.4791, 0.4749, 0.4718])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  1  stage  23  ep  99999   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([ 9, 10,  4,  9,  9,  0,  0,  7, 10, 10,  9,  0, 11,  9,  7,  0,  9,  8,\n",
      "         7,  8,  8,  9,  9,  9,  0])\n",
      "loss=  tensor(0.0078, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.7901, 0.7901, 0.7901, 0.7901, 0.7901, 0.7901, 0.7901, 0.7901, 0.7901,\n",
      "        0.7901, 0.7901, 0.7901, 0.7901, 0.7901, 0.7901, 0.7901, 0.7901, 0.7901,\n",
      "        0.7901, 0.7901, 0.7901, 0.7901, 0.7901, 0.7901, 0.3823]) return=  106695.37744149746\n",
      "probs of actions:  tensor([0.4556, 0.1081, 0.0112, 0.4412, 0.4102, 0.1664, 0.2821, 0.0526, 0.1414,\n",
      "        0.1179, 0.4751, 0.2725, 0.0115, 0.4704, 0.0645, 0.2235, 0.4843, 0.0924,\n",
      "        0.0722, 0.1001, 0.0992, 0.3777, 0.4121, 0.6827, 0.9964],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5031, 0.5469, 0.5947, 0.5226, 0.4935, 0.4803, 0.4345, 0.3968, 0.3897,\n",
      "        0.3977, 0.4057, 0.4152, 0.3756, 0.3937, 0.4012, 0.4030, 0.3707, 0.3823,\n",
      "        0.3882, 0.3869, 0.3901, 0.3909, 0.3959, 0.3997, 0.4106])\n",
      "finalReturns:  tensor([0.0202, 0.0283])\n",
      "----------------------------------------\n",
      "iter  1  stage  22  ep  99999   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([10, 10,  9, 10,  9, 10, 10,  9, 10, 10, 16, 10,  9, 10,  9, 10, 10, 10,\n",
      "        10, 11, 16,  9, 11, 10,  0])\n",
      "loss=  tensor(0.2371, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2213, 1.2213, 1.2213, 1.2213, 1.2213, 1.2213, 1.2213, 1.2213, 1.2213,\n",
      "        1.2213, 1.2213, 1.2213, 1.2213, 1.2213, 1.2213, 1.2213, 1.2213, 1.2213,\n",
      "        1.2213, 1.2213, 1.2213, 1.2213, 1.2213, 0.7816, 0.3779]) return=  113419.76902528579\n",
      "probs of actions:  tensor([0.5822, 0.5528, 0.2460, 0.5698, 0.2272, 0.5105, 0.5893, 0.2325, 0.5383,\n",
      "        0.5680, 0.0183, 0.5190, 0.2591, 0.5497, 0.2466, 0.5711, 0.5934, 0.5512,\n",
      "        0.5538, 0.0713, 0.0181, 0.2689, 0.0841, 0.4928, 0.9997],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5012, 0.5506, 0.5911, 0.5393, 0.5089, 0.4798, 0.4634, 0.4531, 0.4389,\n",
      "        0.4330, 0.4131, 0.4455, 0.4398, 0.4290, 0.4276, 0.4199, 0.4189, 0.4181,\n",
      "        0.4175, 0.4150, 0.4044, 0.4407, 0.4276, 0.4295, 0.4360])\n",
      "finalReturns:  tensor([0.0718, 0.0839, 0.0582])\n",
      "----------------------------------------\n",
      "iter  1  stage  21  ep  99999   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([20, 16, 20, 20, 16, 16, 20, 20, 20, 20, 20, 10, 14, 20, 16, 16, 20, 16,\n",
      "        16, 20, 16, 16, 16, 14,  0])\n",
      "loss=  tensor(0.8845, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.1640, 1.1640, 1.1640, 1.1640, 1.1640, 1.1640, 1.1640, 1.1640, 1.1640,\n",
      "        1.1640, 1.1640, 1.1640, 1.1640, 1.1640, 1.1640, 1.1640, 1.1640, 1.1640,\n",
      "        1.1640, 1.1640, 1.1640, 1.1640, 0.8126, 0.5105, 0.2430]) return=  91897.17702893353\n",
      "probs of actions:  tensor([0.5741, 0.2196, 0.5670, 0.5593, 0.1998, 0.2072, 0.5479, 0.5890, 0.5455,\n",
      "        0.5652, 0.5592, 0.0451, 0.0846, 0.5561, 0.2024, 0.2001, 0.5917, 0.1945,\n",
      "        0.2057, 0.5518, 0.2183, 0.1572, 0.1927, 0.1121, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5731, 0.4861, 0.4486, 0.4358, 0.4027, 0.3643, 0.3593, 0.3555,\n",
      "        0.3527, 0.3506, 0.3791, 0.3378, 0.3062, 0.3299, 0.3251, 0.3071, 0.3306,\n",
      "        0.3256, 0.3075, 0.3309, 0.3259, 0.3221, 0.3252, 0.3369])\n",
      "finalReturns:  tensor([0.1460, 0.1716, 0.1516, 0.0939])\n",
      "----------------------------------------\n",
      "iter  1  stage  20  ep  99999   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20,  0])\n",
      "loss=  tensor(0.0227, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.4830, 1.4830, 1.4830, 1.4830, 1.4830, 1.4830, 1.4830, 1.4830, 1.4830,\n",
      "        1.4830, 1.4830, 1.4830, 1.4830, 1.4830, 1.4830, 1.4830, 1.4830, 1.4830,\n",
      "        1.4830, 1.4830, 1.4830, 1.0978, 0.7724, 0.4884, 0.2338]) return=  95926.91902269641\n",
      "probs of actions:  tensor([0.9743, 0.9730, 0.9708, 0.9723, 0.9681, 0.9578, 0.9686, 0.9745, 0.9643,\n",
      "        0.9707, 0.9698, 0.9638, 0.9648, 0.9697, 0.9709, 0.9736, 0.9771, 0.9706,\n",
      "        0.9658, 0.9669, 0.9809, 0.9906, 0.9810, 0.9655, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5587, 0.5007, 0.4591, 0.4290, 0.4071, 0.3910, 0.3791, 0.3703,\n",
      "        0.3637, 0.3588, 0.3552, 0.3525, 0.3505, 0.3489, 0.3478, 0.3470, 0.3463,\n",
      "        0.3458, 0.3455, 0.3452, 0.3450, 0.3449, 0.3447, 0.3847])\n",
      "finalReturns:  tensor([0.2814, 0.3214, 0.3019, 0.2410, 0.1509])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  19  ep  75221   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([16, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20,  0])\n",
      "loss=  tensor(0.0016, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.2476, 2.2476, 2.2476, 2.2476, 2.2476, 2.2476, 2.2476, 2.2476, 2.2476,\n",
      "        2.2476, 2.2476, 2.2476, 2.2476, 2.2476, 2.2476, 2.2476, 2.2476, 2.2476,\n",
      "        2.2476, 2.2476, 1.7571, 1.3343, 0.9589, 0.6172, 0.2998]) return=  116915.56065675804\n",
      "probs of actions:  tensor([0.0013, 0.9986, 0.9984, 0.9986, 0.9983, 0.9976, 0.9984, 0.9988, 0.9981,\n",
      "        0.9985, 0.9985, 0.9982, 0.9982, 0.9985, 0.9986, 0.9988, 0.9990, 0.9986,\n",
      "        0.9983, 0.9991, 0.9993, 0.9998, 0.9994, 0.9986, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4856, 0.5433, 0.5192, 0.5015, 0.4884, 0.4786, 0.4714, 0.4660, 0.4620,\n",
      "        0.4590, 0.4567, 0.4550, 0.4538, 0.4528, 0.4521, 0.4516, 0.4512, 0.4509,\n",
      "        0.4507, 0.4505, 0.4504, 0.4503, 0.4502, 0.4502, 0.4901])\n",
      "finalReturns:  tensor([0.4940, 0.5340, 0.5065, 0.4316, 0.3231, 0.1903])\n",
      "----------------------------------------\n",
      "iter  1  stage  18  ep  10555   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([16, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20,  0])\n",
      "loss=  tensor(0.0016, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.5351, 2.5351, 2.5351, 2.5351, 2.5351, 2.5351, 2.5351, 2.5351, 2.5351,\n",
      "        2.5351, 2.5351, 2.5351, 2.5351, 2.5351, 2.5351, 2.5351, 2.5351, 2.5351,\n",
      "        2.5351, 2.0444, 1.6215, 1.2460, 0.9042, 0.5868, 0.2870]) return=  116915.56065675804\n",
      "probs of actions:  tensor([8.3469e-04, 9.9912e-01, 9.9896e-01, 9.9913e-01, 9.9889e-01, 9.9846e-01,\n",
      "        9.9898e-01, 9.9922e-01, 9.9876e-01, 9.9908e-01, 9.9905e-01, 9.9887e-01,\n",
      "        9.9881e-01, 9.9905e-01, 9.9910e-01, 9.9924e-01, 9.9937e-01, 9.9911e-01,\n",
      "        9.9914e-01, 9.9956e-01, 9.9962e-01, 9.9994e-01, 9.9966e-01, 9.9913e-01,\n",
      "        1.0000e+00], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4856, 0.5433, 0.5192, 0.5015, 0.4884, 0.4786, 0.4714, 0.4660, 0.4620,\n",
      "        0.4590, 0.4567, 0.4550, 0.4538, 0.4528, 0.4521, 0.4516, 0.4512, 0.4509,\n",
      "        0.4507, 0.4505, 0.4504, 0.4503, 0.4502, 0.4502, 0.4901])\n",
      "finalReturns:  tensor([0.6572, 0.6972, 0.6697, 0.5948, 0.4862, 0.3535, 0.2032])\n",
      "----------------------------------------\n",
      "iter  1  stage  17  ep  51   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20,  0])\n",
      "loss=  tensor(0.0027, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.1125, 2.1125, 2.1125, 2.1125, 2.1125, 2.1125, 2.1125, 2.1125, 2.1125,\n",
      "        2.1125, 2.1125, 2.1125, 2.1125, 2.1125, 2.1125, 2.1125, 2.1125, 2.1125,\n",
      "        1.7262, 1.4000, 1.1155, 0.8604, 0.6264, 0.4075, 0.1997]) return=  95926.91902269641\n",
      "probs of actions:  tensor([0.9991, 0.9991, 0.9989, 0.9991, 0.9988, 0.9983, 0.9988, 0.9991, 0.9986,\n",
      "        0.9989, 0.9989, 0.9986, 0.9986, 0.9989, 0.9989, 0.9991, 0.9992, 0.9990,\n",
      "        0.9990, 0.9995, 0.9995, 0.9999, 0.9996, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5587, 0.5007, 0.4591, 0.4290, 0.4071, 0.3910, 0.3791, 0.3703,\n",
      "        0.3637, 0.3588, 0.3552, 0.3525, 0.3505, 0.3489, 0.3478, 0.3470, 0.3463,\n",
      "        0.3458, 0.3455, 0.3452, 0.3450, 0.3449, 0.3447, 0.3847])\n",
      "finalReturns:  tensor([0.6896, 0.7296, 0.7100, 0.6490, 0.5588, 0.4478, 0.3219, 0.1849])\n",
      "----------------------------------------\n",
      "iter  1  stage  16  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20,  0])\n",
      "loss=  tensor(0.0038, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.3083, 2.3083, 2.3083, 2.3083, 2.3083, 2.3083, 2.3083, 2.3083, 2.3083,\n",
      "        2.3083, 2.3083, 2.3083, 2.3083, 2.3083, 2.3083, 2.3083, 2.3083, 1.9214,\n",
      "        1.5947, 1.3099, 1.0547, 0.8205, 0.6015, 0.3936, 0.1938]) return=  95926.91902269641\n",
      "probs of actions:  tensor([0.9991, 0.9991, 0.9989, 0.9991, 0.9988, 0.9983, 0.9988, 0.9991, 0.9986,\n",
      "        0.9989, 0.9989, 0.9986, 0.9986, 0.9989, 0.9989, 0.9991, 0.9992, 0.9990,\n",
      "        0.9990, 0.9995, 0.9995, 0.9999, 0.9996, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5587, 0.5007, 0.4591, 0.4290, 0.4071, 0.3910, 0.3791, 0.3703,\n",
      "        0.3637, 0.3588, 0.3552, 0.3525, 0.3505, 0.3489, 0.3478, 0.3470, 0.3463,\n",
      "        0.3458, 0.3455, 0.3452, 0.3450, 0.3449, 0.3447, 0.3847])\n",
      "finalReturns:  tensor([0.8407, 0.8807, 0.8611, 0.8000, 0.7098, 0.5988, 0.4728, 0.3358, 0.1909])\n",
      "----------------------------------------\n",
      "iter  1  stage  15  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20,  0])\n",
      "loss=  tensor(0.0052, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.5005, 2.5005, 2.5005, 2.5005, 2.5005, 2.5005, 2.5005, 2.5005, 2.5005,\n",
      "        2.5005, 2.5005, 2.5005, 2.5005, 2.5005, 2.5005, 2.5005, 2.1127, 1.7854,\n",
      "        1.5002, 1.2447, 1.0103, 0.7912, 0.5831, 0.3833, 0.1894]) return=  95926.91902269641\n",
      "probs of actions:  tensor([0.9991, 0.9991, 0.9989, 0.9991, 0.9988, 0.9983, 0.9988, 0.9991, 0.9986,\n",
      "        0.9989, 0.9989, 0.9986, 0.9986, 0.9989, 0.9989, 0.9991, 0.9992, 0.9990,\n",
      "        0.9990, 0.9995, 0.9995, 0.9999, 0.9996, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5587, 0.5007, 0.4591, 0.4290, 0.4071, 0.3910, 0.3791, 0.3703,\n",
      "        0.3637, 0.3588, 0.3552, 0.3525, 0.3505, 0.3489, 0.3478, 0.3470, 0.3463,\n",
      "        0.3458, 0.3455, 0.3452, 0.3450, 0.3449, 0.3447, 0.3847])\n",
      "finalReturns:  tensor([0.9963, 1.0363, 1.0166, 0.9555, 0.8653, 0.7542, 0.6281, 0.4911, 0.3461,\n",
      "        0.1952])\n",
      "----------------------------------------\n",
      "iter  1  stage  14  ep  37   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20,  0])\n",
      "loss=  tensor(0.0065, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.6904, 2.6904, 2.6904, 2.6904, 2.6904, 2.6904, 2.6904, 2.6904, 2.6904,\n",
      "        2.6904, 2.6904, 2.6904, 2.6904, 2.6904, 2.6904, 2.3015, 1.9734, 1.6877,\n",
      "        1.4317, 1.1971, 0.9777, 0.7696, 0.5696, 0.3756, 0.1862]) return=  95926.91902269641\n",
      "probs of actions:  tensor([0.9992, 0.9991, 0.9990, 0.9991, 0.9988, 0.9983, 0.9989, 0.9991, 0.9986,\n",
      "        0.9989, 0.9989, 0.9987, 0.9986, 0.9989, 0.9990, 0.9991, 0.9993, 0.9991,\n",
      "        0.9990, 0.9995, 0.9995, 0.9999, 0.9996, 0.9990, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5587, 0.5007, 0.4591, 0.4290, 0.4071, 0.3910, 0.3791, 0.3703,\n",
      "        0.3637, 0.3588, 0.3552, 0.3525, 0.3505, 0.3489, 0.3478, 0.3470, 0.3463,\n",
      "        0.3458, 0.3455, 0.3452, 0.3450, 0.3449, 0.3447, 0.3847])\n",
      "finalReturns:  tensor([1.1554, 1.1954, 1.1756, 1.1144, 1.0240, 0.9129, 0.7867, 0.6497, 0.5047,\n",
      "        0.3538, 0.1985])\n",
      "----------------------------------------\n",
      "iter  1  stage  13  ep  122   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20,  0])\n",
      "loss=  tensor(0.0081, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.8792, 2.8792, 2.8792, 2.8792, 2.8792, 2.8792, 2.8792, 2.8792, 2.8792,\n",
      "        2.8792, 2.8792, 2.8792, 2.8792, 2.8792, 2.4887, 2.1597, 1.8732, 1.6167,\n",
      "        1.3817, 1.1621, 0.9537, 0.7536, 0.5595, 0.3700, 0.1837]) return=  95926.91902269641\n",
      "probs of actions:  tensor([0.9991, 0.9991, 0.9989, 0.9991, 0.9988, 0.9983, 0.9988, 0.9991, 0.9985,\n",
      "        0.9989, 0.9989, 0.9986, 0.9986, 0.9990, 0.9991, 0.9992, 0.9994, 0.9991,\n",
      "        0.9990, 0.9994, 0.9995, 0.9999, 0.9996, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5587, 0.5007, 0.4591, 0.4290, 0.4071, 0.3910, 0.3791, 0.3703,\n",
      "        0.3637, 0.3588, 0.3552, 0.3525, 0.3505, 0.3489, 0.3478, 0.3470, 0.3463,\n",
      "        0.3458, 0.3455, 0.3452, 0.3450, 0.3449, 0.3447, 0.3847])\n",
      "finalReturns:  tensor([1.3170, 1.3570, 1.3372, 1.2759, 1.1854, 1.0741, 0.9479, 0.8108, 0.6657,\n",
      "        0.5147, 0.3594, 0.2009])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  12  ep  263   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20,  0])\n",
      "loss=  tensor(0.0084, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.0679, 3.0679, 3.0679, 3.0679, 3.0679, 3.0679, 3.0679, 3.0679, 3.0679,\n",
      "        3.0679, 3.0679, 3.0679, 3.0679, 2.6754, 2.3450, 2.0575, 1.8003, 1.5648,\n",
      "        1.3448, 1.1362, 0.9359, 0.7417, 0.5520, 0.3657, 0.1819]) return=  95926.91902269641\n",
      "probs of actions:  tensor([0.9993, 0.9992, 0.9991, 0.9992, 0.9989, 0.9985, 0.9990, 0.9992, 0.9987,\n",
      "        0.9990, 0.9990, 0.9988, 0.9990, 0.9992, 0.9993, 0.9994, 0.9995, 0.9993,\n",
      "        0.9991, 0.9995, 0.9996, 0.9999, 0.9996, 0.9991, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5587, 0.5007, 0.4591, 0.4290, 0.4071, 0.3910, 0.3791, 0.3703,\n",
      "        0.3637, 0.3588, 0.3552, 0.3525, 0.3505, 0.3489, 0.3478, 0.3470, 0.3463,\n",
      "        0.3458, 0.3455, 0.3452, 0.3450, 0.3449, 0.3447, 0.3847])\n",
      "finalReturns:  tensor([1.4808, 1.5208, 1.5008, 1.4393, 1.3487, 1.2373, 1.1109, 0.9737, 0.8286,\n",
      "        0.6775, 0.5222, 0.3637, 0.2027])\n",
      "----------------------------------------\n",
      "iter  1  stage  11  ep  203   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20,  0])\n",
      "loss=  tensor(0.0104, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.2577, 3.2577, 3.2577, 3.2577, 3.2577, 3.2577, 3.2577, 3.2577, 3.2577,\n",
      "        3.2577, 3.2577, 3.2577, 2.8625, 2.5301, 2.2414, 1.9833, 1.7471, 1.5266,\n",
      "        1.3176, 1.1171, 0.9227, 0.7329, 0.5465, 0.3626, 0.1806]) return=  95926.91902269641\n",
      "probs of actions:  tensor([0.9992, 0.9992, 0.9990, 0.9992, 0.9989, 0.9984, 0.9990, 0.9992, 0.9987,\n",
      "        0.9990, 0.9990, 0.9990, 0.9990, 0.9993, 0.9993, 0.9995, 0.9995, 0.9993,\n",
      "        0.9991, 0.9995, 0.9996, 0.9999, 0.9996, 0.9990, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5587, 0.5007, 0.4591, 0.4290, 0.4071, 0.3910, 0.3791, 0.3703,\n",
      "        0.3637, 0.3588, 0.3552, 0.3525, 0.3505, 0.3489, 0.3478, 0.3470, 0.3463,\n",
      "        0.3458, 0.3455, 0.3452, 0.3450, 0.3449, 0.3447, 0.3847])\n",
      "finalReturns:  tensor([1.6463, 1.6863, 1.6661, 1.6044, 1.5136, 1.4020, 1.2755, 1.1381, 0.9929,\n",
      "        0.8418, 0.6864, 0.5278, 0.3668, 0.2041])\n",
      "----------------------------------------\n",
      "iter  1  stage  10  ep  31   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20,  0])\n",
      "loss=  tensor(0.0128, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.4496, 3.4496, 3.4496, 3.4496, 3.4496, 3.4496, 3.4496, 3.4496, 3.4496,\n",
      "        3.4496, 3.4496, 3.0507, 2.7159, 2.4254, 2.1660, 1.9289, 1.7078, 1.4984,\n",
      "        1.2975, 1.1028, 0.9128, 0.7263, 0.5423, 0.3602, 0.1796]) return=  95926.91902269641\n",
      "probs of actions:  tensor([0.9992, 0.9992, 0.9990, 0.9991, 0.9989, 0.9984, 0.9989, 0.9992, 0.9987,\n",
      "        0.9990, 0.9990, 0.9990, 0.9990, 0.9993, 0.9993, 0.9995, 0.9995, 0.9993,\n",
      "        0.9990, 0.9995, 0.9996, 0.9999, 0.9996, 0.9990, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5587, 0.5007, 0.4591, 0.4290, 0.4071, 0.3910, 0.3791, 0.3703,\n",
      "        0.3637, 0.3588, 0.3552, 0.3525, 0.3505, 0.3489, 0.3478, 0.3470, 0.3463,\n",
      "        0.3458, 0.3455, 0.3452, 0.3450, 0.3449, 0.3447, 0.3847])\n",
      "finalReturns:  tensor([1.8132, 1.8532, 1.8329, 1.7709, 1.6798, 1.5679, 1.4412, 1.3037, 1.1583,\n",
      "        1.0071, 0.8516, 0.6930, 0.5319, 0.3692, 0.2051])\n",
      "----------------------------------------\n",
      "iter  1  stage  9  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20,  0])\n",
      "loss=  tensor(0.0154, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.6449, 3.6449, 3.6449, 3.6449, 3.6449, 3.6449, 3.6449, 3.6449, 3.6449,\n",
      "        3.6449, 3.2412, 2.9030, 2.6101, 2.3491, 2.1108, 1.8889, 1.6788, 1.4775,\n",
      "        1.2825, 1.0922, 0.9055, 0.7214, 0.5392, 0.3585, 0.1788]) return=  95926.91902269641\n",
      "probs of actions:  tensor([0.9992, 0.9992, 0.9990, 0.9991, 0.9989, 0.9984, 0.9989, 0.9992, 0.9987,\n",
      "        0.9990, 0.9990, 0.9990, 0.9990, 0.9993, 0.9993, 0.9995, 0.9995, 0.9993,\n",
      "        0.9990, 0.9995, 0.9996, 0.9999, 0.9996, 0.9990, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5587, 0.5007, 0.4591, 0.4290, 0.4071, 0.3910, 0.3791, 0.3703,\n",
      "        0.3637, 0.3588, 0.3552, 0.3525, 0.3505, 0.3489, 0.3478, 0.3470, 0.3463,\n",
      "        0.3458, 0.3455, 0.3452, 0.3450, 0.3449, 0.3447, 0.3847])\n",
      "finalReturns:  tensor([1.9816, 2.0216, 2.0010, 1.9386, 1.8471, 1.7349, 1.6079, 1.4702, 1.3246,\n",
      "        1.1733, 1.0177, 0.8590, 0.6979, 0.5351, 0.3709, 0.2058])\n",
      "----------------------------------------\n",
      "iter  1  stage  8  ep  4590   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20,  0])\n",
      "loss=  tensor(0.0115, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.8453, 3.8453, 3.8453, 3.8453, 3.8453, 3.8453, 3.8453, 3.8453, 3.8453,\n",
      "        3.4351, 3.0924, 2.7964, 2.5332, 2.2933, 2.0702, 1.8593, 1.6573, 1.4619,\n",
      "        1.2713, 1.0843, 0.9000, 0.7177, 0.5369, 0.3571, 0.1783]) return=  95926.91902269641\n",
      "probs of actions:  tensor([0.9994, 0.9993, 0.9992, 0.9993, 0.9991, 0.9987, 0.9991, 0.9993, 0.9990,\n",
      "        0.9998, 0.9993, 0.9994, 0.9992, 0.9998, 0.9996, 0.9998, 0.9997, 0.9994,\n",
      "        0.9992, 0.9996, 0.9997, 1.0000, 0.9997, 0.9992, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5587, 0.5007, 0.4591, 0.4290, 0.4071, 0.3910, 0.3791, 0.3703,\n",
      "        0.3637, 0.3588, 0.3552, 0.3525, 0.3505, 0.3489, 0.3478, 0.3470, 0.3463,\n",
      "        0.3458, 0.3455, 0.3452, 0.3450, 0.3449, 0.3447, 0.3847])\n",
      "finalReturns:  tensor([2.1515, 2.1915, 2.1704, 2.1076, 2.0155, 1.9029, 1.7756, 1.6375, 1.4917,\n",
      "        1.3402, 1.1845, 1.0256, 0.8645, 0.7016, 0.5374, 0.3722, 0.2064])\n",
      "----------------------------------------\n",
      "iter  1  stage  7  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20,  0])\n",
      "loss=  tensor(0.0136, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.0530, 4.0530, 4.0530, 4.0530, 4.0530, 4.0530, 4.0530, 4.0530, 3.6339,\n",
      "        3.2852, 2.9850, 2.7188, 2.4768, 2.2522, 2.0402, 1.8374, 1.6413, 1.4503,\n",
      "        1.2630, 1.0784, 0.8959, 0.7149, 0.5351, 0.3562, 0.1778]) return=  95926.91902269641\n",
      "probs of actions:  tensor([0.9994, 0.9993, 0.9992, 0.9993, 0.9991, 0.9987, 0.9991, 0.9993, 0.9990,\n",
      "        0.9998, 0.9993, 0.9994, 0.9992, 0.9998, 0.9996, 0.9998, 0.9997, 0.9994,\n",
      "        0.9992, 0.9996, 0.9997, 1.0000, 0.9997, 0.9992, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5587, 0.5007, 0.4591, 0.4290, 0.4071, 0.3910, 0.3791, 0.3703,\n",
      "        0.3637, 0.3588, 0.3552, 0.3525, 0.3505, 0.3489, 0.3478, 0.3470, 0.3463,\n",
      "        0.3458, 0.3455, 0.3452, 0.3450, 0.3449, 0.3447, 0.3847])\n",
      "finalReturns:  tensor([2.3229, 2.3629, 2.3413, 2.2778, 2.1851, 2.0719, 1.9440, 1.8056, 1.6595,\n",
      "        1.5077, 1.3518, 1.1928, 1.0315, 0.8686, 0.7043, 0.5391, 0.3732, 0.2068])\n",
      "----------------------------------------\n",
      "iter  1  stage  6  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20,  0])\n",
      "loss=  tensor(0.0162, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.2707, 4.2707, 4.2707, 4.2707, 4.2707, 4.2707, 4.2707, 3.8397, 3.4828,\n",
      "        3.1770, 2.9068, 2.6620, 2.4353, 2.2218, 2.0179, 1.8210, 1.6294, 1.4416,\n",
      "        1.2567, 1.0740, 0.8928, 0.7129, 0.5338, 0.3554, 0.1775]) return=  95926.91902269641\n",
      "probs of actions:  tensor([0.9994, 0.9993, 0.9992, 0.9993, 0.9991, 0.9987, 0.9991, 0.9993, 0.9990,\n",
      "        0.9998, 0.9993, 0.9994, 0.9992, 0.9998, 0.9996, 0.9998, 0.9997, 0.9994,\n",
      "        0.9992, 0.9996, 0.9997, 1.0000, 0.9997, 0.9992, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5587, 0.5007, 0.4591, 0.4290, 0.4071, 0.3910, 0.3791, 0.3703,\n",
      "        0.3637, 0.3588, 0.3552, 0.3525, 0.3505, 0.3489, 0.3478, 0.3470, 0.3463,\n",
      "        0.3458, 0.3455, 0.3452, 0.3450, 0.3449, 0.3447, 0.3847])\n",
      "finalReturns:  tensor([2.4962, 2.5362, 2.5140, 2.4495, 2.3560, 2.2420, 2.1134, 1.9745, 1.8279,\n",
      "        1.6758, 1.5196, 1.3605, 1.1990, 1.0359, 0.8716, 0.7064, 0.5404, 0.3740,\n",
      "        0.2071])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  5  ep  9276   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([20, 20, 20, 20, 16, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20,  0])\n",
      "loss=  tensor(0.0145, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.4575, 4.4575, 4.4575, 4.4575, 4.4575, 4.4575, 4.0237, 3.6649, 3.3577,\n",
      "        3.0866, 2.8411, 2.6139, 2.4000, 2.1958, 1.9988, 1.8070, 1.6192, 1.4342,\n",
      "        1.2514, 1.0702, 0.8902, 0.7111, 0.5327, 0.3548, 0.1772]) return=  95556.53901947841\n",
      "probs of actions:  tensor([9.9949e-01, 9.9946e-01, 9.9936e-01, 9.9944e-01, 7.3200e-04, 9.9900e-01,\n",
      "        9.9951e-01, 9.9947e-01, 9.9918e-01, 9.9991e-01, 9.9956e-01, 9.9956e-01,\n",
      "        9.9941e-01, 9.9991e-01, 9.9975e-01, 9.9983e-01, 9.9981e-01, 9.9959e-01,\n",
      "        9.9938e-01, 9.9969e-01, 9.9974e-01, 1.0000e+00, 9.9982e-01, 9.9942e-01,\n",
      "        1.0000e+00], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5587, 0.5007, 0.4591, 0.4434, 0.3938, 0.3812, 0.3718, 0.3649,\n",
      "        0.3597, 0.3559, 0.3530, 0.3508, 0.3492, 0.3480, 0.3471, 0.3464, 0.3459,\n",
      "        0.3455, 0.3453, 0.3450, 0.3449, 0.3448, 0.3447, 0.3846])\n",
      "finalReturns:  tensor([2.6650, 2.7050, 2.6826, 2.6180, 2.5242, 2.4100, 2.2813, 2.1422, 1.9956,\n",
      "        1.8434, 1.6872, 1.5279, 1.3665, 1.2034, 1.0390, 0.8738, 0.7078, 0.5413,\n",
      "        0.3745, 0.2074])\n",
      "----------------------------------------\n",
      "iter  1  stage  4  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20,  0])\n",
      "loss=  tensor(0.0170, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.7526, 4.7526, 4.7526, 4.7526, 4.7526, 4.2835, 3.9008, 3.5771, 3.2943,\n",
      "        3.0405, 2.8074, 2.5891, 2.3818, 2.1824, 1.9889, 1.7998, 1.6138, 1.4303,\n",
      "        1.2486, 1.0682, 0.8888, 0.7102, 0.5321, 0.3544, 0.1771]) return=  95926.91902269641\n",
      "probs of actions:  tensor([0.9995, 0.9995, 0.9994, 0.9994, 0.9993, 0.9990, 0.9995, 0.9995, 0.9992,\n",
      "        0.9999, 0.9996, 0.9996, 0.9994, 0.9999, 0.9998, 0.9998, 0.9998, 0.9996,\n",
      "        0.9994, 0.9997, 0.9997, 1.0000, 0.9998, 0.9994, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5587, 0.5007, 0.4591, 0.4290, 0.4071, 0.3910, 0.3791, 0.3703,\n",
      "        0.3637, 0.3588, 0.3552, 0.3525, 0.3505, 0.3489, 0.3478, 0.3470, 0.3463,\n",
      "        0.3458, 0.3455, 0.3452, 0.3450, 0.3449, 0.3447, 0.3847])\n",
      "finalReturns:  tensor([2.8504, 2.8904, 2.8660, 2.7988, 2.7025, 2.5860, 2.4554, 2.3148, 2.1670,\n",
      "        2.0138, 1.8569, 1.6971, 1.5352, 1.3718, 1.2072, 1.0417, 0.8756, 0.7091,\n",
      "        0.5421, 0.3749, 0.2075])\n",
      "----------------------------------------\n",
      "iter  1  stage  3  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20,  0])\n",
      "loss=  tensor(0.0192, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.0293, 5.0293, 5.0293, 5.0293, 4.5301, 4.1271, 3.7894, 3.4968, 3.2361,\n",
      "        2.9979, 2.7761, 2.5661, 2.3648, 2.1699, 1.9797, 1.7930, 1.6089, 1.4267,\n",
      "        1.2460, 1.0663, 0.8875, 0.7093, 0.5316, 0.3541, 0.1770]) return=  95926.91902269641\n",
      "probs of actions:  tensor([0.9995, 0.9995, 0.9994, 0.9994, 0.9993, 0.9990, 0.9995, 0.9995, 0.9992,\n",
      "        0.9999, 0.9996, 0.9996, 0.9994, 0.9999, 0.9998, 0.9998, 0.9998, 0.9996,\n",
      "        0.9994, 0.9997, 0.9997, 1.0000, 0.9998, 0.9994, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5587, 0.5007, 0.4591, 0.4290, 0.4071, 0.3910, 0.3791, 0.3703,\n",
      "        0.3637, 0.3588, 0.3552, 0.3525, 0.3505, 0.3489, 0.3478, 0.3470, 0.3463,\n",
      "        0.3458, 0.3455, 0.3452, 0.3450, 0.3449, 0.3447, 0.3847])\n",
      "finalReturns:  tensor([3.0328, 3.0728, 3.0468, 2.9775, 2.8790, 2.7607, 2.6286, 2.4867, 2.3378,\n",
      "        2.1839, 2.0264, 1.8661, 1.7039, 1.5402, 1.3754, 1.2098, 1.0436, 0.8769,\n",
      "        0.7099, 0.5427, 0.3753, 0.2077])\n",
      "----------------------------------------\n",
      "iter  1  stage  2  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20,  0])\n",
      "loss=  tensor(0.0217, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.3425, 5.3425, 5.3425, 4.8018, 4.3708, 4.0140, 3.7081, 3.4380, 3.1931,\n",
      "        2.9665, 2.7529, 2.5490, 2.3522, 2.1605, 1.9728, 1.7879, 1.6051, 1.4240,\n",
      "        1.2440, 1.0650, 0.8866, 0.7087, 0.5311, 0.3539, 0.1769]) return=  95926.91902269641\n",
      "probs of actions:  tensor([0.9995, 0.9995, 0.9994, 0.9994, 0.9993, 0.9990, 0.9995, 0.9995, 0.9992,\n",
      "        0.9999, 0.9996, 0.9996, 0.9994, 0.9999, 0.9998, 0.9998, 0.9998, 0.9996,\n",
      "        0.9994, 0.9997, 0.9997, 1.0000, 0.9998, 0.9994, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5587, 0.5007, 0.4591, 0.4290, 0.4071, 0.3910, 0.3791, 0.3703,\n",
      "        0.3637, 0.3588, 0.3552, 0.3525, 0.3505, 0.3489, 0.3478, 0.3470, 0.3463,\n",
      "        0.3458, 0.3455, 0.3452, 0.3450, 0.3449, 0.3447, 0.3847])\n",
      "finalReturns:  tensor([3.2203, 3.2603, 3.2321, 3.1600, 3.0587, 2.9379, 2.8037, 2.6601, 2.5099,\n",
      "        2.3549, 2.1966, 2.0357, 1.8730, 1.7090, 1.5439, 1.3781, 1.2117, 1.0450,\n",
      "        0.8779, 0.7106, 0.5431, 0.3755, 0.2078])\n",
      "----------------------------------------\n",
      "iter  1  stage  1  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20,  0])\n",
      "loss=  tensor(0.0241, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.7070, 5.7070, 5.1084, 4.6387, 4.2556, 3.9315, 3.6486, 3.3946, 3.1614,\n",
      "        2.9431, 2.7357, 2.5362, 2.3427, 2.1535, 1.9676, 1.7841, 1.6023, 1.4219,\n",
      "        1.2426, 1.0639, 0.8858, 0.7082, 0.5308, 0.3537, 0.1768]) return=  95926.91902269641\n",
      "probs of actions:  tensor([0.9995, 0.9995, 0.9994, 0.9994, 0.9993, 0.9990, 0.9995, 0.9995, 0.9992,\n",
      "        0.9999, 0.9996, 0.9996, 0.9994, 0.9999, 0.9998, 0.9998, 0.9998, 0.9996,\n",
      "        0.9994, 0.9997, 0.9997, 1.0000, 0.9998, 0.9994, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5587, 0.5007, 0.4591, 0.4290, 0.4071, 0.3910, 0.3791, 0.3703,\n",
      "        0.3637, 0.3588, 0.3552, 0.3525, 0.3505, 0.3489, 0.3478, 0.3470, 0.3463,\n",
      "        0.3458, 0.3455, 0.3452, 0.3450, 0.3449, 0.3447, 0.3847])\n",
      "finalReturns:  tensor([3.4144, 3.4544, 3.4234, 3.3474, 3.2424, 3.1183, 2.9813, 2.8354, 2.6835,\n",
      "        2.5271, 2.3677, 2.2060, 2.0427, 1.8782, 1.7128, 1.5467, 1.3801, 1.2132,\n",
      "        1.0460, 0.8786, 0.7111, 0.5434, 0.3757, 0.2079])\n",
      "----------------------------------------\n",
      "iter  1  stage  0  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20,  0])\n",
      "loss=  tensor(0.0264, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.9753, 5.4641, 4.9403, 4.5206, 4.1715, 3.8710, 3.6046, 3.3625, 3.1378,\n",
      "        2.9257, 2.7228, 2.5267, 2.3357, 2.1483, 1.9637, 1.7812, 1.6003, 1.4204,\n",
      "        1.2415, 1.0631, 0.8853, 0.7078, 0.5306, 0.3536, 0.1767]) return=  95926.91902269641\n",
      "probs of actions:  tensor([0.9995, 0.9995, 0.9994, 0.9994, 0.9993, 0.9990, 0.9995, 0.9995, 0.9992,\n",
      "        0.9999, 0.9996, 0.9996, 0.9994, 0.9999, 0.9998, 0.9998, 0.9998, 0.9996,\n",
      "        0.9994, 0.9997, 0.9997, 1.0000, 0.9998, 0.9994, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4712, 0.5587, 0.5007, 0.4591, 0.4290, 0.4071, 0.3910, 0.3791, 0.3703,\n",
      "        0.3637, 0.3588, 0.3552, 0.3525, 0.3505, 0.3489, 0.3478, 0.3470, 0.3463,\n",
      "        0.3458, 0.3455, 0.3452, 0.3450, 0.3449, 0.3447, 0.3847])\n",
      "finalReturns:  tensor([3.6174, 3.6574, 3.6225, 3.5415, 3.4315, 3.3029, 3.1622, 3.0134, 2.8590,\n",
      "        2.7009, 2.5400, 2.3772, 2.2131, 2.0479, 1.8820, 1.7156, 1.5488, 1.3817,\n",
      "        1.2143, 1.0468, 0.8792, 0.7114, 0.5436, 0.3758, 0.2079])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1683019988 saved\n",
      "[700369, 'tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])', 95926.91902269641, 83461.56370671562, 0.026398321613669395, 1e-05, 1, 0, 'tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\\n        20, 20, 20, 20, 20, 20,  0])', '[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\\n 1.]', '0,[1e-05,1][1, 10000, 1, 1],1683019988', 25, 50, 166678.00013679263, 199440.72542558872, 81175.85726043607, 135011.53866666666, 132074.72800000003, 95926.91902269641, 95926.91902269641, 113529.90192566635, 113488.54729640356, 94070.91886744411, 95926.91902269641, 113529.90192566635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy reset\n",
      "----------------------------------------\n",
      "iter  2  stage  24  ep  99999   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.4335, 0.4335, 0.4335, 0.4335, 0.4335, 0.4335, 0.4335, 0.4335, 0.4335,\n",
      "        0.4335, 0.4335, 0.4335, 0.4335, 0.4335, 0.4335, 0.4335, 0.4335, 0.4335,\n",
      "        0.4335, 0.4335, 0.4335, 0.4335, 0.4335, 0.4335, 0.4335]) return=  123480.07884867929\n",
      "probs of actions:  tensor([0.8697, 0.8787, 0.8837, 0.8861, 0.9065, 0.8957, 0.9092, 0.9046, 0.0033,\n",
      "        0.8968, 0.0717, 0.8905, 0.8930, 0.8939, 0.8652, 0.9085, 0.8924, 0.8947,\n",
      "        0.8934, 0.8794, 0.8891, 0.8795, 0.8696, 0.8838, 0.9802],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5238, 0.5334, 0.5406, 0.5460, 0.5501, 0.5532, 0.5555, 0.5548,\n",
      "        0.5774, 0.5389, 0.5146, 0.4932, 0.4774, 0.4658, 0.4571, 0.4507, 0.4459,\n",
      "        0.4423, 0.4396, 0.4376, 0.4361, 0.4350, 0.4342, 0.4335])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  2  stage  23  ep  99999   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([11, 10, 11,  8,  9,  0,  0, 11, 10,  9,  0,  6,  0,  8,  6, 11,  9,  6,\n",
      "         0,  4,  1,  6,  9, 12,  0])\n",
      "loss=  tensor(0.0690, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.8430, 0.8430, 0.8430, 0.8430, 0.8430, 0.8430, 0.8430, 0.8430, 0.8430,\n",
      "        0.8430, 0.8430, 0.8430, 0.8430, 0.8430, 0.8430, 0.8430, 0.8430, 0.8430,\n",
      "        0.8430, 0.8430, 0.8430, 0.8430, 0.8430, 0.8430, 0.4124]) return=  113897.17236003252\n",
      "probs of actions:  tensor([0.3751, 0.0684, 0.3539, 0.0686, 0.1139, 0.2947, 0.1850, 0.3508, 0.0738,\n",
      "        0.1131, 0.1590, 0.0437, 0.2453, 0.0760, 0.0570, 0.3085, 0.1086, 0.0428,\n",
      "        0.2804, 0.0067, 0.0272, 0.0505, 0.1165, 0.0645, 0.9907],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5544, 0.5332, 0.5284, 0.5081, 0.5061, 0.4672, 0.4270, 0.4449,\n",
      "        0.4555, 0.4667, 0.4351, 0.4379, 0.4113, 0.4251, 0.4183, 0.4402, 0.4516,\n",
      "        0.4502, 0.4251, 0.4223, 0.4059, 0.4079, 0.4162, 0.4518])\n",
      "finalReturns:  tensor([0.0250, 0.0394])\n",
      "----------------------------------------\n",
      "iter  2  stage  22  ep  99999   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,  0, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0053, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.3927, 1.3927, 1.3927, 1.3927, 1.3927, 1.3927, 1.3927, 1.3927, 1.3927,\n",
      "        1.3927, 1.3927, 1.3927, 1.3927, 1.3927, 1.3927, 1.3927, 1.3927, 1.3927,\n",
      "        1.3927, 1.3927, 1.3927, 1.3927, 1.3927, 0.8954, 0.4345]) return=  124264.57333060534\n",
      "probs of actions:  tensor([0.9427, 0.9337, 0.9446, 0.9470, 0.9577, 0.9128, 0.9357, 0.9532, 0.9385,\n",
      "        0.9313, 0.9501, 0.9403, 0.9314, 0.9393, 0.9421, 0.0346, 0.9357, 0.9234,\n",
      "        0.9208, 0.9168, 0.9496, 0.9306, 0.9677, 0.9730, 0.9974],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.5051, 0.4545, 0.4637,\n",
      "        0.4707, 0.4760, 0.4800, 0.4830, 0.4852, 0.4869, 0.5003])\n",
      "finalReturns:  tensor([0.0797, 0.0918, 0.0658])\n",
      "----------------------------------------\n",
      "iter  2  stage  21  ep  99999   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 12, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0080, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.8357, 1.8357, 1.8357, 1.8357, 1.8357, 1.8357, 1.8357, 1.8357, 1.8357,\n",
      "        1.8357, 1.8357, 1.8357, 1.8357, 1.8357, 1.8357, 1.8357, 1.8357, 1.8357,\n",
      "        1.8357, 1.8357, 1.8357, 1.8357, 1.3278, 0.8593, 0.4193]) return=  125646.00419333146\n",
      "probs of actions:  tensor([0.9796, 0.9785, 0.9838, 0.9819, 0.9860, 0.9745, 0.9773, 0.9863, 0.9773,\n",
      "        0.9804, 0.9844, 0.9810, 0.9799, 0.9795, 0.9794, 0.9820, 0.9782, 0.9743,\n",
      "        0.9818, 0.9728, 0.0102, 0.9734, 0.9912, 0.9842, 0.9991],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4899, 0.4957, 0.4948, 0.4941, 0.5057])\n",
      "finalReturns:  tensor([0.1547, 0.1668, 0.1405, 0.0864])\n",
      "----------------------------------------\n",
      "iter  2  stage  20  ep  99999   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11,  0])\n",
      "loss=  tensor(0.0292, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.2295, 2.2295, 2.2295, 2.2295, 2.2295, 2.2295, 2.2295, 2.2295, 2.2295,\n",
      "        2.2295, 2.2295, 2.2295, 2.2295, 2.2295, 2.2295, 2.2295, 2.2295, 2.2295,\n",
      "        2.2295, 2.2295, 2.2295, 1.7252, 1.2592, 0.8210, 0.4030]) return=  125571.79187222246\n",
      "probs of actions:  tensor([0.9680, 0.9685, 0.9766, 0.9720, 0.9780, 0.9648, 0.9629, 0.9802, 0.9639,\n",
      "        0.9731, 0.9777, 0.9711, 0.9713, 0.9694, 0.9686, 0.9748, 0.9664, 0.9592,\n",
      "        0.9771, 0.9606, 0.9766, 0.9474, 0.9886, 0.9610, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4923, 0.4922, 0.4922, 0.4921, 0.4921, 0.5042])\n",
      "finalReturns:  tensor([0.2433, 0.2554, 0.2292, 0.1753, 0.1011])\n",
      "----------------------------------------\n",
      "iter  2  stage  19  ep  99999   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 12, 12, 11, 11, 11,  0])\n",
      "loss=  tensor(1.7093, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.6218, 2.6218, 2.6218, 2.6218, 2.6218, 2.6218, 2.6218, 2.6218, 2.6218,\n",
      "        2.6218, 2.6218, 2.6218, 2.6218, 2.6218, 2.6218, 2.6218, 2.6218, 2.6218,\n",
      "        2.6218, 2.6218, 2.1174, 1.6514, 1.2131, 0.7951, 0.3921]) return=  125731.65338382448\n",
      "probs of actions:  tensor([0.8931, 0.8975, 0.9188, 0.9044, 0.9218, 0.8893, 0.8750, 0.9309, 0.8813,\n",
      "        0.9124, 0.9239, 0.9041, 0.9058, 0.8980, 0.8968, 0.9130, 0.8879, 0.8648,\n",
      "        0.9240, 0.1488, 0.0878, 0.8260, 0.9609, 0.8325, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5523, 0.5369, 0.5255, 0.5170, 0.5107, 0.5060, 0.5025, 0.4998,\n",
      "        0.4979, 0.4964, 0.4953, 0.4945, 0.4939, 0.4934, 0.4930, 0.4928, 0.4926,\n",
      "        0.4924, 0.4900, 0.4935, 0.4984, 0.4968, 0.4956, 0.5068])\n",
      "finalReturns:  tensor([0.3593, 0.3737, 0.3463, 0.2861, 0.2073, 0.1147])\n",
      "----------------------------------------\n",
      "iter  2  stage  18  ep  99999   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 11, 12, 11, 12, 12, 12, 12, 12, 12, 11, 11, 12, 12,\n",
      "        12, 11, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(1.3323, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.8592, 2.8592, 2.8592, 2.8592, 2.8592, 2.8592, 2.8592, 2.8592, 2.8592,\n",
      "        2.8592, 2.8592, 2.8592, 2.8592, 2.8592, 2.8592, 2.8592, 2.8592, 2.8592,\n",
      "        2.8592, 2.3724, 1.9258, 1.5082, 1.1117, 0.7307, 0.3611]) return=  121796.3805407076\n",
      "probs of actions:  tensor([0.7982, 0.7813, 0.7673, 0.7875, 0.7589, 0.2079, 0.8283, 0.2679, 0.8133,\n",
      "        0.7479, 0.7430, 0.7761, 0.7679, 0.7932, 0.2071, 0.2261, 0.8062, 0.8390,\n",
      "        0.7591, 0.1343, 0.8476, 0.8865, 0.7336, 0.9199, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.5537, 0.5337, 0.5188, 0.5079, 0.5020, 0.4901, 0.4887, 0.4802,\n",
      "        0.4790, 0.4782, 0.4775, 0.4770, 0.4767, 0.4787, 0.4750, 0.4699, 0.4714,\n",
      "        0.4724, 0.4755, 0.4703, 0.4716, 0.4726, 0.4734, 0.4883])\n",
      "finalReturns:  tensor([0.4650, 0.4794, 0.4505, 0.3978, 0.3226, 0.2310, 0.1272])\n",
      "----------------------------------------\n",
      "iter  2  stage  17  ep  99999   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 11, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0642, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.2258, 3.2258, 3.2258, 3.2258, 3.2258, 3.2258, 3.2258, 3.2258, 3.2258,\n",
      "        3.2258, 3.2258, 3.2258, 3.2258, 3.2258, 3.2258, 3.2258, 3.2258, 3.2258,\n",
      "        2.7355, 2.2864, 1.8670, 1.4692, 1.0872, 0.7169, 0.3553]) return=  122213.81800409526\n",
      "probs of actions:  tensor([0.9715, 0.9674, 0.9688, 0.9709, 0.9668, 0.9693, 0.9768, 0.9617, 0.9737,\n",
      "        0.0387, 0.9627, 0.9676, 0.9658, 0.9710, 0.9709, 0.9687, 0.9726, 0.9801,\n",
      "        0.9701, 0.9851, 0.9858, 0.9873, 0.9717, 0.9934, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.5537, 0.5337, 0.5188, 0.5079, 0.4997, 0.4936, 0.4891, 0.4857,\n",
      "        0.4855, 0.4778, 0.4772, 0.4768, 0.4765, 0.4763, 0.4761, 0.4760, 0.4759,\n",
      "        0.4758, 0.4758, 0.4757, 0.4757, 0.4757, 0.4757, 0.4900])\n",
      "finalReturns:  tensor([0.5944, 0.6088, 0.5821, 0.5258, 0.4478, 0.3541, 0.2487, 0.1348])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  2  stage  16  ep  99999   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0178, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.5788, 3.5788, 3.5788, 3.5788, 3.5788, 3.5788, 3.5788, 3.5788, 3.5788,\n",
      "        3.5788, 3.5788, 3.5788, 3.5788, 3.5788, 3.5788, 3.5788, 3.5788, 3.0878,\n",
      "        2.6382, 2.2184, 1.8203, 1.4382, 1.0677, 0.7059, 0.3506]) return=  122329.26544005591\n",
      "probs of actions:  tensor([0.9930, 0.9917, 0.9927, 0.9930, 0.9921, 0.9921, 0.9943, 0.9909, 0.9935,\n",
      "        0.9904, 0.9909, 0.9920, 0.9915, 0.9928, 0.9930, 0.9926, 0.9944, 0.9954,\n",
      "        0.9950, 0.9971, 0.9974, 0.9972, 0.9951, 0.9989, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.5537, 0.5337, 0.5188, 0.5079, 0.4997, 0.4936, 0.4891, 0.4857,\n",
      "        0.4832, 0.4813, 0.4798, 0.4788, 0.4780, 0.4774, 0.4769, 0.4766, 0.4764,\n",
      "        0.4762, 0.4760, 0.4759, 0.4758, 0.4758, 0.4757, 0.4901])\n",
      "finalReturns:  tensor([0.7197, 0.7341, 0.7074, 0.6510, 0.5731, 0.4793, 0.3739, 0.2599, 0.1395])\n",
      "----------------------------------------\n",
      "iter  2  stage  15  ep  99999   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0061, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.9270, 3.9270, 3.9270, 3.9270, 3.9270, 3.9270, 3.9270, 3.9270, 3.9270,\n",
      "        3.9270, 3.9270, 3.9270, 3.9270, 3.9270, 3.9270, 3.9270, 3.4356, 2.9858,\n",
      "        2.5658, 2.1676, 1.7854, 1.4148, 1.0530, 0.6976, 0.3470]) return=  122329.26544005591\n",
      "probs of actions:  tensor([0.9975, 0.9970, 0.9975, 0.9976, 0.9973, 0.9971, 0.9980, 0.9969, 0.9977,\n",
      "        0.9966, 0.9968, 0.9972, 0.9970, 0.9974, 0.9976, 0.9985, 0.9987, 0.9989,\n",
      "        0.9986, 0.9990, 0.9995, 0.9995, 0.9989, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.5537, 0.5337, 0.5188, 0.5079, 0.4997, 0.4936, 0.4891, 0.4857,\n",
      "        0.4832, 0.4813, 0.4798, 0.4788, 0.4780, 0.4774, 0.4769, 0.4766, 0.4764,\n",
      "        0.4762, 0.4760, 0.4759, 0.4758, 0.4758, 0.4757, 0.4901])\n",
      "finalReturns:  tensor([0.8485, 0.8629, 0.8361, 0.7797, 0.7018, 0.6080, 0.5026, 0.3886, 0.2682,\n",
      "        0.1431])\n",
      "----------------------------------------\n",
      "iter  2  stage  15  ep  117999   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0040, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.9270, 3.9270, 3.9270, 3.9270, 3.9270, 3.9270, 3.9270, 3.9270, 3.9270,\n",
      "        3.9270, 3.9270, 3.9270, 3.9270, 3.9270, 3.9270, 3.9270, 3.4356, 2.9858,\n",
      "        2.5658, 2.1676, 1.7854, 1.4148, 1.0530, 0.6976, 0.3470]) return=  122329.26544005591\n",
      "probs of actions:  tensor([0.9982, 0.9978, 0.9982, 0.9983, 0.9981, 0.9979, 0.9985, 0.9978, 0.9983,\n",
      "        0.9975, 0.9977, 0.9980, 0.9978, 0.9981, 0.9983, 0.9990, 0.9990, 0.9992,\n",
      "        0.9994, 0.9993, 0.9997, 0.9997, 0.9993, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.5537, 0.5337, 0.5188, 0.5079, 0.4997, 0.4936, 0.4891, 0.4857,\n",
      "        0.4832, 0.4813, 0.4798, 0.4788, 0.4780, 0.4774, 0.4769, 0.4766, 0.4764,\n",
      "        0.4762, 0.4760, 0.4759, 0.4758, 0.4758, 0.4757, 0.4901])\n",
      "finalReturns:  tensor([0.8485, 0.8629, 0.8361, 0.7797, 0.7018, 0.6080, 0.5026, 0.3886, 0.2682,\n",
      "        0.1431])\n",
      "----------------------------------------\n",
      "iter  2  stage  14  ep  908   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0052, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.5483, 4.5483, 4.5483, 4.5483, 4.5483, 4.5483, 4.5483, 4.5483, 4.5483,\n",
      "        4.5483, 4.5483, 4.5483, 4.5483, 4.5483, 4.5483, 4.0289, 3.5520, 3.1059,\n",
      "        2.6822, 2.2749, 1.8797, 1.4934, 1.1138, 0.7392, 0.3682]) return=  127915.76530179875\n",
      "probs of actions:  tensor([0.0017, 0.9979, 0.9983, 0.9983, 0.9982, 0.9980, 0.9986, 0.9979, 0.9985,\n",
      "        0.9977, 0.9979, 0.9982, 0.9979, 0.9983, 0.9990, 0.9991, 0.9991, 0.9993,\n",
      "        0.9994, 0.9993, 0.9997, 0.9997, 0.9994, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5500, 0.5383, 0.5296, 0.5232, 0.5183, 0.5147, 0.5120, 0.5100,\n",
      "        0.5085, 0.5074, 0.5065, 0.5059, 0.5054, 0.5051, 0.5048, 0.5046, 0.5045,\n",
      "        0.5043, 0.5043, 0.5042, 0.5041, 0.5041, 0.5041, 0.5185])\n",
      "finalReturns:  tensor([1.0141, 1.0285, 1.0006, 0.9421, 0.8614, 0.7644, 0.6553, 0.5374, 0.4128,\n",
      "        0.2834, 0.1503])\n",
      "----------------------------------------\n",
      "iter  2  stage  13  ep  3619   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0067, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.9158, 4.9158, 4.9158, 4.9158, 4.9158, 4.9158, 4.9158, 4.9158, 4.9158,\n",
      "        4.9158, 4.9158, 4.9158, 4.9158, 4.9158, 4.3960, 3.9188, 3.4725, 3.0487,\n",
      "        2.6413, 2.2460, 1.8597, 1.4800, 1.1054, 0.7344, 0.3662]) return=  127915.76530179875\n",
      "probs of actions:  tensor([0.0018, 0.9979, 0.9983, 0.9983, 0.9982, 0.9980, 0.9986, 0.9979, 0.9984,\n",
      "        0.9977, 0.9979, 0.9982, 0.9979, 0.9990, 0.9990, 0.9991, 0.9991, 0.9993,\n",
      "        0.9994, 0.9993, 0.9997, 0.9997, 0.9994, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4991, 0.5500, 0.5383, 0.5296, 0.5232, 0.5183, 0.5147, 0.5120, 0.5100,\n",
      "        0.5085, 0.5074, 0.5065, 0.5059, 0.5054, 0.5051, 0.5048, 0.5046, 0.5045,\n",
      "        0.5043, 0.5043, 0.5042, 0.5041, 0.5041, 0.5041, 0.5185])\n",
      "finalReturns:  tensor([1.1521, 1.1665, 1.1386, 1.0801, 0.9994, 0.9023, 0.7932, 0.6753, 0.5507,\n",
      "        0.4213, 0.2882, 0.1523])\n",
      "----------------------------------------\n",
      "iter  2  stage  12  ep  18243   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0071, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.9612, 4.9612, 4.9612, 4.9612, 4.9612, 4.9612, 4.9612, 4.9612, 4.9612,\n",
      "        4.9612, 4.9612, 4.9612, 4.9612, 4.4680, 4.0168, 3.5959, 3.1970, 2.8143,\n",
      "        2.4434, 2.0812, 1.7256, 1.3749, 1.0278, 0.6833, 0.3409]) return=  122329.26544005591\n",
      "probs of actions:  tensor([0.9984, 0.9981, 0.9984, 0.9985, 0.9983, 0.9981, 0.9987, 0.9981, 0.9985,\n",
      "        0.9978, 0.9980, 0.9983, 0.9990, 0.9992, 0.9991, 0.9992, 0.9991, 0.9993,\n",
      "        0.9999, 0.9993, 0.9997, 0.9999, 0.9995, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.5537, 0.5337, 0.5188, 0.5079, 0.4997, 0.4936, 0.4891, 0.4857,\n",
      "        0.4832, 0.4813, 0.4798, 0.4788, 0.4780, 0.4774, 0.4769, 0.4766, 0.4764,\n",
      "        0.4762, 0.4760, 0.4759, 0.4758, 0.4758, 0.4757, 0.4901])\n",
      "finalReturns:  tensor([1.2484, 1.2628, 1.2360, 1.1795, 1.1015, 1.0077, 0.9022, 0.7882, 0.6677,\n",
      "        0.5426, 0.4139, 0.2825, 0.1492])\n",
      "----------------------------------------\n",
      "iter  2  stage  11  ep  5010   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0066, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.3049, 5.3049, 5.3049, 5.3049, 5.3049, 5.3049, 5.3049, 5.3049, 5.3049,\n",
      "        5.3049, 5.3049, 5.3049, 4.8106, 4.3587, 3.9372, 3.5379, 3.1548, 2.7837,\n",
      "        2.4214, 2.0657, 1.7149, 1.3677, 1.0232, 0.6807, 0.3398]) return=  122329.26544005591\n",
      "probs of actions:  tensor([0.9988, 0.9985, 0.9988, 0.9988, 0.9987, 0.9986, 0.9990, 0.9986, 0.9989,\n",
      "        0.9984, 0.9985, 0.9990, 0.9993, 0.9995, 0.9994, 0.9994, 0.9994, 0.9995,\n",
      "        1.0000, 0.9995, 0.9998, 0.9999, 0.9997, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.5537, 0.5337, 0.5188, 0.5079, 0.4997, 0.4936, 0.4891, 0.4857,\n",
      "        0.4832, 0.4813, 0.4798, 0.4788, 0.4780, 0.4774, 0.4769, 0.4766, 0.4764,\n",
      "        0.4762, 0.4760, 0.4759, 0.4758, 0.4758, 0.4757, 0.4901])\n",
      "finalReturns:  tensor([1.3846, 1.3990, 1.3722, 1.3157, 1.2376, 1.1437, 1.0382, 0.9241, 0.8037,\n",
      "        0.6785, 0.5498, 0.4184, 0.2851, 0.1503])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  2  stage  10  ep  12616   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0071, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.6490, 5.6490, 5.6490, 5.6490, 5.6490, 5.6490, 5.6490, 5.6490, 5.6490,\n",
      "        5.6490, 5.6490, 5.1533, 4.7004, 4.2782, 3.8783, 3.4949, 3.1234, 2.7610,\n",
      "        2.4051, 2.0541, 1.7068, 1.3623, 1.0198, 0.6788, 0.3390]) return=  122329.26544005591\n",
      "probs of actions:  tensor([0.9989, 0.9987, 0.9989, 0.9990, 0.9989, 0.9987, 0.9991, 0.9987, 0.9990,\n",
      "        0.9985, 0.9990, 0.9991, 0.9994, 0.9997, 0.9995, 0.9995, 0.9996, 0.9995,\n",
      "        1.0000, 0.9996, 0.9998, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.5537, 0.5337, 0.5188, 0.5079, 0.4997, 0.4936, 0.4891, 0.4857,\n",
      "        0.4832, 0.4813, 0.4798, 0.4788, 0.4780, 0.4774, 0.4769, 0.4766, 0.4764,\n",
      "        0.4762, 0.4760, 0.4759, 0.4758, 0.4758, 0.4757, 0.4901])\n",
      "finalReturns:  tensor([1.5217, 1.5361, 1.5093, 1.4527, 1.3745, 1.2806, 1.1751, 1.0610, 0.9405,\n",
      "        0.8153, 0.6865, 0.5552, 0.4218, 0.2870, 0.1511])\n",
      "----------------------------------------\n",
      "iter  2  stage  9  ep  510   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0090, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.9943, 5.9943, 5.9943, 5.9943, 5.9943, 5.9943, 5.9943, 5.9943, 5.9943,\n",
      "        5.9943, 5.4967, 5.0424, 4.6192, 4.2186, 3.8346, 3.4628, 3.1001, 2.7440,\n",
      "        2.3929, 2.0454, 1.7008, 1.3582, 1.0172, 0.6773, 0.3383]) return=  122329.26544005591\n",
      "probs of actions:  tensor([0.9989, 0.9987, 0.9990, 0.9990, 0.9989, 0.9987, 0.9991, 0.9988, 0.9990,\n",
      "        0.9990, 0.9990, 0.9991, 0.9994, 0.9997, 0.9995, 0.9996, 0.9996, 0.9995,\n",
      "        1.0000, 0.9996, 0.9998, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.5537, 0.5337, 0.5188, 0.5079, 0.4997, 0.4936, 0.4891, 0.4857,\n",
      "        0.4832, 0.4813, 0.4798, 0.4788, 0.4780, 0.4774, 0.4769, 0.4766, 0.4764,\n",
      "        0.4762, 0.4760, 0.4759, 0.4758, 0.4758, 0.4757, 0.4901])\n",
      "finalReturns:  tensor([1.6596, 1.6740, 1.6471, 1.5904, 1.5122, 1.4182, 1.3126, 1.1985, 1.0779,\n",
      "        0.9527, 0.8240, 0.6926, 0.5592, 0.4244, 0.2885, 0.1518])\n",
      "----------------------------------------\n",
      "iter  2  stage  8  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0113, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.3414, 6.3414, 6.3414, 6.3414, 6.3414, 6.3414, 6.3414, 6.3414, 6.3414,\n",
      "        5.8413, 5.3852, 4.9607, 4.5592, 4.1745, 3.8022, 3.4390, 3.0826, 2.7313,\n",
      "        2.3837, 2.0390, 1.6963, 1.3552, 1.0153, 0.6763, 0.3379]) return=  122329.26544005591\n",
      "probs of actions:  tensor([0.9989, 0.9987, 0.9990, 0.9990, 0.9989, 0.9987, 0.9991, 0.9988, 0.9990,\n",
      "        0.9990, 0.9990, 0.9991, 0.9994, 0.9997, 0.9995, 0.9996, 0.9996, 0.9995,\n",
      "        1.0000, 0.9996, 0.9998, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.5537, 0.5337, 0.5188, 0.5079, 0.4997, 0.4936, 0.4891, 0.4857,\n",
      "        0.4832, 0.4813, 0.4798, 0.4788, 0.4780, 0.4774, 0.4769, 0.4766, 0.4764,\n",
      "        0.4762, 0.4760, 0.4759, 0.4758, 0.4758, 0.4757, 0.4901])\n",
      "finalReturns:  tensor([1.7982, 1.8126, 1.7855, 1.7288, 1.6505, 1.5564, 1.4507, 1.3365, 1.2159,\n",
      "        1.0906, 0.9619, 0.8304, 0.6971, 0.5623, 0.4263, 0.2896, 0.1522])\n",
      "----------------------------------------\n",
      "iter  2  stage  7  ep  179   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0132, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.6914, 6.6914, 6.6914, 6.6914, 6.6914, 6.6914, 6.6914, 6.6914, 6.1879,\n",
      "        5.7293, 5.3031, 4.9003, 4.5146, 4.1416, 3.7779, 3.4212, 3.0696, 2.7218,\n",
      "        2.3769, 2.0341, 1.6929, 1.3529, 1.0139, 0.6754, 0.3375]) return=  122329.26544005591\n",
      "probs of actions:  tensor([0.9989, 0.9987, 0.9990, 0.9990, 0.9989, 0.9987, 0.9991, 0.9990, 0.9991,\n",
      "        0.9991, 0.9990, 0.9991, 0.9994, 0.9997, 0.9995, 0.9996, 0.9996, 0.9995,\n",
      "        1.0000, 0.9996, 0.9998, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.5537, 0.5337, 0.5188, 0.5079, 0.4997, 0.4936, 0.4891, 0.4857,\n",
      "        0.4832, 0.4813, 0.4798, 0.4788, 0.4780, 0.4774, 0.4769, 0.4766, 0.4764,\n",
      "        0.4762, 0.4760, 0.4759, 0.4758, 0.4758, 0.4757, 0.4901])\n",
      "finalReturns:  tensor([1.9373, 1.9517, 1.9246, 1.8677, 1.7892, 1.6950, 1.5892, 1.4749, 1.3543,\n",
      "        1.2290, 1.1001, 0.9687, 0.8353, 0.7005, 0.5645, 0.4278, 0.2904, 0.1526])\n",
      "----------------------------------------\n",
      "iter  2  stage  6  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0154, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.0453, 7.0453, 7.0453, 7.0453, 7.0453, 7.0453, 7.0453, 6.5372, 6.0754,\n",
      "        5.6468, 5.2423, 4.8554, 4.4815, 4.1171, 3.7599, 3.4079, 3.0598, 2.7147,\n",
      "        2.3717, 2.0305, 1.6904, 1.3512, 1.0128, 0.6748, 0.3373]) return=  122329.26544005591\n",
      "probs of actions:  tensor([0.9989, 0.9987, 0.9990, 0.9990, 0.9989, 0.9987, 0.9991, 0.9990, 0.9991,\n",
      "        0.9991, 0.9990, 0.9991, 0.9994, 0.9997, 0.9995, 0.9996, 0.9996, 0.9995,\n",
      "        1.0000, 0.9996, 0.9998, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.5537, 0.5337, 0.5188, 0.5079, 0.4997, 0.4936, 0.4891, 0.4857,\n",
      "        0.4832, 0.4813, 0.4798, 0.4788, 0.4780, 0.4774, 0.4769, 0.4766, 0.4764,\n",
      "        0.4762, 0.4760, 0.4759, 0.4758, 0.4758, 0.4757, 0.4901])\n",
      "finalReturns:  tensor([2.0770, 2.0914, 2.0642, 2.0071, 1.9284, 1.8340, 1.7281, 1.6137, 1.4930,\n",
      "        1.3676, 1.2387, 1.1072, 0.9738, 0.8389, 0.7030, 0.5662, 0.4288, 0.2910,\n",
      "        0.1528])\n",
      "----------------------------------------\n",
      "iter  2  stage  5  ep  179   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0167, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.4046, 7.4046, 7.4046, 7.4046, 7.4046, 7.4046, 6.8905, 6.4243, 5.9926,\n",
      "        5.5858, 5.1973, 4.8221, 4.4568, 4.0989, 3.7464, 3.3979, 3.0525, 2.7094,\n",
      "        2.3679, 2.0277, 1.6885, 1.3500, 1.0120, 0.6744, 0.3371]) return=  122329.26544005591\n",
      "probs of actions:  tensor([0.9990, 0.9988, 0.9990, 0.9990, 0.9990, 0.9990, 0.9993, 0.9991, 0.9991,\n",
      "        0.9992, 0.9991, 0.9992, 0.9994, 0.9998, 0.9995, 0.9996, 0.9997, 0.9995,\n",
      "        1.0000, 0.9996, 0.9998, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.5537, 0.5337, 0.5188, 0.5079, 0.4997, 0.4936, 0.4891, 0.4857,\n",
      "        0.4832, 0.4813, 0.4798, 0.4788, 0.4780, 0.4774, 0.4769, 0.4766, 0.4764,\n",
      "        0.4762, 0.4760, 0.4759, 0.4758, 0.4758, 0.4757, 0.4901])\n",
      "finalReturns:  tensor([2.2174, 2.2318, 2.2043, 2.1470, 2.0681, 1.9735, 1.8674, 1.7528, 1.6320,\n",
      "        1.5065, 1.3775, 1.2460, 1.1126, 0.9777, 0.8417, 0.7049, 0.5675, 0.4296,\n",
      "        0.2915, 0.1530])\n",
      "----------------------------------------\n",
      "iter  2  stage  4  ep  42   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0198, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.7714, 7.7714, 7.7714, 7.7714, 7.7714, 7.2491, 6.7771, 6.3412, 5.9313,\n",
      "        5.5405, 5.1637, 4.7972, 4.4384, 4.0852, 3.7363, 3.3905, 3.0470, 2.7054,\n",
      "        2.3650, 2.0257, 1.6871, 1.3490, 1.0114, 0.6740, 0.3369]) return=  122329.26544005591\n",
      "probs of actions:  tensor([0.9990, 0.9988, 0.9990, 0.9990, 0.9990, 0.9990, 0.9993, 0.9991, 0.9991,\n",
      "        0.9992, 0.9991, 0.9991, 0.9994, 0.9998, 0.9995, 0.9996, 0.9997, 0.9995,\n",
      "        1.0000, 0.9996, 0.9998, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.5537, 0.5337, 0.5188, 0.5079, 0.4997, 0.4936, 0.4891, 0.4857,\n",
      "        0.4832, 0.4813, 0.4798, 0.4788, 0.4780, 0.4774, 0.4769, 0.4766, 0.4764,\n",
      "        0.4762, 0.4760, 0.4759, 0.4758, 0.4758, 0.4757, 0.4901])\n",
      "finalReturns:  tensor([2.3585, 2.3729, 2.3452, 2.2875, 2.2082, 2.1134, 2.0070, 1.8922, 1.7713,\n",
      "        1.6456, 1.5166, 1.3850, 1.2515, 1.1165, 0.9805, 0.8437, 0.7063, 0.5684,\n",
      "        0.4303, 0.2918, 0.1532])\n",
      "----------------------------------------\n",
      "iter  2  stage  3  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0227, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.1482, 8.1482, 8.1482, 8.1482, 7.6150, 7.1352, 6.6936, 6.2797, 5.8859,\n",
      "        5.5069, 5.1387, 4.7787, 4.4246, 4.0750, 3.7287, 3.3849, 3.0429, 2.7024,\n",
      "        2.3629, 2.0241, 1.6860, 1.3483, 1.0109, 0.6738, 0.3368]) return=  122329.26544005591\n",
      "probs of actions:  tensor([0.9990, 0.9988, 0.9990, 0.9990, 0.9990, 0.9990, 0.9993, 0.9991, 0.9991,\n",
      "        0.9992, 0.9991, 0.9991, 0.9994, 0.9998, 0.9995, 0.9996, 0.9997, 0.9995,\n",
      "        1.0000, 0.9996, 0.9998, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.5537, 0.5337, 0.5188, 0.5079, 0.4997, 0.4936, 0.4891, 0.4857,\n",
      "        0.4832, 0.4813, 0.4798, 0.4788, 0.4780, 0.4774, 0.4769, 0.4766, 0.4764,\n",
      "        0.4762, 0.4760, 0.4759, 0.4758, 0.4758, 0.4757, 0.4901])\n",
      "finalReturns:  tensor([2.5005, 2.5149, 2.4868, 2.4287, 2.3490, 2.2537, 2.1470, 2.0320, 1.9108,\n",
      "        1.7850, 1.6559, 1.5242, 1.3906, 1.2556, 1.1195, 0.9827, 0.8452, 0.7074,\n",
      "        0.5692, 0.4307, 0.2921, 0.1533])\n",
      "----------------------------------------\n",
      "iter  2  stage  2  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0259, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.5388, 8.5388, 8.5388, 7.9907, 7.5004, 7.0513, 6.6318, 6.2340, 5.8521,\n",
      "        5.4817, 5.1201, 4.7648, 4.4143, 4.0673, 3.7230, 3.3807, 3.0399, 2.7002,\n",
      "        2.3613, 2.0230, 1.6852, 1.3478, 1.0106, 0.6736, 0.3367]) return=  122329.26544005591\n",
      "probs of actions:  tensor([0.9990, 0.9988, 0.9990, 0.9990, 0.9990, 0.9990, 0.9993, 0.9991, 0.9991,\n",
      "        0.9992, 0.9991, 0.9991, 0.9994, 0.9998, 0.9995, 0.9996, 0.9997, 0.9995,\n",
      "        1.0000, 0.9996, 0.9998, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.5537, 0.5337, 0.5188, 0.5079, 0.4997, 0.4936, 0.4891, 0.4857,\n",
      "        0.4832, 0.4813, 0.4798, 0.4788, 0.4780, 0.4774, 0.4769, 0.4766, 0.4764,\n",
      "        0.4762, 0.4760, 0.4759, 0.4758, 0.4758, 0.4757, 0.4901])\n",
      "finalReturns:  tensor([2.6436, 2.6580, 2.6294, 2.5707, 2.4905, 2.3946, 2.2875, 2.1722, 2.0507,\n",
      "        1.9247, 1.7953, 1.6635, 1.5298, 1.3948, 1.2586, 1.1218, 0.9843, 0.8464,\n",
      "        0.7082, 0.5697, 0.4310, 0.2923, 0.1534])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  2  stage  1  ep  130   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0258, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.9480, 8.9480, 8.3799, 7.8753, 7.4160, 6.9892, 6.5860, 6.2001, 5.8268,\n",
      "        5.4630, 5.1061, 4.7544, 4.4066, 4.0616, 3.7188, 3.3776, 3.0376, 2.6985,\n",
      "        2.3601, 2.0221, 1.6846, 1.3473, 1.0103, 0.6734, 0.3367]) return=  122329.26544005591\n",
      "probs of actions:  tensor([0.9990, 0.9990, 0.9992, 0.9992, 0.9992, 0.9991, 0.9994, 0.9993, 0.9991,\n",
      "        0.9993, 0.9991, 0.9992, 0.9995, 0.9998, 0.9995, 0.9996, 0.9997, 0.9996,\n",
      "        1.0000, 0.9996, 0.9999, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.5537, 0.5337, 0.5188, 0.5079, 0.4997, 0.4936, 0.4891, 0.4857,\n",
      "        0.4832, 0.4813, 0.4798, 0.4788, 0.4780, 0.4774, 0.4769, 0.4766, 0.4764,\n",
      "        0.4762, 0.4760, 0.4759, 0.4758, 0.4758, 0.4757, 0.4901])\n",
      "finalReturns:  tensor([2.7881, 2.8025, 2.7734, 2.7139, 2.6328, 2.5363, 2.4286, 2.3128, 2.1909,\n",
      "        2.0646, 1.9351, 1.8031, 1.6692, 1.5341, 1.3979, 1.2609, 1.1234, 0.9855,\n",
      "        0.8473, 0.7088, 0.5701, 0.4313, 0.2924, 0.1534])\n",
      "----------------------------------------\n",
      "iter  2  stage  0  ep  0   adversary:  AdversaryModes.fight_lb_132\n",
      "  actions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12,  0])\n",
      "loss=  tensor(0.0291, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.2984, 8.7872, 8.2633, 7.7903, 7.3535, 6.9431, 6.5519, 6.1747, 5.8080,\n",
      "        5.4490, 5.0957, 4.7466, 4.4008, 4.0573, 3.7156, 3.3752, 3.0359, 2.6972,\n",
      "        2.3591, 2.0215, 1.6842, 1.3470, 1.0101, 0.6733, 0.3366]) return=  122329.26544005591\n",
      "probs of actions:  tensor([0.9990, 0.9990, 0.9992, 0.9992, 0.9992, 0.9991, 0.9994, 0.9993, 0.9991,\n",
      "        0.9993, 0.9991, 0.9992, 0.9995, 0.9998, 0.9995, 0.9996, 0.9997, 0.9996,\n",
      "        1.0000, 0.9996, 0.9999, 1.0000, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.5537, 0.5337, 0.5188, 0.5079, 0.4997, 0.4936, 0.4891, 0.4857,\n",
      "        0.4832, 0.4813, 0.4798, 0.4788, 0.4780, 0.4774, 0.4769, 0.4766, 0.4764,\n",
      "        0.4762, 0.4760, 0.4759, 0.4758, 0.4758, 0.4757, 0.4901])\n",
      "finalReturns:  tensor([2.9345, 2.9489, 2.9190, 2.8584, 2.7763, 2.6789, 2.5704, 2.4539, 2.3316,\n",
      "        2.2049, 2.0751, 1.9428, 1.8088, 1.6735, 1.5372, 1.4002, 1.2627, 1.1247,\n",
      "        0.9864, 0.8479, 0.7092, 0.5704, 0.4315, 0.2925, 0.1535])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1683049303 saved\n",
      "[1239451, 'tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])', 122329.26544005591, 95669.58206529099, 0.029126999899744987, 1e-05, 1, 0, 'tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\\n        12, 12, 12, 12, 12, 12,  0])', '[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\\n 1.]', '0,[1e-05,1][1, 10000, 1, 1],1683049303', 25, 50, 157611.33342506486, 175538.2609849781, 68694.09895647192, 135313.23466666666, 132439.05866666668, 122329.26544005591, 122329.26544005591, 130254.64915679736, 130254.64915679736, 80045.82189636558, 122329.26544005591, 130254.64915679736]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x648 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy reset\n",
      "----------------------------------------\n",
      "iter  0  stage  24  ep  99999   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 0, 0, 0, 1, 0, 0,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.4671, 0.4671, 0.4671, 0.4671, 0.4671, 0.4671, 0.4671, 0.4671, 0.4671,\n",
      "        0.4671, 0.4671, 0.4671, 0.4671, 0.4671, 0.4671, 0.4671, 0.4671, 0.4671,\n",
      "        0.4671, 0.4671, 0.4671, 0.4671, 0.4671, 0.4671, 0.4671]) return=  118465.88424035063\n",
      "probs of actions:  tensor([0.9225, 0.9092, 0.0133, 0.9142, 0.9103, 0.9245, 0.9002, 0.9208, 0.9088,\n",
      "        0.9158, 0.9157, 0.9228, 0.9095, 0.8977, 0.0451, 0.9123, 0.9046, 0.0062,\n",
      "        0.9008, 0.9034, 0.9077, 0.0532, 0.9217, 0.8866, 0.9858],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.4988, 0.4892, 0.4897, 0.4828, 0.4776, 0.4738, 0.4709, 0.4688,\n",
      "        0.4672, 0.4660, 0.4651, 0.4644, 0.4639, 0.4634, 0.4667, 0.4656, 0.4632,\n",
      "        0.4779, 0.4740, 0.4711, 0.4688, 0.4707, 0.4686, 0.4671])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  0  stage  23  ep  99999   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([ 6,  0,  0,  9,  0,  0,  0,  9,  7,  0,  0, 12,  1, 13,  0,  0,  7,  8,\n",
      "         0,  9,  0,  7,  9,  9,  0])\n",
      "loss=  tensor(0.0365, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.0671, 1.0671, 1.0671, 1.0671, 1.0671, 1.0671, 1.0671, 1.0671, 1.0671,\n",
      "        1.0671, 1.0671, 1.0671, 1.0671, 1.0671, 1.0671, 1.0671, 1.0671, 1.0671,\n",
      "        1.0671, 1.0671, 1.0671, 1.0671, 1.0671, 1.0671, 0.5230]) return=  128711.60607356537\n",
      "probs of actions:  tensor([0.0279, 0.3195, 0.2609, 0.3040, 0.4547, 0.4141, 0.3812, 0.2469, 0.0893,\n",
      "        0.2782, 0.3144, 0.0407, 0.0230, 0.0346, 0.2974, 0.2232, 0.0981, 0.1333,\n",
      "        0.3403, 0.2705, 0.3291, 0.1090, 0.2630, 0.2326, 0.9983],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5076, 0.5202, 0.5054, 0.4864, 0.5183, 0.5040, 0.4934, 0.4775, 0.5065,\n",
      "        0.5240, 0.5082, 0.4821, 0.5306, 0.4999, 0.5501, 0.5274, 0.5059, 0.5171,\n",
      "        0.5368, 0.5095, 0.5360, 0.5122, 0.5202, 0.5360, 0.5561])\n",
      "finalReturns:  tensor([0.0250, 0.0331])\n",
      "----------------------------------------\n",
      "iter  0  stage  22  ep  99999   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([17, 21, 21, 21, 21, 21, 14, 21,  7, 11, 21, 21, 13,  9, 21, 21, 13, 21,\n",
      "        21, 21, 21, 21, 21, 17,  0])\n",
      "loss=  tensor(0.4933, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.9320, 0.9320, 0.9320, 0.9320, 0.9320, 0.9320, 0.9320, 0.9320, 0.9320,\n",
      "        0.9320, 0.9320, 0.9320, 0.9320, 0.9320, 0.9320, 0.9320, 0.9320, 0.9320,\n",
      "        0.9320, 0.9320, 0.9320, 0.9320, 0.9320, 0.5697, 0.2649]) return=  90665.36375873255\n",
      "probs of actions:  tensor([0.0206, 0.6746, 0.6706, 0.7113, 0.6599, 0.7061, 0.0543, 0.6362, 0.0054,\n",
      "        0.0176, 0.6451, 0.6355, 0.1559, 0.0499, 0.6243, 0.6552, 0.1977, 0.5576,\n",
      "        0.6014, 0.5997, 0.6143, 0.6144, 0.9103, 0.0191, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4823, 0.5165, 0.5707, 0.5052, 0.4586, 0.4250, 0.4250, 0.3600, 0.3919,\n",
      "        0.3367, 0.2820, 0.2939, 0.3302, 0.3225, 0.2632, 0.2796, 0.3193, 0.2785,\n",
      "        0.2913, 0.3010, 0.3084, 0.3140, 0.3182, 0.3366, 0.3559])\n",
      "finalReturns:  tensor([0.0787, 0.1228, 0.0910])\n",
      "----------------------------------------\n",
      "iter  0  stage  21  ep  99999   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 13,  0])\n",
      "loss=  tensor(0.4876, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.5149, 1.5149, 1.5149, 1.5149, 1.5149, 1.5149, 1.5149, 1.5149, 1.5149,\n",
      "        1.5149, 1.5149, 1.5149, 1.5149, 1.5149, 1.5149, 1.5149, 1.5149, 1.5149,\n",
      "        1.5149, 1.5149, 1.5149, 1.5149, 1.0522, 0.6582, 0.3121]) return=  109929.65918662024\n",
      "probs of actions:  tensor([0.9783, 0.9750, 0.9743, 0.9797, 0.9723, 0.9814, 0.9655, 0.9731, 0.9711,\n",
      "        0.9687, 0.9743, 0.9726, 0.9711, 0.9715, 0.9721, 0.9763, 0.9660, 0.9519,\n",
      "        0.9693, 0.9662, 0.9683, 0.9910, 0.9983, 0.1137, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4671, 0.5316, 0.5021, 0.4805, 0.4646, 0.4528, 0.4441, 0.4376, 0.4327,\n",
      "        0.4291, 0.4264, 0.4244, 0.4228, 0.4217, 0.4208, 0.4202, 0.4197, 0.4194,\n",
      "        0.4191, 0.4189, 0.4188, 0.4186, 0.4186, 0.4457, 0.4357])\n",
      "finalReturns:  tensor([0.2037, 0.2478, 0.2232, 0.1236])\n",
      "----------------------------------------\n",
      "iter  0  stage  20  ep  99999   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21,  0])\n",
      "loss=  tensor(0.0092, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.8031, 1.8031, 1.8031, 1.8031, 1.8031, 1.8031, 1.8031, 1.8031, 1.8031,\n",
      "        1.8031, 1.8031, 1.8031, 1.8031, 1.8031, 1.8031, 1.8031, 1.8031, 1.8031,\n",
      "        1.8031, 1.8031, 1.8031, 1.3403, 0.9462, 0.6001, 0.2879]) return=  109925.7013290539\n",
      "probs of actions:  tensor([0.9984, 0.9981, 0.9979, 0.9986, 0.9977, 0.9987, 0.9970, 0.9980, 0.9974,\n",
      "        0.9973, 0.9981, 0.9979, 0.9977, 0.9978, 0.9978, 0.9981, 0.9972, 0.9956,\n",
      "        0.9976, 0.9975, 0.9985, 0.9999, 1.0000, 0.9697, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4671, 0.5316, 0.5021, 0.4805, 0.4646, 0.4528, 0.4441, 0.4376, 0.4327,\n",
      "        0.4291, 0.4264, 0.4244, 0.4228, 0.4217, 0.4208, 0.4202, 0.4197, 0.4194,\n",
      "        0.4191, 0.4189, 0.4188, 0.4186, 0.4186, 0.4185, 0.4625])\n",
      "finalReturns:  tensor([0.3339, 0.3780, 0.3534, 0.2810, 0.1747])\n",
      "----------------------------------------\n",
      "iter  0  stage  19  ep  956   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        13, 21, 21, 21, 21, 21,  0])\n",
      "loss=  tensor(0.0114, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.9943, 1.9943, 1.9943, 1.9943, 1.9943, 1.9943, 1.9943, 1.9943, 1.9943,\n",
      "        1.9943, 1.9943, 1.9943, 1.9943, 1.9943, 1.9943, 1.9943, 1.9943, 1.9943,\n",
      "        1.9943, 1.9943, 1.5581, 1.1825, 0.8495, 0.5466, 0.2654]) return=  109311.80214161768\n",
      "probs of actions:  tensor([0.9987, 0.9985, 0.9984, 0.9989, 0.9982, 0.9989, 0.9977, 0.9984, 0.9980,\n",
      "        0.9978, 0.9985, 0.9985, 0.9982, 0.9983, 0.9983, 0.9985, 0.9978, 0.9966,\n",
      "        0.0010, 0.9990, 0.9989, 1.0000, 1.0000, 0.9682, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4671, 0.5316, 0.5021, 0.4805, 0.4646, 0.4528, 0.4441, 0.4376, 0.4327,\n",
      "        0.4291, 0.4264, 0.4244, 0.4228, 0.4217, 0.4208, 0.4202, 0.4197, 0.4194,\n",
      "        0.4463, 0.3921, 0.3986, 0.4035, 0.4071, 0.4099, 0.4561])\n",
      "finalReturns:  tensor([0.4730, 0.5171, 0.4941, 0.4237, 0.3194, 0.1907])\n",
      "----------------------------------------\n",
      "iter  0  stage  18  ep  722   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21,  0])\n",
      "loss=  tensor(0.0127, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.3320, 2.3320, 2.3320, 2.3320, 2.3320, 2.3320, 2.3320, 2.3320, 2.3320,\n",
      "        2.3320, 2.3320, 2.3320, 2.3320, 2.3320, 2.3320, 2.3320, 2.3320, 2.3320,\n",
      "        2.3320, 1.8688, 1.4745, 1.1282, 0.8159, 0.5279, 0.2575]) return=  109925.7013290539\n",
      "probs of actions:  tensor([0.9988, 0.9987, 0.9985, 0.9990, 0.9984, 0.9990, 0.9979, 0.9986, 0.9981,\n",
      "        0.9980, 0.9986, 0.9986, 0.9984, 0.9984, 0.9984, 0.9987, 0.9980, 0.9969,\n",
      "        0.9990, 0.9993, 0.9990, 1.0000, 1.0000, 0.9694, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4671, 0.5316, 0.5021, 0.4805, 0.4646, 0.4528, 0.4441, 0.4376, 0.4327,\n",
      "        0.4291, 0.4264, 0.4244, 0.4228, 0.4217, 0.4208, 0.4202, 0.4197, 0.4194,\n",
      "        0.4191, 0.4189, 0.4188, 0.4186, 0.4186, 0.4185, 0.4625])\n",
      "finalReturns:  tensor([0.6430, 0.6871, 0.6625, 0.5900, 0.4837, 0.3531, 0.2050])\n",
      "----------------------------------------\n",
      "iter  0  stage  17  ep  50770   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21,  0])\n",
      "loss=  tensor(0.0102, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.5810, 2.5810, 2.5810, 2.5810, 2.5810, 2.5810, 2.5810, 2.5810, 2.5810,\n",
      "        2.5810, 2.5810, 2.5810, 2.5810, 2.5810, 2.5810, 2.5810, 2.5810, 2.5810,\n",
      "        2.1175, 1.7230, 1.3766, 1.0642, 0.7761, 0.5057, 0.2481]) return=  109925.7013290539\n",
      "probs of actions:  tensor([0.9994, 0.9993, 0.9992, 0.9995, 0.9991, 0.9995, 0.9989, 0.9993, 0.9990,\n",
      "        0.9989, 0.9993, 0.9993, 0.9992, 0.9992, 0.9992, 0.9993, 0.9989, 0.9990,\n",
      "        0.9997, 1.0000, 0.9999, 1.0000, 1.0000, 0.9761, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4671, 0.5316, 0.5021, 0.4805, 0.4646, 0.4528, 0.4441, 0.4376, 0.4327,\n",
      "        0.4291, 0.4264, 0.4244, 0.4228, 0.4217, 0.4208, 0.4202, 0.4197, 0.4194,\n",
      "        0.4191, 0.4189, 0.4188, 0.4186, 0.4186, 0.4185, 0.4625])\n",
      "finalReturns:  tensor([0.8134, 0.8575, 0.8329, 0.7604, 0.6541, 0.5235, 0.3753, 0.2144])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  0  stage  16  ep  39   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21,  0])\n",
      "loss=  tensor(0.0118, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.8233, 2.8233, 2.8233, 2.8233, 2.8233, 2.8233, 2.8233, 2.8233, 2.8233,\n",
      "        2.8233, 2.8233, 2.8233, 2.8233, 2.8233, 2.8233, 2.8233, 2.8233, 2.3595,\n",
      "        1.9647, 1.6181, 1.3056, 1.0175, 0.7470, 0.4894, 0.2412]) return=  109925.7013290539\n",
      "probs of actions:  tensor([0.9994, 0.9993, 0.9992, 0.9995, 0.9991, 0.9995, 0.9989, 0.9993, 0.9990,\n",
      "        0.9989, 0.9993, 0.9993, 0.9992, 0.9992, 0.9992, 0.9993, 0.9990, 0.9990,\n",
      "        0.9997, 1.0000, 0.9999, 1.0000, 1.0000, 0.9763, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4671, 0.5316, 0.5021, 0.4805, 0.4646, 0.4528, 0.4441, 0.4376, 0.4327,\n",
      "        0.4291, 0.4264, 0.4244, 0.4228, 0.4217, 0.4208, 0.4202, 0.4197, 0.4194,\n",
      "        0.4191, 0.4189, 0.4188, 0.4186, 0.4186, 0.4185, 0.4625])\n",
      "finalReturns:  tensor([0.9908, 1.0349, 1.0103, 0.9377, 0.8314, 0.7007, 0.5526, 0.3917, 0.2213])\n",
      "----------------------------------------\n",
      "iter  0  stage  15  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21,  0])\n",
      "loss=  tensor(0.0134, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.0609, 3.0609, 3.0609, 3.0609, 3.0609, 3.0609, 3.0609, 3.0609, 3.0609,\n",
      "        3.0609, 3.0609, 3.0609, 3.0609, 3.0609, 3.0609, 3.0609, 2.5966, 2.2016,\n",
      "        1.8547, 1.5420, 1.2538, 0.9832, 0.7255, 0.4773, 0.2361]) return=  109925.7013290539\n",
      "probs of actions:  tensor([0.9994, 0.9993, 0.9992, 0.9995, 0.9991, 0.9995, 0.9989, 0.9993, 0.9990,\n",
      "        0.9989, 0.9993, 0.9993, 0.9992, 0.9992, 0.9992, 0.9993, 0.9990, 0.9990,\n",
      "        0.9997, 1.0000, 0.9999, 1.0000, 1.0000, 0.9763, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4671, 0.5316, 0.5021, 0.4805, 0.4646, 0.4528, 0.4441, 0.4376, 0.4327,\n",
      "        0.4291, 0.4264, 0.4244, 0.4228, 0.4217, 0.4208, 0.4202, 0.4197, 0.4194,\n",
      "        0.4191, 0.4189, 0.4188, 0.4186, 0.4186, 0.4185, 0.4625])\n",
      "finalReturns:  tensor([1.1734, 1.2175, 1.1928, 1.1203, 1.0138, 0.8832, 0.7350, 0.5741, 0.4037,\n",
      "        0.2265])\n",
      "----------------------------------------\n",
      "iter  0  stage  14  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21,  0])\n",
      "loss=  tensor(0.0151, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.2953, 3.2953, 3.2953, 3.2953, 3.2953, 3.2953, 3.2953, 3.2953, 3.2953,\n",
      "        3.2953, 3.2953, 3.2953, 3.2953, 3.2953, 3.2953, 2.8304, 2.4348, 2.0877,\n",
      "        1.7748, 1.4864, 1.2157, 0.9579, 0.7097, 0.4684, 0.2323]) return=  109925.7013290539\n",
      "probs of actions:  tensor([0.9994, 0.9993, 0.9992, 0.9995, 0.9991, 0.9995, 0.9989, 0.9993, 0.9990,\n",
      "        0.9989, 0.9993, 0.9993, 0.9992, 0.9992, 0.9992, 0.9993, 0.9990, 0.9990,\n",
      "        0.9997, 1.0000, 0.9999, 1.0000, 1.0000, 0.9763, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4671, 0.5316, 0.5021, 0.4805, 0.4646, 0.4528, 0.4441, 0.4376, 0.4327,\n",
      "        0.4291, 0.4264, 0.4244, 0.4228, 0.4217, 0.4208, 0.4202, 0.4197, 0.4194,\n",
      "        0.4191, 0.4189, 0.4188, 0.4186, 0.4186, 0.4185, 0.4625])\n",
      "finalReturns:  tensor([1.3598, 1.4039, 1.3793, 1.3067, 1.2002, 1.0695, 0.9213, 0.7603, 0.5899,\n",
      "        0.4127, 0.2303])\n",
      "----------------------------------------\n",
      "iter  0  stage  13  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21,  0])\n",
      "loss=  tensor(0.0171, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.5276, 3.5276, 3.5276, 3.5276, 3.5276, 3.5276, 3.5276, 3.5276, 3.5276,\n",
      "        3.5276, 3.5276, 3.5276, 3.5276, 3.5276, 3.0618, 2.6657, 2.3181, 2.0049,\n",
      "        1.7163, 1.4455, 1.1876, 0.9392, 0.6979, 0.4617, 0.2294]) return=  109925.7013290539\n",
      "probs of actions:  tensor([0.9995, 0.9993, 0.9992, 0.9995, 0.9991, 0.9995, 0.9989, 0.9993, 0.9990,\n",
      "        0.9989, 0.9993, 0.9993, 0.9992, 0.9992, 0.9992, 0.9993, 0.9990, 0.9990,\n",
      "        0.9997, 1.0000, 0.9999, 1.0000, 1.0000, 0.9763, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4671, 0.5316, 0.5021, 0.4805, 0.4646, 0.4528, 0.4441, 0.4376, 0.4327,\n",
      "        0.4291, 0.4264, 0.4244, 0.4228, 0.4217, 0.4208, 0.4202, 0.4197, 0.4194,\n",
      "        0.4191, 0.4189, 0.4188, 0.4186, 0.4186, 0.4185, 0.4625])\n",
      "finalReturns:  tensor([1.5493, 1.5934, 1.5686, 1.4960, 1.3894, 1.2587, 1.1104, 0.9494, 0.7790,\n",
      "        0.6017, 0.4193, 0.2331])\n",
      "----------------------------------------\n",
      "iter  0  stage  12  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21,  0])\n",
      "loss=  tensor(0.0192, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.7587, 3.7587, 3.7587, 3.7587, 3.7587, 3.7587, 3.7587, 3.7587, 3.7587,\n",
      "        3.7587, 3.7587, 3.7587, 3.7587, 3.2918, 2.8949, 2.5468, 2.2332, 1.9443,\n",
      "        1.6732, 1.4152, 1.1667, 0.9253, 0.6891, 0.4567, 0.2273]) return=  109925.7013290539\n",
      "probs of actions:  tensor([0.9995, 0.9993, 0.9992, 0.9995, 0.9991, 0.9995, 0.9989, 0.9993, 0.9990,\n",
      "        0.9989, 0.9993, 0.9993, 0.9992, 0.9992, 0.9992, 0.9993, 0.9990, 0.9990,\n",
      "        0.9997, 1.0000, 0.9999, 1.0000, 1.0000, 0.9763, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4671, 0.5316, 0.5021, 0.4805, 0.4646, 0.4528, 0.4441, 0.4376, 0.4327,\n",
      "        0.4291, 0.4264, 0.4244, 0.4228, 0.4217, 0.4208, 0.4202, 0.4197, 0.4194,\n",
      "        0.4191, 0.4189, 0.4188, 0.4186, 0.4186, 0.4185, 0.4625])\n",
      "finalReturns:  tensor([1.7410, 1.7851, 1.7603, 1.6875, 1.5809, 1.4501, 1.3018, 1.1407, 0.9703,\n",
      "        0.7929, 0.6105, 0.4243, 0.2352])\n",
      "----------------------------------------\n",
      "iter  0  stage  11  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21,  0])\n",
      "loss=  tensor(0.0211, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.9896, 3.9896, 3.9896, 3.9896, 3.9896, 3.9896, 3.9896, 3.9896, 3.9896,\n",
      "        3.9896, 3.9896, 3.9896, 3.5211, 3.1232, 2.7744, 2.4602, 2.1710, 1.8996,\n",
      "        1.6414, 1.3928, 1.1512, 0.9149, 0.6825, 0.4530, 0.2257]) return=  109925.7013290539\n",
      "probs of actions:  tensor([0.9995, 0.9993, 0.9992, 0.9995, 0.9991, 0.9995, 0.9989, 0.9993, 0.9990,\n",
      "        0.9989, 0.9993, 0.9993, 0.9992, 0.9992, 0.9992, 0.9993, 0.9990, 0.9990,\n",
      "        0.9997, 1.0000, 0.9999, 1.0000, 1.0000, 0.9763, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4671, 0.5316, 0.5021, 0.4805, 0.4646, 0.4528, 0.4441, 0.4376, 0.4327,\n",
      "        0.4291, 0.4264, 0.4244, 0.4228, 0.4217, 0.4208, 0.4202, 0.4197, 0.4194,\n",
      "        0.4191, 0.4189, 0.4188, 0.4186, 0.4186, 0.4185, 0.4625])\n",
      "finalReturns:  tensor([1.9345, 1.9786, 1.9537, 1.8808, 1.7741, 1.6431, 1.4947, 1.3336, 1.1631,\n",
      "        0.9857, 0.8033, 0.6170, 0.4280, 0.2368])\n",
      "----------------------------------------\n",
      "iter  0  stage  10  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21,  0])\n",
      "loss=  tensor(0.0233, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.2210, 4.2210, 4.2210, 4.2210, 4.2210, 4.2210, 4.2210, 4.2210, 4.2210,\n",
      "        4.2210, 4.2210, 3.7505, 3.3512, 3.0014, 2.6866, 2.3968, 2.1251, 1.8666,\n",
      "        1.6178, 1.3761, 1.1397, 0.9072, 0.6777, 0.4503, 0.2245]) return=  109925.7013290539\n",
      "probs of actions:  tensor([0.9995, 0.9993, 0.9992, 0.9995, 0.9991, 0.9995, 0.9989, 0.9993, 0.9990,\n",
      "        0.9989, 0.9993, 0.9993, 0.9992, 0.9992, 0.9992, 0.9993, 0.9990, 0.9990,\n",
      "        0.9997, 1.0000, 0.9999, 1.0000, 1.0000, 0.9763, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4671, 0.5316, 0.5021, 0.4805, 0.4646, 0.4528, 0.4441, 0.4376, 0.4327,\n",
      "        0.4291, 0.4264, 0.4244, 0.4228, 0.4217, 0.4208, 0.4202, 0.4197, 0.4194,\n",
      "        0.4191, 0.4189, 0.4188, 0.4186, 0.4186, 0.4185, 0.4625])\n",
      "finalReturns:  tensor([2.1294, 2.1735, 2.1485, 2.0755, 1.9686, 1.8375, 1.6890, 1.5278, 1.3572,\n",
      "        1.1798, 0.9973, 0.8110, 0.6219, 0.4308, 0.2380])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  0  stage  9  ep  40   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21,  0])\n",
      "loss=  tensor(0.0255, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.4539, 4.4539, 4.4539, 4.4539, 4.4539, 4.4539, 4.4539, 4.4539, 4.4539,\n",
      "        4.4539, 3.9807, 3.5795, 3.2284, 2.9127, 2.6222, 2.3500, 2.0912, 1.8421,\n",
      "        1.6002, 1.3637, 1.1311, 0.9014, 0.6740, 0.4482, 0.2236]) return=  109925.7013290539\n",
      "probs of actions:  tensor([0.9994, 0.9993, 0.9992, 0.9995, 0.9991, 0.9995, 0.9989, 0.9993, 0.9990,\n",
      "        0.9990, 0.9994, 0.9994, 0.9992, 0.9993, 0.9993, 0.9993, 0.9990, 0.9990,\n",
      "        0.9997, 1.0000, 0.9999, 1.0000, 1.0000, 0.9760, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4671, 0.5316, 0.5021, 0.4805, 0.4646, 0.4528, 0.4441, 0.4376, 0.4327,\n",
      "        0.4291, 0.4264, 0.4244, 0.4228, 0.4217, 0.4208, 0.4202, 0.4197, 0.4194,\n",
      "        0.4191, 0.4189, 0.4188, 0.4186, 0.4186, 0.4185, 0.4625])\n",
      "finalReturns:  tensor([2.3256, 2.3697, 2.3445, 2.2713, 2.1642, 2.0330, 1.8843, 1.7229, 1.5523,\n",
      "        1.3748, 1.1922, 1.0059, 0.8168, 0.6256, 0.4328, 0.2389])\n",
      "----------------------------------------\n",
      "iter  0  stage  8  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21,  0])\n",
      "loss=  tensor(0.0287, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.6893, 4.6893, 4.6893, 4.6893, 4.6893, 4.6893, 4.6893, 4.6893, 4.6893,\n",
      "        4.2125, 3.8088, 3.4559, 3.1389, 2.8476, 2.5747, 2.3154, 2.0660, 1.8238,\n",
      "        1.5871, 1.3544, 1.1246, 0.8971, 0.6713, 0.4467, 0.2230]) return=  109925.7013290539\n",
      "probs of actions:  tensor([0.9994, 0.9993, 0.9992, 0.9995, 0.9991, 0.9995, 0.9989, 0.9993, 0.9990,\n",
      "        0.9990, 0.9994, 0.9994, 0.9992, 0.9993, 0.9993, 0.9993, 0.9990, 0.9990,\n",
      "        0.9997, 1.0000, 0.9999, 1.0000, 1.0000, 0.9759, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4671, 0.5316, 0.5021, 0.4805, 0.4646, 0.4528, 0.4441, 0.4376, 0.4327,\n",
      "        0.4291, 0.4264, 0.4244, 0.4228, 0.4217, 0.4208, 0.4202, 0.4197, 0.4194,\n",
      "        0.4191, 0.4189, 0.4188, 0.4186, 0.4186, 0.4185, 0.4625])\n",
      "finalReturns:  tensor([2.5229, 2.5670, 2.5416, 2.4681, 2.3608, 2.2293, 2.0804, 1.9189, 1.7481,\n",
      "        1.5705, 1.3879, 1.2015, 1.0123, 0.8211, 0.6283, 0.4344, 0.2396])\n",
      "----------------------------------------\n",
      "iter  0  stage  7  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21,  0])\n",
      "loss=  tensor(0.0314, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.9285, 4.9285, 4.9285, 4.9285, 4.9285, 4.9285, 4.9285, 4.9285, 4.4468,\n",
      "        4.0397, 3.6845, 3.3658, 3.0733, 2.7996, 2.5396, 2.2897, 2.0472, 1.8102,\n",
      "        1.5773, 1.3474, 1.1198, 0.8939, 0.6692, 0.4455, 0.2225]) return=  109925.7013290539\n",
      "probs of actions:  tensor([0.9994, 0.9993, 0.9992, 0.9995, 0.9991, 0.9995, 0.9989, 0.9993, 0.9990,\n",
      "        0.9990, 0.9994, 0.9994, 0.9993, 0.9993, 0.9993, 0.9993, 0.9990, 0.9990,\n",
      "        0.9997, 1.0000, 0.9999, 1.0000, 1.0000, 0.9759, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4671, 0.5316, 0.5021, 0.4805, 0.4646, 0.4528, 0.4441, 0.4376, 0.4327,\n",
      "        0.4291, 0.4264, 0.4244, 0.4228, 0.4217, 0.4208, 0.4202, 0.4197, 0.4194,\n",
      "        0.4191, 0.4189, 0.4188, 0.4186, 0.4186, 0.4185, 0.4625])\n",
      "finalReturns:  tensor([2.7213, 2.7654, 2.7398, 2.6659, 2.5582, 2.4264, 2.2773, 2.1155, 1.9446,\n",
      "        1.7669, 1.5841, 1.3977, 1.2084, 1.0172, 0.8243, 0.6304, 0.4355, 0.2401])\n",
      "----------------------------------------\n",
      "iter  0  stage  6  ep  40   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21,  0])\n",
      "loss=  tensor(0.0331, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.1729, 5.1729, 5.1729, 5.1729, 5.1729, 5.1729, 5.1729, 4.6847, 4.2732,\n",
      "        3.9148, 3.5939, 3.2997, 3.0249, 2.7641, 2.5135, 2.2706, 2.0333, 1.8001,\n",
      "        1.5700, 1.3423, 1.1162, 0.8915, 0.6677, 0.4446, 0.2221]) return=  109925.7013290539\n",
      "probs of actions:  tensor([0.9995, 0.9993, 0.9992, 0.9996, 0.9991, 0.9995, 0.9990, 0.9993, 0.9991,\n",
      "        0.9991, 0.9994, 0.9994, 0.9993, 0.9993, 0.9993, 0.9994, 0.9991, 0.9990,\n",
      "        0.9997, 1.0000, 0.9999, 1.0000, 1.0000, 0.9763, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4671, 0.5316, 0.5021, 0.4805, 0.4646, 0.4528, 0.4441, 0.4376, 0.4327,\n",
      "        0.4291, 0.4264, 0.4244, 0.4228, 0.4217, 0.4208, 0.4202, 0.4197, 0.4194,\n",
      "        0.4191, 0.4189, 0.4188, 0.4186, 0.4186, 0.4185, 0.4625])\n",
      "finalReturns:  tensor([2.9210, 2.9651, 2.9390, 2.8647, 2.7565, 2.6243, 2.4748, 2.3128, 2.1416,\n",
      "        1.9637, 1.7808, 1.5943, 1.4050, 1.2136, 1.0208, 0.8268, 0.6319, 0.4364,\n",
      "        0.2404])\n",
      "----------------------------------------\n",
      "iter  0  stage  5  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 13, 21,\n",
      "        21, 21, 21, 21, 21, 21,  0])\n",
      "loss=  tensor(12.9543, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.4246, 5.4246, 5.4246, 5.4246, 5.4246, 5.4246, 4.9277, 4.5102, 4.1476,\n",
      "        3.8237, 3.5274, 3.2510, 2.9890, 2.7377, 2.4941, 2.2563, 2.0228, 1.7925,\n",
      "        1.5646, 1.3384, 1.1135, 0.8897, 0.6665, 0.4440, 0.2218]) return=  109226.96100827814\n",
      "probs of actions:  tensor([9.9946e-01, 9.9933e-01, 9.9919e-01, 9.9955e-01, 9.9913e-01, 9.9952e-01,\n",
      "        9.9900e-01, 9.9935e-01, 9.9914e-01, 9.9909e-01, 9.9942e-01, 9.9944e-01,\n",
      "        9.9932e-01, 9.9934e-01, 9.9934e-01, 9.9939e-01, 5.4956e-04, 9.9907e-01,\n",
      "        9.9969e-01, 1.0000e+00, 9.9992e-01, 1.0000e+00, 1.0000e+00, 9.7597e-01,\n",
      "        1.0000e+00], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4671, 0.5316, 0.5021, 0.4805, 0.4646, 0.4528, 0.4441, 0.4376, 0.4327,\n",
      "        0.4291, 0.4264, 0.4244, 0.4228, 0.4217, 0.4208, 0.4202, 0.4469, 0.3925,\n",
      "        0.3989, 0.4037, 0.4073, 0.4101, 0.4121, 0.4137, 0.4589])\n",
      "finalReturns:  tensor([3.0521, 3.0962, 3.0697, 2.9947, 2.8859, 2.7531, 2.6032, 2.4408, 2.2693,\n",
      "        2.0912, 1.9081, 1.7214, 1.5048, 1.3402, 1.1675, 0.9886, 0.8051, 0.6182,\n",
      "        0.4286, 0.2371])\n",
      "----------------------------------------\n",
      "iter  0  stage  4  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21,  0])\n",
      "loss=  tensor(0.0390, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.6865, 5.6865, 5.6865, 5.6865, 5.6865, 5.1778, 4.7521, 4.3839, 4.0560,\n",
      "        3.7569, 3.4784, 3.2149, 2.9624, 2.7181, 2.4797, 2.2457, 2.0150, 1.7868,\n",
      "        1.5605, 1.3355, 1.1115, 0.8883, 0.6657, 0.4435, 0.2216]) return=  109925.7013290539\n",
      "probs of actions:  tensor([0.9995, 0.9993, 0.9992, 0.9996, 0.9991, 0.9995, 0.9990, 0.9993, 0.9991,\n",
      "        0.9991, 0.9994, 0.9994, 0.9993, 0.9993, 0.9993, 0.9994, 0.9990, 0.9990,\n",
      "        0.9997, 1.0000, 0.9999, 1.0000, 1.0000, 0.9762, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4671, 0.5316, 0.5021, 0.4805, 0.4646, 0.4528, 0.4441, 0.4376, 0.4327,\n",
      "        0.4291, 0.4264, 0.4244, 0.4228, 0.4217, 0.4208, 0.4202, 0.4197, 0.4194,\n",
      "        0.4191, 0.4189, 0.4188, 0.4186, 0.4186, 0.4185, 0.4625])\n",
      "finalReturns:  tensor([3.3248, 3.3689, 3.3417, 3.2659, 3.1562, 3.0226, 2.8720, 2.7091, 2.5373,\n",
      "        2.3588, 2.1755, 1.9886, 1.7991, 1.6075, 1.4145, 1.2204, 1.0255, 0.8299,\n",
      "        0.6339, 0.4375, 0.2409])\n",
      "----------------------------------------\n",
      "iter  0  stage  3  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21,  0])\n",
      "loss=  tensor(0.0413, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.9621, 5.9621, 5.9621, 5.9621, 5.4375, 5.0009, 4.6251, 4.2918, 3.9888,\n",
      "        3.7076, 3.4421, 3.1881, 2.9426, 2.7034, 2.4688, 2.2377, 2.0092, 1.7826,\n",
      "        1.5574, 1.3333, 1.1100, 0.8873, 0.6650, 0.4431, 0.2215]) return=  109925.7013290539\n",
      "probs of actions:  tensor([0.9995, 0.9993, 0.9992, 0.9996, 0.9991, 0.9995, 0.9990, 0.9993, 0.9991,\n",
      "        0.9991, 0.9994, 0.9994, 0.9993, 0.9993, 0.9993, 0.9994, 0.9990, 0.9990,\n",
      "        0.9997, 1.0000, 0.9999, 1.0000, 1.0000, 0.9761, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4671, 0.5316, 0.5021, 0.4805, 0.4646, 0.4528, 0.4441, 0.4376, 0.4327,\n",
      "        0.4291, 0.4264, 0.4244, 0.4228, 0.4217, 0.4208, 0.4202, 0.4197, 0.4194,\n",
      "        0.4191, 0.4189, 0.4188, 0.4186, 0.4186, 0.4185, 0.4625])\n",
      "finalReturns:  tensor([3.5297, 3.5738, 3.5457, 3.4688, 3.3579, 3.2234, 3.0719, 2.9084, 2.7359,\n",
      "        2.5571, 2.3734, 2.1863, 1.9966, 1.8049, 1.6118, 1.4176, 1.2226, 1.0270,\n",
      "        0.8309, 0.6345, 0.4379, 0.2411])\n",
      "----------------------------------------\n",
      "iter  0  stage  2  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21,  0])\n",
      "loss=  tensor(0.0450, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.2565, 6.2565, 6.2565, 5.7103, 5.2590, 4.8729, 4.5324, 4.2242, 3.9392,\n",
      "        3.6710, 3.4151, 3.1681, 2.9278, 2.6925, 2.4608, 2.2318, 2.0048, 1.7794,\n",
      "        1.5551, 1.3317, 1.1089, 0.8865, 0.6646, 0.4429, 0.2214]) return=  109925.7013290539\n",
      "probs of actions:  tensor([0.9995, 0.9993, 0.9992, 0.9996, 0.9991, 0.9995, 0.9990, 0.9993, 0.9991,\n",
      "        0.9991, 0.9994, 0.9994, 0.9993, 0.9993, 0.9993, 0.9994, 0.9990, 0.9990,\n",
      "        0.9997, 1.0000, 0.9999, 1.0000, 1.0000, 0.9760, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4671, 0.5316, 0.5021, 0.4805, 0.4646, 0.4528, 0.4441, 0.4376, 0.4327,\n",
      "        0.4291, 0.4264, 0.4244, 0.4228, 0.4217, 0.4208, 0.4202, 0.4197, 0.4194,\n",
      "        0.4191, 0.4189, 0.4188, 0.4186, 0.4186, 0.4185, 0.4625])\n",
      "finalReturns:  tensor([3.7373, 3.7814, 3.7523, 3.6737, 3.5614, 3.4255, 3.2730, 3.1085, 2.9353,\n",
      "        2.7559, 2.5718, 2.3844, 2.1944, 2.0025, 1.8093, 1.6150, 1.4199, 1.2242,\n",
      "        1.0281, 0.8317, 0.6350, 0.4382, 0.2412])\n",
      "----------------------------------------\n",
      "iter  0  stage  1  ep  0   adversary:  AdversaryModes.fight_125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actions:  tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21,  0])\n",
      "loss=  tensor(0.0484, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.5768, 6.5768, 6.0011, 5.5297, 5.1298, 4.7795, 4.4644, 4.1744, 3.9025,\n",
      "        3.6439, 3.3950, 3.1533, 2.9168, 2.6843, 2.4547, 2.2273, 2.0016, 1.7770,\n",
      "        1.5534, 1.3304, 1.1080, 0.8860, 0.6642, 0.4426, 0.2213]) return=  109925.7013290539\n",
      "probs of actions:  tensor([0.9995, 0.9993, 0.9992, 0.9996, 0.9991, 0.9995, 0.9990, 0.9993, 0.9991,\n",
      "        0.9991, 0.9994, 0.9994, 0.9993, 0.9993, 0.9993, 0.9994, 0.9990, 0.9990,\n",
      "        0.9997, 1.0000, 0.9999, 1.0000, 1.0000, 0.9760, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4671, 0.5316, 0.5021, 0.4805, 0.4646, 0.4528, 0.4441, 0.4376, 0.4327,\n",
      "        0.4291, 0.4264, 0.4244, 0.4228, 0.4217, 0.4208, 0.4202, 0.4197, 0.4194,\n",
      "        0.4191, 0.4189, 0.4188, 0.4186, 0.4186, 0.4185, 0.4625])\n",
      "finalReturns:  tensor([3.9487, 3.9928, 3.9620, 3.8815, 3.7671, 3.6294, 3.4754, 3.3097, 3.1356,\n",
      "        2.9554, 2.7708, 2.5829, 2.3926, 2.2004, 2.0070, 1.8125, 1.6174, 1.4216,\n",
      "        1.2254, 1.0290, 0.8323, 0.6354, 0.4384, 0.2413])\n",
      "----------------------------------------\n",
      "iter  0  stage  0  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21,  0])\n",
      "loss=  tensor(0.0513, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.8278, 6.3166, 5.8178, 5.3989, 5.0355, 4.7109, 4.4142, 4.1374, 3.8753,\n",
      "        3.6237, 3.3800, 3.1421, 2.9086, 2.6782, 2.4502, 2.2240, 1.9991, 1.7752,\n",
      "        1.5521, 1.3295, 1.1074, 0.8855, 0.6639, 0.4425, 0.2212]) return=  109925.7013290539\n",
      "probs of actions:  tensor([0.9995, 0.9993, 0.9992, 0.9996, 0.9991, 0.9995, 0.9990, 0.9993, 0.9991,\n",
      "        0.9991, 0.9994, 0.9994, 0.9993, 0.9993, 0.9993, 0.9994, 0.9990, 0.9990,\n",
      "        0.9997, 1.0000, 0.9999, 1.0000, 1.0000, 0.9759, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4671, 0.5316, 0.5021, 0.4805, 0.4646, 0.4528, 0.4441, 0.4376, 0.4327,\n",
      "        0.4291, 0.4264, 0.4244, 0.4228, 0.4217, 0.4208, 0.4202, 0.4197, 0.4194,\n",
      "        0.4191, 0.4189, 0.4188, 0.4186, 0.4186, 0.4185, 0.4625])\n",
      "finalReturns:  tensor([4.1648, 4.2089, 4.1760, 4.0928, 3.9758, 3.8357, 3.6796, 3.5123, 3.3370,\n",
      "        3.1558, 2.9704, 2.7819, 2.5911, 2.3987, 2.2050, 2.0103, 1.8150, 1.6191,\n",
      "        1.4229, 1.2264, 1.0296, 0.8327, 0.6357, 0.4385, 0.2413])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1683106228 saved\n",
      "[652587, 'tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])', 109925.7013290539, 84773.34866887607, 0.051301419734954834, 1e-05, 1, 0, 'tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\\n        21, 21, 21, 21, 21, 21,  0])', '[1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\\n 1.   1.   1.   1.   1.   1.   1.   1.   1.   0.98 1.  ]', '0,[1e-05,1][1, 10000, 1, 1],1683106228', 25, 50, 167645.58347602683, 202386.15908071544, 82634.78314716663, 134766.63466666665, 131821.99466666667, 92498.06441395788, 92568.85885052304, 109925.70132905389, 109925.70132905389, 95781.68158887929, 92498.7981199441, 109927.0996773476]\n",
      "policy reset\n",
      "----------------------------------------\n",
      "iter  1  stage  24  ep  99999   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.4633, 0.4633, 0.4633, 0.4633, 0.4633, 0.4633, 0.4633, 0.4633, 0.4633,\n",
      "        0.4633, 0.4633, 0.4633, 0.4633, 0.4633, 0.4633, 0.4633, 0.4633, 0.4633,\n",
      "        0.4633, 0.4633, 0.4633, 0.4633, 0.4633, 0.4633, 0.4633]) return=  117778.10729910324\n",
      "probs of actions:  tensor([0.9298, 0.9438, 0.0328, 0.9247, 0.9325, 0.9293, 0.9304, 0.9386, 0.9360,\n",
      "        0.9318, 0.8983, 0.9275, 0.9239, 0.9333, 0.9266, 0.9286, 0.9126, 0.9336,\n",
      "        0.0313, 0.8994, 0.9284, 0.9285, 0.9281, 0.9292, 0.9939],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.4988, 0.4895, 0.4862, 0.4802, 0.4757, 0.4724, 0.4699, 0.4680,\n",
      "        0.4666, 0.4655, 0.4648, 0.4642, 0.4637, 0.4634, 0.4631, 0.4630, 0.4628,\n",
      "        0.4626, 0.4660, 0.4651, 0.4644, 0.4639, 0.4636, 0.4633])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "iter  1  stage  23  ep  99999   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([17,  0,  0,  8,  1, 17,  9,  0,  6,  7,  9, 13,  3,  8, 10, 10,  8, 10,\n",
      "        10,  8, 29,  8,  8, 10,  0])\n",
      "loss=  tensor(0.0178, grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.8227, 0.8227, 0.8227, 0.8227, 0.8227, 0.8227, 0.8227, 0.8227, 0.8227,\n",
      "        0.8227, 0.8227, 0.8227, 0.8227, 0.8227, 0.8227, 0.8227, 0.8227, 0.8227,\n",
      "        0.8227, 0.8227, 0.8227, 0.8227, 0.8227, 0.8227, 0.3931]) return=  110886.5992611681\n",
      "probs of actions:  tensor([6.7410e-03, 3.9277e-01, 2.1554e-01, 1.2243e-01, 2.4300e-02, 4.6671e-03,\n",
      "        1.0503e-01, 1.7088e-01, 4.1501e-02, 6.3581e-02, 9.4326e-02, 3.5382e-02,\n",
      "        1.1196e-02, 1.0751e-01, 1.9669e-01, 2.2438e-01, 1.1883e-01, 2.5325e-01,\n",
      "        2.4511e-01, 1.2646e-01, 7.9849e-05, 1.2561e-01, 1.2209e-01, 4.4844e-01,\n",
      "        9.9323e-01], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4823, 0.5606, 0.5352, 0.5101, 0.5313, 0.4884, 0.5573, 0.5210, 0.4544,\n",
      "        0.4280, 0.4096, 0.3961, 0.4214, 0.3908, 0.3845, 0.3887, 0.3955, 0.3879,\n",
      "        0.3913, 0.3975, 0.3153, 0.4585, 0.4382, 0.4196, 0.4251])\n",
      "finalReturns:  tensor([0.0220, 0.0320])\n",
      "----------------------------------------\n",
      "iter  1  stage  22  ep  99999   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([12, 15, 15, 15, 15, 12, 15, 15, 12, 13, 15, 15, 15, 15, 13, 15, 15, 15,\n",
      "        15, 15, 15, 13, 15, 13,  0])\n",
      "loss=  tensor(0.2627, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.3032, 1.3032, 1.3032, 1.3032, 1.3032, 1.3032, 1.3032, 1.3032, 1.3032,\n",
      "        1.3032, 1.3032, 1.3032, 1.3032, 1.3032, 1.3032, 1.3032, 1.3032, 1.3032,\n",
      "        1.3032, 1.3032, 1.3032, 1.3032, 1.3032, 0.8249, 0.3951]) return=  119446.69263091798\n",
      "probs of actions:  tensor([0.0268, 0.6716, 0.6783, 0.6497, 0.7063, 0.0258, 0.6930, 0.7128, 0.0190,\n",
      "        0.1823, 0.6454, 0.6303, 0.6423, 0.5961, 0.1839, 0.6566, 0.6457, 0.6843,\n",
      "        0.7225, 0.6714, 0.6469, 0.2010, 0.8696, 0.1107, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4968, 0.5196, 0.5546, 0.5312, 0.5140, 0.5094, 0.4811, 0.4768, 0.4817,\n",
      "        0.4663, 0.4546, 0.4569, 0.4587, 0.4600, 0.4666, 0.4548, 0.4571, 0.4588,\n",
      "        0.4601, 0.4611, 0.4618, 0.4680, 0.4558, 0.4635, 0.4750])\n",
      "finalReturns:  tensor([0.0911, 0.1136, 0.0799])\n",
      "----------------------------------------\n",
      "iter  1  stage  21  ep  99999   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0023, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.6186, 1.6186, 1.6186, 1.6186, 1.6186, 1.6186, 1.6186, 1.6186, 1.6186,\n",
      "        1.6186, 1.6186, 1.6186, 1.6186, 1.6186, 1.6186, 1.6186, 1.6186, 1.6186,\n",
      "        1.6186, 1.6186, 1.6186, 1.6186, 1.1523, 0.7359, 0.3551]) return=  117051.59338141434\n",
      "probs of actions:  tensor([0.9903, 0.9901, 0.9906, 0.9895, 0.9921, 0.9890, 0.9906, 0.9925, 0.9923,\n",
      "        0.9895, 0.9863, 0.9882, 0.9871, 0.9857, 0.9883, 0.9888, 0.9872, 0.9912,\n",
      "        0.9917, 0.9882, 0.9884, 0.9916, 0.9985, 0.9974, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5307, 0.5632, 0.5319, 0.5091, 0.4922, 0.4798, 0.4705, 0.4636,\n",
      "        0.4585, 0.4547, 0.4518, 0.4497, 0.4481, 0.4469, 0.4460, 0.4453, 0.4448,\n",
      "        0.4444, 0.4442, 0.4439, 0.4438, 0.4437, 0.4436, 0.4660])\n",
      "finalReturns:  tensor([0.1785, 0.2010, 0.1737, 0.1109])\n",
      "----------------------------------------\n",
      "iter  1  stage  20  ep  40198   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([15, 13, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0006, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.3391, 2.3391, 2.3391, 2.3391, 2.3391, 2.3391, 2.3391, 2.3391, 2.3391,\n",
      "        2.3391, 2.3391, 2.3391, 2.3391, 2.3391, 2.3391, 2.3391, 2.3391, 2.3391,\n",
      "        2.3391, 2.3391, 2.3391, 1.7892, 1.2936, 0.8369, 0.4082]) return=  132803.10474323918\n",
      "probs of actions:  tensor([0.9989, 0.0011, 0.9989, 0.9988, 0.9992, 0.9988, 0.9990, 0.9993, 0.9992,\n",
      "        0.9989, 0.9986, 0.9988, 0.9986, 0.9985, 0.9988, 0.9989, 0.9986, 0.9992,\n",
      "        0.9991, 0.9987, 0.9991, 0.9995, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5363, 0.5556, 0.5413, 0.5436, 0.5394, 0.5364, 0.5341, 0.5323,\n",
      "        0.5310, 0.5301, 0.5294, 0.5288, 0.5284, 0.5281, 0.5279, 0.5277, 0.5276,\n",
      "        0.5275, 0.5274, 0.5273, 0.5273, 0.5273, 0.5273, 0.5497])\n",
      "finalReturns:  tensor([0.3198, 0.3423, 0.3106, 0.2401, 0.1415])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  19  ep  392   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0011, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.2790, 2.2790, 2.2790, 2.2790, 2.2790, 2.2790, 2.2790, 2.2790, 2.2790,\n",
      "        2.2790, 2.2790, 2.2790, 2.2790, 2.2790, 2.2790, 2.2790, 2.2790, 2.2790,\n",
      "        2.2790, 2.2790, 1.8123, 1.3957, 1.0148, 0.6595, 0.3229]) return=  117051.59338141434\n",
      "probs of actions:  tensor([0.9990, 0.9990, 0.9990, 0.9989, 0.9992, 0.9988, 0.9990, 0.9992, 0.9992,\n",
      "        0.9989, 0.9985, 0.9987, 0.9985, 0.9984, 0.9987, 0.9988, 0.9985, 0.9991,\n",
      "        0.9991, 0.9990, 0.9990, 0.9994, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5307, 0.5632, 0.5319, 0.5091, 0.4922, 0.4798, 0.4705, 0.4636,\n",
      "        0.4585, 0.4547, 0.4518, 0.4497, 0.4481, 0.4469, 0.4460, 0.4453, 0.4448,\n",
      "        0.4444, 0.4442, 0.4439, 0.4438, 0.4437, 0.4436, 0.4660])\n",
      "finalReturns:  tensor([0.4061, 0.4286, 0.4013, 0.3385, 0.2501, 0.1431])\n",
      "----------------------------------------\n",
      "iter  1  stage  18  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0019, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.5927, 2.5927, 2.5927, 2.5927, 2.5927, 2.5927, 2.5927, 2.5927, 2.5927,\n",
      "        2.5927, 2.5927, 2.5927, 2.5927, 2.5927, 2.5927, 2.5927, 2.5927, 2.5927,\n",
      "        2.5927, 2.1258, 1.7090, 1.3279, 0.9725, 0.6358, 0.3129]) return=  117051.59338141434\n",
      "probs of actions:  tensor([0.9990, 0.9990, 0.9990, 0.9989, 0.9992, 0.9988, 0.9990, 0.9992, 0.9992,\n",
      "        0.9989, 0.9985, 0.9987, 0.9985, 0.9984, 0.9987, 0.9988, 0.9985, 0.9991,\n",
      "        0.9991, 0.9990, 0.9990, 0.9994, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5307, 0.5632, 0.5319, 0.5091, 0.4922, 0.4798, 0.4705, 0.4636,\n",
      "        0.4585, 0.4547, 0.4518, 0.4497, 0.4481, 0.4469, 0.4460, 0.4453, 0.4448,\n",
      "        0.4444, 0.4442, 0.4439, 0.4438, 0.4437, 0.4436, 0.4660])\n",
      "finalReturns:  tensor([0.5368, 0.5593, 0.5320, 0.4692, 0.3808, 0.2738, 0.1531])\n",
      "----------------------------------------\n",
      "iter  1  stage  17  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0029, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.8994, 2.8994, 2.8994, 2.8994, 2.8994, 2.8994, 2.8994, 2.8994, 2.8994,\n",
      "        2.8994, 2.8994, 2.8994, 2.8994, 2.8994, 2.8994, 2.8994, 2.8994, 2.8994,\n",
      "        2.4321, 2.0150, 1.6337, 1.2782, 0.9414, 0.6184, 0.3054]) return=  117051.59338141434\n",
      "probs of actions:  tensor([0.9990, 0.9990, 0.9990, 0.9989, 0.9992, 0.9988, 0.9990, 0.9992, 0.9992,\n",
      "        0.9989, 0.9985, 0.9987, 0.9985, 0.9984, 0.9987, 0.9988, 0.9985, 0.9991,\n",
      "        0.9991, 0.9990, 0.9990, 0.9994, 0.9999, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5307, 0.5632, 0.5319, 0.5091, 0.4922, 0.4798, 0.4705, 0.4636,\n",
      "        0.4585, 0.4547, 0.4518, 0.4497, 0.4481, 0.4469, 0.4460, 0.4453, 0.4448,\n",
      "        0.4444, 0.4442, 0.4439, 0.4438, 0.4437, 0.4436, 0.4660])\n",
      "finalReturns:  tensor([0.6750, 0.6975, 0.6702, 0.6073, 0.5189, 0.4119, 0.2912, 0.1606])\n",
      "----------------------------------------\n",
      "iter  1  stage  16  ep  4983   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0028, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.2010, 3.2010, 3.2010, 3.2010, 3.2010, 3.2010, 3.2010, 3.2010, 3.2010,\n",
      "        3.2010, 3.2010, 3.2010, 3.2010, 3.2010, 3.2010, 3.2010, 3.2010, 2.7332,\n",
      "        2.3157, 1.9342, 1.5785, 1.2416, 0.9184, 0.6054, 0.2999]) return=  117051.59338141434\n",
      "probs of actions:  tensor([0.9992, 0.9992, 0.9992, 0.9991, 0.9994, 0.9991, 0.9992, 0.9994, 0.9994,\n",
      "        0.9991, 0.9988, 0.9990, 0.9988, 0.9988, 0.9990, 0.9991, 0.9990, 0.9998,\n",
      "        0.9994, 0.9993, 0.9993, 0.9996, 1.0000, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5307, 0.5632, 0.5319, 0.5091, 0.4922, 0.4798, 0.4705, 0.4636,\n",
      "        0.4585, 0.4547, 0.4518, 0.4497, 0.4481, 0.4469, 0.4460, 0.4453, 0.4448,\n",
      "        0.4444, 0.4442, 0.4439, 0.4438, 0.4437, 0.4436, 0.4660])\n",
      "finalReturns:  tensor([0.8187, 0.8412, 0.8139, 0.7510, 0.6625, 0.5555, 0.4348, 0.3041, 0.1661])\n",
      "----------------------------------------\n",
      "iter  1  stage  15  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0041, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.4991, 3.4991, 3.4991, 3.4991, 3.4991, 3.4991, 3.4991, 3.4991, 3.4991,\n",
      "        3.4991, 3.4991, 3.4991, 3.4991, 3.4991, 3.4991, 3.4991, 3.0306, 2.6127,\n",
      "        2.2308, 1.8748, 1.5378, 1.2145, 0.9014, 0.5958, 0.2958]) return=  117051.59338141434\n",
      "probs of actions:  tensor([0.9992, 0.9992, 0.9992, 0.9991, 0.9994, 0.9991, 0.9992, 0.9994, 0.9994,\n",
      "        0.9991, 0.9988, 0.9990, 0.9988, 0.9988, 0.9990, 0.9991, 0.9990, 0.9998,\n",
      "        0.9994, 0.9993, 0.9993, 0.9996, 1.0000, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5307, 0.5632, 0.5319, 0.5091, 0.4922, 0.4798, 0.4705, 0.4636,\n",
      "        0.4585, 0.4547, 0.4518, 0.4497, 0.4481, 0.4469, 0.4460, 0.4453, 0.4448,\n",
      "        0.4444, 0.4442, 0.4439, 0.4438, 0.4437, 0.4436, 0.4660])\n",
      "finalReturns:  tensor([0.9666, 0.9891, 0.9617, 0.8988, 0.8103, 0.7032, 0.5825, 0.4518, 0.3138,\n",
      "        0.1702])\n",
      "----------------------------------------\n",
      "iter  1  stage  14  ep  20   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0055, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.7950, 3.7950, 3.7950, 3.7950, 3.7950, 3.7950, 3.7950, 3.7950, 3.7950,\n",
      "        3.7950, 3.7950, 3.7950, 3.7950, 3.7950, 3.7950, 3.3256, 2.9070, 2.5247,\n",
      "        2.1684, 1.8311, 1.5076, 1.1944, 0.8887, 0.5887, 0.2928]) return=  117051.59338141434\n",
      "probs of actions:  tensor([0.9992, 0.9992, 0.9992, 0.9991, 0.9994, 0.9991, 0.9992, 0.9994, 0.9994,\n",
      "        0.9991, 0.9988, 0.9990, 0.9988, 0.9988, 0.9990, 0.9991, 0.9990, 0.9998,\n",
      "        0.9994, 0.9993, 0.9993, 0.9996, 1.0000, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5307, 0.5632, 0.5319, 0.5091, 0.4922, 0.4798, 0.4705, 0.4636,\n",
      "        0.4585, 0.4547, 0.4518, 0.4497, 0.4481, 0.4469, 0.4460, 0.4453, 0.4448,\n",
      "        0.4444, 0.4442, 0.4439, 0.4438, 0.4437, 0.4436, 0.4660])\n",
      "finalReturns:  tensor([1.1176, 1.1401, 1.1127, 1.0497, 0.9612, 0.8541, 0.7333, 0.6026, 0.4645,\n",
      "        0.3209, 0.1732])\n",
      "----------------------------------------\n",
      "iter  1  stage  13  ep  207   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0071, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.0897, 4.0897, 4.0897, 4.0897, 4.0897, 4.0897, 4.0897, 4.0897, 4.0897,\n",
      "        4.0897, 4.0897, 4.0897, 4.0897, 4.0897, 3.6191, 3.1997, 2.8167, 2.4600,\n",
      "        2.1223, 1.7987, 1.4853, 1.1795, 0.8793, 0.5833, 0.2905]) return=  117051.59338141434\n",
      "probs of actions:  tensor([0.9992, 0.9992, 0.9992, 0.9991, 0.9993, 0.9990, 0.9992, 0.9994, 0.9994,\n",
      "        0.9991, 0.9988, 0.9990, 0.9988, 0.9990, 0.9990, 0.9992, 0.9990, 0.9998,\n",
      "        0.9994, 0.9993, 0.9993, 0.9995, 1.0000, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5307, 0.5632, 0.5319, 0.5091, 0.4922, 0.4798, 0.4705, 0.4636,\n",
      "        0.4585, 0.4547, 0.4518, 0.4497, 0.4481, 0.4469, 0.4460, 0.4453, 0.4448,\n",
      "        0.4444, 0.4442, 0.4439, 0.4438, 0.4437, 0.4436, 0.4660])\n",
      "finalReturns:  tensor([1.2710, 1.2935, 1.2661, 1.2030, 1.1144, 1.0072, 0.8865, 0.7557, 0.6176,\n",
      "        0.4739, 0.3262, 0.1755])\n",
      "----------------------------------------\n",
      "iter  1  stage  12  ep  66   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([15, 13, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 13, 15, 15, 15,  0])\n",
      "loss=  tensor(6.1737, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.2969, 5.2969, 5.2969, 5.2969, 5.2969, 5.2969, 5.2969, 5.2969, 5.2969,\n",
      "        5.2969, 5.2969, 5.2969, 5.2969, 4.7456, 4.2490, 3.7915, 3.3623, 2.9537,\n",
      "        2.5602, 2.1779, 1.8039, 1.4360, 1.0727, 0.7128, 0.3554]) return=  132656.87375173956\n",
      "probs of actions:  tensor([9.9921e-01, 8.1956e-04, 9.9919e-01, 9.9913e-01, 9.9937e-01, 9.9911e-01,\n",
      "        9.9927e-01, 9.9944e-01, 9.9943e-01, 9.9919e-01, 9.9892e-01, 9.9912e-01,\n",
      "        9.9905e-01, 9.9916e-01, 9.9913e-01, 9.9935e-01, 9.9912e-01, 9.9982e-01,\n",
      "        9.9947e-01, 9.9936e-01, 6.5138e-04, 9.9961e-01, 9.9997e-01, 9.9984e-01,\n",
      "        1.0000e+00], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5363, 0.5556, 0.5413, 0.5436, 0.5394, 0.5364, 0.5341, 0.5323,\n",
      "        0.5310, 0.5301, 0.5294, 0.5288, 0.5284, 0.5281, 0.5279, 0.5277, 0.5276,\n",
      "        0.5275, 0.5274, 0.5329, 0.5199, 0.5217, 0.5231, 0.5466])\n",
      "finalReturns:  tensor([1.5707, 1.5932, 1.5614, 1.4908, 1.3922, 1.2731, 1.1390, 0.9938, 0.8404,\n",
      "        0.6753, 0.5187, 0.3569, 0.1912])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "iter  1  stage  11  ep  19   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0115, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.6792, 4.6792, 4.6792, 4.6792, 4.6792, 4.6792, 4.6792, 4.6792, 4.6792,\n",
      "        4.6792, 4.6792, 4.6792, 4.2048, 3.7828, 3.3979, 3.0398, 2.7012, 2.3768,\n",
      "        2.0628, 1.7566, 1.4562, 1.1600, 0.8670, 0.5764, 0.2875]) return=  117051.59338141434\n",
      "probs of actions:  tensor([0.9992, 0.9992, 0.9992, 0.9991, 0.9993, 0.9990, 0.9992, 0.9994, 0.9993,\n",
      "        0.9991, 0.9987, 0.9990, 0.9989, 0.9990, 0.9990, 0.9992, 0.9990, 0.9998,\n",
      "        0.9994, 0.9992, 0.9992, 0.9995, 1.0000, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5307, 0.5632, 0.5319, 0.5091, 0.4922, 0.4798, 0.4705, 0.4636,\n",
      "        0.4585, 0.4547, 0.4518, 0.4497, 0.4481, 0.4469, 0.4460, 0.4453, 0.4448,\n",
      "        0.4444, 0.4442, 0.4439, 0.4438, 0.4437, 0.4436, 0.4660])\n",
      "finalReturns:  tensor([1.5830, 1.6055, 1.5779, 1.5147, 1.4259, 1.3185, 1.1976, 1.0667, 0.9285,\n",
      "        0.7848, 0.6370, 0.4862, 0.3332, 0.1785])\n",
      "----------------------------------------\n",
      "iter  1  stage  10  ep  178   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0126, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.9759, 4.9759, 4.9759, 4.9759, 4.9759, 4.9759, 4.9759, 4.9759, 4.9759,\n",
      "        4.9759, 4.9759, 4.4987, 4.0746, 3.6883, 3.3292, 2.9898, 2.6648, 2.3505,\n",
      "        2.0440, 1.7433, 1.4469, 1.1538, 0.8631, 0.5741, 0.2866]) return=  117051.59338141434\n",
      "probs of actions:  tensor([0.9992, 0.9992, 0.9992, 0.9991, 0.9994, 0.9991, 0.9992, 0.9994, 0.9994,\n",
      "        0.9991, 0.9990, 0.9992, 0.9991, 0.9992, 0.9990, 0.9993, 0.9990, 0.9998,\n",
      "        0.9994, 0.9993, 0.9992, 0.9996, 1.0000, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5307, 0.5632, 0.5319, 0.5091, 0.4922, 0.4798, 0.4705, 0.4636,\n",
      "        0.4585, 0.4547, 0.4518, 0.4497, 0.4481, 0.4469, 0.4460, 0.4453, 0.4448,\n",
      "        0.4444, 0.4442, 0.4439, 0.4438, 0.4437, 0.4436, 0.4660])\n",
      "finalReturns:  tensor([1.7410, 1.7635, 1.7358, 1.6724, 1.5834, 1.4759, 1.3549, 1.2239, 1.0856,\n",
      "        0.9419, 0.7941, 0.6432, 0.4902, 0.3354, 0.1794])\n",
      "----------------------------------------\n",
      "iter  1  stage  9  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0149, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.2754, 5.2754, 5.2754, 5.2754, 5.2754, 5.2754, 5.2754, 5.2754, 5.2754,\n",
      "        5.2754, 4.7944, 4.3676, 3.9793, 3.6188, 3.2784, 2.9527, 2.6378, 2.3308,\n",
      "        2.0299, 1.7333, 1.4400, 1.1491, 0.8601, 0.5725, 0.2859]) return=  117051.59338141434\n",
      "probs of actions:  tensor([0.9992, 0.9992, 0.9992, 0.9991, 0.9994, 0.9991, 0.9992, 0.9994, 0.9994,\n",
      "        0.9991, 0.9990, 0.9992, 0.9991, 0.9992, 0.9990, 0.9993, 0.9990, 0.9998,\n",
      "        0.9994, 0.9993, 0.9992, 0.9996, 1.0000, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5307, 0.5632, 0.5319, 0.5091, 0.4922, 0.4798, 0.4705, 0.4636,\n",
      "        0.4585, 0.4547, 0.4518, 0.4497, 0.4481, 0.4469, 0.4460, 0.4453, 0.4448,\n",
      "        0.4444, 0.4442, 0.4439, 0.4438, 0.4437, 0.4436, 0.4660])\n",
      "finalReturns:  tensor([1.9000, 1.9225, 1.8946, 1.8310, 1.7419, 1.6342, 1.5130, 1.3820, 1.2435,\n",
      "        1.0997, 0.9519, 0.8010, 0.6479, 0.4931, 0.3371, 0.1802])\n",
      "----------------------------------------\n",
      "iter  1  stage  8  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0167, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.5790, 5.5790, 5.5790, 5.5790, 5.5790, 5.5790, 5.5790, 5.5790, 5.5790,\n",
      "        5.0929, 4.6624, 4.2716, 3.9092, 3.5674, 3.2407, 2.9251, 2.6176, 2.3162,\n",
      "        2.0193, 1.7258, 1.4348, 1.1457, 0.8579, 0.5712, 0.2853]) return=  117051.59338141434\n",
      "probs of actions:  tensor([0.9992, 0.9992, 0.9992, 0.9991, 0.9994, 0.9991, 0.9992, 0.9994, 0.9994,\n",
      "        0.9991, 0.9990, 0.9992, 0.9991, 0.9992, 0.9990, 0.9993, 0.9990, 0.9998,\n",
      "        0.9994, 0.9993, 0.9992, 0.9996, 1.0000, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5307, 0.5632, 0.5319, 0.5091, 0.4922, 0.4798, 0.4705, 0.4636,\n",
      "        0.4585, 0.4547, 0.4518, 0.4497, 0.4481, 0.4469, 0.4460, 0.4453, 0.4448,\n",
      "        0.4444, 0.4442, 0.4439, 0.4438, 0.4437, 0.4436, 0.4660])\n",
      "finalReturns:  tensor([2.0601, 2.0826, 2.0545, 1.9906, 1.9012, 1.7933, 1.6719, 1.5406, 1.4021,\n",
      "        1.2582, 1.1102, 0.9593, 0.8062, 0.6514, 0.4953, 0.3384, 0.1807])\n",
      "----------------------------------------\n",
      "iter  1  stage  7  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 13, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(9.7105, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.8884, 5.8884, 5.8884, 5.8884, 5.8884, 5.8884, 5.8884, 5.8884, 5.3954,\n",
      "        4.9602, 4.5659, 4.2010, 3.8574, 3.5293, 3.2127, 2.9045, 2.6026, 2.3053,\n",
      "        2.0115, 1.7202, 1.4309, 1.1430, 0.8563, 0.5703, 0.2849]) return=  116862.23949259547\n",
      "probs of actions:  tensor([9.9923e-01, 9.9920e-01, 9.9920e-01, 9.9914e-01, 9.9936e-01, 9.9906e-01,\n",
      "        9.9921e-01, 9.9939e-01, 9.9938e-01, 9.9910e-01, 9.9900e-01, 9.9920e-01,\n",
      "        9.9909e-01, 9.9916e-01, 9.9903e-01, 9.9927e-01, 9.7531e-04, 9.9980e-01,\n",
      "        9.9940e-01, 9.9928e-01, 9.9924e-01, 9.9955e-01, 9.9997e-01, 9.9983e-01,\n",
      "        1.0000e+00], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5307, 0.5632, 0.5319, 0.5091, 0.4922, 0.4798, 0.4705, 0.4636,\n",
      "        0.4585, 0.4547, 0.4518, 0.4497, 0.4481, 0.4469, 0.4460, 0.4509, 0.4380,\n",
      "        0.4393, 0.4403, 0.4411, 0.4416, 0.4420, 0.4424, 0.4651])\n",
      "finalReturns:  tensor([2.2022, 2.2247, 2.1963, 2.1321, 2.0423, 1.9341, 1.8124, 1.6810, 1.5423,\n",
      "        1.3982, 1.2446, 1.1004, 0.9523, 0.8013, 0.6481, 0.4932, 0.3372, 0.1802])\n",
      "----------------------------------------\n",
      "iter  1  stage  6  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0211, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.2060, 6.2060, 6.2060, 6.2060, 6.2060, 6.2060, 6.2060, 5.7037, 5.2620,\n",
      "        4.8630, 4.4948, 4.1487, 3.8189, 3.5010, 3.1918, 2.8891, 2.5913, 2.2971,\n",
      "        2.0056, 1.7160, 1.4280, 1.1411, 0.8550, 0.5696, 0.2846]) return=  117051.59338141434\n",
      "probs of actions:  tensor([0.9992, 0.9992, 0.9992, 0.9991, 0.9994, 0.9991, 0.9992, 0.9994, 0.9994,\n",
      "        0.9991, 0.9990, 0.9992, 0.9991, 0.9992, 0.9990, 0.9993, 0.9990, 0.9998,\n",
      "        0.9994, 0.9993, 0.9992, 0.9996, 1.0000, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5307, 0.5632, 0.5319, 0.5091, 0.4922, 0.4798, 0.4705, 0.4636,\n",
      "        0.4585, 0.4547, 0.4518, 0.4497, 0.4481, 0.4469, 0.4460, 0.4453, 0.4448,\n",
      "        0.4444, 0.4442, 0.4439, 0.4438, 0.4437, 0.4436, 0.4660])\n",
      "finalReturns:  tensor([2.3834, 2.4059, 2.3771, 2.3124, 2.2221, 2.1135, 1.9915, 1.8597, 1.7208,\n",
      "        1.5766, 1.4284, 1.2773, 1.1240, 0.9691, 0.8130, 0.6559, 0.4982, 0.3400,\n",
      "        0.1814])\n",
      "----------------------------------------\n",
      "iter  1  stage  5  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0240, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.5346, 6.5346, 6.5346, 6.5346, 6.5346, 6.5346, 6.0199, 5.5694, 5.1642,\n",
      "        4.7915, 4.4422, 4.1100, 3.7903, 3.4798, 3.1762, 2.8777, 2.5829, 2.2910,\n",
      "        2.0012, 1.7129, 1.4258, 1.1396, 0.8541, 0.5691, 0.2844]) return=  117051.59338141434\n",
      "probs of actions:  tensor([0.9992, 0.9992, 0.9992, 0.9991, 0.9994, 0.9991, 0.9992, 0.9994, 0.9994,\n",
      "        0.9991, 0.9990, 0.9992, 0.9991, 0.9992, 0.9990, 0.9993, 0.9990, 0.9998,\n",
      "        0.9994, 0.9993, 0.9992, 0.9996, 1.0000, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5307, 0.5632, 0.5319, 0.5091, 0.4922, 0.4798, 0.4705, 0.4636,\n",
      "        0.4585, 0.4547, 0.4518, 0.4497, 0.4481, 0.4469, 0.4460, 0.4453, 0.4448,\n",
      "        0.4444, 0.4442, 0.4439, 0.4438, 0.4437, 0.4436, 0.4660])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalReturns:  tensor([2.5469, 2.5694, 2.5402, 2.4748, 2.3839, 2.2747, 2.1522, 2.0201, 1.8809,\n",
      "        1.7364, 1.5880, 1.4368, 1.2834, 1.1284, 0.9722, 0.8152, 0.6574, 0.4992,\n",
      "        0.3405, 0.1816])\n",
      "----------------------------------------\n",
      "iter  1  stage  4  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([15, 13, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0261, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.1639, 8.1639, 8.1639, 8.1639, 8.1639, 7.5979, 7.0908, 6.6257, 6.1910,\n",
      "        5.7784, 5.3820, 4.9976, 4.6219, 4.2528, 3.8886, 3.5281, 3.1702, 2.8144,\n",
      "        2.4601, 2.1069, 1.7546, 1.4030, 1.0518, 0.7009, 0.3504]) return=  132803.10474323918\n",
      "probs of actions:  tensor([9.9923e-01, 8.0644e-04, 9.9920e-01, 9.9914e-01, 9.9938e-01, 9.9912e-01,\n",
      "        9.9928e-01, 9.9945e-01, 9.9944e-01, 9.9920e-01, 9.9912e-01, 9.9930e-01,\n",
      "        9.9920e-01, 9.9926e-01, 9.9915e-01, 9.9936e-01, 9.9914e-01, 9.9982e-01,\n",
      "        9.9948e-01, 9.9937e-01, 9.9934e-01, 9.9961e-01, 9.9997e-01, 9.9985e-01,\n",
      "        1.0000e+00], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5363, 0.5556, 0.5413, 0.5436, 0.5394, 0.5364, 0.5341, 0.5323,\n",
      "        0.5310, 0.5301, 0.5294, 0.5288, 0.5284, 0.5281, 0.5279, 0.5277, 0.5276,\n",
      "        0.5275, 0.5274, 0.5273, 0.5273, 0.5273, 0.5273, 0.5497])\n",
      "finalReturns:  tensor([2.9945, 3.0170, 2.9847, 2.9134, 2.8140, 2.6943, 2.5596, 2.4140, 2.2603,\n",
      "        2.1006, 1.9364, 1.7689, 1.5988, 1.4270, 1.2537, 1.0794, 0.9043, 0.7286,\n",
      "        0.5525, 0.3761, 0.1994])\n",
      "----------------------------------------\n",
      "iter  1  stage  3  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0295, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.2430, 7.2430, 7.2430, 7.2430, 6.6886, 6.2103, 5.7855, 5.3986, 5.0391,\n",
      "        4.6994, 4.3742, 4.0597, 3.7530, 3.4523, 3.1559, 2.8627, 2.5719, 2.2830,\n",
      "        1.9954, 1.7088, 1.4230, 1.1377, 0.8529, 0.5684, 0.2841]) return=  117051.59338141434\n",
      "probs of actions:  tensor([0.9992, 0.9992, 0.9992, 0.9991, 0.9993, 0.9991, 0.9992, 0.9994, 0.9994,\n",
      "        0.9991, 0.9990, 0.9992, 0.9991, 0.9992, 0.9990, 0.9993, 0.9990, 0.9998,\n",
      "        0.9994, 0.9993, 0.9992, 0.9995, 1.0000, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5307, 0.5632, 0.5319, 0.5091, 0.4922, 0.4798, 0.4705, 0.4636,\n",
      "        0.4585, 0.4547, 0.4518, 0.4497, 0.4481, 0.4469, 0.4460, 0.4453, 0.4448,\n",
      "        0.4444, 0.4442, 0.4439, 0.4438, 0.4437, 0.4436, 0.4660])\n",
      "finalReturns:  tensor([2.8795, 2.9020, 2.8712, 2.8039, 2.7110, 2.6000, 2.4761, 2.3427, 2.2026,\n",
      "        2.0573, 1.9084, 1.7567, 1.6030, 1.4478, 1.2914, 1.1342, 0.9763, 0.8180,\n",
      "        0.6593, 0.5004, 0.3412, 0.1819])\n",
      "----------------------------------------\n",
      "iter  1  stage  2  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0325, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.6361, 7.6361, 7.6361, 7.0504, 6.5504, 6.1103, 5.7124, 5.3450, 4.9996,\n",
      "        4.6702, 4.3525, 4.0436, 3.7411, 3.4435, 3.1493, 2.8579, 2.5684, 2.2804,\n",
      "        1.9935, 1.7075, 1.4220, 1.1371, 0.8525, 0.5681, 0.2840]) return=  117051.59338141434\n",
      "probs of actions:  tensor([0.9992, 0.9992, 0.9992, 0.9991, 0.9993, 0.9991, 0.9992, 0.9994, 0.9994,\n",
      "        0.9991, 0.9990, 0.9992, 0.9991, 0.9992, 0.9990, 0.9993, 0.9990, 0.9998,\n",
      "        0.9994, 0.9993, 0.9992, 0.9995, 1.0000, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5307, 0.5632, 0.5319, 0.5091, 0.4922, 0.4798, 0.4705, 0.4636,\n",
      "        0.4585, 0.4547, 0.4518, 0.4497, 0.4481, 0.4469, 0.4460, 0.4453, 0.4448,\n",
      "        0.4444, 0.4442, 0.4439, 0.4438, 0.4437, 0.4436, 0.4660])\n",
      "finalReturns:  tensor([3.0496, 3.0721, 3.0402, 2.9713, 2.8769, 2.7646, 2.6395, 2.5053, 2.3644,\n",
      "        2.2186, 2.0692, 1.9172, 1.7633, 1.6078, 1.4513, 1.2940, 1.1361, 0.9777,\n",
      "        0.8189, 0.6599, 0.5008, 0.3414, 0.1820])\n",
      "----------------------------------------\n",
      "iter  1  stage  1  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 13, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(9.9530, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.9931, 7.9931, 7.4399, 6.9102, 6.4492, 6.0365, 5.6584, 5.3053, 4.9702,\n",
      "        4.6484, 4.3364, 4.0316, 3.7322, 3.4369, 3.1444, 2.8543, 2.5657, 2.2785,\n",
      "        1.9921, 1.7065, 1.4214, 1.1366, 0.8522, 0.5680, 0.2839]) return=  116862.23949259547\n",
      "probs of actions:  tensor([9.9922e-01, 9.9919e-01, 9.9919e-01, 9.9913e-01, 9.9935e-01, 9.9905e-01,\n",
      "        9.9921e-01, 9.9939e-01, 9.9937e-01, 9.9910e-01, 9.9899e-01, 9.9919e-01,\n",
      "        9.9908e-01, 9.9915e-01, 9.9902e-01, 9.9926e-01, 9.9577e-04, 9.9979e-01,\n",
      "        9.9939e-01, 9.9927e-01, 9.9923e-01, 9.9955e-01, 9.9997e-01, 9.9982e-01,\n",
      "        1.0000e+00], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5307, 0.5632, 0.5319, 0.5091, 0.4922, 0.4798, 0.4705, 0.4636,\n",
      "        0.4585, 0.4547, 0.4518, 0.4497, 0.4481, 0.4469, 0.4460, 0.4509, 0.4380,\n",
      "        0.4393, 0.4403, 0.4411, 0.4416, 0.4420, 0.4424, 0.4651])\n",
      "finalReturns:  tensor([3.2044, 3.2269, 3.1934, 3.1225, 3.0261, 2.9120, 2.7854, 2.6499, 2.5081,\n",
      "        2.3616, 2.2116, 2.0592, 1.9049, 1.7492, 1.5925, 1.4350, 1.2714, 1.1197,\n",
      "        0.9660, 0.8108, 0.6545, 0.4973, 0.3395, 0.1812])\n",
      "----------------------------------------\n",
      "iter  1  stage  0  ep  0   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15,  0])\n",
      "loss=  tensor(0.0391, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.3070, 7.7957, 7.2969, 6.8074, 6.3746, 5.9820, 5.6184, 5.2757, 4.9483,\n",
      "        4.6322, 4.3243, 4.0227, 3.7256, 3.4319, 3.1408, 2.8516, 2.5638, 2.2770,\n",
      "        1.9911, 1.7057, 1.4208, 1.1363, 0.8520, 0.5679, 0.2839]) return=  117051.59338141434\n",
      "probs of actions:  tensor([0.9992, 0.9992, 0.9992, 0.9991, 0.9993, 0.9990, 0.9992, 0.9994, 0.9994,\n",
      "        0.9991, 0.9990, 0.9992, 0.9991, 0.9991, 0.9990, 0.9993, 0.9990, 0.9998,\n",
      "        0.9994, 0.9993, 0.9992, 0.9995, 1.0000, 0.9998, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4887, 0.5307, 0.5632, 0.5319, 0.5091, 0.4922, 0.4798, 0.4705, 0.4636,\n",
      "        0.4585, 0.4547, 0.4518, 0.4497, 0.4481, 0.4469, 0.4460, 0.4453, 0.4448,\n",
      "        0.4444, 0.4442, 0.4439, 0.4438, 0.4437, 0.4436, 0.4660])\n",
      "finalReturns:  tensor([3.3982, 3.4207, 3.3888, 3.3152, 3.2161, 3.0996, 2.9710, 2.8339, 2.6908,\n",
      "        2.5433, 2.3926, 2.2396, 2.0848, 1.9288, 1.7718, 1.6141, 1.4559, 1.2974,\n",
      "        1.1385, 0.9794, 0.8201, 0.6607, 0.5013, 0.3417, 0.1821])\n",
      "0,[1e-05,1][1, 10000, 1, 1],1683118086 saved\n",
      "[526084, 'tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])', 117051.59338141434, 92893.76204687786, 0.03911364823579788, 1e-05, 1, 0, 'tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\\n        15, 15, 15, 15, 15, 15,  0])', '[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\\n 1.]', '0,[1e-05,1][1, 10000, 1, 1],1683118086', 25, 50, 161287.58344151574, 184572.3091501231, 73445.38232037451, 135545.41866666666, 132647.75466666667, 112524.66365021843, 112524.66365021843, 117051.59338141435, 117034.7937498715, 85375.85726043607, 112524.66365021843, 117051.59338141435]\n",
      "policy reset\n",
      "----------------------------------------\n",
      "iter  2  stage  24  ep  99999   adversary:  AdversaryModes.fight_125\n",
      "  actions:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.4645, 0.4645, 0.4645, 0.4645, 0.4645, 0.4645, 0.4645, 0.4645, 0.4645,\n",
      "        0.4645, 0.4645, 0.4645, 0.4645, 0.4645, 0.4645, 0.4645, 0.4645, 0.4645,\n",
      "        0.4645, 0.4645, 0.4645, 0.4645, 0.4645, 0.4645, 0.4645]) return=  118139.09703589762\n",
      "probs of actions:  tensor([0.9190, 0.8833, 0.9029, 0.8854, 0.9188, 0.9049, 0.8844, 0.9179, 0.8879,\n",
      "        0.9124, 0.8865, 0.0176, 0.8999, 0.9145, 0.9051, 0.8961, 0.8862, 0.0062,\n",
      "        0.8904, 0.8916, 0.9037, 0.8957, 0.9033, 0.8869, 0.9912],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.4988, 0.4896, 0.4827, 0.4776, 0.4738, 0.4709, 0.4688, 0.4672,\n",
      "        0.4660, 0.4651, 0.4640, 0.4707, 0.4687, 0.4671, 0.4659, 0.4650, 0.4635,\n",
      "        0.4742, 0.4712, 0.4690, 0.4673, 0.4661, 0.4652, 0.4645])\n",
      "finalReturns:  tensor([0.])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m neuralNet\u001b[38;5;241m=\u001b[39mNNBase(num_input\u001b[38;5;241m=\u001b[39mgame\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39mgame\u001b[38;5;241m.\u001b[39madvHistoryNum, lr\u001b[38;5;241m=\u001b[39mhyperParams[\u001b[38;5;241m0\u001b[39m],num_actions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m      8\u001b[0m algorithm \u001b[38;5;241m=\u001b[39m ReinforceAlgorithm(game, neuralNet, numberIterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, numberEpisodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3_000_000\u001b[39m, discountFactor \u001b[38;5;241m=\u001b[39mhyperParams[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 11\u001b[0m \u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprint_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcodeParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconverge_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\EquiLearn\\PGM_base\\learningBase.py:136\u001b[0m, in \u001b[0;36mReinforceAlgorithm.solver\u001b[1;34m(self, print_step, options, converge_break)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mappend([])\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stage \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_stage_onwards\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumberEpisodes\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mprob_break_limit_ln\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobBreakLn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconverge_break\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_save\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m axs[\u001b[38;5;28miter\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mscatter(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturns[\u001b[38;5;28miter\u001b[39m])), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturns[\u001b[38;5;28miter\u001b[39m])\n\u001b[0;32m    140\u001b[0m axs[\u001b[38;5;28miter\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mscatter(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss[\u001b[38;5;28miter\u001b[39m])), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss[\u001b[38;5;28miter\u001b[39m])\n",
      "File \u001b[1;32m~\\Documents\\EquiLearn\\PGM_base\\learningBase.py:200\u001b[0m, in \u001b[0;36mReinforceAlgorithm.learn_stage_onwards\u001b[1;34m(self, iter, stage, episodes, print_step, prob_break_limit_ln, options, lr, just_stage, write_save)\u001b[0m\n\u001b[0;32m    198\u001b[0m actionsLogProbs \u001b[38;5;241m=\u001b[39m action_logprobs[stage:]\n\u001b[0;32m    199\u001b[0m discRewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturnsComputation(rewards\u001b[38;5;241m=\u001b[39mrewards)\n\u001b[1;32m--> 200\u001b[0m baseRewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomputeBase\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitDemand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdemandPotential\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstartStage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39moptions[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    202\u001b[0m baseDiscReturns \u001b[38;5;241m=\u001b[39m discRewards\u001b[38;5;241m-\u001b[39mbaseRewards\n\u001b[0;32m    203\u001b[0m finalReturns \u001b[38;5;241m=\u001b[39m baseDiscReturns[stage:]\n",
      "File \u001b[1;32m~\\Documents\\EquiLearn\\PGM_base\\learningBase.py:299\u001b[0m, in \u001b[0;36mReinforceAlgorithm.computeBase\u001b[1;34m(self, advPrices, startStage, initDemand)\u001b[0m\n\u001b[0;32m    297\u001b[0m     profit[i] \u001b[38;5;241m=\u001b[39m (demand\u001b[38;5;241m-\u001b[39mprice)\u001b[38;5;241m*\u001b[39m(price\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mcosts[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    298\u001b[0m     demand \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (advPrices[i]\u001b[38;5;241m-\u001b[39mprice)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m--> 299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturnsComputation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\EquiLearn\\PGM_base\\learningBase.py:84\u001b[0m, in \u001b[0;36mReinforceAlgorithm.returnsComputation\u001b[1;34m(self, rewards, episodeMemory)\u001b[0m\n\u001b[0;32m     82\u001b[0m discRewards[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m rewards[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rewards)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 84\u001b[0m     discRewards[i] \u001b[38;5;241m=\u001b[39m rewards[i] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdiscRewards\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m discRewards\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for adv in range(len(AdversaryModes)):\n",
    "    adversaryProbs=torch.zeros(len(AdversaryModes))\n",
    "    adversaryProbs[adv]=1\n",
    "    game = Model(totalDemand = 400, \n",
    "                   tupleCosts = (57, 71),\n",
    "                  totalStages = 25, adversaryProbs=adversaryProbs, advHistoryNum=0)\n",
    "    neuralNet=NNBase(num_input=game.T+2+game.advHistoryNum, lr=hyperParams[0],num_actions=50)\n",
    "    algorithm = ReinforceAlgorithm(game, neuralNet, numberIterations=3, numberEpisodes=3_000_000, discountFactor =hyperParams[1])\n",
    "\n",
    "\n",
    "    algorithm.solver(print_step=100_000,options=codeParams,converge_break=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
