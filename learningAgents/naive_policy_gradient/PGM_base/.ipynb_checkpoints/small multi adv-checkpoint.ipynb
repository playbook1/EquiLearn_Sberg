{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learningBase import ReinforceAlgorithm\n",
    "from environmentModelBase import Model, AdversaryModes\n",
    "from neuralNetworkSimple import NNBase\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversaryProbs=torch.zeros(len(AdversaryModes))\n",
    "adversaryProbs[0]=1\n",
    "game = Model(totalDemand = 400, \n",
    "               tupleCosts = (57, 71),\n",
    "              totalStages = 25, adversaryProbs=adversaryProbs, advHistoryNum=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperParams=[0.0001, 1, 0]\n",
    "codeParams=[1, 10000, 1, 1]\n",
    "\n",
    "lr=hyperParams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy reset\n",
      "----------------------------------------\n",
      "24527   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0, 18,  0,  0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5968, 0.5968, 0.5968, 0.5968, 0.5968, 0.5968, 0.5968, 0.5968, 0.5968,\n",
      "        0.5968, 0.5968, 0.5968, 0.5968, 0.5968, 0.5968, 0.5968, 0.5968, 0.5968,\n",
      "        0.5968, 0.5968, 0.5968, 0.5968, 0.5968, 0.5968, 0.5968]) return=  140305.1458599319\n",
      "probs of actions:  tensor([9.7228e-01, 9.7357e-01, 9.7232e-01, 9.7705e-01, 9.6317e-01, 9.7690e-01,\n",
      "        9.7655e-01, 9.7618e-01, 9.7250e-01, 9.8279e-01, 9.8160e-01, 9.7764e-01,\n",
      "        9.7504e-01, 9.7618e-01, 9.6984e-01, 9.7103e-01, 9.7842e-01, 9.7346e-01,\n",
      "        9.7620e-01, 9.8331e-01, 9.6818e-01, 9.7596e-01, 1.3203e-04, 9.6867e-01,\n",
      "        9.9901e-01], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5366, 0.5495, 0.5560, 0.5592, 0.5609, 0.5617, 0.5621, 0.5623,\n",
      "        0.5624, 0.5624, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625,\n",
      "        0.5625, 0.5625, 0.5625, 0.5625, 0.5301, 0.6320, 0.5968])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "25755   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([ 0, 26,  0, 26, 26, 26, 26,  0, 26, 26,  0, 26, 26, 26,  0, 26, 26, 26,\n",
      "        26, 26, 26, 26,  0, 26,  0])\n",
      "loss=  tensor(3.7815e-05, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2759, 1.2759, 1.2759, 1.2759, 1.2759, 1.2759, 1.2759, 1.2759, 1.2759,\n",
      "        1.2759, 1.2759, 1.2759, 1.2759, 1.2759, 1.2759, 1.2759, 1.2759, 1.2759,\n",
      "        1.2759, 1.2759, 1.2759, 1.2759, 1.2759, 1.2759, 0.6121]) return=  163403.48619982437\n",
      "probs of actions:  tensor([0.2713, 0.9437, 0.2477, 0.9074, 0.7945, 0.9064, 0.8652, 0.1833, 0.8615,\n",
      "        0.8040, 0.3010, 0.8593, 0.8648, 0.7408, 0.1000, 0.8775, 0.6739, 0.8823,\n",
      "        0.8630, 0.7801, 0.8445, 0.8598, 0.1583, 0.9990, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.4690, 0.6500, 0.5379, 0.6197, 0.6626, 0.6845, 0.7632, 0.5914,\n",
      "        0.6480, 0.7447, 0.5828, 0.6435, 0.6748, 0.7583, 0.5892, 0.6468, 0.6765,\n",
      "        0.6916, 0.6992, 0.7030, 0.7049, 0.7734, 0.5962, 0.7180])\n",
      "finalReturns:  tensor([0.0383, 0.1059])\n",
      "----------------------------------------\n",
      "614   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26,  0, 26, 26,  0])\n",
      "loss=  tensor(0.0002, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.8390, 1.8390, 1.8390, 1.8390, 1.8390, 1.8390, 1.8390, 1.8390, 1.8390,\n",
      "        1.8390, 1.8390, 1.8390, 1.8390, 1.8390, 1.8390, 1.8390, 1.8390, 1.8390,\n",
      "        1.8390, 1.8390, 1.8390, 1.8390, 1.8390, 1.1748, 0.5625]) return=  170660.4533652142\n",
      "probs of actions:  tensor([0.9908, 0.9982, 0.9919, 0.9974, 0.9944, 0.9979, 0.9970, 0.9954, 0.9965,\n",
      "        0.9951, 0.9921, 0.9969, 0.9968, 0.9932, 0.9974, 0.9966, 0.9910, 0.9972,\n",
      "        0.9969, 0.9943, 0.9961, 0.0026, 0.9991, 0.9999, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7744, 0.5966, 0.6507, 0.7461])\n",
      "finalReturns:  tensor([0.1543, 0.2219, 0.1836])\n",
      "----------------------------------------\n",
      "26   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26,  0])\n",
      "loss=  tensor(0.0007, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.5582, 2.5582, 2.5582, 2.5582, 2.5582, 2.5582, 2.5582, 2.5582, 2.5582,\n",
      "        2.5582, 2.5582, 2.5582, 2.5582, 2.5582, 2.5582, 2.5582, 2.5582, 2.5582,\n",
      "        2.5582, 2.5582, 2.5582, 2.5582, 1.7838, 1.1196, 0.5324]) return=  171931.00017309192\n",
      "probs of actions:  tensor([0.9934, 0.9986, 0.9940, 0.9980, 0.9958, 0.9984, 0.9977, 0.9967, 0.9974,\n",
      "        0.9964, 0.9942, 0.9978, 0.9976, 0.9951, 0.9981, 0.9975, 0.9933, 0.9979,\n",
      "        0.9977, 0.9958, 0.9971, 0.9990, 0.9992, 0.9999, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7744])\n",
      "finalReturns:  tensor([0.3366, 0.4042, 0.3616, 0.2420])\n",
      "----------------------------------------\n",
      "21   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26,  0])\n",
      "loss=  tensor(0.0012, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.0514, 3.0514, 3.0514, 3.0514, 3.0514, 3.0514, 3.0514, 3.0514, 3.0514,\n",
      "        3.0514, 3.0514, 3.0514, 3.0514, 3.0514, 3.0514, 3.0514, 3.0514, 3.0514,\n",
      "        3.0514, 3.0514, 3.0514, 2.2770, 1.6128, 1.0256, 0.4932]) return=  171931.00017309192\n",
      "probs of actions:  tensor([0.9955, 0.9990, 0.9959, 0.9986, 0.9971, 0.9990, 0.9985, 0.9978, 0.9982,\n",
      "        0.9976, 0.9961, 0.9985, 0.9984, 0.9968, 0.9987, 0.9983, 0.9954, 0.9986,\n",
      "        0.9985, 0.9972, 0.9990, 0.9996, 0.9994, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7744])\n",
      "finalReturns:  tensor([0.5502, 0.6178, 0.5752, 0.4556, 0.2812])\n",
      "----------------------------------------\n",
      "16   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26,  0])\n",
      "loss=  tensor(0.0016, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.5161, 3.5161, 3.5161, 3.5161, 3.5161, 3.5161, 3.5161, 3.5161, 3.5161,\n",
      "        3.5161, 3.5161, 3.5161, 3.5161, 3.5161, 3.5161, 3.5161, 3.5161, 3.5161,\n",
      "        3.5161, 3.5161, 2.7417, 2.0775, 1.4903, 0.9579, 0.4647]) return=  171931.00017309192\n",
      "probs of actions:  tensor([0.9972, 0.9993, 0.9974, 0.9991, 0.9981, 0.9993, 0.9990, 0.9986, 0.9988,\n",
      "        0.9985, 0.9975, 0.9991, 0.9989, 0.9980, 0.9991, 0.9989, 0.9970, 0.9991,\n",
      "        0.9990, 0.9990, 0.9996, 0.9998, 0.9996, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7744])\n",
      "finalReturns:  tensor([0.7923, 0.8599, 0.8173, 0.6977, 0.5233, 0.3097])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26,  0])\n",
      "loss=  tensor(0.0031, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.9600, 3.9600, 3.9600, 3.9600, 3.9600, 3.9600, 3.9600, 3.9600, 3.9600,\n",
      "        3.9600, 3.9600, 3.9600, 3.9600, 3.9600, 3.9600, 3.9600, 3.9600, 3.9600,\n",
      "        3.9600, 3.1856, 2.5214, 1.9343, 1.4018, 0.9086, 0.4439]) return=  171931.00017309192\n",
      "probs of actions:  tensor([0.9972, 0.9994, 0.9975, 0.9991, 0.9981, 0.9994, 0.9990, 0.9986, 0.9989,\n",
      "        0.9985, 0.9976, 0.9991, 0.9990, 0.9980, 0.9992, 0.9989, 0.9971, 0.9991,\n",
      "        0.9990, 0.9991, 0.9996, 0.9998, 0.9996, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7744])\n",
      "finalReturns:  tensor([1.0552, 1.1228, 1.0802, 0.9605, 0.7862, 0.5726, 0.3305])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26,  0])\n",
      "loss=  tensor(0.0047, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.3887, 4.3887, 4.3887, 4.3887, 4.3887, 4.3887, 4.3887, 4.3887, 4.3887,\n",
      "        4.3887, 4.3887, 4.3887, 4.3887, 4.3887, 4.3887, 4.3887, 4.3887, 4.3887,\n",
      "        3.6143, 2.9500, 2.3629, 1.8305, 1.3373, 0.8726, 0.4286]) return=  171931.00017309192\n",
      "probs of actions:  tensor([0.9973, 0.9994, 0.9975, 0.9991, 0.9982, 0.9994, 0.9991, 0.9987, 0.9989,\n",
      "        0.9985, 0.9977, 0.9991, 0.9990, 0.9981, 0.9992, 0.9989, 0.9972, 0.9991,\n",
      "        0.9991, 0.9991, 0.9996, 0.9998, 0.9996, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7744])\n",
      "finalReturns:  tensor([1.3333, 1.4009, 1.3584, 1.2387, 1.0643, 0.8507, 0.6086, 0.3458])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "12   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26,  0])\n",
      "loss=  tensor(0.0037, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.8060, 4.8060, 4.8060, 4.8060, 4.8060, 4.8060, 4.8060, 4.8060, 4.8060,\n",
      "        4.8060, 4.8060, 4.8060, 4.8060, 4.8060, 4.8060, 4.8060, 4.8060, 4.0316,\n",
      "        3.3674, 2.7802, 2.2478, 1.7546, 1.2899, 0.8460, 0.4174]) return=  171931.00017309192\n",
      "probs of actions:  tensor([0.9985, 0.9996, 0.9987, 0.9995, 0.9990, 0.9997, 0.9995, 0.9993, 0.9994,\n",
      "        0.9992, 0.9988, 0.9995, 0.9995, 0.9990, 0.9996, 0.9994, 0.9991, 0.9997,\n",
      "        0.9997, 0.9997, 0.9998, 0.9999, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7744])\n",
      "finalReturns:  tensor([1.6228, 1.6904, 1.6478, 1.5281, 1.3538, 1.1402, 0.8981, 0.6352, 0.3570])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26,  0])\n",
      "loss=  tensor(0.0051, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.2150, 5.2150, 5.2150, 5.2150, 5.2150, 5.2150, 5.2150, 5.2150, 5.2150,\n",
      "        5.2150, 5.2150, 5.2150, 5.2150, 5.2150, 5.2150, 5.2150, 4.4406, 3.7764,\n",
      "        3.1892, 2.6568, 2.1636, 1.6989, 1.2550, 0.8263, 0.4090]) return=  171931.00017309192\n",
      "probs of actions:  tensor([0.9986, 0.9997, 0.9987, 0.9995, 0.9990, 0.9997, 0.9995, 0.9993, 0.9994,\n",
      "        0.9993, 0.9988, 0.9996, 0.9995, 0.9990, 0.9996, 0.9994, 0.9991, 0.9997,\n",
      "        0.9997, 0.9997, 0.9998, 0.9999, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7744])\n",
      "finalReturns:  tensor([1.9206, 1.9882, 1.9456, 1.8260, 1.6516, 1.4380, 1.1959, 0.9330, 0.6549,\n",
      "        0.3654])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26,  0])\n",
      "loss=  tensor(0.0061, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.6177, 5.6177, 5.6177, 5.6177, 5.6177, 5.6177, 5.6177, 5.6177, 5.6177,\n",
      "        5.6177, 5.6177, 5.6177, 5.6177, 5.6177, 5.6177, 4.8434, 4.1791, 3.5920,\n",
      "        3.0596, 2.5664, 2.1017, 1.6578, 1.2291, 0.8118, 0.4028]) return=  171931.00017309192\n",
      "probs of actions:  tensor([0.9987, 0.9997, 0.9988, 0.9996, 0.9991, 0.9997, 0.9996, 0.9993, 0.9995,\n",
      "        0.9993, 0.9989, 0.9996, 0.9995, 0.9991, 0.9996, 0.9995, 0.9992, 0.9997,\n",
      "        0.9997, 0.9997, 0.9998, 0.9999, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7744])\n",
      "finalReturns:  tensor([2.2246, 2.2922, 2.2497, 2.1300, 1.9556, 1.7420, 1.4999, 1.2370, 0.9589,\n",
      "        0.6694, 0.3716])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26,  0])\n",
      "loss=  tensor(0.0084, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.0158, 6.0158, 6.0158, 6.0158, 6.0158, 6.0158, 6.0158, 6.0158, 6.0158,\n",
      "        6.0158, 6.0158, 6.0158, 6.0158, 6.0158, 5.2415, 4.5773, 3.9901, 3.4577,\n",
      "        2.9645, 2.4998, 2.0559, 1.6272, 1.2099, 0.8009, 0.3981]) return=  171931.00017309192\n",
      "probs of actions:  tensor([0.9987, 0.9997, 0.9989, 0.9996, 0.9991, 0.9997, 0.9996, 0.9994, 0.9995,\n",
      "        0.9993, 0.9990, 0.9996, 0.9995, 0.9991, 0.9996, 0.9995, 0.9993, 0.9997,\n",
      "        0.9997, 0.9997, 0.9998, 0.9999, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7744])\n",
      "finalReturns:  tensor([2.5333, 2.6009, 2.5583, 2.4387, 2.2643, 2.0507, 1.8086, 1.5457, 1.2676,\n",
      "        0.9781, 0.6803, 0.3763])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26,  0])\n",
      "loss=  tensor(0.0097, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.4105, 6.4105, 6.4105, 6.4105, 6.4105, 6.4105, 6.4105, 6.4105, 6.4105,\n",
      "        6.4105, 6.4105, 6.4105, 6.4105, 5.6361, 4.9719, 4.3848, 3.8524, 3.3592,\n",
      "        2.8945, 2.4506, 2.0219, 1.6046, 1.1956, 0.7928, 0.3947]) return=  171931.00017309192\n",
      "probs of actions:  tensor([0.9988, 0.9997, 0.9989, 0.9996, 0.9992, 0.9997, 0.9996, 0.9994, 0.9995,\n",
      "        0.9994, 0.9990, 0.9996, 0.9996, 0.9992, 0.9997, 0.9995, 0.9993, 0.9998,\n",
      "        0.9998, 0.9998, 0.9998, 0.9999, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7744])\n",
      "finalReturns:  tensor([2.8454, 2.9130, 2.8704, 2.7508, 2.5764, 2.3628, 2.1207, 1.8578, 1.5797,\n",
      "        1.2902, 0.9924, 0.6884, 0.3797])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26,  0])\n",
      "loss=  tensor(0.0106, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.8024, 6.8024, 6.8024, 6.8024, 6.8024, 6.8024, 6.8024, 6.8024, 6.8024,\n",
      "        6.8024, 6.8024, 6.8024, 6.0282, 5.3640, 4.7769, 4.2445, 3.7513, 3.2866,\n",
      "        2.8427, 2.4140, 1.9967, 1.5877, 1.1849, 0.7868, 0.3921]) return=  171931.00017309192\n",
      "probs of actions:  tensor([0.9989, 0.9997, 0.9990, 0.9996, 0.9992, 0.9997, 0.9996, 0.9994, 0.9995,\n",
      "        0.9994, 0.9991, 0.9996, 0.9996, 0.9993, 0.9997, 0.9996, 0.9994, 0.9998,\n",
      "        0.9998, 0.9998, 0.9999, 0.9999, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7744])\n",
      "finalReturns:  tensor([3.1601, 3.2277, 3.1851, 3.0655, 2.8911, 2.6775, 2.4354, 2.1725, 1.8944,\n",
      "        1.6049, 1.3071, 1.0031, 0.6944, 0.3823])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26,  0])\n",
      "loss=  tensor(0.0133, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.1923, 7.1923, 7.1923, 7.1923, 7.1923, 7.1923, 7.1923, 7.1923, 7.1923,\n",
      "        7.1923, 7.1923, 6.4182, 5.7541, 5.1670, 4.6346, 4.1415, 3.6767, 3.2328,\n",
      "        2.8042, 2.3868, 1.9779, 1.5751, 1.1769, 0.7823, 0.3902]) return=  171931.00017309192\n",
      "probs of actions:  tensor([0.9989, 0.9997, 0.9990, 0.9996, 0.9993, 0.9998, 0.9996, 0.9995, 0.9996,\n",
      "        0.9994, 0.9991, 0.9997, 0.9996, 0.9993, 0.9997, 0.9996, 0.9994, 0.9998,\n",
      "        0.9998, 0.9998, 0.9999, 0.9999, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7744])\n",
      "finalReturns:  tensor([3.4767, 3.5443, 3.5017, 3.3821, 3.2077, 2.9941, 2.7520, 2.4892, 2.2110,\n",
      "        1.9216, 1.6237, 1.3197, 1.0111, 0.6989, 0.3842])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26,  0])\n",
      "loss=  tensor(0.0147, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.5805, 7.5805, 7.5805, 7.5805, 7.5805, 7.5805, 7.5805, 7.5805, 7.5805,\n",
      "        7.5805, 6.8067, 6.1427, 5.5557, 5.0233, 4.5302, 4.0655, 3.6215, 3.1929,\n",
      "        2.7756, 2.3666, 1.9638, 1.5657, 1.1710, 0.7789, 0.3887]) return=  171931.00017309192\n",
      "probs of actions:  tensor([0.9990, 0.9997, 0.9991, 0.9997, 0.9993, 0.9998, 0.9997, 0.9995, 0.9996,\n",
      "        0.9995, 0.9992, 0.9997, 0.9997, 0.9994, 0.9997, 0.9997, 0.9995, 0.9998,\n",
      "        0.9998, 0.9998, 0.9999, 0.9999, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7744])\n",
      "finalReturns:  tensor([3.7948, 3.8624, 3.8198, 3.7002, 3.5258, 3.3122, 3.0701, 2.8072, 2.5291,\n",
      "        2.2396, 1.9418, 1.6378, 1.3291, 1.0170, 0.7023, 0.3857])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26,  0])\n",
      "loss=  tensor(0.0154, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.9671, 7.9671, 7.9671, 7.9671, 7.9671, 7.9671, 7.9671, 7.9671, 7.9671,\n",
      "        7.1938, 6.5301, 5.9432, 5.4109, 4.9178, 4.4531, 4.0092, 3.5805, 3.1632,\n",
      "        2.7542, 2.3514, 1.9533, 1.5586, 1.1665, 0.7764, 0.3876]) return=  171931.00017309192\n",
      "probs of actions:  tensor([0.9991, 0.9998, 0.9992, 0.9997, 0.9994, 0.9998, 0.9997, 0.9996, 0.9996,\n",
      "        0.9995, 0.9993, 0.9997, 0.9997, 0.9995, 0.9998, 0.9997, 0.9995, 0.9998,\n",
      "        0.9998, 0.9998, 0.9999, 0.9999, 0.9998, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7744])\n",
      "finalReturns:  tensor([4.1139, 4.1815, 4.1389, 4.0193, 3.8450, 3.6314, 3.3893, 3.1264, 2.8482,\n",
      "        2.5588, 2.2610, 1.9570, 1.6483, 1.3362, 1.0215, 0.7048, 0.3868])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26,  0])\n",
      "loss=  tensor(0.0162, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.3518, 8.3518, 8.3518, 8.3518, 8.3518, 8.3518, 8.3518, 8.3518, 7.5796,\n",
      "        6.9165, 6.3298, 5.7976, 5.3045, 4.8399, 4.3960, 3.9674, 3.5500, 3.1410,\n",
      "        2.7383, 2.3401, 1.9454, 1.5534, 1.1632, 0.7745, 0.3868]) return=  171931.00017309192\n",
      "probs of actions:  tensor([0.9991, 0.9998, 0.9992, 0.9997, 0.9994, 0.9998, 0.9997, 0.9996, 0.9997,\n",
      "        0.9996, 0.9994, 0.9998, 0.9997, 0.9995, 0.9998, 0.9997, 0.9996, 0.9998,\n",
      "        0.9998, 0.9998, 0.9999, 0.9999, 0.9999, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7744])\n",
      "finalReturns:  tensor([4.4337, 4.5013, 4.4588, 4.3392, 4.1649, 3.9513, 3.7092, 3.4464, 3.1682,\n",
      "        2.8788, 2.5810, 2.2769, 1.9683, 1.6562, 1.3414, 1.0248, 0.7067, 0.3876])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26,  0])\n",
      "loss=  tensor(0.0162, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.7337, 8.7337, 8.7337, 8.7337, 8.7337, 8.7337, 8.7337, 7.9639, 7.3017,\n",
      "        6.7156, 6.1836, 5.6907, 5.2261, 4.7822, 4.3536, 3.9362, 3.5273, 3.1245,\n",
      "        2.7263, 2.3317, 1.9396, 1.5494, 1.1607, 0.7730, 0.3862]) return=  171931.00017309192\n",
      "probs of actions:  tensor([0.9992, 0.9998, 0.9993, 0.9997, 0.9994, 0.9998, 0.9997, 0.9996, 0.9997,\n",
      "        0.9996, 0.9994, 0.9998, 0.9998, 0.9996, 0.9998, 0.9997, 0.9996, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7744])\n",
      "finalReturns:  tensor([4.7540, 4.8216, 4.7792, 4.6597, 4.4854, 4.2719, 4.0298, 3.7669, 3.4888,\n",
      "        3.1993, 2.9015, 2.5975, 2.2889, 1.9767, 1.6620, 1.3454, 1.0273, 0.7082,\n",
      "        0.3882])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26,  0])\n",
      "loss=  tensor(0.0156, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.1110, 9.1110, 9.1110, 9.1110, 9.1110, 9.1110, 8.3456, 7.6856, 7.1004,\n",
      "        6.5689, 6.0762, 5.6117, 5.1679, 4.7393, 4.3220, 3.9130, 3.5102, 3.1121,\n",
      "        2.7174, 2.3253, 1.9352, 1.5464, 1.1588, 0.7720, 0.3858]) return=  171931.00017309192\n",
      "probs of actions:  tensor([0.9993, 0.9998, 0.9994, 0.9997, 0.9995, 0.9998, 0.9998, 0.9997, 0.9997,\n",
      "        0.9997, 0.9995, 0.9998, 0.9998, 0.9996, 0.9998, 0.9998, 0.9997, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 1.0000, 0.9999, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7744])\n",
      "finalReturns:  tensor([5.0745, 5.1421, 5.0999, 4.9805, 4.8063, 4.5928, 4.3508, 4.0880, 3.8098,\n",
      "        3.5204, 3.2226, 2.9185, 2.6099, 2.2978, 1.9831, 1.6664, 1.3484, 1.0292,\n",
      "        0.7092, 0.3886])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26,  0])\n",
      "loss=  tensor(0.0166, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.4794, 9.4794, 9.4794, 9.4794, 9.4794, 8.7231, 8.0672, 7.4840, 6.9535,\n",
      "        6.4612, 5.9969, 5.5532, 5.1247, 4.7074, 4.2984, 3.8957, 3.4975, 3.1029,\n",
      "        2.7108, 2.3206, 1.9319, 1.5442, 1.1574, 0.7712, 0.3854]) return=  171931.00017309192\n",
      "probs of actions:  tensor([0.9993, 0.9998, 0.9994, 0.9998, 0.9995, 0.9998, 0.9998, 0.9997, 0.9998,\n",
      "        0.9997, 0.9996, 0.9998, 0.9998, 0.9997, 0.9999, 0.9998, 0.9997, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 1.0000, 0.9999, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7744])\n",
      "finalReturns:  tensor([5.3948, 5.4624, 5.4205, 5.3014, 5.1275, 4.9141, 4.6721, 4.4093, 4.1312,\n",
      "        3.8417, 3.5439, 3.2399, 2.9313, 2.6191, 2.3044, 1.9878, 1.6697, 1.3506,\n",
      "        1.0306, 0.7100, 0.3890])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26,  0])\n",
      "loss=  tensor(0.0163, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.8309, 9.8309, 9.8309, 9.8309, 9.0923, 8.4448, 7.8656, 7.3369, 6.8455,\n",
      "        6.3817, 5.9382, 5.5097, 5.0925, 4.6836, 4.2808, 3.8827, 3.4880, 3.0959,\n",
      "        2.7058, 2.3170, 1.9294, 1.5426, 1.1564, 0.7706, 0.3852]) return=  171931.00017309192\n",
      "probs of actions:  tensor([0.9994, 0.9998, 0.9995, 0.9998, 0.9996, 0.9999, 0.9998, 0.9997, 0.9998,\n",
      "        0.9997, 0.9996, 0.9999, 0.9998, 0.9997, 0.9999, 0.9998, 0.9997, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 1.0000, 0.9999, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7744])\n",
      "finalReturns:  tensor([5.7143, 5.7819, 5.7407, 5.6222, 5.4486, 5.2354, 4.9936, 4.7309, 4.4528,\n",
      "        4.1634, 3.8656, 3.5615, 3.2529, 2.9408, 2.6261, 2.3094, 1.9914, 1.6722,\n",
      "        1.3522, 1.0316, 0.7106, 0.3892])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26,  0])\n",
      "loss=  tensor(0.0176, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.1492, 10.1492, 10.1492,  9.4457,  8.8147,  8.2433,  7.7183,  7.2288,\n",
      "         6.7658,  6.3227,  5.8945,  5.4774,  5.0685,  4.6658,  4.2677,  3.8730,\n",
      "         3.4809,  3.0907,  2.7020,  2.3144,  1.9275,  1.5413,  1.1556,  0.7701,\n",
      "         0.3850]) return=  171931.00017309192\n",
      "probs of actions:  tensor([0.9995, 0.9999, 0.9995, 0.9998, 0.9996, 0.9999, 0.9998, 0.9998, 0.9998,\n",
      "        0.9998, 0.9997, 0.9999, 0.9999, 0.9998, 0.9999, 0.9998, 0.9998, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 1.0000, 0.9999, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7744])\n",
      "finalReturns:  tensor([6.0318, 6.0994, 6.0595, 5.9422, 5.7694, 5.5567, 5.3151, 5.0525, 4.7745,\n",
      "        4.4851, 4.1874, 3.8834, 3.5747, 3.2626, 2.9479, 2.6313, 2.3132, 1.9940,\n",
      "        1.6741, 1.3535, 1.0324, 0.7111, 0.3894])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26,  0])\n",
      "loss=  tensor(0.0167, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.4043, 10.4043,  9.7682,  9.1696,  8.6136,  8.0961,  7.6101,  7.1489,\n",
      "         6.7067,  6.2789,  5.8620,  5.4532,  5.0505,  4.6525,  4.2578,  3.8657,\n",
      "         3.4756,  3.0868,  2.6992,  2.3124,  1.9262,  1.5404,  1.1550,  0.7698,\n",
      "         0.3848]) return=  171931.00017309192\n",
      "probs of actions:  tensor([0.9995, 0.9999, 0.9996, 0.9998, 0.9997, 0.9999, 0.9999, 0.9998, 0.9998,\n",
      "        0.9998, 0.9997, 0.9999, 0.9999, 0.9998, 0.9999, 0.9999, 0.9998, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 1.0000, 0.9999, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7744])\n",
      "finalReturns:  tensor([6.3452, 6.4128, 6.3756, 6.2606, 6.0894, 5.8776, 5.6365, 5.3742, 5.0963,\n",
      "        4.8070, 4.5093, 4.2053, 3.8967, 3.5846, 3.2699, 2.9532, 2.6352, 2.3160,\n",
      "        1.9960, 1.6754, 1.3544, 1.0330, 0.7114, 0.3896])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26,  0])\n",
      "loss=  tensor(0.0178, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.5430, 10.0317,  9.4952,  8.9696,  8.4668,  7.9880,  7.5303,  7.0898,\n",
      "         6.6628,  6.2463,  5.8377,  5.4352,  5.0371,  4.6425,  4.2504,  3.8603,\n",
      "         3.4716,  3.0839,  2.6971,  2.3109,  1.9251,  1.5397,  1.1545,  0.7696,\n",
      "         0.3847]) return=  171931.00017309192\n",
      "probs of actions:  tensor([0.9996, 0.9999, 0.9996, 0.9998, 0.9997, 0.9999, 0.9999, 0.9998, 0.9999,\n",
      "        0.9998, 0.9998, 0.9999, 0.9999, 0.9998, 0.9999, 0.9999, 0.9998, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 1.0000, 0.9999, 1.0000, 1.0000],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4436, 0.5684, 0.6359, 0.6709, 0.6888, 0.6978, 0.7023, 0.7045, 0.7057,\n",
      "        0.7062, 0.7065, 0.7067, 0.7067, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068,\n",
      "        0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7068, 0.7744])\n",
      "finalReturns:  tensor([6.6501, 6.7177, 6.6859, 6.5756, 6.4075, 6.1975, 5.9575, 5.6957, 5.4181,\n",
      "        5.1290, 4.8313, 4.5273, 4.2187, 3.9066, 3.5919, 3.2753, 2.9572, 2.6381,\n",
      "        2.3181, 1.9975, 1.6765, 1.3551, 1.0335, 0.7116, 0.3897])\n",
      "0,[0.0001,1][1, 10000, 1, 1],1682338032 saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50996, 'tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])', 171931.00017309192, 61130.999905586235, 0.01784641481935978, 0.0001, 1, 0, 'tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\\n        26, 26, 26, 26, 26, 26,  0])', '[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\\n 1.]', '0,[0.0001,1][1, 10000, 1, 1],1682338032', 25, 50, 171931.00017309192, 216972.079356517, 90141.68158887929, 132851.47466666668, 129867.688, 75586.00175631171, 75586.00175631171, 91712.14211264676, 91712.14211264676, 103918.78070576812, 75586.00175631171, 91712.14211264676]\n",
      "policy reset\n",
      "----------------------------------------\n",
      "20196   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "loss=  tensor(-0., grad_fn=<NegBackward0>)   ,  base rewards= tensor([0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625,\n",
      "        0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625,\n",
      "        0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625]) return=  139591.33336462575\n",
      "probs of actions:  tensor([0.9829, 0.9817, 0.9833, 0.9813, 0.9814, 0.9830, 0.9766, 0.9877, 0.9829,\n",
      "        0.9777, 0.9770, 0.9807, 0.9820, 0.9828, 0.9853, 0.9854, 0.9831, 0.9799,\n",
      "        0.9780, 0.9840, 0.9796, 0.9798, 0.9782, 0.9862, 0.9990],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5366, 0.5495, 0.5560, 0.5592, 0.5609, 0.5617, 0.5621, 0.5623,\n",
      "        0.5624, 0.5624, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625,\n",
      "        0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625, 0.5625])\n",
      "finalReturns:  tensor([0.])\n",
      "----------------------------------------\n",
      "29231   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([ 0,  4,  0,  0,  0,  0,  0,  0,  0, 18, 18,  0, 18,  0,  0,  2,  0,  0,\n",
      "         2,  0, 18, 18,  0, 18,  0])\n",
      "loss=  tensor(0.0002, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.2033, 1.2033, 1.2033, 1.2033, 1.2033, 1.2033, 1.2033, 1.2033, 1.2033,\n",
      "        1.2033, 1.2033, 1.2033, 1.2033, 1.2033, 1.2033, 1.2033, 1.2033, 1.2033,\n",
      "        1.2033, 1.2033, 1.2033, 1.2033, 1.2033, 1.2033, 0.5884]) return=  145639.90478618248\n",
      "probs of actions:  tensor([0.8123, 0.0217, 0.8376, 0.8246, 0.8538, 0.8365, 0.6731, 0.8068, 0.8012,\n",
      "        0.5065, 0.1790, 0.7986, 0.3048, 0.8478, 0.8667, 0.0651, 0.7699, 0.7195,\n",
      "        0.0874, 0.7782, 0.1389, 0.1057, 0.7413, 0.9990, 0.9982],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.5112, 0.5350, 0.5644, 0.5634, 0.5630, 0.5627, 0.5626, 0.5626, 0.5625,\n",
      "        0.5301, 0.5996, 0.6683, 0.5819, 0.6591, 0.6099, 0.5855, 0.5818, 0.5721,\n",
      "        0.5669, 0.5724, 0.5351, 0.6022, 0.6697, 0.5825, 0.6595])\n",
      "finalReturns:  tensor([0.0387, 0.0711])\n",
      "----------------------------------------\n",
      "5854   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,  0, 18, 18, 18,\n",
      "        18, 18, 18,  0, 18, 18,  0])\n",
      "loss=  tensor(0.0003, grad_fn=<NegBackward0>)   ,  base rewards= tensor([1.7903, 1.7903, 1.7903, 1.7903, 1.7903, 1.7903, 1.7903, 1.7903, 1.7903,\n",
      "        1.7903, 1.7903, 1.7903, 1.7903, 1.7903, 1.7903, 1.7903, 1.7903, 1.7903,\n",
      "        1.7903, 1.7903, 1.7903, 1.7903, 1.7903, 1.1589, 0.5624]) return=  162500.80801629333\n",
      "probs of actions:  tensor([0.9176, 0.9049, 0.8263, 0.8896, 0.8708, 0.9044, 0.9630, 0.9452, 0.9347,\n",
      "        0.9912, 0.9645, 0.9213, 0.9762, 0.8875, 0.1613, 0.8918, 0.9604, 0.9668,\n",
      "        0.9129, 0.9288, 0.9479, 0.0403, 0.9990, 0.9999, 0.9990],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6601, 0.6667, 0.6699, 0.6716, 0.6724,\n",
      "        0.6728, 0.6730, 0.6731, 0.6731, 0.6732, 0.7056, 0.5996, 0.6359, 0.6544,\n",
      "        0.6638, 0.6685, 0.6708, 0.7044, 0.5991, 0.6356, 0.6867])\n",
      "finalReturns:  tensor([0.1310, 0.1634, 0.1243])\n",
      "----------------------------------------\n",
      "225   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0006, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.4587, 2.4587, 2.4587, 2.4587, 2.4587, 2.4587, 2.4587, 2.4587, 2.4587,\n",
      "        2.4587, 2.4587, 2.4587, 2.4587, 2.4587, 2.4587, 2.4587, 2.4587, 2.4587,\n",
      "        2.4587, 2.4587, 2.4587, 2.4587, 1.7531, 1.1211, 0.5416]) return=  164632.33345850307\n",
      "probs of actions:  tensor([0.9828, 0.9791, 0.9639, 0.9793, 0.9724, 0.9812, 0.9923, 0.9902, 0.9876,\n",
      "        0.9981, 0.9930, 0.9858, 0.9954, 0.9777, 0.9626, 0.9789, 0.9934, 0.9941,\n",
      "        0.9832, 0.9868, 0.9900, 0.9990, 0.9996, 1.0000, 0.9989],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6601, 0.6667, 0.6699, 0.6716, 0.6724,\n",
      "        0.6728, 0.6730, 0.6731, 0.6731, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732,\n",
      "        0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.7056])\n",
      "finalReturns:  tensor([0.2665, 0.2989, 0.2577, 0.1640])\n",
      "----------------------------------------\n",
      "61   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0010, grad_fn=<NegBackward0>)   ,  base rewards= tensor([2.9728, 2.9728, 2.9728, 2.9728, 2.9728, 2.9728, 2.9728, 2.9728, 2.9728,\n",
      "        2.9728, 2.9728, 2.9728, 2.9728, 2.9728, 2.9728, 2.9728, 2.9728, 2.9728,\n",
      "        2.9728, 2.9728, 2.9728, 2.2672, 1.6351, 1.0556, 0.5140]) return=  164632.33345850307\n",
      "probs of actions:  tensor([0.9912, 0.9893, 0.9827, 0.9902, 0.9873, 0.9906, 0.9962, 0.9955, 0.9940,\n",
      "        0.9990, 0.9965, 0.9933, 0.9976, 0.9892, 0.9819, 0.9897, 0.9967, 0.9971,\n",
      "        0.9915, 0.9931, 0.9990, 0.9995, 0.9998, 1.0000, 0.9988],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6601, 0.6667, 0.6699, 0.6716, 0.6724,\n",
      "        0.6728, 0.6730, 0.6731, 0.6731, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732,\n",
      "        0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.7056])\n",
      "finalReturns:  tensor([0.4256, 0.4580, 0.4169, 0.3232, 0.1916])\n",
      "----------------------------------------\n",
      "41   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0014, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.4666, 3.4666, 3.4666, 3.4666, 3.4666, 3.4666, 3.4666, 3.4666, 3.4666,\n",
      "        3.4666, 3.4666, 3.4666, 3.4666, 3.4666, 3.4666, 3.4666, 3.4666, 3.4666,\n",
      "        3.4666, 3.4666, 2.7610, 2.1289, 1.5494, 1.0078, 0.4938]) return=  164632.33345850307\n",
      "probs of actions:  tensor([0.9953, 0.9943, 0.9912, 0.9949, 0.9935, 0.9952, 0.9979, 0.9978, 0.9969,\n",
      "        0.9995, 0.9981, 0.9966, 0.9987, 0.9945, 0.9906, 0.9946, 0.9983, 0.9985,\n",
      "        0.9955, 0.9990, 0.9996, 0.9997, 0.9998, 1.0000, 0.9987],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6601, 0.6667, 0.6699, 0.6716, 0.6724,\n",
      "        0.6728, 0.6730, 0.6731, 0.6731, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732,\n",
      "        0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.7056])\n",
      "finalReturns:  tensor([0.6050, 0.6374, 0.5963, 0.5026, 0.3710, 0.2118])\n",
      "----------------------------------------\n",
      "26   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0018, grad_fn=<NegBackward0>)   ,  base rewards= tensor([3.9455, 3.9455, 3.9455, 3.9455, 3.9455, 3.9455, 3.9455, 3.9455, 3.9455,\n",
      "        3.9455, 3.9455, 3.9455, 3.9455, 3.9455, 3.9455, 3.9455, 3.9455, 3.9455,\n",
      "        3.9455, 3.2399, 2.6078, 2.0283, 1.4867, 0.9727, 0.4789]) return=  164632.33345850307\n",
      "probs of actions:  tensor([0.9972, 0.9965, 0.9948, 0.9969, 0.9961, 0.9970, 0.9987, 0.9987, 0.9981,\n",
      "        0.9996, 0.9989, 0.9980, 0.9991, 0.9967, 0.9943, 0.9967, 0.9989, 0.9991,\n",
      "        0.9990, 0.9995, 0.9998, 0.9998, 0.9999, 1.0000, 0.9987],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6601, 0.6667, 0.6699, 0.6716, 0.6724,\n",
      "        0.6728, 0.6730, 0.6731, 0.6731, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732,\n",
      "        0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.7056])\n",
      "finalReturns:  tensor([0.7993, 0.8317, 0.7906, 0.6969, 0.5653, 0.4061, 0.2267])\n",
      "----------------------------------------\n",
      "1   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0030, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.4134, 4.4134, 4.4134, 4.4134, 4.4134, 4.4134, 4.4134, 4.4134, 4.4134,\n",
      "        4.4134, 4.4134, 4.4134, 4.4134, 4.4134, 4.4134, 4.4134, 4.4134, 4.4134,\n",
      "        3.7078, 3.0757, 2.4962, 1.9546, 1.4406, 0.9468, 0.4679]) return=  164632.33345850307\n",
      "probs of actions:  tensor([0.9972, 0.9966, 0.9950, 0.9970, 0.9963, 0.9971, 0.9987, 0.9987, 0.9982,\n",
      "        0.9997, 0.9989, 0.9980, 0.9992, 0.9968, 0.9945, 0.9968, 0.9989, 0.9991,\n",
      "        0.9991, 0.9996, 0.9998, 0.9998, 0.9999, 1.0000, 0.9987],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6601, 0.6667, 0.6699, 0.6716, 0.6724,\n",
      "        0.6728, 0.6730, 0.6731, 0.6731, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732,\n",
      "        0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.7056])\n",
      "finalReturns:  tensor([1.0046, 1.0370, 0.9959, 0.9022, 0.7706, 0.6114, 0.4320, 0.2377])\n",
      "----------------------------------------\n",
      "1   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0044, grad_fn=<NegBackward0>)   ,  base rewards= tensor([4.8731, 4.8731, 4.8731, 4.8731, 4.8731, 4.8731, 4.8731, 4.8731, 4.8731,\n",
      "        4.8731, 4.8731, 4.8731, 4.8731, 4.8731, 4.8731, 4.8731, 4.8731, 4.1675,\n",
      "        3.5355, 2.9560, 2.4143, 1.9003, 1.4065, 0.9276, 0.4597]) return=  164632.33345850307\n",
      "probs of actions:  tensor([0.9973, 0.9967, 0.9951, 0.9971, 0.9964, 0.9972, 0.9988, 0.9988, 0.9983,\n",
      "        0.9997, 0.9989, 0.9981, 0.9992, 0.9969, 0.9946, 0.9969, 0.9990, 0.9992,\n",
      "        0.9992, 0.9996, 0.9998, 0.9998, 0.9999, 1.0000, 0.9986],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6601, 0.6667, 0.6699, 0.6716, 0.6724,\n",
      "        0.6728, 0.6730, 0.6731, 0.6731, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732,\n",
      "        0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.7056])\n",
      "finalReturns:  tensor([1.2181, 1.2505, 1.2093, 1.1156, 0.9841, 0.8249, 0.6455, 0.4512, 0.2459])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "15   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0036, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.3267, 5.3267, 5.3267, 5.3267, 5.3267, 5.3267, 5.3267, 5.3267, 5.3267,\n",
      "        5.3267, 5.3267, 5.3267, 5.3267, 5.3267, 5.3267, 5.3267, 4.6211, 3.9891,\n",
      "        3.4096, 2.8680, 2.3539, 1.8601, 1.3812, 0.9133, 0.4536]) return=  164632.33345850307\n",
      "probs of actions:  tensor([0.9984, 0.9980, 0.9971, 0.9982, 0.9979, 0.9983, 0.9992, 0.9993, 0.9990,\n",
      "        0.9998, 0.9993, 0.9989, 0.9995, 0.9982, 0.9968, 0.9991, 0.9997, 0.9997,\n",
      "        0.9996, 0.9998, 0.9999, 0.9999, 0.9999, 1.0000, 0.9985],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6601, 0.6667, 0.6699, 0.6716, 0.6724,\n",
      "        0.6728, 0.6730, 0.6731, 0.6731, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732,\n",
      "        0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.7056])\n",
      "finalReturns:  tensor([1.4377, 1.4701, 1.4289, 1.3352, 1.2036, 1.0445, 0.8651, 0.6708, 0.4655,\n",
      "        0.2520])\n",
      "----------------------------------------\n",
      "14   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0036, grad_fn=<NegBackward0>)   ,  base rewards= tensor([5.7758, 5.7758, 5.7758, 5.7758, 5.7758, 5.7758, 5.7758, 5.7758, 5.7758,\n",
      "        5.7758, 5.7758, 5.7758, 5.7758, 5.7758, 5.7758, 5.0702, 4.4382, 3.8587,\n",
      "        3.3171, 2.8030, 2.3092, 1.8303, 1.3624, 0.9027, 0.4491]) return=  164632.33345850307\n",
      "probs of actions:  tensor([0.9991, 0.9988, 0.9984, 0.9990, 0.9988, 0.9990, 0.9995, 0.9996, 0.9994,\n",
      "        0.9999, 0.9996, 0.9993, 0.9997, 0.9990, 0.9990, 0.9997, 0.9999, 0.9999,\n",
      "        0.9998, 0.9999, 0.9999, 0.9999, 0.9999, 1.0000, 0.9984],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6601, 0.6667, 0.6699, 0.6716, 0.6724,\n",
      "        0.6728, 0.6730, 0.6731, 0.6731, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732,\n",
      "        0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.7056])\n",
      "finalReturns:  tensor([1.6618, 1.6942, 1.6530, 1.5593, 1.4277, 1.2686, 1.0892, 0.8949, 0.6896,\n",
      "        0.4761, 0.2565])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([18, 18, 18, 18,  0, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0056, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.2209, 6.2209, 6.2209, 6.2209, 6.2209, 6.2209, 6.2209, 6.2209, 6.2209,\n",
      "        6.2209, 6.2209, 6.2209, 6.2209, 6.2209, 5.5156, 4.8837, 4.3043, 3.7627,\n",
      "        3.2487, 2.7549, 2.2760, 1.8081, 1.3484, 0.8948, 0.4457]) return=  163476.0224004586\n",
      "probs of actions:  tensor([9.9908e-01, 9.9883e-01, 9.9843e-01, 9.9899e-01, 7.8089e-04, 9.9918e-01,\n",
      "        9.9949e-01, 9.9957e-01, 9.9941e-01, 9.9986e-01, 9.9962e-01, 9.9937e-01,\n",
      "        9.9969e-01, 9.9901e-01, 9.9912e-01, 9.9968e-01, 9.9988e-01, 9.9989e-01,\n",
      "        9.9982e-01, 9.9987e-01, 9.9993e-01, 9.9994e-01, 9.9993e-01, 9.9999e-01,\n",
      "        9.9840e-01], grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6925, 0.5934, 0.6327, 0.6528, 0.6630,\n",
      "        0.6681, 0.6706, 0.6719, 0.6726, 0.6729, 0.6730, 0.6731, 0.6732, 0.6732,\n",
      "        0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.7056])\n",
      "finalReturns:  tensor([1.8893, 1.9217, 1.8805, 1.7868, 1.6552, 1.4961, 1.3167, 1.1224, 0.9171,\n",
      "        0.7036, 0.4840, 0.2599])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0064, grad_fn=<NegBackward0>)   ,  base rewards= tensor([6.6645, 6.6645, 6.6645, 6.6645, 6.6645, 6.6645, 6.6645, 6.6645, 6.6645,\n",
      "        6.6645, 6.6645, 6.6645, 6.6645, 5.9590, 5.3270, 4.7475, 4.2059, 3.6919,\n",
      "        3.1981, 2.7192, 2.2513, 1.7916, 1.3379, 0.8889, 0.4432]) return=  164632.33345850307\n",
      "probs of actions:  tensor([0.9991, 0.9989, 0.9985, 0.9990, 0.9989, 0.9991, 0.9996, 0.9996, 0.9994,\n",
      "        0.9999, 0.9996, 0.9994, 0.9997, 0.9991, 0.9992, 0.9997, 0.9999, 0.9999,\n",
      "        0.9998, 0.9999, 0.9999, 0.9999, 0.9999, 1.0000, 0.9984],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6601, 0.6667, 0.6699, 0.6716, 0.6724,\n",
      "        0.6728, 0.6730, 0.6731, 0.6731, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732,\n",
      "        0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.7056])\n",
      "finalReturns:  tensor([2.1194, 2.1518, 2.1106, 2.0169, 1.8853, 1.7261, 1.5467, 1.3524, 1.1471,\n",
      "        0.9336, 0.7141, 0.4899, 0.2624])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0079, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.1057, 7.1057, 7.1057, 7.1057, 7.1057, 7.1057, 7.1057, 7.1057, 7.1057,\n",
      "        7.1057, 7.1057, 7.1057, 6.4002, 5.7682, 5.1888, 4.6472, 4.1331, 3.6393,\n",
      "        3.1604, 2.6925, 2.2328, 1.7792, 1.3301, 0.8844, 0.4413]) return=  164632.33345850307\n",
      "probs of actions:  tensor([0.9991, 0.9989, 0.9985, 0.9991, 0.9989, 0.9991, 0.9996, 0.9996, 0.9995,\n",
      "        0.9999, 0.9996, 0.9994, 0.9997, 0.9991, 0.9992, 0.9997, 0.9999, 0.9999,\n",
      "        0.9998, 0.9999, 0.9999, 0.9999, 0.9999, 1.0000, 0.9984],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6601, 0.6667, 0.6699, 0.6716, 0.6724,\n",
      "        0.6728, 0.6730, 0.6731, 0.6731, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732,\n",
      "        0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.7056])\n",
      "finalReturns:  tensor([2.3513, 2.3837, 2.3425, 2.2488, 2.1172, 1.9581, 1.7787, 1.5844, 1.3791,\n",
      "        1.1656, 0.9460, 0.7219, 0.4944, 0.2643])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0087, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.5454, 7.5454, 7.5454, 7.5454, 7.5454, 7.5454, 7.5454, 7.5454, 7.5454,\n",
      "        7.5454, 7.5454, 6.8400, 6.2080, 5.6286, 5.0870, 4.5730, 4.0792, 3.6003,\n",
      "        3.1324, 2.6727, 2.2190, 1.7700, 1.3243, 0.8811, 0.4398]) return=  164632.33345850307\n",
      "probs of actions:  tensor([0.9992, 0.9989, 0.9986, 0.9991, 0.9990, 0.9991, 0.9996, 0.9996, 0.9995,\n",
      "        0.9999, 0.9997, 0.9994, 0.9997, 0.9992, 0.9993, 0.9997, 0.9999, 0.9999,\n",
      "        0.9998, 0.9999, 0.9999, 0.9999, 0.9999, 1.0000, 0.9984],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6601, 0.6667, 0.6699, 0.6716, 0.6724,\n",
      "        0.6728, 0.6730, 0.6731, 0.6731, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732,\n",
      "        0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.7056])\n",
      "finalReturns:  tensor([2.5846, 2.6170, 2.5759, 2.4822, 2.3506, 2.1914, 2.0120, 1.8177, 1.6124,\n",
      "        1.3989, 1.1794, 0.9552, 0.7277, 0.4977, 0.2658])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0089, grad_fn=<NegBackward0>)   ,  base rewards= tensor([7.9837, 7.9837, 7.9837, 7.9837, 7.9837, 7.9837, 7.9837, 7.9837, 7.9837,\n",
      "        7.9837, 7.2786, 6.6467, 6.0673, 5.5258, 5.0118, 4.5180, 4.0390, 3.5712,\n",
      "        3.1114, 2.6578, 2.2087, 1.7630, 1.3199, 0.8786, 0.4388]) return=  164632.33345850307\n",
      "probs of actions:  tensor([0.9992, 0.9990, 0.9986, 0.9991, 0.9990, 0.9992, 0.9996, 0.9996, 0.9995,\n",
      "        0.9999, 0.9997, 0.9995, 0.9997, 0.9993, 0.9993, 0.9997, 0.9999, 0.9999,\n",
      "        0.9998, 0.9999, 0.9999, 0.9999, 0.9999, 1.0000, 0.9984],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6601, 0.6667, 0.6699, 0.6716, 0.6724,\n",
      "        0.6728, 0.6730, 0.6731, 0.6731, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732,\n",
      "        0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.7056])\n",
      "finalReturns:  tensor([2.8190, 2.8514, 2.8103, 2.7166, 2.5850, 2.4258, 2.2464, 2.0521, 1.8468,\n",
      "        1.6334, 1.4138, 1.1897, 0.9622, 0.7321, 0.5002, 0.2668])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0101, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.4209, 8.4209, 8.4209, 8.4209, 8.4209, 8.4209, 8.4209, 8.4209, 8.4209,\n",
      "        7.7162, 7.0845, 6.5052, 5.9637, 5.4497, 4.9559, 4.4770, 4.0091, 3.5494,\n",
      "        3.0958, 2.6467, 2.2010, 1.7579, 1.3166, 0.8768, 0.4380]) return=  164632.33345850307\n",
      "probs of actions:  tensor([0.9992, 0.9990, 0.9987, 0.9992, 0.9991, 0.9992, 0.9996, 0.9997, 0.9995,\n",
      "        0.9999, 0.9997, 0.9995, 0.9998, 0.9993, 0.9994, 0.9998, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 1.0000, 0.9984],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6601, 0.6667, 0.6699, 0.6716, 0.6724,\n",
      "        0.6728, 0.6730, 0.6731, 0.6731, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732,\n",
      "        0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.7056])\n",
      "finalReturns:  tensor([3.0542, 3.0866, 3.0455, 2.9518, 2.8202, 2.6610, 2.4816, 2.2874, 2.0821,\n",
      "        1.8686, 1.6490, 1.4249, 1.1974, 0.9673, 0.7354, 0.5020, 0.2676])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0108, grad_fn=<NegBackward0>)   ,  base rewards= tensor([8.8568, 8.8568, 8.8568, 8.8568, 8.8568, 8.8568, 8.8568, 8.8568, 8.1528,\n",
      "        7.5215, 6.9424, 6.4010, 5.8871, 5.3933, 4.9144, 4.4465, 3.9868, 3.5332,\n",
      "        3.0841, 2.6384, 2.1953, 1.7540, 1.3142, 0.8754, 0.4374]) return=  164632.33345850307\n",
      "probs of actions:  tensor([0.9993, 0.9990, 0.9987, 0.9992, 0.9991, 0.9992, 0.9996, 0.9997, 0.9996,\n",
      "        0.9999, 0.9997, 0.9996, 0.9998, 0.9994, 0.9994, 0.9998, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 1.0000, 0.9983],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6601, 0.6667, 0.6699, 0.6716, 0.6724,\n",
      "        0.6728, 0.6730, 0.6731, 0.6731, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732,\n",
      "        0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.7056])\n",
      "finalReturns:  tensor([3.2900, 3.3224, 3.2812, 3.1876, 3.0560, 2.8968, 2.7175, 2.5232, 2.3179,\n",
      "        2.1044, 1.8848, 1.6607, 1.4332, 1.2031, 0.9712, 0.7378, 0.5034, 0.2682])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0117, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.2906, 9.2906, 9.2906, 9.2906, 9.2906, 9.2906, 9.2906, 8.5882, 7.9578,\n",
      "        7.3790, 6.8378, 6.3239, 5.8302, 5.3513, 4.8834, 4.4237, 3.9701, 3.5210,\n",
      "        3.0754, 2.6322, 2.1909, 1.7511, 1.3123, 0.8743, 0.4369]) return=  164632.33345850307\n",
      "probs of actions:  tensor([0.9993, 0.9991, 0.9988, 0.9992, 0.9991, 0.9993, 0.9996, 0.9997, 0.9996,\n",
      "        0.9999, 0.9997, 0.9996, 0.9998, 0.9994, 0.9995, 0.9998, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 1.0000, 0.9999, 1.0000, 0.9983],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6601, 0.6667, 0.6699, 0.6716, 0.6724,\n",
      "        0.6728, 0.6730, 0.6731, 0.6731, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732,\n",
      "        0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.7056])\n",
      "finalReturns:  tensor([3.5261, 3.5585, 3.5174, 3.4238, 3.2922, 3.1331, 2.9537, 2.7594, 2.5541,\n",
      "        2.3406, 2.1211, 1.8969, 1.6694, 1.4394, 1.2075, 0.9741, 0.7397, 0.5045,\n",
      "        0.2687])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0138, grad_fn=<NegBackward0>)   ,  base rewards= tensor([9.7209, 9.7209, 9.7209, 9.7209, 9.7209, 9.7209, 9.0218, 8.3929, 7.8149,\n",
      "        7.2740, 6.7604, 6.2667, 5.7879, 5.3200, 4.8603, 4.4067, 3.9577, 3.5120,\n",
      "        3.0688, 2.6275, 2.1877, 1.7489, 1.3109, 0.8735, 0.4366]) return=  164632.33345850307\n",
      "probs of actions:  tensor([0.9993, 0.9991, 0.9989, 0.9993, 0.9992, 0.9993, 0.9997, 0.9997, 0.9996,\n",
      "        0.9999, 0.9998, 0.9996, 0.9998, 0.9995, 0.9995, 0.9998, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 1.0000, 0.9999, 1.0000, 0.9983],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6601, 0.6667, 0.6699, 0.6716, 0.6724,\n",
      "        0.6728, 0.6730, 0.6731, 0.6731, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732,\n",
      "        0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.7056])\n",
      "finalReturns:  tensor([3.7624, 3.7948, 3.7538, 3.6602, 3.5288, 3.3696, 3.1903, 2.9960, 2.7907,\n",
      "        2.5772, 2.3577, 2.1335, 1.9060, 1.6760, 1.4441, 1.2107, 0.9763, 0.7411,\n",
      "        0.5053, 0.2690])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0163, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.1448, 10.1448, 10.1448, 10.1448, 10.1448,  9.4522,  8.8264,  8.2499,\n",
      "         7.7097,  7.1964,  6.7029,  6.2242,  5.7563,  5.2967,  4.8431,  4.3940,\n",
      "         3.9483,  3.5052,  3.0639,  2.6241,  2.1853,  1.7473,  1.3099,  0.8730,\n",
      "         0.4364]) return=  164632.33345850307\n",
      "probs of actions:  tensor([0.9993, 0.9992, 0.9989, 0.9993, 0.9992, 0.9993, 0.9997, 0.9997, 0.9996,\n",
      "        0.9999, 0.9998, 0.9997, 0.9998, 0.9995, 0.9996, 0.9998, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 1.0000, 1.0000, 0.9999, 1.0000, 0.9983],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6601, 0.6667, 0.6699, 0.6716, 0.6724,\n",
      "        0.6728, 0.6730, 0.6731, 0.6731, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732,\n",
      "        0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.7056])\n",
      "finalReturns:  tensor([3.9987, 4.0311, 3.9903, 3.8969, 3.7655, 3.6064, 3.4271, 3.2328, 3.0276,\n",
      "        2.8141, 2.5945, 2.3704, 2.1429, 1.9128, 1.6809, 1.4475, 1.2131, 0.9779,\n",
      "        0.7421, 0.5058, 0.2692])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0183, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.5560, 10.5560, 10.5560, 10.5560,  9.8764,  9.2568,  8.6832,  8.1445,\n",
      "         7.6319,  7.1387,  6.6602,  6.1924,  5.7328,  5.2792,  4.8302,  4.3845,\n",
      "         3.9413,  3.5001,  3.0602,  2.6214,  2.1835,  1.7461,  1.3091,  0.8725,\n",
      "         0.4362]) return=  164632.33345850307\n",
      "probs of actions:  tensor([0.9994, 0.9992, 0.9990, 0.9993, 0.9993, 0.9994, 0.9997, 0.9998, 0.9997,\n",
      "        0.9999, 0.9998, 0.9997, 0.9998, 0.9996, 0.9996, 0.9998, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 1.0000, 1.0000, 0.9999, 1.0000, 0.9983],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6601, 0.6667, 0.6699, 0.6716, 0.6724,\n",
      "        0.6728, 0.6730, 0.6731, 0.6731, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732,\n",
      "        0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.7056])\n",
      "finalReturns:  tensor([4.2346, 4.2670, 4.2265, 4.1334, 4.0022, 3.8433, 3.6640, 3.4698, 3.2646,\n",
      "        3.0511, 2.8315, 2.6074, 2.3799, 2.1499, 1.9179, 1.6846, 1.4502, 1.2149,\n",
      "        0.9791, 0.7429, 0.5063, 0.2694])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0216, grad_fn=<NegBackward0>)   ,  base rewards= tensor([10.9428, 10.9428, 10.9428, 10.2887,  9.6813,  9.1136,  8.5777,  8.0665,\n",
      "         7.5741,  7.0958,  6.6283,  6.1687,  5.7152,  5.2662,  4.8205,  4.3773,\n",
      "         3.9361,  3.4962,  3.0575,  2.6195,  2.1821,  1.7451,  1.3085,  0.8722,\n",
      "         0.4360]) return=  164632.33345850307\n",
      "probs of actions:  tensor([0.9994, 0.9993, 0.9990, 0.9994, 0.9993, 0.9995, 0.9997, 0.9998, 0.9997,\n",
      "        0.9999, 0.9998, 0.9997, 0.9998, 0.9996, 0.9996, 0.9998, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 1.0000, 1.0000, 0.9999, 1.0000, 0.9983],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6601, 0.6667, 0.6699, 0.6716, 0.6724,\n",
      "        0.6728, 0.6730, 0.6731, 0.6731, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732,\n",
      "        0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.7056])\n",
      "finalReturns:  tensor([4.4695, 4.5019, 4.4621, 4.3697, 4.2389, 4.0802, 3.9011, 3.7069, 3.5017,\n",
      "        3.2883, 3.0687, 2.8446, 2.6171, 2.3870, 2.1551, 1.9218, 1.6873, 1.4521,\n",
      "        1.2163, 0.9801, 0.7435, 0.5066, 0.2696])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0235, grad_fn=<NegBackward0>)   ,  base rewards= tensor([11.2821, 11.2821, 10.6776, 10.0943,  9.5383,  9.0082,  8.4997,  8.0087,\n",
      "         7.5311,  7.0639,  6.6045,  6.1510,  5.7020,  5.2564,  4.8132,  4.3720,\n",
      "         3.9322,  3.4934,  3.0554,  2.6180,  2.1811,  1.7445,  1.3081,  0.8719,\n",
      "         0.4359]) return=  164632.33345850307\n",
      "probs of actions:  tensor([0.9994, 0.9993, 0.9991, 0.9994, 0.9994, 0.9995, 0.9998, 0.9998, 0.9997,\n",
      "        0.9999, 0.9998, 0.9997, 0.9999, 0.9997, 0.9997, 0.9998, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 0.9982],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6601, 0.6667, 0.6699, 0.6716, 0.6724,\n",
      "        0.6728, 0.6730, 0.6731, 0.6731, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732,\n",
      "        0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.7056])\n",
      "finalReturns:  tensor([4.7023, 4.7347, 4.6963, 4.6051, 4.4751, 4.3169, 4.1381, 3.9441, 3.7389,\n",
      "        3.5255, 3.3060, 3.0819, 2.8544, 2.6243, 2.3924, 2.1590, 1.9246, 1.6894,\n",
      "        1.4536, 1.2173, 0.9807, 0.7439, 0.5069, 0.2697])\n",
      "----------------------------------------\n",
      "0   adversary:  AdversaryModes.myopic\n",
      "  actions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18,  0])\n",
      "loss=  tensor(0.0243, grad_fn=<NegBackward0>)   ,  base rewards= tensor([11.5326, 11.0213, 10.4848,  9.9519,  9.4330,  8.9302,  8.4418,  7.9656,\n",
      "         7.4991,  7.0400,  6.5867,  6.1378,  5.6922,  5.2491,  4.8078,  4.3680,\n",
      "         3.9292,  3.4912,  3.0538,  2.6169,  2.1803,  1.7439,  1.3078,  0.8718,\n",
      "         0.4358]) return=  164632.33345850307\n",
      "probs of actions:  tensor([0.9995, 0.9994, 0.9992, 0.9995, 0.9995, 0.9996, 0.9998, 0.9998, 0.9998,\n",
      "        0.9999, 0.9998, 0.9998, 0.9999, 0.9997, 0.9997, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 0.9982],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "rewards:  tensor([0.4788, 0.5721, 0.6217, 0.6472, 0.6601, 0.6667, 0.6699, 0.6716, 0.6724,\n",
      "        0.6728, 0.6730, 0.6731, 0.6731, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732,\n",
      "        0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.6732, 0.7056])\n",
      "finalReturns:  tensor([4.9307, 4.9631, 4.9275, 4.8387, 4.7104, 4.5531, 4.3748, 4.1811, 3.9761,\n",
      "        3.7628, 3.5433, 3.3192, 3.0917, 2.8617, 2.6298, 2.3964, 2.1620, 1.9268,\n",
      "        1.6910, 1.4547, 1.2181, 0.9813, 0.7442, 0.5070, 0.2698])\n",
      "0,[0.0001,1][1, 10000, 1, 1],1682339633 saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55690, 'tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])', 164632.33345850307, 70408.33325584729, 0.024329129606485367, 0.0001, 1, 0, 'tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\\n        18, 18, 18, 18, 18, 18,  0])', '[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\\n 1.]', '0,[0.0001,1][1, 10000, 1, 1],1682339633', 25, 50, 164632.33345850307, 193521.20851536895, 78111.91688437786, 135363.21866666665, 132442.06666666665, 102604.52812037412, 102604.52812037412, 120699.79187222247, 120699.79187222247, 90621.14382460734, 102604.52812037412, 120699.79187222247]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "neuralNet=NNBase(num_input=game.T+2+game.advHistoryNum, lr=hyperParams[0],num_actions=50)\n",
    "algorithm = ReinforceAlgorithm(game, neuralNet, numberIterations=2, numberEpisodes=3_000_000, discountFactor =hyperParams[1])\n",
    "\n",
    "\n",
    "algorithm.solver(print_step=100_000,options=codeParams,converge_break=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
