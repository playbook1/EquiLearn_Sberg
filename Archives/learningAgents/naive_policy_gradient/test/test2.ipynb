{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learningAgents import ReinforceAlgorithm\n",
    "from environmentModel import Model, AdversaryModes\n",
    "from NeuralNetwork import NeuralNetwork\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adversaryProbs=torch.zeros(len(AdversaryModes))\n",
    "adversaryProbs[0]=1\n",
    "adversaryProbs[1]=0\n",
    "adversaryProbs[8]=0\n",
    "game = Model(totalDemand = 400, \n",
    "               tupleCosts = (57, 71),\n",
    "              totalStages = 3, adversaryProbs=adversaryProbs, advHistoryNum=0)\n",
    "adversaryProbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralNet=NeuralNetwork(num_input=3+game.advHistoryNum, lr=0.0009,num_actions=4)\n",
    "algorithm = ReinforceAlgorithm(game, neuralNet, numberIterations=1, numberEpisodes=1_000_000, discountFactor =0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "0   actions:  tensor([3, 1, 3])\n",
      "loss=  tensor(8.5972, grad_fn=<DivBackward0>)   , return=  16156.8125\n",
      "discReturns/1000= tensor([6.4214, 6.5907, 5.5786])\n",
      "actionProbs tensor([[0.3286, 0.3392, 0.3316, 0.3318],\n",
      "        [0.3324, 0.3323, 0.3339, 0.3332],\n",
      "        [0.3390, 0.3285, 0.3346, 0.3350]], grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "1000   actions:  tensor([3, 3, 2])\n",
      "loss=  tensor(10.7480, grad_fn=<DivBackward0>)   , return=  16228.8125\n",
      "discReturns/1000= tensor([6.4230, 6.5987, 5.6586])\n",
      "actionProbs tensor([[0.2824, 0.1744, 0.5969, 0.3024],\n",
      "        [0.3242, 0.3013, 0.2806, 0.3261],\n",
      "        [0.3934, 0.5243, 0.1225, 0.3715]], grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "2000   actions:  tensor([2, 0, 1])\n",
      "loss=  tensor(7.8665, grad_fn=<DivBackward0>)   , return=  16077.953125\n",
      "discReturns/1000= tensor([6.4173, 6.5452, 5.5306])\n",
      "actionProbs tensor([[0.2951, 0.2840, 0.4037, 0.2193],\n",
      "        [0.3287, 0.3363, 0.3281, 0.3129],\n",
      "        [0.3762, 0.3797, 0.2682, 0.4677]], grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "3000   actions:  tensor([0, 3, 2])\n",
      "loss=  tensor(8.4739, grad_fn=<DivBackward0>)   , return=  16071.078125\n",
      "discReturns/1000= tensor([6.4077, 6.4770, 5.6023])\n",
      "actionProbs tensor([[0.3476, 0.3499, 0.3314, 0.2664],\n",
      "        [0.3336, 0.3336, 0.3341, 0.3275],\n",
      "        [0.3188, 0.3165, 0.3345, 0.4061]], grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "4000   actions:  tensor([0, 2, 0])\n",
      "loss=  tensor(8.5201, grad_fn=<DivBackward0>)   , return=  16042.703125\n",
      "discReturns/1000= tensor([6.4073, 6.4753, 5.5689])\n",
      "actionProbs tensor([[0.3301, 0.3539, 0.3489, 0.2708],\n",
      "        [0.3341, 0.3316, 0.3342, 0.3313],\n",
      "        [0.3357, 0.3145, 0.3169, 0.3980]], grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "5000   actions:  tensor([2, 3, 3])\n",
      "loss=  tensor(7.9764, grad_fn=<DivBackward0>)   , return=  16173.078125\n",
      "discReturns/1000= tensor([6.4197, 6.5570, 5.6348])\n",
      "actionProbs tensor([[0.3087, 0.3108, 0.4016, 0.2911],\n",
      "        [0.3351, 0.3348, 0.3288, 0.3303],\n",
      "        [0.3562, 0.3544, 0.2696, 0.3787]], grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "6000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(8.6772, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[0.3256, 0.3489, 0.3854, 0.3115],\n",
      "        [0.3412, 0.3360, 0.3336, 0.3221],\n",
      "        [0.3332, 0.3151, 0.2811, 0.3665]], grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "7000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(8.6203, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[0.2421, 0.2995, 0.3926, 0.3446],\n",
      "        [0.3361, 0.3361, 0.3369, 0.3209],\n",
      "        [0.4218, 0.3644, 0.2705, 0.3346]], grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "8000   actions:  tensor([3, 2, 2])\n",
      "loss=  tensor(8.6322, grad_fn=<DivBackward0>)   , return=  16196.25\n",
      "discReturns/1000= tensor([6.4225, 6.5962, 5.6210])\n",
      "actionProbs tensor([[1.5882e-05, 2.5634e-04, 2.1196e-04, 4.6841e-01],\n",
      "        [5.5804e-03, 2.0103e-02, 1.8710e-02, 3.0317e-01],\n",
      "        [9.9440e-01, 9.7964e-01, 9.8108e-01, 2.2842e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "9000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(3.3007, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[7.8612e-07, 8.0090e-05, 2.1400e-05, 7.3793e-02],\n",
      "        [1.1663e-03, 1.0852e-02, 5.6212e-03, 2.2881e-01],\n",
      "        [9.9883e-01, 9.8907e-01, 9.9436e-01, 6.9740e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "10000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6138, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.5829e-07, 2.6023e-05, 1.4338e-05, 6.2198e-02],\n",
      "        [6.7807e-04, 7.3606e-03, 5.3987e-03, 2.1184e-01],\n",
      "        [9.9932e-01, 9.9261e-01, 9.9459e-01, 7.2596e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "11000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.5315, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[6.3690e-08, 2.1721e-06, 1.8790e-06, 5.8306e-02],\n",
      "        [3.1426e-04, 1.6998e-03, 1.5175e-03, 1.9521e-01],\n",
      "        [9.9969e-01, 9.9830e-01, 9.9848e-01, 7.4649e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "12000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.3721, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[3.0246e-08, 2.9942e-06, 6.2092e-07, 2.2259e-01],\n",
      "        [1.3759e-04, 1.3827e-03, 5.9490e-04, 2.9462e-01],\n",
      "        [9.9986e-01, 9.9861e-01, 9.9940e-01, 4.8279e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "13000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.2460, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.4388e-07, 1.4764e-05, 8.8994e-06, 7.0919e-01],\n",
      "        [7.5112e-04, 6.0675e-03, 4.9288e-03, 2.0312e-01],\n",
      "        [9.9925e-01, 9.9392e-01, 9.9506e-01, 8.7698e-02]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "14000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(3.5155, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[5.7373e-06, 1.2727e-05, 9.9829e-05, 9.2344e-02],\n",
      "        [6.8391e-03, 9.1174e-03, 2.1370e-02, 2.4354e-01],\n",
      "        [9.9316e-01, 9.9087e-01, 9.7853e-01, 6.6412e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "15000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6073, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[7.2063e-08, 1.4871e-06, 5.4445e-06, 5.7875e-02],\n",
      "        [1.0088e-03, 3.5806e-03, 5.7766e-03, 1.9221e-01],\n",
      "        [9.9899e-01, 9.9642e-01, 9.9422e-01, 7.4992e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "16000   actions:  tensor([3, 3, 2])\n",
      "loss=  tensor(2.2634, grad_fn=<DivBackward0>)   , return=  16228.8125\n",
      "discReturns/1000= tensor([6.4230, 6.5987, 5.6586])\n",
      "actionProbs tensor([[2.3458e-08, 1.3848e-06, 3.8618e-06, 6.5873e-01],\n",
      "        [6.5929e-04, 3.6538e-03, 5.0796e-03, 2.0044e-01],\n",
      "        [9.9934e-01, 9.9634e-01, 9.9492e-01, 1.4083e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "17000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.5754, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[9.1633e-08, 3.2491e-06, 6.3334e-06, 1.4541e-01],\n",
      "        [1.9192e-03, 7.9411e-03, 8.8721e-03, 2.4797e-01],\n",
      "        [9.9808e-01, 9.9206e-01, 9.9112e-01, 6.0662e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "18000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6652, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[8.7142e-09, 5.7233e-07, 2.0734e-06, 7.3182e-02],\n",
      "        [1.0711e-03, 5.5944e-03, 7.0187e-03, 1.7468e-01],\n",
      "        [9.9893e-01, 9.9441e-01, 9.9298e-01, 7.5214e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "19000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(4.6548, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[2.3689e-09, 1.4372e-07, 7.7128e-07, 5.2753e-01],\n",
      "        [1.4396e-03, 6.2140e-03, 7.3469e-03, 1.6393e-01],\n",
      "        [9.9856e-01, 9.9379e-01, 9.9265e-01, 3.0855e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "20000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(3.1496, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[6.8194e-11, 3.5509e-09, 3.7815e-07, 9.7024e-02],\n",
      "        [7.4135e-04, 2.4334e-03, 7.2527e-03, 1.2607e-01],\n",
      "        [9.9926e-01, 9.9757e-01, 9.9275e-01, 7.7691e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "21000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.9890, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[2.4957e-13, 7.0856e-10, 5.4380e-07, 4.6608e-01],\n",
      "        [3.6818e-04, 5.4993e-03, 1.4038e-02, 5.9072e-02],\n",
      "        [9.9963e-01, 9.9450e-01, 9.8596e-01, 4.7485e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "22000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6375, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[3.9071e-18, 2.9552e-15, 1.4249e-12, 5.2359e-04],\n",
      "        [1.1079e-06, 9.8759e-06, 3.3254e-05, 1.0676e-02],\n",
      "        [1.0000e+00, 9.9999e-01, 9.9997e-01, 9.8880e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "23000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6188, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.8160e-20, 1.5631e-17, 6.2342e-15, 4.3529e-05],\n",
      "        [6.7610e-08, 6.8124e-07, 2.1015e-06, 2.8402e-03],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9712e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "24000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6150, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[7.1741e-22, 6.0180e-19, 1.9990e-16, 8.3961e-06],\n",
      "        [1.0992e-08, 1.1309e-07, 3.2102e-07, 1.1235e-03],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9887e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "25000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6137, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[5.6349e-23, 4.3728e-20, 1.2745e-17, 2.2390e-06],\n",
      "        [2.6384e-09, 2.6724e-08, 7.2393e-08, 5.4415e-04],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9945e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "26000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6131, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[5.8834e-24, 4.1747e-21, 1.0797e-18, 6.4847e-07],\n",
      "        [7.4794e-10, 7.3190e-09, 1.9164e-08, 2.8236e-04],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9972e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "27000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6128, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[7.1702e-25, 4.5489e-22, 1.0513e-19, 1.9951e-07],\n",
      "        [2.2860e-10, 2.1289e-09, 5.3839e-09, 1.4996e-04],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9985e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "28000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6127, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[9.6553e-26, 5.3853e-23, 1.1272e-20, 6.4691e-08],\n",
      "        [7.3224e-11, 6.4136e-10, 1.5694e-09, 8.1081e-05],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9992e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "29000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6126, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.4280e-26, 6.9684e-24, 1.3631e-21, 2.2850e-08],\n",
      "        [2.4239e-11, 1.9774e-10, 4.6941e-10, 4.4414e-05],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9996e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "30000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6126, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[2.2357e-27, 9.5202e-25, 1.7646e-22, 8.4435e-09],\n",
      "        [8.2161e-12, 6.1965e-11, 1.4308e-10, 2.4566e-05],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9998e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "31000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[3.6423e-28, 1.3466e-25, 2.3902e-23, 3.2023e-09],\n",
      "        [2.8345e-12, 1.9648e-11, 4.4254e-11, 1.3690e-05],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "32000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[5.0123e-29, 1.6581e-26, 3.0050e-24, 1.1724e-09],\n",
      "        [8.5735e-13, 5.6297e-12, 1.3259e-11, 7.6426e-06],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "33000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[7.5761e-30, 2.2692e-27, 4.0730e-25, 4.2561e-10],\n",
      "        [2.7162e-13, 1.6986e-12, 4.1303e-12, 4.2544e-06],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "34000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.2227e-30, 3.3221e-28, 5.7755e-26, 1.5748e-10],\n",
      "        [8.9462e-14, 5.3186e-13, 1.3159e-12, 2.3862e-06],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "35000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[2.0729e-31, 5.1493e-29, 8.5921e-27, 6.0657e-11],\n",
      "        [3.0096e-14, 1.7012e-13, 4.2481e-13, 1.3546e-06],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "36000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[3.7310e-32, 8.3595e-30, 1.3158e-27, 2.3438e-11],\n",
      "        [1.0473e-14, 5.5746e-14, 1.3883e-13, 7.6901e-07],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "37000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[6.6903e-33, 1.4119e-30, 2.0903e-28, 9.4402e-12],\n",
      "        [3.6922e-15, 1.8844e-14, 4.6065e-14, 4.4068e-07],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "38000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[2.0833e-33, 4.2062e-31, 5.7528e-29, 4.7857e-12],\n",
      "        [1.5681e-15, 7.4584e-15, 1.7252e-14, 2.5816e-07],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "39000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[3.3734e-33, 8.7529e-31, 1.1683e-28, 6.4321e-12],\n",
      "        [1.1013e-15, 4.9850e-15, 1.0875e-14, 1.5901e-07],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "40000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.0015e-31, 5.9655e-29, 1.2531e-26, 4.7397e-10],\n",
      "        [7.4240e-16, 3.0552e-15, 6.6707e-15, 1.5914e-07],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "41000   actions:  tensor([3, 3, 2])\n",
      "loss=  tensor(2.6148, grad_fn=<DivBackward0>)   , return=  16228.8125\n",
      "discReturns/1000= tensor([6.4230, 6.5987, 5.6586])\n",
      "actionProbs tensor([[1.9873e-30, 2.4235e-27, 9.0478e-25, 7.2979e-09],\n",
      "        [4.0457e-16, 1.6398e-15, 2.9693e-15, 9.7609e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "42000   actions:  tensor([3, 3, 2])\n",
      "loss=  tensor(2.6147, grad_fn=<DivBackward0>)   , return=  16228.8125\n",
      "discReturns/1000= tensor([6.4230, 6.5987, 5.6586])\n",
      "actionProbs tensor([[2.0055e-26, 8.1203e-24, 5.0581e-21, 1.4950e-04],\n",
      "        [8.8139e-14, 2.0936e-13, 7.1152e-13, 8.6945e-05],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9976e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "43000   actions:  tensor([3, 3, 2])\n",
      "loss=  tensor(2.6146, grad_fn=<DivBackward0>)   , return=  16228.8125\n",
      "discReturns/1000= tensor([6.4230, 6.5987, 5.6586])\n",
      "actionProbs tensor([[9.4274e-26, 3.6564e-23, 1.8969e-20, 2.4114e-04],\n",
      "        [2.6514e-13, 6.4059e-13, 2.0255e-12, 1.4879e-04],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9961e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "44000   actions:  tensor([3, 3, 2])\n",
      "loss=  tensor(2.6147, grad_fn=<DivBackward0>)   , return=  16228.8125\n",
      "discReturns/1000= tensor([6.4230, 6.5987, 5.6586])\n",
      "actionProbs tensor([[1.2787e-25, 4.5322e-23, 1.9264e-20, 1.5595e-04],\n",
      "        [2.9997e-13, 6.7166e-13, 1.9108e-12, 1.1545e-04],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9973e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "45000   actions:  tensor([3, 3, 2])\n",
      "loss=  tensor(2.6147, grad_fn=<DivBackward0>)   , return=  16228.8125\n",
      "discReturns/1000= tensor([6.4230, 6.5987, 5.6586])\n",
      "actionProbs tensor([[1.6320e-25, 5.2615e-23, 1.8190e-20, 9.4858e-05],\n",
      "        [3.2291e-13, 6.6516e-13, 1.6934e-12, 8.5278e-05],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9982e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "46000   actions:  tensor([3, 3, 2])\n",
      "loss=  tensor(2.6148, grad_fn=<DivBackward0>)   , return=  16228.8125\n",
      "discReturns/1000= tensor([6.4230, 6.5987, 5.6586])\n",
      "actionProbs tensor([[1.3016e-25, 3.6331e-23, 9.7064e-21, 3.5978e-05],\n",
      "        [2.3154e-13, 4.1376e-13, 9.0716e-13, 4.2732e-05],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9992e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "47000   actions:  tensor([3, 3, 2])\n",
      "loss=  tensor(2.6148, grad_fn=<DivBackward0>)   , return=  16228.8125\n",
      "discReturns/1000= tensor([6.4230, 6.5987, 5.6586])\n",
      "actionProbs tensor([[2.2762e-25, 5.5365e-23, 1.0154e-20, 2.1103e-05],\n",
      "        [3.5141e-13, 5.8887e-13, 1.0934e-12, 4.1770e-05],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9994e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "48000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[7.5066e-26, 1.8331e-23, 2.4647e-21, 5.8317e-06],\n",
      "        [1.5379e-13, 2.9672e-13, 5.1798e-13, 2.1936e-05],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9997e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "49000   actions:  tensor([3, 3, 2])\n",
      "loss=  tensor(2.6148, grad_fn=<DivBackward0>)   , return=  16228.8125\n",
      "discReturns/1000= tensor([6.4230, 6.5987, 5.6586])\n",
      "actionProbs tensor([[6.4663e-26, 1.5262e-23, 1.9625e-21, 4.4625e-06],\n",
      "        [1.3677e-13, 2.5809e-13, 4.4273e-13, 1.8447e-05],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9998e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "50000   actions:  tensor([3, 3, 2])\n",
      "loss=  tensor(2.6148, grad_fn=<DivBackward0>)   , return=  16228.8125\n",
      "discReturns/1000= tensor([6.4230, 6.5987, 5.6586])\n",
      "actionProbs tensor([[3.1816e-26, 6.9637e-24, 7.9317e-22, 1.9920e-06],\n",
      "        [7.9254e-14, 1.4695e-13, 2.4362e-13, 1.0745e-05],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "51000   actions:  tensor([3, 3, 2])\n",
      "loss=  tensor(2.6148, grad_fn=<DivBackward0>)   , return=  16228.8125\n",
      "discReturns/1000= tensor([6.4230, 6.5987, 5.6586])\n",
      "actionProbs tensor([[2.0470e-26, 4.4672e-24, 5.5397e-22, 2.1002e-06],\n",
      "        [6.1377e-14, 1.1628e-13, 2.0750e-13, 1.1920e-05],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "52000   actions:  tensor([3, 3, 2])\n",
      "loss=  tensor(2.5966, grad_fn=<DivBackward0>)   , return=  16228.8125\n",
      "discReturns/1000= tensor([6.4230, 6.5987, 5.6586])\n",
      "actionProbs tensor([[1.7470e-22, 3.6839e-20, 9.7810e-18, 1.5452e-02],\n",
      "        [3.7168e-11, 1.2197e-10, 5.0884e-10, 2.2963e-02],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.6158e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "53000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6602, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[2.5258e-22, 4.0124e-20, 7.0588e-18, 1.1410e-02],\n",
      "        [4.9757e-11, 1.4744e-10, 5.2348e-10, 2.1620e-02],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.6697e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "54000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6408, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[2.8273e-22, 3.1113e-20, 3.5620e-18, 5.1045e-03],\n",
      "        [5.0078e-11, 1.3584e-10, 4.0033e-10, 1.4648e-02],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.8025e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "55000   actions:  tensor([3, 3, 2])\n",
      "loss=  tensor(2.6081, grad_fn=<DivBackward0>)   , return=  16228.8125\n",
      "discReturns/1000= tensor([6.4230, 6.5987, 5.6586])\n",
      "actionProbs tensor([[5.2250e-22, 3.2030e-20, 2.1185e-18, 2.5598e-03],\n",
      "        [8.1195e-11, 1.7004e-10, 3.6710e-10, 1.1610e-02],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.8583e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "56000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6112, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[5.4141e-22, 1.6291e-20, 4.7984e-19, 1.4872e-03],\n",
      "        [8.7602e-11, 1.3350e-10, 1.8220e-10, 9.1650e-03],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.8935e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "57000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6137, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[2.6464e-22, 4.7960e-21, 8.0554e-20, 5.8641e-04],\n",
      "        [5.4853e-11, 7.2635e-11, 8.1047e-11, 5.7412e-03],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9367e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "58000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.0738, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.8691e-27, 1.9781e-25, 1.2452e-21, 9.9400e-01],\n",
      "        [5.8825e-13, 2.1512e-12, 3.9599e-11, 5.6991e-03],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 2.9822e-04]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "59000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.0733, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[4.8424e-30, 3.4585e-28, 4.0191e-23, 9.9976e-01],\n",
      "        [4.8354e-14, 1.5089e-13, 9.0972e-12, 2.3453e-04],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.3764e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "60000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.0737, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.8777e-30, 1.1759e-28, 3.3625e-24, 9.9992e-01],\n",
      "        [3.0546e-14, 8.9629e-14, 2.5763e-12, 8.1645e-05],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.9947e-07]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "61000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.0737, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.2329e-30, 7.3419e-29, 2.6653e-24, 9.9995e-01],\n",
      "        [2.4882e-14, 7.1238e-14, 2.2975e-12, 4.7615e-05],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 7.3821e-08]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "62000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6173, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[2.2428e-16, 5.4426e-19, 1.1395e-19, 6.0320e-05],\n",
      "        [6.4070e-07, 8.8627e-08, 1.3839e-08, 2.8990e-03],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9704e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "63000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6138, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[2.6640e-18, 5.6356e-21, 1.1191e-21, 8.4603e-06],\n",
      "        [4.2868e-08, 5.9147e-09, 8.4791e-10, 8.3519e-04],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9916e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "64000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6131, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[2.1529e-19, 4.3584e-22, 8.9382e-23, 2.4855e-06],\n",
      "        [9.1362e-09, 1.3237e-09, 1.8871e-10, 3.8797e-04],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9961e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "65000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6128, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[2.3482e-20, 4.5172e-23, 9.6586e-24, 8.4316e-07],\n",
      "        [2.3014e-09, 3.4660e-10, 4.9776e-11, 1.9638e-04],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9980e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "66000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6127, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[3.0078e-21, 5.4506e-24, 1.2119e-24, 3.0772e-07],\n",
      "        [6.3221e-10, 9.7885e-11, 1.4202e-11, 1.0362e-04],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9990e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "67000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6126, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[4.1509e-22, 7.0941e-25, 1.6510e-25, 1.1655e-07],\n",
      "        [1.8361e-10, 2.8906e-11, 4.2489e-12, 5.6081e-05],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9994e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "68000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[6.0079e-23, 9.7317e-26, 2.3145e-26, 4.5222e-08],\n",
      "        [5.5113e-11, 8.7740e-12, 1.2996e-12, 3.0823e-05],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9997e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "69000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[9.0559e-24, 1.3978e-26, 3.5313e-27, 1.7882e-08],\n",
      "        [1.6891e-11, 2.7133e-12, 4.0858e-13, 1.7112e-05],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9998e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "70000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.4082e-24, 2.0991e-27, 5.1479e-28, 7.1969e-09],\n",
      "        [5.2565e-12, 8.5205e-13, 1.2734e-13, 9.5711e-06],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "71000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[2.2375e-25, 3.2064e-28, 1.5914e-28, 2.9086e-09],\n",
      "        [1.6534e-12, 2.6960e-13, 4.7012e-14, 5.3814e-06],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "72000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[3.6233e-26, 4.9814e-29, 1.6749e-28, 1.1903e-09],\n",
      "        [5.2413e-13, 8.5377e-14, 2.1259e-14, 3.0378e-06],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "73000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[6.0195e-27, 8.3778e-30, 7.0729e-30, 5.0994e-10],\n",
      "        [1.6803e-13, 2.7586e-14, 4.5082e-15, 1.7221e-06],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "74000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.3908e-27, 1.9039e-30, 5.0544e-31, 2.0618e-10],\n",
      "        [6.6118e-14, 1.1053e-14, 1.3870e-15, 9.8805e-07],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "75000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[4.4481e-28, 9.3874e-31, 1.2025e-31, 1.1008e-10],\n",
      "        [3.0795e-14, 5.5739e-15, 5.8589e-16, 5.7094e-07],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "76000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5887e-28, 4.6031e-31, 2.8797e-32, 4.1960e-11],\n",
      "        [1.5845e-14, 3.1508e-15, 2.8278e-16, 3.2636e-07],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "77000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[5.2130e-29, 2.1514e-31, 6.0983e-33, 1.5508e-11],\n",
      "        [7.8280e-15, 1.6891e-15, 1.3220e-16, 1.8806e-07],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "78000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[2.4475e-29, 2.3883e-31, 4.9154e-33, 1.3756e-11],\n",
      "        [4.3354e-15, 1.0485e-15, 7.6219e-17, 1.1493e-07],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "79000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[8.3994e-30, 1.1124e-31, 1.6827e-33, 5.5961e-12],\n",
      "        [2.1868e-15, 5.6889e-16, 3.8875e-17, 6.3054e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "80000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.3113e-29, 3.2348e-31, 5.7587e-33, 6.5808e-12],\n",
      "        [2.4269e-15, 1.0088e-15, 7.3743e-17, 5.9587e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "81000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.1732e-29, 1.2504e-31, 4.3971e-33, 3.9116e-12],\n",
      "        [2.2302e-15, 1.1576e-15, 1.3110e-16, 5.9551e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "82000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[7.8684e-30, 5.2519e-32, 4.6627e-34, 1.5016e-12],\n",
      "        [1.7309e-15, 1.6430e-15, 1.5096e-16, 6.0270e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "83000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[2.5110e-29, 1.2370e-32, 4.3814e-35, 2.2260e-13],\n",
      "        [2.2945e-15, 2.6768e-15, 2.2243e-16, 5.0896e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "84000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5531e-29, 7.3333e-34, 3.5366e-37, 4.8166e-14],\n",
      "        [1.4154e-15, 7.7630e-16, 3.7224e-17, 4.5317e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "85000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.6127e-29, 7.5802e-34, 3.6583e-37, 4.8422e-14],\n",
      "        [1.4285e-15, 7.8366e-16, 3.7632e-17, 4.5261e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "86000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.3835e-29, 6.6002e-34, 3.1704e-37, 4.9812e-14],\n",
      "        [1.3705e-15, 7.5028e-16, 3.5768e-17, 4.6535e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "87000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.4956e-29, 7.0715e-34, 3.4007e-37, 4.9833e-14],\n",
      "        [1.3976e-15, 7.6530e-16, 3.6589e-17, 4.6251e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "88000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[2.4000e-29, 9.8438e-34, 4.5667e-37, 6.4257e-14],\n",
      "        [1.7625e-15, 8.9945e-16, 4.2141e-17, 5.2574e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "89000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[2.6820e-29, 1.1194e-33, 5.2367e-37, 5.9158e-14],\n",
      "        [1.8724e-15, 9.6147e-16, 4.5367e-17, 5.0668e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "90000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[2.4606e-29, 9.8015e-34, 4.4903e-37, 6.3654e-14],\n",
      "        [1.7711e-15, 8.9183e-16, 4.1523e-17, 5.2494e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "91000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[3.2360e-29, 1.4315e-33, 7.0003e-37, 5.9446e-14],\n",
      "        [2.0407e-15, 1.0812e-15, 5.2251e-17, 5.1341e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "92000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.9414e-29, 6.6677e-34, 2.8304e-37, 6.6143e-14],\n",
      "        [1.5134e-15, 7.1712e-16, 3.1978e-17, 5.3563e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "93000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.6889e-29, 5.2409e-34, 2.1186e-37, 6.5500e-14],\n",
      "        [1.3715e-15, 6.2469e-16, 2.7112e-17, 5.3518e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "94000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.0275e-29, 2.4301e-34, 8.4747e-38, 6.5187e-14],\n",
      "        [1.0216e-15, 4.1354e-16, 1.6554e-17, 5.3194e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "95000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.1126e-29, 2.5734e-34, 9.1760e-38, 6.2970e-14],\n",
      "        [1.0396e-15, 4.2134e-16, 1.7064e-17, 5.3053e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "96000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[5.8224e-30, 9.5969e-35, 2.8464e-38, 6.2221e-14],\n",
      "        [7.2019e-16, 2.5070e-16, 9.1913e-18, 5.2319e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "97000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[3.2065e-30, 3.7761e-35, 9.5308e-39, 6.1571e-14],\n",
      "        [5.1122e-16, 1.5337e-16, 5.1546e-18, 5.1912e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "98000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[2.9563e-30, 3.1358e-35, 7.9720e-39, 5.9314e-14],\n",
      "        [4.8116e-16, 1.3913e-16, 4.6909e-18, 5.1675e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "99000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[8.0543e-31, 4.7623e-36, 8.6052e-40, 6.5490e-14],\n",
      "        [2.4658e-16, 5.3376e-17, 1.5001e-18, 5.2474e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "100000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[5.4635e-31, 2.6503e-36, 4.6041e-40, 6.5054e-14],\n",
      "        [2.0538e-16, 4.0440e-17, 1.1126e-18, 5.2442e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "101000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[3.0203e-31, 1.0959e-36, 1.6918e-40, 6.7644e-14],\n",
      "        [1.5440e-16, 2.6475e-17, 6.8287e-19, 5.3014e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "102000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.2709e-31, 3.0078e-37, 3.7045e-41, 7.5843e-14],\n",
      "        [1.0309e-16, 1.4519e-17, 3.3016e-19, 5.4757e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "103000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[8.1421e-32, 1.5067e-37, 1.6457e-41, 7.8565e-14],\n",
      "        [8.7862e-17, 1.1230e-17, 2.3726e-19, 5.4934e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "104000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.8873e-37, 4.7602e-42, 4.2039e-45, 1.9790e-12],\n",
      "        [2.7513e-16, 4.6195e-18, 1.9904e-19, 4.9710e-06],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "105000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[4.7906e-38, 2.1033e-42, 2.8026e-45, 6.4944e-13],\n",
      "        [7.8344e-17, 2.4211e-18, 1.1836e-19, 2.2332e-06],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "106000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.6862e-38, 1.1028e-42, 1.4013e-45, 1.7954e-13],\n",
      "        [3.0328e-17, 1.4663e-18, 8.0054e-20, 9.6521e-07],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "107000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[7.9353e-39, 6.8243e-43, 1.4013e-45, 4.3357e-14],\n",
      "        [1.5323e-17, 1.0117e-18, 6.0479e-20, 4.0732e-07],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "108000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[4.7913e-39, 5.0026e-43, 1.4013e-45, 1.0318e-14],\n",
      "        [9.5287e-18, 7.7581e-19, 4.9788e-20, 1.7708e-07],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "109000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[3.1908e-39, 3.2931e-43, 0.0000e+00, 2.5508e-15],\n",
      "        [7.6201e-18, 6.7524e-19, 4.5302e-20, 8.2816e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "110000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[2.5676e-39, 2.4102e-43, 0.0000e+00, 5.9536e-16],\n",
      "        [7.3417e-18, 6.5218e-19, 4.5018e-20, 3.9248e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "111000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.8935e-39, 1.6115e-43, 0.0000e+00, 1.0157e-16],\n",
      "        [7.3779e-18, 7.1442e-19, 5.3493e-20, 1.5934e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "112000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.6522e-39, 1.3452e-43, 0.0000e+00, 3.7066e-17],\n",
      "        [7.3918e-18, 7.3682e-19, 5.6928e-20, 9.6029e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "113000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.6087e-39, 1.2892e-43, 0.0000e+00, 3.0270e-17],\n",
      "        [7.3930e-18, 7.3652e-19, 5.7016e-20, 8.6855e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "114000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.6064e-39, 1.2892e-43, 0.0000e+00, 2.9921e-17],\n",
      "        [7.3932e-18, 7.3650e-19, 5.7021e-20, 8.6362e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "115000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.6057e-39, 1.2752e-43, 0.0000e+00, 2.9937e-17],\n",
      "        [7.3937e-18, 7.3653e-19, 5.7024e-20, 8.6384e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "116000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.6038e-39, 1.2752e-43, 0.0000e+00, 2.9754e-17],\n",
      "        [7.3938e-18, 7.3654e-19, 5.7030e-20, 8.6120e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "117000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.6183e-39, 1.2892e-43, 0.0000e+00, 3.1109e-17],\n",
      "        [7.3925e-18, 7.3571e-19, 5.6897e-20, 8.8068e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "118000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.5864e-39, 1.2612e-43, 0.0000e+00, 2.9128e-17],\n",
      "        [7.3958e-18, 7.3630e-19, 5.7007e-20, 8.5183e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "119000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.6573e-39, 1.3452e-43, 0.0000e+00, 3.2232e-17],\n",
      "        [7.3874e-18, 7.3513e-19, 5.6828e-20, 8.9761e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "120000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.6511e-39, 1.3312e-43, 0.0000e+00, 3.0789e-17],\n",
      "        [7.3869e-18, 7.3514e-19, 5.6884e-20, 8.7795e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "121000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.6290e-39, 1.3032e-43, 0.0000e+00, 3.0619e-17],\n",
      "        [7.3904e-18, 7.3522e-19, 5.6875e-20, 8.7478e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "122000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.4394e-39, 1.0790e-43, 0.0000e+00, 2.8643e-17],\n",
      "        [7.4241e-18, 7.3841e-19, 5.7075e-20, 8.3820e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "123000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.4349e-39, 1.0510e-43, 0.0000e+00, 2.8183e-17],\n",
      "        [7.4592e-18, 7.4011e-19, 5.7031e-20, 8.3129e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "124000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.6571e-39, 1.3312e-43, 0.0000e+00, 3.1732e-17],\n",
      "        [7.4034e-18, 7.3119e-19, 5.6300e-20, 8.9222e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "125000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5408e-39, 1.1631e-43, 0.0000e+00, 3.1216e-17],\n",
      "        [7.4834e-18, 7.3825e-19, 5.6699e-20, 8.7826e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "126000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.5243e-39, 1.1070e-43, 0.0000e+00, 3.1139e-17],\n",
      "        [7.5767e-18, 7.4862e-19, 5.7495e-20, 8.7414e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "127000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.6025e-39, 1.2051e-43, 0.0000e+00, 3.3686e-17],\n",
      "        [7.5059e-18, 7.3852e-19, 5.6759e-20, 9.1374e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "128000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[3.2208e-39, 2.5083e-43, 0.0000e+00, 5.4811e-17],\n",
      "        [1.0739e-17, 8.4081e-19, 6.2721e-20, 1.3324e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "129000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[5.8640e-39, 3.5313e-43, 0.0000e+00, 8.4189e-17],\n",
      "        [1.8742e-17, 1.0775e-18, 7.6623e-20, 1.8911e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "130000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[3.0615e-39, 2.0599e-43, 0.0000e+00, 6.5303e-17],\n",
      "        [1.0849e-17, 8.2122e-19, 6.1223e-20, 1.4536e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "131000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[9.8355e-39, 3.0969e-43, 0.0000e+00, 1.8091e-16],\n",
      "        [3.5533e-17, 1.2629e-18, 8.3663e-20, 3.2818e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "132000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.1029e-39, 6.4460e-44, 0.0000e+00, 2.3127e-17],\n",
      "        [4.5117e-18, 3.6069e-19, 2.2465e-20, 7.7436e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "133000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.0282e-39, 5.7453e-44, 0.0000e+00, 2.4178e-17],\n",
      "        [4.3360e-18, 3.4153e-19, 2.1213e-20, 7.9291e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "134000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[2.1359e-39, 8.6881e-44, 0.0000e+00, 5.1547e-17],\n",
      "        [8.7586e-18, 3.9836e-19, 2.1838e-20, 1.4774e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "135000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.0055e-39, 4.4842e-44, 0.0000e+00, 3.8171e-17],\n",
      "        [4.7736e-18, 2.9060e-19, 1.7042e-20, 1.0952e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "136000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.5148e-39, 4.2039e-44, 0.0000e+00, 6.1914e-17],\n",
      "        [7.9626e-18, 3.2346e-19, 1.7569e-20, 1.6192e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "137000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[7.5151e-40, 8.4078e-45, 0.0000e+00, 5.4501e-17],\n",
      "        [8.2618e-18, 1.8038e-19, 7.1660e-21, 1.7649e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "138000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.5659e-39, 1.1210e-44, 0.0000e+00, 1.0605e-16],\n",
      "        [1.9358e-17, 2.3194e-19, 8.1877e-21, 3.0974e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "139000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[6.0086e-41, 0.0000e+00, 0.0000e+00, 1.3005e-17],\n",
      "        [1.5720e-18, 4.4167e-20, 1.3951e-21, 6.6579e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "140000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[9.8429e-41, 1.4013e-45, 0.0000e+00, 1.8478e-17],\n",
      "        [2.6743e-18, 5.4302e-20, 1.6037e-21, 9.1709e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "141000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[8.4285e-41, 0.0000e+00, 0.0000e+00, 1.8488e-17],\n",
      "        [2.4418e-18, 5.0433e-20, 1.5008e-21, 8.9810e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "142000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.0390e-40, 0.0000e+00, 0.0000e+00, 2.6222e-17],\n",
      "        [3.4685e-18, 5.4218e-20, 1.5309e-21, 1.1809e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "143000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[2.1544e-40, 0.0000e+00, 0.0000e+00, 6.2788e-17],\n",
      "        [8.9921e-18, 6.9300e-20, 1.7026e-21, 2.3589e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "144000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[5.5245e-41, 0.0000e+00, 0.0000e+00, 3.6247e-17],\n",
      "        [5.8156e-18, 3.4379e-20, 6.5319e-22, 1.8018e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "145000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[8.9188e-41, 0.0000e+00, 0.0000e+00, 6.2231e-17],\n",
      "        [1.0676e-17, 4.0313e-20, 7.0394e-22, 2.7684e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "146000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.4896e-41, 0.0000e+00, 0.0000e+00, 1.1819e-17],\n",
      "        [1.8565e-18, 1.5014e-20, 2.4675e-22, 8.5666e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "147000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.3951e-41, 0.0000e+00, 0.0000e+00, 1.2753e-17],\n",
      "        [1.8846e-18, 1.4452e-20, 2.3622e-22, 8.9691e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "148000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.6240e-41, 0.0000e+00, 0.0000e+00, 1.9420e-17],\n",
      "        [2.6818e-18, 1.5117e-20, 2.3329e-22, 1.2205e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "149000   actions:  tensor([3, 3, 0])\n",
      "loss=  tensor(2.6167, grad_fn=<DivBackward0>)   , return=  16232.8125\n",
      "discReturns/1000= tensor([6.4232, 6.5995, 5.6626])\n",
      "actionProbs tensor([[1.2640e-40, 0.0000e+00, 0.0000e+00, 9.3251e-17],\n",
      "        [2.4981e-17, 3.9981e-20, 4.5771e-22, 4.5085e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "150000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.7071, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.4076e-31, 2.1052e-41, 6.3479e-43, 2.4593e-05],\n",
      "        [3.3831e-11, 1.6656e-16, 2.8445e-17, 6.4203e-02],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.3577e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "151000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[5.1055e-38, 2.3770e-40, 1.0650e-43, 1.5637e-13],\n",
      "        [2.7834e-15, 1.7154e-17, 2.9404e-19, 7.3951e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "152000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[4.0633e-38, 2.2259e-40, 9.9492e-44, 1.2535e-13],\n",
      "        [2.8592e-15, 1.8412e-17, 3.0808e-19, 5.9124e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "153000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[3.0560e-38, 2.1572e-40, 9.6690e-44, 1.1453e-13],\n",
      "        [3.0047e-15, 2.0552e-17, 3.3874e-19, 5.8804e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "154000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[5.1691e-39, 2.8880e-40, 2.0179e-43, 9.9402e-14],\n",
      "        [2.3882e-15, 4.5686e-17, 8.4324e-19, 5.8455e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "155000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.2951e-39, 2.8714e-40, 3.0408e-43, 7.4446e-14],\n",
      "        [2.5122e-15, 8.5294e-17, 1.7224e-18, 5.5241e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "156000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[6.3100e-41, 7.7409e-41, 4.5122e-43, 4.5472e-14],\n",
      "        [1.6626e-15, 1.2417e-16, 4.8170e-18, 5.5286e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "157000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[3.5775e-42, 2.0113e-41, 3.7835e-43, 1.9918e-14],\n",
      "        [1.7161e-15, 2.0246e-16, 1.1775e-17, 5.7496e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "158000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.6535e-43, 6.5945e-42, 1.4854e-43, 4.5869e-15],\n",
      "        [2.4718e-15, 3.9677e-16, 2.2406e-17, 5.3915e-08],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "159000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[0.0000e+00, 2.2421e-44, 1.4013e-45, 1.5701e-16],\n",
      "        [6.3377e-20, 1.8617e-18, 2.4358e-19, 3.9182e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "160000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[0.0000e+00, 4.2039e-45, 0.0000e+00, 3.6972e-17],\n",
      "        [1.9796e-21, 3.1146e-19, 5.3898e-20, 1.3000e-09],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "161000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[0.0000e+00, 1.4013e-45, 0.0000e+00, 1.2919e-17],\n",
      "        [2.0771e-22, 9.6441e-20, 2.0043e-20, 5.9030e-10],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "162000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 5.7820e-18],\n",
      "        [4.6104e-23, 4.5958e-20, 1.0848e-20, 3.3229e-10],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "163000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3776e-18],\n",
      "        [2.2572e-23, 3.4786e-20, 8.8804e-21, 2.5130e-10],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "164000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2604e-18],\n",
      "        [2.1924e-23, 3.7911e-20, 9.9234e-21, 2.4621e-10],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "165000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1932e-18],\n",
      "        [2.0925e-23, 4.3163e-20, 1.1772e-20, 2.3830e-10],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "166000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 4.4360e-19],\n",
      "        [2.0675e-23, 5.5935e-20, 1.6157e-20, 2.2702e-10],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "167000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[0.0000e+00, 1.4013e-45, 0.0000e+00, 8.8888e-20],\n",
      "        [2.2481e-23, 7.2445e-20, 2.2818e-20, 1.6437e-10],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "168000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[0.0000e+00, 1.4013e-45, 0.0000e+00, 1.9164e-20],\n",
      "        [1.9074e-23, 5.7219e-20, 1.9817e-20, 9.1859e-11],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "169000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[0.0000e+00, 1.4013e-45, 0.0000e+00, 1.0385e-20],\n",
      "        [1.6896e-23, 4.6934e-20, 1.7603e-20, 6.0078e-11],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "170000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[0.0000e+00, 7.0065e-45, 1.4013e-45, 1.0274e-20],\n",
      "        [1.5576e-23, 3.9897e-20, 1.6105e-20, 4.3965e-11],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "171000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[0.0000e+00, 3.5032e-44, 4.2039e-45, 1.3996e-20],\n",
      "        [1.4897e-23, 3.4995e-20, 1.5158e-20, 3.4766e-11],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "172000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[0.0000e+00, 1.6395e-43, 1.5414e-44, 2.1318e-20],\n",
      "        [1.4806e-23, 3.1736e-20, 1.4722e-20, 2.8995e-11],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "173000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[0.0000e+00, 7.6651e-43, 6.5861e-44, 3.4579e-20],\n",
      "        [1.5319e-23, 2.9822e-20, 1.4789e-20, 2.4964e-11],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "174000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[0.0000e+00, 2.9806e-42, 2.4242e-43, 4.7838e-20],\n",
      "        [1.6505e-23, 2.9254e-20, 1.5453e-20, 2.1414e-11],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "175000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[0.0000e+00, 7.8655e-42, 6.0396e-43, 4.0423e-20],\n",
      "        [1.7601e-23, 3.0685e-20, 1.6572e-20, 1.5193e-11],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "176000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.4013e-45, 1.3255e-41, 9.9072e-43, 2.6027e-20],\n",
      "        [1.8913e-23, 3.3285e-20, 1.8076e-20, 1.0709e-11],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "177000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.4013e-45, 1.8857e-41, 1.4013e-42, 1.5866e-20],\n",
      "        [2.0763e-23, 3.6762e-20, 2.0099e-20, 7.7878e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "178000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.4013e-45, 2.5389e-41, 1.8861e-42, 1.0062e-20],\n",
      "        [2.2654e-23, 4.0252e-20, 2.2160e-20, 5.8969e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "179000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[2.8026e-45, 3.2725e-41, 2.4397e-42, 6.5470e-21],\n",
      "        [2.4530e-23, 4.3663e-20, 2.4216e-20, 4.5872e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "180000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[2.8026e-45, 4.0719e-41, 3.0534e-42, 4.3624e-21],\n",
      "        [2.6370e-23, 4.6964e-20, 2.6243e-20, 3.6467e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "181000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[4.2039e-45, 4.9174e-41, 3.7148e-42, 2.9865e-21],\n",
      "        [2.8128e-23, 5.0084e-20, 2.8191e-20, 2.9584e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "182000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[4.2039e-45, 5.7925e-41, 4.4099e-42, 2.1079e-21],\n",
      "        [2.9794e-23, 5.3024e-20, 3.0051e-20, 2.4483e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "183000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[5.6052e-45, 6.6825e-41, 5.1259e-42, 1.5372e-21],\n",
      "        [3.1352e-23, 5.5759e-20, 3.1801e-20, 2.0656e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "184000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[5.6052e-45, 7.5721e-41, 5.8504e-42, 1.1571e-21],\n",
      "        [3.2797e-23, 5.8287e-20, 3.3441e-20, 1.7746e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "185000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[7.0065e-45, 8.4539e-41, 6.5763e-42, 8.9681e-22],\n",
      "        [3.4128e-23, 6.0609e-20, 3.4958e-20, 1.5494e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "186000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[8.4078e-45, 9.3325e-41, 7.3050e-42, 7.1371e-22],\n",
      "        [3.5363e-23, 6.2758e-20, 3.6377e-20, 1.3723e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "187000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[8.4078e-45, 1.0199e-40, 8.0308e-42, 5.8067e-22],\n",
      "        [3.6509e-23, 6.4748e-20, 3.7698e-20, 1.2300e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "188000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[8.6881e-44, 7.6976e-41, 5.8869e-42, 8.6214e-21],\n",
      "        [2.5185e-22, 8.7766e-20, 4.8330e-20, 8.9408e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "189000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.2331e-43, 1.0684e-40, 8.2242e-42, 4.2162e-21],\n",
      "        [2.8037e-22, 9.8426e-20, 5.4710e-20, 6.1374e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "190000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.4994e-43, 1.2739e-40, 9.8581e-42, 2.9073e-21],\n",
      "        [2.9771e-22, 1.0480e-19, 5.8599e-20, 5.0460e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "191000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3375e-40, 1.0370e-41, 2.6274e-21],\n",
      "        [3.0286e-22, 1.0667e-19, 5.9753e-20, 4.7838e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "192000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3478e-40, 1.0455e-41, 2.5861e-21],\n",
      "        [3.0370e-22, 1.0698e-19, 5.9947e-20, 4.7441e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "193000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3465e-40, 1.0445e-41, 2.5910e-21],\n",
      "        [3.0357e-22, 1.0693e-19, 5.9919e-20, 4.7488e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "194000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3494e-40, 1.0468e-41, 2.5798e-21],\n",
      "        [3.0385e-22, 1.0702e-19, 5.9974e-20, 4.7380e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "195000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3504e-40, 1.0476e-41, 2.5759e-21],\n",
      "        [3.0392e-22, 1.0705e-19, 5.9990e-20, 4.7344e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "196000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3540e-40, 1.0506e-41, 2.5620e-21],\n",
      "        [3.0422e-22, 1.0716e-19, 6.0058e-20, 4.7208e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "197000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3513e-40, 1.0483e-41, 2.5722e-21],\n",
      "        [3.0400e-22, 1.0708e-19, 6.0007e-20, 4.7306e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "198000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3453e-40, 1.0434e-41, 2.5951e-21],\n",
      "        [3.0349e-22, 1.0689e-19, 5.9894e-20, 4.7529e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "199000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3430e-40, 1.0416e-41, 2.6041e-21],\n",
      "        [3.0329e-22, 1.0682e-19, 5.9853e-20, 4.7615e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "200000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5695e-43, 1.3343e-40, 1.0343e-41, 2.6382e-21],\n",
      "        [3.0256e-22, 1.0657e-19, 5.9686e-20, 4.7941e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "201000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3496e-40, 1.0469e-41, 2.5791e-21],\n",
      "        [3.0385e-22, 1.0702e-19, 5.9974e-20, 4.7373e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "202000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3475e-40, 1.0452e-41, 2.5864e-21],\n",
      "        [3.0369e-22, 1.0696e-19, 5.9937e-20, 4.7444e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "203000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3492e-40, 1.0466e-41, 2.5801e-21],\n",
      "        [3.0382e-22, 1.0701e-19, 5.9969e-20, 4.7383e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "204000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3487e-40, 1.0462e-41, 2.5821e-21],\n",
      "        [3.0379e-22, 1.0699e-19, 5.9959e-20, 4.7402e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "205000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3515e-40, 1.0485e-41, 2.5715e-21],\n",
      "        [3.0401e-22, 1.0708e-19, 6.0011e-20, 4.7302e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "206000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3451e-40, 1.0433e-41, 2.5959e-21],\n",
      "        [3.0348e-22, 1.0689e-19, 5.9888e-20, 4.7535e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "207000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3490e-40, 1.0463e-41, 2.5813e-21],\n",
      "        [3.0382e-22, 1.0701e-19, 5.9963e-20, 4.7396e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "208000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3485e-40, 1.0459e-41, 2.5832e-21],\n",
      "        [3.0376e-22, 1.0698e-19, 5.9952e-20, 4.7412e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "209000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.6115e-43, 1.3587e-40, 1.0545e-41, 2.5439e-21],\n",
      "        [3.0463e-22, 1.0729e-19, 6.0144e-20, 4.7031e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "210000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3632e-40, 1.0581e-41, 2.5272e-21],\n",
      "        [3.0501e-22, 1.0743e-19, 6.0229e-20, 4.6868e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "211000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3495e-40, 1.0468e-41, 2.5791e-21],\n",
      "        [3.0384e-22, 1.0701e-19, 5.9963e-20, 4.7373e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "212000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3487e-40, 1.0461e-41, 2.5822e-21],\n",
      "        [3.0378e-22, 1.0699e-19, 5.9954e-20, 4.7404e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "213000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.6115e-43, 1.3616e-40, 1.0569e-41, 2.5332e-21],\n",
      "        [3.0487e-22, 1.0738e-19, 6.0200e-20, 4.6928e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "214000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3547e-40, 1.0511e-41, 2.5592e-21],\n",
      "        [3.0429e-22, 1.0717e-19, 6.0069e-20, 4.7181e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "215000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3584e-40, 1.0542e-41, 2.5451e-21],\n",
      "        [3.0462e-22, 1.0728e-19, 6.0139e-20, 4.7043e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "216000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3517e-40, 1.0486e-41, 2.5703e-21],\n",
      "        [3.0403e-22, 1.0708e-19, 6.0009e-20, 4.7289e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "217000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3488e-40, 1.0462e-41, 2.5812e-21],\n",
      "        [3.0380e-22, 1.0699e-19, 5.9956e-20, 4.7393e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "218000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3442e-40, 1.0424e-41, 2.5984e-21],\n",
      "        [3.0339e-22, 1.0685e-19, 5.9868e-20, 4.7558e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "219000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3533e-40, 1.0500e-41, 2.5642e-21],\n",
      "        [3.0417e-22, 1.0713e-19, 6.0040e-20, 4.7230e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "220000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3439e-40, 1.0421e-41, 2.5997e-21],\n",
      "        [3.0338e-22, 1.0685e-19, 5.9864e-20, 4.7573e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "221000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3512e-40, 1.0482e-41, 2.5720e-21],\n",
      "        [3.0400e-22, 1.0707e-19, 6.0000e-20, 4.7303e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "222000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3483e-40, 1.0458e-41, 2.5832e-21],\n",
      "        [3.0375e-22, 1.0698e-19, 5.9947e-20, 4.7413e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "223000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3434e-40, 1.0417e-41, 2.6022e-21],\n",
      "        [3.0333e-22, 1.0683e-19, 5.9853e-20, 4.7594e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "224000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3585e-40, 1.0542e-41, 2.5446e-21],\n",
      "        [3.0460e-22, 1.0728e-19, 6.0135e-20, 4.7040e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "225000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3516e-40, 1.0485e-41, 2.5709e-21],\n",
      "        [3.0402e-22, 1.0708e-19, 6.0005e-20, 4.7293e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "226000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3560e-40, 1.0521e-41, 2.5541e-21],\n",
      "        [3.0438e-22, 1.0721e-19, 6.0089e-20, 4.7130e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "227000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3543e-40, 1.0507e-41, 2.5606e-21],\n",
      "        [3.0426e-22, 1.0716e-19, 6.0058e-20, 4.7194e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "228000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3567e-40, 1.0527e-41, 2.5513e-21],\n",
      "        [3.0447e-22, 1.0723e-19, 6.0106e-20, 4.7103e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "229000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3576e-40, 1.0535e-41, 2.5479e-21],\n",
      "        [3.0454e-22, 1.0726e-19, 6.0122e-20, 4.7070e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "230000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3608e-40, 1.0562e-41, 2.5354e-21],\n",
      "        [3.0482e-22, 1.0736e-19, 6.0181e-20, 4.6949e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "231000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3501e-40, 1.0473e-41, 2.5760e-21],\n",
      "        [3.0390e-22, 1.0703e-19, 5.9981e-20, 4.7342e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "232000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3455e-40, 1.0435e-41, 2.5935e-21],\n",
      "        [3.0352e-22, 1.0690e-19, 5.9894e-20, 4.7513e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "233000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3533e-40, 1.0499e-41, 2.5641e-21],\n",
      "        [3.0418e-22, 1.0713e-19, 6.0042e-20, 4.7227e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "234000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3460e-40, 1.0438e-41, 2.5917e-21],\n",
      "        [3.0356e-22, 1.0691e-19, 5.9903e-20, 4.7494e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "235000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3415e-40, 1.0400e-41, 2.6091e-21],\n",
      "        [3.0317e-22, 1.0677e-19, 5.9817e-20, 4.7663e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "236000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3421e-40, 1.0406e-41, 2.6072e-21],\n",
      "        [3.0323e-22, 1.0679e-19, 5.9828e-20, 4.7644e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "237000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3443e-40, 1.0424e-41, 2.5985e-21],\n",
      "        [3.0341e-22, 1.0686e-19, 5.9868e-20, 4.7559e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "238000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3438e-40, 1.0420e-41, 2.6003e-21],\n",
      "        [3.0336e-22, 1.0684e-19, 5.9859e-20, 4.7577e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "239000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3502e-40, 1.0473e-41, 2.5757e-21],\n",
      "        [3.0390e-22, 1.0703e-19, 5.9978e-20, 4.7339e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "240000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3539e-40, 1.0503e-41, 2.5621e-21],\n",
      "        [3.0423e-22, 1.0715e-19, 6.0053e-20, 4.7208e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "241000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3586e-40, 1.0542e-41, 2.5444e-21],\n",
      "        [3.0462e-22, 1.0728e-19, 6.0135e-20, 4.7036e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "242000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3548e-40, 1.0511e-41, 2.5586e-21],\n",
      "        [3.0431e-22, 1.0717e-19, 6.0066e-20, 4.7175e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "243000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3459e-40, 1.0438e-41, 2.5922e-21],\n",
      "        [3.0354e-22, 1.0691e-19, 5.9901e-20, 4.7499e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "244000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.6115e-43, 1.3584e-40, 1.0541e-41, 2.5449e-21],\n",
      "        [3.0461e-22, 1.0728e-19, 6.0135e-20, 4.7043e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "245000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3521e-40, 1.0489e-41, 2.5687e-21],\n",
      "        [3.0407e-22, 1.0708e-19, 6.0012e-20, 4.7272e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "246000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3457e-40, 1.0435e-41, 2.5934e-21],\n",
      "        [3.0354e-22, 1.0690e-19, 5.9892e-20, 4.7512e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "247000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3600e-40, 1.0555e-41, 2.5389e-21],\n",
      "        [3.0474e-22, 1.0732e-19, 6.0161e-20, 4.6982e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "248000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3551e-40, 1.0513e-41, 2.5573e-21],\n",
      "        [3.0433e-22, 1.0718e-19, 6.0069e-20, 4.7161e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "249000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3522e-40, 1.0489e-41, 2.5684e-21],\n",
      "        [3.0408e-22, 1.0709e-19, 6.0016e-20, 4.7269e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "250000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3524e-40, 1.0490e-41, 2.5674e-21],\n",
      "        [3.0410e-22, 1.0709e-19, 6.0016e-20, 4.7260e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "251000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3448e-40, 1.0427e-41, 2.5965e-21],\n",
      "        [3.0344e-22, 1.0687e-19, 5.9873e-20, 4.7539e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "252000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3422e-40, 1.0406e-41, 2.6063e-21],\n",
      "        [3.0324e-22, 1.0679e-19, 5.9824e-20, 4.7636e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "253000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3489e-40, 1.0462e-41, 2.5800e-21],\n",
      "        [3.0381e-22, 1.0699e-19, 5.9952e-20, 4.7381e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "254000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3447e-40, 1.0427e-41, 2.5968e-21],\n",
      "        [3.0343e-22, 1.0686e-19, 5.9873e-20, 4.7544e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "255000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3524e-40, 1.0490e-41, 2.5669e-21],\n",
      "        [3.0409e-22, 1.0710e-19, 6.0020e-20, 4.7253e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "256000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3606e-40, 1.0559e-41, 2.5353e-21],\n",
      "        [3.0480e-22, 1.0734e-19, 6.0172e-20, 4.6948e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "257000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3476e-40, 1.0451e-41, 2.5850e-21],\n",
      "        [3.0369e-22, 1.0695e-19, 5.9925e-20, 4.7430e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "258000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3465e-40, 1.0440e-41, 2.5896e-21],\n",
      "        [3.0358e-22, 1.0692e-19, 5.9899e-20, 4.7474e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "259000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3489e-40, 1.0462e-41, 2.5798e-21],\n",
      "        [3.0380e-22, 1.0699e-19, 5.9952e-20, 4.7380e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "260000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3502e-40, 1.0472e-41, 2.5751e-21],\n",
      "        [3.0391e-22, 1.0703e-19, 5.9974e-20, 4.7335e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "261000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3509e-40, 1.0478e-41, 2.5724e-21],\n",
      "        [3.0397e-22, 1.0705e-19, 5.9989e-20, 4.7309e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "262000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3530e-40, 1.0494e-41, 2.5644e-21],\n",
      "        [3.0416e-22, 1.0711e-19, 6.0027e-20, 4.7231e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "263000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3416e-40, 1.0402e-41, 2.6081e-21],\n",
      "        [3.0318e-22, 1.0677e-19, 5.9813e-20, 4.7652e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "264000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3550e-40, 1.0511e-41, 2.5567e-21],\n",
      "        [3.0432e-22, 1.0717e-19, 6.0066e-20, 4.7155e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "265000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3556e-40, 1.0515e-41, 2.5545e-21],\n",
      "        [3.0435e-22, 1.0719e-19, 6.0073e-20, 4.7135e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "266000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3455e-40, 1.0434e-41, 2.5924e-21],\n",
      "        [3.0352e-22, 1.0689e-19, 5.9888e-20, 4.7501e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "267000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3460e-40, 1.0438e-41, 2.5910e-21],\n",
      "        [3.0356e-22, 1.0690e-19, 5.9901e-20, 4.7488e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "268000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3419e-40, 1.0403e-41, 2.6068e-21],\n",
      "        [3.0320e-22, 1.0677e-19, 5.9817e-20, 4.7639e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "269000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3493e-40, 1.0463e-41, 2.5783e-21],\n",
      "        [3.0383e-22, 1.0700e-19, 5.9956e-20, 4.7365e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "270000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3539e-40, 1.0503e-41, 2.5608e-21],\n",
      "        [3.0423e-22, 1.0714e-19, 6.0047e-20, 4.7194e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "271000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3458e-40, 1.0434e-41, 2.5919e-21],\n",
      "        [3.0354e-22, 1.0689e-19, 5.9890e-20, 4.7497e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "272000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3469e-40, 1.0445e-41, 2.5876e-21],\n",
      "        [3.0363e-22, 1.0692e-19, 5.9908e-20, 4.7455e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "273000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3548e-40, 1.0510e-41, 2.5572e-21],\n",
      "        [3.0430e-22, 1.0716e-19, 6.0060e-20, 4.7161e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "274000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3505e-40, 1.0475e-41, 2.5735e-21],\n",
      "        [3.0394e-22, 1.0704e-19, 5.9981e-20, 4.7316e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "275000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3541e-40, 1.0503e-41, 2.5599e-21],\n",
      "        [3.0424e-22, 1.0714e-19, 6.0044e-20, 4.7187e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "276000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3436e-40, 1.0417e-41, 2.5997e-21],\n",
      "        [3.0336e-22, 1.0683e-19, 5.9850e-20, 4.7571e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "277000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3473e-40, 1.0448e-41, 2.5857e-21],\n",
      "        [3.0369e-22, 1.0694e-19, 5.9919e-20, 4.7436e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "278000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3465e-40, 1.0440e-41, 2.5887e-21],\n",
      "        [3.0360e-22, 1.0691e-19, 5.9901e-20, 4.7465e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "279000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3482e-40, 1.0455e-41, 2.5823e-21],\n",
      "        [3.0375e-22, 1.0696e-19, 5.9936e-20, 4.7404e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "280000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3534e-40, 1.0497e-41, 2.5622e-21],\n",
      "        [3.0420e-22, 1.0712e-19, 6.0036e-20, 4.7211e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "281000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3554e-40, 1.0513e-41, 2.5551e-21],\n",
      "        [3.0435e-22, 1.0718e-19, 6.0064e-20, 4.7140e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "282000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3483e-40, 1.0455e-41, 2.5817e-21],\n",
      "        [3.0375e-22, 1.0697e-19, 5.9937e-20, 4.7399e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "283000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3520e-40, 1.0486e-41, 2.5675e-21],\n",
      "        [3.0406e-22, 1.0708e-19, 6.0007e-20, 4.7261e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "284000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3478e-40, 1.0451e-41, 2.5833e-21],\n",
      "        [3.0371e-22, 1.0695e-19, 5.9926e-20, 4.7413e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "285000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3500e-40, 1.0469e-41, 2.5754e-21],\n",
      "        [3.0389e-22, 1.0702e-19, 5.9963e-20, 4.7337e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "286000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3502e-40, 1.0471e-41, 2.5746e-21],\n",
      "        [3.0392e-22, 1.0702e-19, 5.9970e-20, 4.7329e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "287000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3514e-40, 1.0482e-41, 2.5695e-21],\n",
      "        [3.0402e-22, 1.0706e-19, 5.9994e-20, 4.7279e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "288000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3475e-40, 1.0448e-41, 2.5846e-21],\n",
      "        [3.0370e-22, 1.0694e-19, 5.9919e-20, 4.7425e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "289000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3546e-40, 1.0507e-41, 2.5574e-21],\n",
      "        [3.0429e-22, 1.0715e-19, 6.0049e-20, 4.7162e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "290000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3531e-40, 1.0494e-41, 2.5635e-21],\n",
      "        [3.0415e-22, 1.0711e-19, 6.0022e-20, 4.7221e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "291000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3462e-40, 1.0438e-41, 2.5892e-21],\n",
      "        [3.0358e-22, 1.0690e-19, 5.9894e-20, 4.7471e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "292000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3491e-40, 1.0462e-41, 2.5781e-21],\n",
      "        [3.0382e-22, 1.0699e-19, 5.9952e-20, 4.7364e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "293000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3498e-40, 1.0468e-41, 2.5753e-21],\n",
      "        [3.0388e-22, 1.0701e-19, 5.9959e-20, 4.7335e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "294000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3393e-40, 1.0382e-41, 2.6158e-21],\n",
      "        [3.0298e-22, 1.0669e-19, 5.9762e-20, 4.7725e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "295000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3463e-40, 1.0438e-41, 2.5891e-21],\n",
      "        [3.0359e-22, 1.0690e-19, 5.9897e-20, 4.7468e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "296000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3482e-40, 1.0454e-41, 2.5816e-21],\n",
      "        [3.0376e-22, 1.0696e-19, 5.9934e-20, 4.7399e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "297000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3526e-40, 1.0490e-41, 2.5650e-21],\n",
      "        [3.0412e-22, 1.0709e-19, 6.0011e-20, 4.7235e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "298000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3505e-40, 1.0473e-41, 2.5726e-21],\n",
      "        [3.0394e-22, 1.0703e-19, 5.9972e-20, 4.7311e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "299000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3541e-40, 1.0503e-41, 2.5589e-21],\n",
      "        [3.0424e-22, 1.0714e-19, 6.0042e-20, 4.7176e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "300000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3517e-40, 1.0482e-41, 2.5682e-21],\n",
      "        [3.0405e-22, 1.0706e-19, 5.9994e-20, 4.7266e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "301000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3463e-40, 1.0438e-41, 2.5885e-21],\n",
      "        [3.0358e-22, 1.0690e-19, 5.9894e-20, 4.7465e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "302000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3505e-40, 1.0473e-41, 2.5720e-21],\n",
      "        [3.0394e-22, 1.0703e-19, 5.9974e-20, 4.7303e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "303000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3442e-40, 1.0420e-41, 2.5966e-21],\n",
      "        [3.0342e-22, 1.0684e-19, 5.9853e-20, 4.7542e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "304000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3513e-40, 1.0480e-41, 2.5696e-21],\n",
      "        [3.0401e-22, 1.0705e-19, 5.9989e-20, 4.7280e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "305000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3482e-40, 1.0454e-41, 2.5809e-21],\n",
      "        [3.0375e-22, 1.0696e-19, 5.9930e-20, 4.7389e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "306000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3543e-40, 1.0504e-41, 2.5579e-21],\n",
      "        [3.0426e-22, 1.0714e-19, 6.0044e-20, 4.7169e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "307000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3513e-40, 1.0479e-41, 2.5698e-21],\n",
      "        [3.0400e-22, 1.0705e-19, 5.9985e-20, 4.7285e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "308000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3424e-40, 1.0406e-41, 2.6032e-21],\n",
      "        [3.0327e-22, 1.0678e-19, 5.9819e-20, 4.7606e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "309000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3502e-40, 1.0471e-41, 2.5736e-21],\n",
      "        [3.0392e-22, 1.0701e-19, 5.9967e-20, 4.7322e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "310000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3491e-40, 1.0461e-41, 2.5775e-21],\n",
      "        [3.0382e-22, 1.0699e-19, 5.9945e-20, 4.7358e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "311000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3534e-40, 1.0496e-41, 2.5613e-21],\n",
      "        [3.0419e-22, 1.0711e-19, 6.0023e-20, 4.7202e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "312000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5695e-43, 1.3339e-40, 1.0336e-41, 2.6361e-21],\n",
      "        [3.0253e-22, 1.0653e-19, 5.9656e-20, 4.7924e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "313000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3480e-40, 1.0452e-41, 2.5809e-21],\n",
      "        [3.0374e-22, 1.0695e-19, 5.9923e-20, 4.7391e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "314000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3436e-40, 1.0416e-41, 2.5983e-21],\n",
      "        [3.0334e-22, 1.0681e-19, 5.9841e-20, 4.7558e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "315000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3457e-40, 1.0433e-41, 2.5903e-21],\n",
      "        [3.0353e-22, 1.0688e-19, 5.9879e-20, 4.7480e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "316000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3376e-40, 1.0365e-41, 2.6215e-21],\n",
      "        [3.0282e-22, 1.0663e-19, 5.9726e-20, 4.7781e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "317000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3562e-40, 1.0518e-41, 2.5502e-21],\n",
      "        [3.0442e-22, 1.0719e-19, 6.0077e-20, 4.7093e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "318000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3490e-40, 1.0459e-41, 2.5773e-21],\n",
      "        [3.0380e-22, 1.0698e-19, 5.9939e-20, 4.7355e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "319000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3533e-40, 1.0494e-41, 2.5609e-21],\n",
      "        [3.0418e-22, 1.0711e-19, 6.0020e-20, 4.7198e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "320000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3528e-40, 1.0490e-41, 2.5626e-21],\n",
      "        [3.0414e-22, 1.0709e-19, 6.0011e-20, 4.7211e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "321000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3402e-40, 1.0386e-41, 2.6111e-21],\n",
      "        [3.0306e-22, 1.0671e-19, 5.9769e-20, 4.7682e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "322000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3476e-40, 1.0448e-41, 2.5822e-21],\n",
      "        [3.0369e-22, 1.0693e-19, 5.9908e-20, 4.7403e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "323000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3503e-40, 1.0471e-41, 2.5720e-21],\n",
      "        [3.0392e-22, 1.0702e-19, 5.9965e-20, 4.7303e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "324000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3473e-40, 1.0445e-41, 2.5834e-21],\n",
      "        [3.0367e-22, 1.0692e-19, 5.9908e-20, 4.7415e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "325000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3539e-40, 1.0500e-41, 2.5584e-21],\n",
      "        [3.0423e-22, 1.0712e-19, 6.0033e-20, 4.7174e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "326000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3519e-40, 1.0483e-41, 2.5656e-21],\n",
      "        [3.0406e-22, 1.0706e-19, 5.9989e-20, 4.7243e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "327000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.6115e-43, 1.3594e-40, 1.0546e-41, 2.5378e-21],\n",
      "        [3.0471e-22, 1.0729e-19, 6.0137e-20, 4.6971e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "328000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3508e-40, 1.0473e-41, 2.5698e-21],\n",
      "        [3.0397e-22, 1.0703e-19, 5.9970e-20, 4.7283e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "329000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3590e-40, 1.0542e-41, 2.5389e-21],\n",
      "        [3.0465e-22, 1.0727e-19, 6.0126e-20, 4.6984e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "330000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3555e-40, 1.0513e-41, 2.5515e-21],\n",
      "        [3.0437e-22, 1.0717e-19, 6.0056e-20, 4.7106e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "331000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3561e-40, 1.0518e-41, 2.5491e-21],\n",
      "        [3.0442e-22, 1.0718e-19, 6.0071e-20, 4.7083e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "332000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3513e-40, 1.0479e-41, 2.5671e-21],\n",
      "        [3.0402e-22, 1.0705e-19, 5.9983e-20, 4.7257e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "333000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3501e-40, 1.0469e-41, 2.5719e-21],\n",
      "        [3.0392e-22, 1.0701e-19, 5.9961e-20, 4.7303e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "334000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3543e-40, 1.0503e-41, 2.5564e-21],\n",
      "        [3.0426e-22, 1.0713e-19, 6.0038e-20, 4.7155e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "335000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3469e-40, 1.0441e-41, 2.5844e-21],\n",
      "        [3.0364e-22, 1.0691e-19, 5.9897e-20, 4.7425e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "336000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3522e-40, 1.0485e-41, 2.5644e-21],\n",
      "        [3.0407e-22, 1.0707e-19, 5.9996e-20, 4.7231e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "337000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3477e-40, 1.0448e-41, 2.5813e-21],\n",
      "        [3.0370e-22, 1.0694e-19, 5.9912e-20, 4.7396e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "338000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3434e-40, 1.0413e-41, 2.5977e-21],\n",
      "        [3.0334e-22, 1.0681e-19, 5.9831e-20, 4.7552e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "339000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3621e-40, 1.0567e-41, 2.5270e-21],\n",
      "        [3.0492e-22, 1.0737e-19, 6.0185e-20, 4.6868e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "340000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3526e-40, 1.0489e-41, 2.5628e-21],\n",
      "        [3.0411e-22, 1.0708e-19, 6.0005e-20, 4.7218e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "341000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3598e-40, 1.0548e-41, 2.5357e-21],\n",
      "        [3.0472e-22, 1.0730e-19, 6.0137e-20, 4.6951e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "342000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3518e-40, 1.0482e-41, 2.5654e-21],\n",
      "        [3.0405e-22, 1.0706e-19, 5.9990e-20, 4.7243e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "343000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3408e-40, 1.0391e-41, 2.6078e-21],\n",
      "        [3.0312e-22, 1.0673e-19, 5.9778e-20, 4.7652e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "344000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3534e-40, 1.0494e-41, 2.5598e-21],\n",
      "        [3.0418e-22, 1.0710e-19, 6.0018e-20, 4.7187e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "345000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3487e-40, 1.0456e-41, 2.5770e-21],\n",
      "        [3.0380e-22, 1.0696e-19, 5.9928e-20, 4.7355e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "346000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3425e-40, 1.0405e-41, 2.6006e-21],\n",
      "        [3.0326e-22, 1.0678e-19, 5.9813e-20, 4.7581e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "347000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3456e-40, 1.0431e-41, 2.5887e-21],\n",
      "        [3.0352e-22, 1.0687e-19, 5.9866e-20, 4.7467e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "348000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3451e-40, 1.0426e-41, 2.5911e-21],\n",
      "        [3.0348e-22, 1.0685e-19, 5.9857e-20, 4.7491e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "349000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3375e-40, 1.0364e-41, 2.6206e-21],\n",
      "        [3.0283e-22, 1.0663e-19, 5.9715e-20, 4.7775e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "350000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3357e-40, 1.0349e-41, 2.6271e-21],\n",
      "        [3.0269e-22, 1.0657e-19, 5.9680e-20, 4.7836e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "351000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3472e-40, 1.0444e-41, 2.5827e-21],\n",
      "        [3.0368e-22, 1.0692e-19, 5.9899e-20, 4.7409e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "352000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3408e-40, 1.0389e-41, 2.6073e-21],\n",
      "        [3.0312e-22, 1.0673e-19, 5.9773e-20, 4.7647e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "353000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3479e-40, 1.0449e-41, 2.5797e-21],\n",
      "        [3.0373e-22, 1.0693e-19, 5.9912e-20, 4.7378e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "354000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3445e-40, 1.0420e-41, 2.5931e-21],\n",
      "        [3.0342e-22, 1.0683e-19, 5.9844e-20, 4.7507e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "355000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3561e-40, 1.0517e-41, 2.5489e-21],\n",
      "        [3.0441e-22, 1.0718e-19, 6.0062e-20, 4.7080e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "356000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3411e-40, 1.0393e-41, 2.6061e-21],\n",
      "        [3.0314e-22, 1.0673e-19, 5.9786e-20, 4.7635e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "357000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3458e-40, 1.0431e-41, 2.5879e-21],\n",
      "        [3.0354e-22, 1.0688e-19, 5.9868e-20, 4.7459e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "358000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3482e-40, 1.0452e-41, 2.5784e-21],\n",
      "        [3.0376e-22, 1.0695e-19, 5.9917e-20, 4.7365e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "359000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3429e-40, 1.0407e-41, 2.5985e-21],\n",
      "        [3.0330e-22, 1.0678e-19, 5.9813e-20, 4.7561e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "360000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3472e-40, 1.0442e-41, 2.5825e-21],\n",
      "        [3.0366e-22, 1.0692e-19, 5.9894e-20, 4.7406e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "361000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3392e-40, 1.0377e-41, 2.6133e-21],\n",
      "        [3.0298e-22, 1.0667e-19, 5.9742e-20, 4.7703e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "362000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3469e-40, 1.0440e-41, 2.5839e-21],\n",
      "        [3.0363e-22, 1.0690e-19, 5.9890e-20, 4.7422e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "363000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3468e-40, 1.0441e-41, 2.5835e-21],\n",
      "        [3.0364e-22, 1.0690e-19, 5.9890e-20, 4.7417e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "364000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3427e-40, 1.0405e-41, 2.5995e-21],\n",
      "        [3.0328e-22, 1.0678e-19, 5.9806e-20, 4.7570e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "365000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3438e-40, 1.0416e-41, 2.5949e-21],\n",
      "        [3.0335e-22, 1.0681e-19, 5.9831e-20, 4.7526e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "366000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3482e-40, 1.0452e-41, 2.5781e-21],\n",
      "        [3.0374e-22, 1.0693e-19, 5.9914e-20, 4.7364e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "367000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3462e-40, 1.0434e-41, 2.5860e-21],\n",
      "        [3.0355e-22, 1.0688e-19, 5.9873e-20, 4.7441e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "368000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3543e-40, 1.0501e-41, 2.5549e-21],\n",
      "        [3.0425e-22, 1.0712e-19, 6.0027e-20, 4.7138e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "369000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3476e-40, 1.0447e-41, 2.5800e-21],\n",
      "        [3.0370e-22, 1.0693e-19, 5.9901e-20, 4.7383e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "370000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3521e-40, 1.0483e-41, 2.5632e-21],\n",
      "        [3.0407e-22, 1.0705e-19, 5.9985e-20, 4.7217e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "371000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3482e-40, 1.0452e-41, 2.5780e-21],\n",
      "        [3.0374e-22, 1.0694e-19, 5.9912e-20, 4.7364e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "372000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3513e-40, 1.0476e-41, 2.5663e-21],\n",
      "        [3.0399e-22, 1.0703e-19, 5.9967e-20, 4.7250e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "373000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3487e-40, 1.0455e-41, 2.5759e-21],\n",
      "        [3.0380e-22, 1.0695e-19, 5.9923e-20, 4.7344e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "374000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3444e-40, 1.0419e-41, 2.5922e-21],\n",
      "        [3.0343e-22, 1.0683e-19, 5.9839e-20, 4.7500e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "375000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3541e-40, 1.0499e-41, 2.5552e-21],\n",
      "        [3.0426e-22, 1.0712e-19, 6.0020e-20, 4.7143e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "376000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3555e-40, 1.0511e-41, 2.5498e-21],\n",
      "        [3.0438e-22, 1.0716e-19, 6.0047e-20, 4.7092e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "377000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3437e-40, 1.0413e-41, 2.5949e-21],\n",
      "        [3.0336e-22, 1.0680e-19, 5.9826e-20, 4.7528e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "378000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3447e-40, 1.0421e-41, 2.5907e-21],\n",
      "        [3.0345e-22, 1.0683e-19, 5.9842e-20, 4.7486e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "379000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3385e-40, 1.0371e-41, 2.6145e-21],\n",
      "        [3.0293e-22, 1.0664e-19, 5.9727e-20, 4.7715e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "380000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3515e-40, 1.0476e-41, 2.5651e-21],\n",
      "        [3.0403e-22, 1.0704e-19, 5.9970e-20, 4.7238e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "381000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3493e-40, 1.0459e-41, 2.5733e-21],\n",
      "        [3.0384e-22, 1.0697e-19, 5.9932e-20, 4.7319e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "382000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3525e-40, 1.0486e-41, 2.5612e-21],\n",
      "        [3.0411e-22, 1.0707e-19, 5.9989e-20, 4.7201e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "383000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3420e-40, 1.0399e-41, 2.6011e-21],\n",
      "        [3.0322e-22, 1.0675e-19, 5.9791e-20, 4.7587e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "384000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3550e-40, 1.0506e-41, 2.5517e-21],\n",
      "        [3.0432e-22, 1.0714e-19, 6.0033e-20, 4.7109e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "385000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3461e-40, 1.0433e-41, 2.5853e-21],\n",
      "        [3.0356e-22, 1.0687e-19, 5.9864e-20, 4.7433e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "386000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3490e-40, 1.0458e-41, 2.5737e-21],\n",
      "        [3.0382e-22, 1.0696e-19, 5.9923e-20, 4.7324e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "387000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3444e-40, 1.0419e-41, 2.5916e-21],\n",
      "        [3.0343e-22, 1.0682e-19, 5.9833e-20, 4.7494e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "388000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3499e-40, 1.0465e-41, 2.5707e-21],\n",
      "        [3.0390e-22, 1.0699e-19, 5.9937e-20, 4.7293e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "389000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3531e-40, 1.0490e-41, 2.5581e-21],\n",
      "        [3.0417e-22, 1.0708e-19, 6.0001e-20, 4.7172e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "390000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3552e-40, 1.0508e-41, 2.5506e-21],\n",
      "        [3.0433e-22, 1.0714e-19, 6.0040e-20, 4.7097e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "391000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3406e-40, 1.0386e-41, 2.6067e-21],\n",
      "        [3.0308e-22, 1.0671e-19, 5.9758e-20, 4.7641e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "392000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3396e-40, 1.0379e-41, 2.6107e-21],\n",
      "        [3.0301e-22, 1.0667e-19, 5.9744e-20, 4.7679e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "393000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3445e-40, 1.0420e-41, 2.5916e-21],\n",
      "        [3.0343e-22, 1.0682e-19, 5.9841e-20, 4.7494e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "394000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3503e-40, 1.0466e-41, 2.5695e-21],\n",
      "        [3.0394e-22, 1.0700e-19, 5.9945e-20, 4.7280e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "395000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3512e-40, 1.0473e-41, 2.5661e-21],\n",
      "        [3.0400e-22, 1.0703e-19, 5.9961e-20, 4.7248e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "396000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3473e-40, 1.0442e-41, 2.5812e-21],\n",
      "        [3.0366e-22, 1.0691e-19, 5.9888e-20, 4.7394e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "397000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3488e-40, 1.0454e-41, 2.5750e-21],\n",
      "        [3.0380e-22, 1.0695e-19, 5.9915e-20, 4.7334e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "398000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3407e-40, 1.0388e-41, 2.6064e-21],\n",
      "        [3.0311e-22, 1.0671e-19, 5.9762e-20, 4.7636e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "399000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3529e-40, 1.0489e-41, 2.5595e-21],\n",
      "        [3.0416e-22, 1.0707e-19, 5.9994e-20, 4.7184e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "400000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3490e-40, 1.0456e-41, 2.5741e-21],\n",
      "        [3.0381e-22, 1.0696e-19, 5.9925e-20, 4.7325e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "401000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3525e-40, 1.0485e-41, 2.5607e-21],\n",
      "        [3.0411e-22, 1.0706e-19, 5.9985e-20, 4.7198e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "402000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3551e-40, 1.0507e-41, 2.5507e-21],\n",
      "        [3.0433e-22, 1.0714e-19, 6.0036e-20, 4.7099e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "403000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3548e-40, 1.0504e-41, 2.5516e-21],\n",
      "        [3.0432e-22, 1.0713e-19, 6.0029e-20, 4.7107e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "404000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3491e-40, 1.0456e-41, 2.5734e-21],\n",
      "        [3.0381e-22, 1.0696e-19, 5.9917e-20, 4.7319e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "405000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3469e-40, 1.0438e-41, 2.5817e-21],\n",
      "        [3.0363e-22, 1.0689e-19, 5.9877e-20, 4.7402e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "406000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3430e-40, 1.0406e-41, 2.5966e-21],\n",
      "        [3.0331e-22, 1.0677e-19, 5.9806e-20, 4.7541e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "407000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3490e-40, 1.0455e-41, 2.5736e-21],\n",
      "        [3.0382e-22, 1.0696e-19, 5.9919e-20, 4.7321e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "408000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3488e-40, 1.0454e-41, 2.5743e-21],\n",
      "        [3.0379e-22, 1.0695e-19, 5.9912e-20, 4.7328e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "409000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3454e-40, 1.0426e-41, 2.5871e-21],\n",
      "        [3.0350e-22, 1.0685e-19, 5.9846e-20, 4.7451e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "410000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3507e-40, 1.0471e-41, 2.5668e-21],\n",
      "        [3.0395e-22, 1.0700e-19, 5.9947e-20, 4.7254e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "411000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3509e-40, 1.0472e-41, 2.5658e-21],\n",
      "        [3.0398e-22, 1.0701e-19, 5.9952e-20, 4.7246e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "412000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3434e-40, 1.0410e-41, 2.5945e-21],\n",
      "        [3.0336e-22, 1.0678e-19, 5.9813e-20, 4.7523e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "413000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3524e-40, 1.0485e-41, 2.5604e-21],\n",
      "        [3.0410e-22, 1.0705e-19, 5.9981e-20, 4.7191e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "414000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3434e-40, 1.0409e-41, 2.5947e-21],\n",
      "        [3.0332e-22, 1.0678e-19, 5.9809e-20, 4.7523e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "415000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3407e-40, 1.0386e-41, 2.6051e-21],\n",
      "        [3.0310e-22, 1.0671e-19, 5.9762e-20, 4.7623e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "416000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3439e-40, 1.0413e-41, 2.5925e-21],\n",
      "        [3.0339e-22, 1.0680e-19, 5.9820e-20, 4.7503e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "417000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3580e-40, 1.0529e-41, 2.5395e-21],\n",
      "        [3.0458e-22, 1.0722e-19, 6.0082e-20, 4.6990e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "418000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3563e-40, 1.0515e-41, 2.5457e-21],\n",
      "        [3.0443e-22, 1.0717e-19, 6.0055e-20, 4.7047e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "419000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3389e-40, 1.0371e-41, 2.6120e-21],\n",
      "        [3.0295e-22, 1.0665e-19, 5.9726e-20, 4.7690e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "420000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3503e-40, 1.0466e-41, 2.5678e-21],\n",
      "        [3.0394e-22, 1.0699e-19, 5.9941e-20, 4.7264e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "421000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3630e-40, 1.0571e-41, 2.5202e-21],\n",
      "        [3.0499e-22, 1.0737e-19, 6.0176e-20, 4.6802e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "422000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3599e-40, 1.0545e-41, 2.5318e-21],\n",
      "        [3.0475e-22, 1.0728e-19, 6.0121e-20, 4.6915e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "423000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3541e-40, 1.0497e-41, 2.5537e-21],\n",
      "        [3.0425e-22, 1.0711e-19, 6.0012e-20, 4.7128e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "424000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3441e-40, 1.0414e-41, 2.5916e-21],\n",
      "        [3.0341e-22, 1.0680e-19, 5.9822e-20, 4.7494e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "425000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3436e-40, 1.0410e-41, 2.5940e-21],\n",
      "        [3.0335e-22, 1.0679e-19, 5.9813e-20, 4.7519e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "426000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3463e-40, 1.0433e-41, 2.5832e-21],\n",
      "        [3.0357e-22, 1.0687e-19, 5.9861e-20, 4.7416e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "427000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3517e-40, 1.0478e-41, 2.5627e-21],\n",
      "        [3.0406e-22, 1.0704e-19, 5.9965e-20, 4.7215e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "428000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3534e-40, 1.0492e-41, 2.5565e-21],\n",
      "        [3.0420e-22, 1.0708e-19, 5.9994e-20, 4.7156e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "429000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3523e-40, 1.0482e-41, 2.5606e-21],\n",
      "        [3.0410e-22, 1.0705e-19, 5.9978e-20, 4.7194e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "430000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3466e-40, 1.0434e-41, 2.5826e-21],\n",
      "        [3.0360e-22, 1.0688e-19, 5.9864e-20, 4.7406e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "431000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3464e-40, 1.0434e-41, 2.5829e-21],\n",
      "        [3.0359e-22, 1.0687e-19, 5.9866e-20, 4.7412e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "432000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3494e-40, 1.0458e-41, 2.5716e-21],\n",
      "        [3.0383e-22, 1.0696e-19, 5.9921e-20, 4.7299e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "433000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3473e-40, 1.0441e-41, 2.5795e-21],\n",
      "        [3.0368e-22, 1.0690e-19, 5.9883e-20, 4.7378e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "434000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3428e-40, 1.0403e-41, 2.5968e-21],\n",
      "        [3.0330e-22, 1.0677e-19, 5.9799e-20, 4.7545e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "435000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3460e-40, 1.0430e-41, 2.5846e-21],\n",
      "        [3.0356e-22, 1.0686e-19, 5.9857e-20, 4.7428e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "436000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3483e-40, 1.0448e-41, 2.5754e-21],\n",
      "        [3.0377e-22, 1.0692e-19, 5.9899e-20, 4.7338e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "437000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3462e-40, 1.0431e-41, 2.5836e-21],\n",
      "        [3.0357e-22, 1.0687e-19, 5.9861e-20, 4.7416e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "438000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3421e-40, 1.0398e-41, 2.5996e-21],\n",
      "        [3.0322e-22, 1.0675e-19, 5.9782e-20, 4.7571e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "439000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3466e-40, 1.0434e-41, 2.5821e-21],\n",
      "        [3.0362e-22, 1.0687e-19, 5.9866e-20, 4.7403e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "440000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3604e-40, 1.0548e-41, 2.5304e-21],\n",
      "        [3.0478e-22, 1.0729e-19, 6.0124e-20, 4.6904e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "441000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3487e-40, 1.0452e-41, 2.5736e-21],\n",
      "        [3.0381e-22, 1.0694e-19, 5.9906e-20, 4.7321e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "442000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3550e-40, 1.0503e-41, 2.5503e-21],\n",
      "        [3.0433e-22, 1.0713e-19, 6.0025e-20, 4.7094e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "443000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3413e-40, 1.0389e-41, 2.6025e-21],\n",
      "        [3.0315e-22, 1.0671e-19, 5.9757e-20, 4.7600e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "444000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3512e-40, 1.0472e-41, 2.5645e-21],\n",
      "        [3.0400e-22, 1.0701e-19, 5.9954e-20, 4.7234e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "445000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3565e-40, 1.0517e-41, 2.5444e-21],\n",
      "        [3.0446e-22, 1.0717e-19, 6.0053e-20, 4.7038e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "446000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3542e-40, 1.0497e-41, 2.5528e-21],\n",
      "        [3.0426e-22, 1.0710e-19, 6.0011e-20, 4.7120e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "447000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.6115e-43, 1.3573e-40, 1.0522e-41, 2.5412e-21],\n",
      "        [3.0453e-22, 1.0720e-19, 6.0066e-20, 4.7007e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "448000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.6115e-43, 1.3567e-40, 1.0518e-41, 2.5437e-21],\n",
      "        [3.0446e-22, 1.0717e-19, 6.0055e-20, 4.7033e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "449000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3502e-40, 1.0463e-41, 2.5681e-21],\n",
      "        [3.0392e-22, 1.0698e-19, 5.9934e-20, 4.7269e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "450000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3439e-40, 1.0412e-41, 2.5925e-21],\n",
      "        [3.0339e-22, 1.0680e-19, 5.9819e-20, 4.7504e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "451000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3548e-40, 1.0501e-41, 2.5507e-21],\n",
      "        [3.0432e-22, 1.0712e-19, 6.0022e-20, 4.7099e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "452000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3553e-40, 1.0506e-41, 2.5489e-21],\n",
      "        [3.0436e-22, 1.0713e-19, 6.0029e-20, 4.7082e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "453000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3531e-40, 1.0487e-41, 2.5574e-21],\n",
      "        [3.0416e-22, 1.0707e-19, 5.9985e-20, 4.7165e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "454000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3421e-40, 1.0398e-41, 2.5989e-21],\n",
      "        [3.0324e-22, 1.0674e-19, 5.9780e-20, 4.7565e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "455000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3538e-40, 1.0493e-41, 2.5545e-21],\n",
      "        [3.0421e-22, 1.0708e-19, 6.0000e-20, 4.7136e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "456000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3409e-40, 1.0388e-41, 2.6037e-21],\n",
      "        [3.0314e-22, 1.0670e-19, 5.9755e-20, 4.7609e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "457000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3489e-40, 1.0454e-41, 2.5726e-21],\n",
      "        [3.0381e-22, 1.0694e-19, 5.9906e-20, 4.7313e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "458000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3489e-40, 1.0454e-41, 2.5730e-21],\n",
      "        [3.0381e-22, 1.0694e-19, 5.9906e-20, 4.7315e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "459000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3429e-40, 1.0403e-41, 2.5958e-21],\n",
      "        [3.0331e-22, 1.0676e-19, 5.9791e-20, 4.7536e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "460000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3486e-40, 1.0451e-41, 2.5739e-21],\n",
      "        [3.0380e-22, 1.0693e-19, 5.9901e-20, 4.7324e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "461000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3537e-40, 1.0493e-41, 2.5548e-21],\n",
      "        [3.0423e-22, 1.0708e-19, 5.9996e-20, 4.7140e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "462000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3441e-40, 1.0413e-41, 2.5913e-21],\n",
      "        [3.0340e-22, 1.0679e-19, 5.9815e-20, 4.7493e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "463000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3405e-40, 1.0382e-41, 2.6052e-21],\n",
      "        [3.0308e-22, 1.0669e-19, 5.9744e-20, 4.7628e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "464000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3502e-40, 1.0462e-41, 2.5678e-21],\n",
      "        [3.0393e-22, 1.0697e-19, 5.9928e-20, 4.7264e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "465000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3419e-40, 1.0395e-41, 2.5994e-21],\n",
      "        [3.0322e-22, 1.0673e-19, 5.9771e-20, 4.7570e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "466000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3419e-40, 1.0395e-41, 2.5995e-21],\n",
      "        [3.0321e-22, 1.0673e-19, 5.9769e-20, 4.7571e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "467000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3411e-40, 1.0388e-41, 2.6026e-21],\n",
      "        [3.0313e-22, 1.0671e-19, 5.9755e-20, 4.7602e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "468000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3544e-40, 1.0497e-41, 2.5517e-21],\n",
      "        [3.0428e-22, 1.0710e-19, 6.0007e-20, 4.7109e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "469000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.6115e-43, 1.3575e-40, 1.0522e-41, 2.5399e-21],\n",
      "        [3.0455e-22, 1.0720e-19, 6.0066e-20, 4.6994e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "470000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3476e-40, 1.0441e-41, 2.5778e-21],\n",
      "        [3.0370e-22, 1.0690e-19, 5.9879e-20, 4.7361e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "471000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3503e-40, 1.0463e-41, 2.5674e-21],\n",
      "        [3.0392e-22, 1.0698e-19, 5.9930e-20, 4.7260e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "472000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3480e-40, 1.0444e-41, 2.5761e-21],\n",
      "        [3.0373e-22, 1.0691e-19, 5.9883e-20, 4.7345e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "473000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3438e-40, 1.0410e-41, 2.5925e-21],\n",
      "        [3.0338e-22, 1.0678e-19, 5.9808e-20, 4.7503e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "474000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3473e-40, 1.0438e-41, 2.5788e-21],\n",
      "        [3.0368e-22, 1.0689e-19, 5.9872e-20, 4.7371e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "475000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3558e-40, 1.0510e-41, 2.5464e-21],\n",
      "        [3.0440e-22, 1.0714e-19, 6.0033e-20, 4.7056e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "476000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3513e-40, 1.0472e-41, 2.5639e-21],\n",
      "        [3.0401e-22, 1.0701e-19, 5.9947e-20, 4.7227e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "477000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.6115e-43, 1.3613e-40, 1.0553e-41, 2.5266e-21],\n",
      "        [3.0486e-22, 1.0731e-19, 6.0133e-20, 4.6865e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "478000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3476e-40, 1.0441e-41, 2.5777e-21],\n",
      "        [3.0370e-22, 1.0690e-19, 5.9877e-20, 4.7361e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "479000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3498e-40, 1.0459e-41, 2.5693e-21],\n",
      "        [3.0390e-22, 1.0696e-19, 5.9915e-20, 4.7280e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "480000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3594e-40, 1.0539e-41, 2.5332e-21],\n",
      "        [3.0470e-22, 1.0725e-19, 6.0099e-20, 4.6929e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "481000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3567e-40, 1.0515e-41, 2.5432e-21],\n",
      "        [3.0447e-22, 1.0717e-19, 6.0047e-20, 4.7028e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "482000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3513e-40, 1.0472e-41, 2.5638e-21],\n",
      "        [3.0400e-22, 1.0701e-19, 5.9948e-20, 4.7225e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "483000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3490e-40, 1.0452e-41, 2.5724e-21],\n",
      "        [3.0382e-22, 1.0694e-19, 5.9904e-20, 4.7311e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "484000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3423e-40, 1.0396e-41, 2.5980e-21],\n",
      "        [3.0325e-22, 1.0674e-19, 5.9775e-20, 4.7558e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "485000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3447e-40, 1.0417e-41, 2.5887e-21],\n",
      "        [3.0345e-22, 1.0681e-19, 5.9817e-20, 4.7467e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "486000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3496e-40, 1.0458e-41, 2.5699e-21],\n",
      "        [3.0386e-22, 1.0696e-19, 5.9915e-20, 4.7286e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "487000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3415e-40, 1.0391e-41, 2.6014e-21],\n",
      "        [3.0318e-22, 1.0671e-19, 5.9758e-20, 4.7590e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "488000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3416e-40, 1.0391e-41, 2.6008e-21],\n",
      "        [3.0318e-22, 1.0671e-19, 5.9762e-20, 4.7584e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "489000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3573e-40, 1.0520e-41, 2.5406e-21],\n",
      "        [3.0454e-22, 1.0719e-19, 6.0056e-20, 4.7003e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "490000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.6115e-43, 1.3572e-40, 1.0520e-41, 2.5410e-21],\n",
      "        [3.0452e-22, 1.0718e-19, 6.0055e-20, 4.7005e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "491000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3574e-40, 1.0522e-41, 2.5400e-21],\n",
      "        [3.0453e-22, 1.0719e-19, 6.0062e-20, 4.6995e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "492000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3513e-40, 1.0471e-41, 2.5634e-21],\n",
      "        [3.0403e-22, 1.0700e-19, 5.9943e-20, 4.7223e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "493000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3476e-40, 1.0440e-41, 2.5774e-21],\n",
      "        [3.0370e-22, 1.0689e-19, 5.9875e-20, 4.7360e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "494000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3427e-40, 1.0402e-41, 2.5959e-21],\n",
      "        [3.0330e-22, 1.0675e-19, 5.9786e-20, 4.7536e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "495000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3397e-40, 1.0377e-41, 2.6079e-21],\n",
      "        [3.0303e-22, 1.0666e-19, 5.9729e-20, 4.7652e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "496000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3454e-40, 1.0423e-41, 2.5857e-21],\n",
      "        [3.0350e-22, 1.0683e-19, 5.9831e-20, 4.7438e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "497000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3446e-40, 1.0416e-41, 2.5889e-21],\n",
      "        [3.0345e-22, 1.0681e-19, 5.9820e-20, 4.7471e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "498000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3516e-40, 1.0473e-41, 2.5622e-21],\n",
      "        [3.0405e-22, 1.0701e-19, 5.9947e-20, 4.7212e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "499000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3389e-40, 1.0368e-41, 2.6111e-21],\n",
      "        [3.0295e-22, 1.0663e-19, 5.9709e-20, 4.7683e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "500000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3523e-40, 1.0479e-41, 2.5595e-21],\n",
      "        [3.0408e-22, 1.0704e-19, 5.9963e-20, 4.7185e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "501000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3423e-40, 1.0396e-41, 2.5972e-21],\n",
      "        [3.0324e-22, 1.0673e-19, 5.9773e-20, 4.7551e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "502000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3373e-40, 1.0356e-41, 2.6169e-21],\n",
      "        [3.0283e-22, 1.0658e-19, 5.9682e-20, 4.7738e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "503000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5695e-43, 1.3298e-40, 1.0294e-41, 2.6463e-21],\n",
      "        [3.0219e-22, 1.0636e-19, 5.9533e-20, 4.8020e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "504000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3485e-40, 1.0448e-41, 2.5734e-21],\n",
      "        [3.0379e-22, 1.0692e-19, 5.9892e-20, 4.7319e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "505000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3494e-40, 1.0455e-41, 2.5700e-21],\n",
      "        [3.0385e-22, 1.0694e-19, 5.9906e-20, 4.7287e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "506000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3490e-40, 1.0452e-41, 2.5715e-21],\n",
      "        [3.0382e-22, 1.0693e-19, 5.9897e-20, 4.7302e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "507000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3544e-40, 1.0497e-41, 2.5509e-21],\n",
      "        [3.0430e-22, 1.0710e-19, 6.0003e-20, 4.7103e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "508000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3550e-40, 1.0501e-41, 2.5490e-21],\n",
      "        [3.0433e-22, 1.0712e-19, 6.0011e-20, 4.7084e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "509000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3430e-40, 1.0402e-41, 2.5946e-21],\n",
      "        [3.0331e-22, 1.0675e-19, 5.9784e-20, 4.7525e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "510000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3573e-40, 1.0520e-41, 2.5400e-21],\n",
      "        [3.0453e-22, 1.0718e-19, 6.0055e-20, 4.6995e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "511000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3458e-40, 1.0426e-41, 2.5833e-21],\n",
      "        [3.0356e-22, 1.0684e-19, 5.9841e-20, 4.7416e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "512000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3464e-40, 1.0430e-41, 2.5810e-21],\n",
      "        [3.0359e-22, 1.0686e-19, 5.9850e-20, 4.7391e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "513000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3437e-40, 1.0409e-41, 2.5917e-21],\n",
      "        [3.0339e-22, 1.0678e-19, 5.9802e-20, 4.7494e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "514000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3429e-40, 1.0400e-41, 2.5950e-21],\n",
      "        [3.0331e-22, 1.0675e-19, 5.9780e-20, 4.7529e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "515000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3483e-40, 1.0445e-41, 2.5741e-21],\n",
      "        [3.0377e-22, 1.0691e-19, 5.9886e-20, 4.7326e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "516000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3531e-40, 1.0485e-41, 2.5556e-21],\n",
      "        [3.0417e-22, 1.0705e-19, 5.9972e-20, 4.7146e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "517000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3513e-40, 1.0471e-41, 2.5623e-21],\n",
      "        [3.0404e-22, 1.0700e-19, 5.9945e-20, 4.7214e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "518000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3505e-40, 1.0463e-41, 2.5656e-21],\n",
      "        [3.0395e-22, 1.0698e-19, 5.9923e-20, 4.7243e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "519000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3490e-40, 1.0452e-41, 2.5710e-21],\n",
      "        [3.0382e-22, 1.0693e-19, 5.9897e-20, 4.7298e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "520000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3342e-40, 1.0329e-41, 2.6283e-21],\n",
      "        [3.0256e-22, 1.0649e-19, 5.9615e-20, 4.7848e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "521000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3416e-40, 1.0391e-41, 2.5992e-21],\n",
      "        [3.0320e-22, 1.0671e-19, 5.9755e-20, 4.7568e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "522000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3437e-40, 1.0407e-41, 2.5913e-21],\n",
      "        [3.0338e-22, 1.0677e-19, 5.9795e-20, 4.7494e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "523000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3474e-40, 1.0438e-41, 2.5773e-21],\n",
      "        [3.0369e-22, 1.0688e-19, 5.9868e-20, 4.7358e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "524000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3516e-40, 1.0473e-41, 2.5610e-21],\n",
      "        [3.0405e-22, 1.0701e-19, 5.9947e-20, 4.7199e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "525000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3498e-40, 1.0458e-41, 2.5677e-21],\n",
      "        [3.0389e-22, 1.0695e-19, 5.9910e-20, 4.7263e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "526000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3508e-40, 1.0465e-41, 2.5642e-21],\n",
      "        [3.0396e-22, 1.0698e-19, 5.9926e-20, 4.7231e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "527000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3394e-40, 1.0372e-41, 2.6080e-21],\n",
      "        [3.0301e-22, 1.0664e-19, 5.9711e-20, 4.7652e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "528000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3389e-40, 1.0368e-41, 2.6096e-21],\n",
      "        [3.0296e-22, 1.0663e-19, 5.9700e-20, 4.7670e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "529000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3411e-40, 1.0385e-41, 2.6015e-21],\n",
      "        [3.0315e-22, 1.0669e-19, 5.9742e-20, 4.7591e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "530000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3437e-40, 1.0407e-41, 2.5911e-21],\n",
      "        [3.0338e-22, 1.0677e-19, 5.9791e-20, 4.7491e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "531000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3473e-40, 1.0437e-41, 2.5774e-21],\n",
      "        [3.0367e-22, 1.0688e-19, 5.9857e-20, 4.7360e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "532000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3429e-40, 1.0400e-41, 2.5943e-21],\n",
      "        [3.0330e-22, 1.0674e-19, 5.9773e-20, 4.7522e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "533000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3476e-40, 1.0440e-41, 2.5763e-21],\n",
      "        [3.0371e-22, 1.0689e-19, 5.9862e-20, 4.7348e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "534000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3495e-40, 1.0455e-41, 2.5694e-21],\n",
      "        [3.0386e-22, 1.0694e-19, 5.9901e-20, 4.7280e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "535000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3527e-40, 1.0480e-41, 2.5569e-21],\n",
      "        [3.0415e-22, 1.0704e-19, 5.9963e-20, 4.7161e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "536000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3434e-40, 1.0405e-41, 2.5926e-21],\n",
      "        [3.0333e-22, 1.0676e-19, 5.9784e-20, 4.7506e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "537000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3457e-40, 1.0424e-41, 2.5834e-21],\n",
      "        [3.0356e-22, 1.0683e-19, 5.9830e-20, 4.7417e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "538000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3412e-40, 1.0385e-41, 2.6013e-21],\n",
      "        [3.0316e-22, 1.0669e-19, 5.9744e-20, 4.7589e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "539000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3408e-40, 1.0384e-41, 2.6022e-21],\n",
      "        [3.0313e-22, 1.0668e-19, 5.9737e-20, 4.7597e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "540000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3517e-40, 1.0472e-41, 2.5610e-21],\n",
      "        [3.0405e-22, 1.0700e-19, 5.9941e-20, 4.7199e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "541000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3418e-40, 1.0391e-41, 2.5990e-21],\n",
      "        [3.0320e-22, 1.0671e-19, 5.9753e-20, 4.7567e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "542000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3445e-40, 1.0414e-41, 2.5880e-21],\n",
      "        [3.0344e-22, 1.0679e-19, 5.9808e-20, 4.7459e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "543000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3414e-40, 1.0386e-41, 2.6005e-21],\n",
      "        [3.0316e-22, 1.0670e-19, 5.9744e-20, 4.7581e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "544000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3347e-40, 1.0332e-41, 2.6266e-21],\n",
      "        [3.0261e-22, 1.0650e-19, 5.9618e-20, 4.7832e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "545000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3398e-40, 1.0374e-41, 2.6064e-21],\n",
      "        [3.0305e-22, 1.0665e-19, 5.9715e-20, 4.7638e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "546000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3505e-40, 1.0462e-41, 2.5653e-21],\n",
      "        [3.0395e-22, 1.0697e-19, 5.9917e-20, 4.7243e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "547000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3489e-40, 1.0449e-41, 2.5712e-21],\n",
      "        [3.0382e-22, 1.0692e-19, 5.9888e-20, 4.7299e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "548000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3499e-40, 1.0458e-41, 2.5676e-21],\n",
      "        [3.0391e-22, 1.0695e-19, 5.9904e-20, 4.7266e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "549000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3422e-40, 1.0395e-41, 2.5969e-21],\n",
      "        [3.0325e-22, 1.0672e-19, 5.9762e-20, 4.7548e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "550000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3405e-40, 1.0379e-41, 2.6040e-21],\n",
      "        [3.0309e-22, 1.0667e-19, 5.9726e-20, 4.7616e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "551000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3538e-40, 1.0489e-41, 2.5528e-21],\n",
      "        [3.0425e-22, 1.0707e-19, 5.9979e-20, 4.7122e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "552000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3453e-40, 1.0420e-41, 2.5844e-21],\n",
      "        [3.0351e-22, 1.0681e-19, 5.9820e-20, 4.7425e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "553000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3440e-40, 1.0409e-41, 2.5899e-21],\n",
      "        [3.0340e-22, 1.0677e-19, 5.9799e-20, 4.7481e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "554000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3503e-40, 1.0461e-41, 2.5660e-21],\n",
      "        [3.0394e-22, 1.0696e-19, 5.9912e-20, 4.7248e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "555000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3476e-40, 1.0438e-41, 2.5759e-21],\n",
      "        [3.0371e-22, 1.0688e-19, 5.9862e-20, 4.7344e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "556000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3444e-40, 1.0413e-41, 2.5882e-21],\n",
      "        [3.0344e-22, 1.0678e-19, 5.9800e-20, 4.7462e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "557000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3385e-40, 1.0364e-41, 2.6113e-21],\n",
      "        [3.0293e-22, 1.0661e-19, 5.9691e-20, 4.7686e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "558000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3480e-40, 1.0442e-41, 2.5743e-21],\n",
      "        [3.0374e-22, 1.0690e-19, 5.9872e-20, 4.7329e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "559000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3480e-40, 1.0441e-41, 2.5743e-21],\n",
      "        [3.0374e-22, 1.0689e-19, 5.9868e-20, 4.7328e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "560000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3401e-40, 1.0375e-41, 2.6047e-21],\n",
      "        [3.0307e-22, 1.0666e-19, 5.9716e-20, 4.7622e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "561000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3530e-40, 1.0482e-41, 2.5552e-21],\n",
      "        [3.0416e-22, 1.0704e-19, 5.9965e-20, 4.7143e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "562000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3499e-40, 1.0456e-41, 2.5667e-21],\n",
      "        [3.0389e-22, 1.0695e-19, 5.9899e-20, 4.7254e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "563000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3415e-40, 1.0388e-41, 2.5995e-21],\n",
      "        [3.0319e-22, 1.0669e-19, 5.9742e-20, 4.7573e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "564000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3447e-40, 1.0413e-41, 2.5870e-21],\n",
      "        [3.0346e-22, 1.0679e-19, 5.9804e-20, 4.7452e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "565000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3401e-40, 1.0375e-41, 2.6047e-21],\n",
      "        [3.0306e-22, 1.0665e-19, 5.9718e-20, 4.7622e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "566000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3492e-40, 1.0451e-41, 2.5697e-21],\n",
      "        [3.0383e-22, 1.0692e-19, 5.9886e-20, 4.7285e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "567000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3489e-40, 1.0448e-41, 2.5708e-21],\n",
      "        [3.0381e-22, 1.0692e-19, 5.9883e-20, 4.7296e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "568000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3490e-40, 1.0448e-41, 2.5703e-21],\n",
      "        [3.0382e-22, 1.0692e-19, 5.9884e-20, 4.7290e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "569000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3440e-40, 1.0407e-41, 2.5893e-21],\n",
      "        [3.0339e-22, 1.0677e-19, 5.9786e-20, 4.7475e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "570000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3462e-40, 1.0427e-41, 2.5809e-21],\n",
      "        [3.0357e-22, 1.0683e-19, 5.9830e-20, 4.7393e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "571000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3529e-40, 1.0480e-41, 2.5556e-21],\n",
      "        [3.0415e-22, 1.0704e-19, 5.9956e-20, 4.7148e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "572000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3452e-40, 1.0417e-41, 2.5847e-21],\n",
      "        [3.0348e-22, 1.0680e-19, 5.9811e-20, 4.7429e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "573000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3476e-40, 1.0438e-41, 2.5758e-21],\n",
      "        [3.0370e-22, 1.0688e-19, 5.9855e-20, 4.7344e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "574000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3494e-40, 1.0452e-41, 2.5688e-21],\n",
      "        [3.0384e-22, 1.0692e-19, 5.9890e-20, 4.7273e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "575000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3428e-40, 1.0398e-41, 2.5941e-21],\n",
      "        [3.0330e-22, 1.0673e-19, 5.9764e-20, 4.7520e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "576000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3347e-40, 1.0330e-41, 2.6259e-21],\n",
      "        [3.0259e-22, 1.0648e-19, 5.9609e-20, 4.7826e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "577000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3444e-40, 1.0412e-41, 2.5879e-21],\n",
      "        [3.0344e-22, 1.0678e-19, 5.9797e-20, 4.7459e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "578000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3520e-40, 1.0473e-41, 2.5586e-21],\n",
      "        [3.0407e-22, 1.0700e-19, 5.9936e-20, 4.7176e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "579000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3521e-40, 1.0475e-41, 2.5583e-21],\n",
      "        [3.0409e-22, 1.0701e-19, 5.9939e-20, 4.7175e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "580000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3432e-40, 1.0400e-41, 2.5921e-21],\n",
      "        [3.0332e-22, 1.0674e-19, 5.9769e-20, 4.7501e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "581000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3451e-40, 1.0416e-41, 2.5851e-21],\n",
      "        [3.0349e-22, 1.0680e-19, 5.9806e-20, 4.7432e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "582000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3551e-40, 1.0499e-41, 2.5470e-21],\n",
      "        [3.0433e-22, 1.0710e-19, 5.9996e-20, 4.7064e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "583000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3500e-40, 1.0458e-41, 2.5660e-21],\n",
      "        [3.0392e-22, 1.0694e-19, 5.9901e-20, 4.7248e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "584000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3389e-40, 1.0365e-41, 2.6091e-21],\n",
      "        [3.0295e-22, 1.0661e-19, 5.9687e-20, 4.7666e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "585000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3478e-40, 1.0438e-41, 2.5744e-21],\n",
      "        [3.0373e-22, 1.0688e-19, 5.9857e-20, 4.7331e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "586000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3496e-40, 1.0454e-41, 2.5674e-21],\n",
      "        [3.0387e-22, 1.0693e-19, 5.9894e-20, 4.7263e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "587000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3432e-40, 1.0400e-41, 2.5922e-21],\n",
      "        [3.0332e-22, 1.0674e-19, 5.9766e-20, 4.7501e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "588000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3411e-40, 1.0382e-41, 2.6003e-21],\n",
      "        [3.0314e-22, 1.0668e-19, 5.9729e-20, 4.7581e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "589000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3458e-40, 1.0423e-41, 2.5822e-21],\n",
      "        [3.0354e-22, 1.0682e-19, 5.9820e-20, 4.7407e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "590000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3434e-40, 1.0403e-41, 2.5915e-21],\n",
      "        [3.0334e-22, 1.0675e-19, 5.9773e-20, 4.7496e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "591000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3412e-40, 1.0384e-41, 2.6000e-21],\n",
      "        [3.0315e-22, 1.0668e-19, 5.9729e-20, 4.7577e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "592000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3375e-40, 1.0353e-41, 2.6145e-21],\n",
      "        [3.0284e-22, 1.0657e-19, 5.9664e-20, 4.7718e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "593000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3477e-40, 1.0437e-41, 2.5751e-21],\n",
      "        [3.0372e-22, 1.0688e-19, 5.9855e-20, 4.7337e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "594000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3418e-40, 1.0389e-41, 2.5975e-21],\n",
      "        [3.0319e-22, 1.0669e-19, 5.9740e-20, 4.7554e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "595000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3420e-40, 1.0391e-41, 2.5970e-21],\n",
      "        [3.0322e-22, 1.0670e-19, 5.9746e-20, 4.7548e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "596000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3460e-40, 1.0423e-41, 2.5816e-21],\n",
      "        [3.0356e-22, 1.0682e-19, 5.9820e-20, 4.7399e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "597000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3489e-40, 1.0447e-41, 2.5703e-21],\n",
      "        [3.0381e-22, 1.0691e-19, 5.9877e-20, 4.7292e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "598000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3505e-40, 1.0461e-41, 2.5642e-21],\n",
      "        [3.0394e-22, 1.0696e-19, 5.9910e-20, 4.7233e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "599000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3497e-40, 1.0454e-41, 2.5673e-21],\n",
      "        [3.0388e-22, 1.0693e-19, 5.9890e-20, 4.7261e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "600000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3529e-40, 1.0480e-41, 2.5552e-21],\n",
      "        [3.0415e-22, 1.0703e-19, 5.9950e-20, 4.7145e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "601000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3448e-40, 1.0413e-41, 2.5860e-21],\n",
      "        [3.0346e-22, 1.0678e-19, 5.9797e-20, 4.7442e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "602000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3499e-40, 1.0455e-41, 2.5660e-21],\n",
      "        [3.0390e-22, 1.0694e-19, 5.9894e-20, 4.7250e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "603000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3534e-40, 1.0485e-41, 2.5529e-21],\n",
      "        [3.0420e-22, 1.0705e-19, 5.9961e-20, 4.7122e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "604000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3535e-40, 1.0485e-41, 2.5531e-21],\n",
      "        [3.0420e-22, 1.0704e-19, 5.9959e-20, 4.7122e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "605000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3459e-40, 1.0421e-41, 2.5816e-21],\n",
      "        [3.0354e-22, 1.0681e-19, 5.9815e-20, 4.7400e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "606000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3502e-40, 1.0456e-41, 2.5653e-21],\n",
      "        [3.0392e-22, 1.0694e-19, 5.9899e-20, 4.7243e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "607000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3356e-40, 1.0337e-41, 2.6214e-21],\n",
      "        [3.0269e-22, 1.0650e-19, 5.9624e-20, 4.7784e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "608000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3445e-40, 1.0410e-41, 2.5867e-21],\n",
      "        [3.0343e-22, 1.0677e-19, 5.9788e-20, 4.7449e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "609000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3454e-40, 1.0417e-41, 2.5831e-21],\n",
      "        [3.0352e-22, 1.0680e-19, 5.9806e-20, 4.7415e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "610000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3533e-40, 1.0483e-41, 2.5528e-21],\n",
      "        [3.0417e-22, 1.0703e-19, 5.9956e-20, 4.7122e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "611000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3461e-40, 1.0423e-41, 2.5803e-21],\n",
      "        [3.0358e-22, 1.0682e-19, 5.9819e-20, 4.7387e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "612000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3484e-40, 1.0441e-41, 2.5716e-21],\n",
      "        [3.0375e-22, 1.0689e-19, 5.9862e-20, 4.7305e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "613000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3541e-40, 1.0489e-41, 2.5500e-21],\n",
      "        [3.0426e-22, 1.0706e-19, 5.9967e-20, 4.7092e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "614000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3553e-40, 1.0500e-41, 2.5451e-21],\n",
      "        [3.0435e-22, 1.0709e-19, 5.9994e-20, 4.7046e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "615000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3546e-40, 1.0494e-41, 2.5475e-21],\n",
      "        [3.0430e-22, 1.0707e-19, 5.9979e-20, 4.7067e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "616000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3472e-40, 1.0433e-41, 2.5757e-21],\n",
      "        [3.0367e-22, 1.0685e-19, 5.9839e-20, 4.7342e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "617000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3440e-40, 1.0405e-41, 2.5883e-21],\n",
      "        [3.0340e-22, 1.0676e-19, 5.9777e-20, 4.7464e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "618000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3495e-40, 1.0452e-41, 2.5667e-21],\n",
      "        [3.0387e-22, 1.0692e-19, 5.9883e-20, 4.7256e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "619000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3436e-40, 1.0402e-41, 2.5894e-21],\n",
      "        [3.0336e-22, 1.0674e-19, 5.9769e-20, 4.7475e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "620000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3471e-40, 1.0431e-41, 2.5759e-21],\n",
      "        [3.0366e-22, 1.0684e-19, 5.9831e-20, 4.7347e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "621000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3429e-40, 1.0396e-41, 2.5921e-21],\n",
      "        [3.0331e-22, 1.0672e-19, 5.9753e-20, 4.7501e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "622000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3447e-40, 1.0410e-41, 2.5847e-21],\n",
      "        [3.0346e-22, 1.0677e-19, 5.9784e-20, 4.7429e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "623000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3461e-40, 1.0423e-41, 2.5793e-21],\n",
      "        [3.0356e-22, 1.0682e-19, 5.9815e-20, 4.7378e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "624000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3510e-40, 1.0463e-41, 2.5604e-21],\n",
      "        [3.0400e-22, 1.0696e-19, 5.9908e-20, 4.7194e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "625000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3489e-40, 1.0445e-41, 2.5685e-21],\n",
      "        [3.0382e-22, 1.0690e-19, 5.9868e-20, 4.7274e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "626000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3433e-40, 1.0399e-41, 2.5903e-21],\n",
      "        [3.0333e-22, 1.0673e-19, 5.9758e-20, 4.7484e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "627000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3431e-40, 1.0398e-41, 2.5907e-21],\n",
      "        [3.0332e-22, 1.0673e-19, 5.9758e-20, 4.7488e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "628000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3403e-40, 1.0375e-41, 2.6021e-21],\n",
      "        [3.0308e-22, 1.0664e-19, 5.9706e-20, 4.7597e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "629000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3437e-40, 1.0403e-41, 2.5884e-21],\n",
      "        [3.0338e-22, 1.0675e-19, 5.9771e-20, 4.7467e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "630000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3476e-40, 1.0434e-41, 2.5738e-21],\n",
      "        [3.0370e-22, 1.0686e-19, 5.9842e-20, 4.7325e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "631000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3473e-40, 1.0433e-41, 2.5747e-21],\n",
      "        [3.0369e-22, 1.0685e-19, 5.9841e-20, 4.7332e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "632000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3425e-40, 1.0392e-41, 2.5931e-21],\n",
      "        [3.0328e-22, 1.0671e-19, 5.9744e-20, 4.7512e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "633000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3506e-40, 1.0459e-41, 2.5620e-21],\n",
      "        [3.0397e-22, 1.0695e-19, 5.9901e-20, 4.7211e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "634000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3500e-40, 1.0454e-41, 2.5645e-21],\n",
      "        [3.0392e-22, 1.0693e-19, 5.9886e-20, 4.7234e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "635000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3496e-40, 1.0451e-41, 2.5662e-21],\n",
      "        [3.0388e-22, 1.0692e-19, 5.9879e-20, 4.7251e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "636000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3475e-40, 1.0434e-41, 2.5739e-21],\n",
      "        [3.0369e-22, 1.0686e-19, 5.9841e-20, 4.7326e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "637000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3531e-40, 1.0480e-41, 2.5528e-21],\n",
      "        [3.0418e-22, 1.0703e-19, 5.9945e-20, 4.7119e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "638000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3422e-40, 1.0389e-41, 2.5942e-21],\n",
      "        [3.0325e-22, 1.0670e-19, 5.9742e-20, 4.7523e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "639000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3471e-40, 1.0431e-41, 2.5755e-21],\n",
      "        [3.0366e-22, 1.0684e-19, 5.9831e-20, 4.7341e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "640000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3436e-40, 1.0402e-41, 2.5887e-21],\n",
      "        [3.0336e-22, 1.0674e-19, 5.9766e-20, 4.7468e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "641000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3568e-40, 1.0511e-41, 2.5385e-21],\n",
      "        [3.0449e-22, 1.0714e-19, 6.0016e-20, 4.6981e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "642000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3517e-40, 1.0468e-41, 2.5578e-21],\n",
      "        [3.0406e-22, 1.0698e-19, 5.9917e-20, 4.7171e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "643000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3419e-40, 1.0388e-41, 2.5949e-21],\n",
      "        [3.0323e-22, 1.0669e-19, 5.9733e-20, 4.7529e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "644000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3418e-40, 1.0385e-41, 2.5957e-21],\n",
      "        [3.0319e-22, 1.0668e-19, 5.9726e-20, 4.7536e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "645000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3451e-40, 1.0413e-41, 2.5829e-21],\n",
      "        [3.0349e-22, 1.0678e-19, 5.9793e-20, 4.7413e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "646000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3519e-40, 1.0471e-41, 2.5563e-21],\n",
      "        [3.0408e-22, 1.0699e-19, 5.9923e-20, 4.7155e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "647000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3399e-40, 1.0370e-41, 2.6027e-21],\n",
      "        [3.0304e-22, 1.0663e-19, 5.9695e-20, 4.7606e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "648000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3518e-40, 1.0469e-41, 2.5571e-21],\n",
      "        [3.0407e-22, 1.0698e-19, 5.9921e-20, 4.7165e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "649000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3477e-40, 1.0435e-41, 2.5728e-21],\n",
      "        [3.0371e-22, 1.0686e-19, 5.9842e-20, 4.7315e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "650000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3430e-40, 1.0396e-41, 2.5909e-21],\n",
      "        [3.0331e-22, 1.0672e-19, 5.9753e-20, 4.7491e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "651000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3457e-40, 1.0417e-41, 2.5808e-21],\n",
      "        [3.0353e-22, 1.0680e-19, 5.9802e-20, 4.7393e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "652000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3451e-40, 1.0413e-41, 2.5832e-21],\n",
      "        [3.0348e-22, 1.0678e-19, 5.9791e-20, 4.7415e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "653000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3375e-40, 1.0350e-41, 2.6124e-21],\n",
      "        [3.0282e-22, 1.0655e-19, 5.9649e-20, 4.7698e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "654000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3466e-40, 1.0426e-41, 2.5770e-21],\n",
      "        [3.0363e-22, 1.0683e-19, 5.9817e-20, 4.7357e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "655000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3375e-40, 1.0350e-41, 2.6121e-21],\n",
      "        [3.0284e-22, 1.0655e-19, 5.9647e-20, 4.7693e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "656000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3447e-40, 1.0410e-41, 2.5838e-21],\n",
      "        [3.0345e-22, 1.0677e-19, 5.9782e-20, 4.7422e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "657000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3443e-40, 1.0406e-41, 2.5858e-21],\n",
      "        [3.0342e-22, 1.0675e-19, 5.9775e-20, 4.7442e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "658000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3346e-40, 1.0326e-41, 2.6231e-21],\n",
      "        [3.0260e-22, 1.0647e-19, 5.9591e-20, 4.7801e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "659000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5695e-43, 1.3326e-40, 1.0309e-41, 2.6312e-21],\n",
      "        [3.0243e-22, 1.0640e-19, 5.9551e-20, 4.7877e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "660000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3406e-40, 1.0377e-41, 2.5996e-21],\n",
      "        [3.0310e-22, 1.0664e-19, 5.9707e-20, 4.7574e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "661000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3441e-40, 1.0405e-41, 2.5864e-21],\n",
      "        [3.0342e-22, 1.0675e-19, 5.9771e-20, 4.7446e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "662000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3485e-40, 1.0441e-41, 2.5693e-21],\n",
      "        [3.0378e-22, 1.0688e-19, 5.9853e-20, 4.7283e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "663000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3400e-40, 1.0371e-41, 2.6023e-21],\n",
      "        [3.0306e-22, 1.0662e-19, 5.9693e-20, 4.7599e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "664000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3563e-40, 1.0506e-41, 2.5399e-21],\n",
      "        [3.0444e-22, 1.0711e-19, 6.0001e-20, 4.6995e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "665000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3501e-40, 1.0454e-41, 2.5632e-21],\n",
      "        [3.0392e-22, 1.0693e-19, 5.9881e-20, 4.7223e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "666000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3404e-40, 1.0374e-41, 2.6010e-21],\n",
      "        [3.0308e-22, 1.0663e-19, 5.9700e-20, 4.7586e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "667000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3441e-40, 1.0406e-41, 2.5859e-21],\n",
      "        [3.0341e-22, 1.0675e-19, 5.9771e-20, 4.7441e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "668000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3387e-40, 1.0360e-41, 2.6074e-21],\n",
      "        [3.0294e-22, 1.0658e-19, 5.9667e-20, 4.7650e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "669000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3392e-40, 1.0364e-41, 2.6050e-21],\n",
      "        [3.0299e-22, 1.0661e-19, 5.9676e-20, 4.7625e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "670000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3367e-40, 1.0343e-41, 2.6150e-21],\n",
      "        [3.0279e-22, 1.0653e-19, 5.9633e-20, 4.7724e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "671000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3492e-40, 1.0447e-41, 2.5665e-21],\n",
      "        [3.0383e-22, 1.0690e-19, 5.9864e-20, 4.7254e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "672000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3430e-40, 1.0396e-41, 2.5899e-21],\n",
      "        [3.0332e-22, 1.0672e-19, 5.9749e-20, 4.7481e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "673000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3332e-40, 1.0314e-41, 2.6288e-21],\n",
      "        [3.0245e-22, 1.0641e-19, 5.9558e-20, 4.7855e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "674000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3468e-40, 1.0427e-41, 2.5760e-21],\n",
      "        [3.0364e-22, 1.0683e-19, 5.9820e-20, 4.7347e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "675000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3465e-40, 1.0424e-41, 2.5767e-21],\n",
      "        [3.0362e-22, 1.0682e-19, 5.9815e-20, 4.7354e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "676000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3491e-40, 1.0445e-41, 2.5671e-21],\n",
      "        [3.0383e-22, 1.0690e-19, 5.9862e-20, 4.7261e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "677000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3457e-40, 1.0416e-41, 2.5802e-21],\n",
      "        [3.0353e-22, 1.0679e-19, 5.9797e-20, 4.7387e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "678000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3520e-40, 1.0469e-41, 2.5560e-21],\n",
      "        [3.0408e-22, 1.0698e-19, 5.9917e-20, 4.7153e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "679000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3489e-40, 1.0444e-41, 2.5677e-21],\n",
      "        [3.0381e-22, 1.0689e-19, 5.9861e-20, 4.7267e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "680000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3509e-40, 1.0459e-41, 2.5599e-21],\n",
      "        [3.0398e-22, 1.0695e-19, 5.9897e-20, 4.7189e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "681000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3385e-40, 1.0357e-41, 2.6076e-21],\n",
      "        [3.0292e-22, 1.0657e-19, 5.9655e-20, 4.7651e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "682000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3480e-40, 1.0435e-41, 2.5707e-21],\n",
      "        [3.0375e-22, 1.0686e-19, 5.9839e-20, 4.7298e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "683000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3496e-40, 1.0449e-41, 2.5642e-21],\n",
      "        [3.0387e-22, 1.0691e-19, 5.9868e-20, 4.7234e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "684000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3409e-40, 1.0378e-41, 2.5976e-21],\n",
      "        [3.0313e-22, 1.0665e-19, 5.9707e-20, 4.7555e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "685000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3476e-40, 1.0433e-41, 2.5720e-21],\n",
      "        [3.0369e-22, 1.0685e-19, 5.9831e-20, 4.7306e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "686000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3495e-40, 1.0449e-41, 2.5649e-21],\n",
      "        [3.0387e-22, 1.0690e-19, 5.9870e-20, 4.7238e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "687000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3445e-40, 1.0407e-41, 2.5836e-21],\n",
      "        [3.0343e-22, 1.0676e-19, 5.9773e-20, 4.7422e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "688000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3427e-40, 1.0392e-41, 2.5911e-21],\n",
      "        [3.0329e-22, 1.0669e-19, 5.9737e-20, 4.7493e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "689000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3406e-40, 1.0375e-41, 2.5987e-21],\n",
      "        [3.0311e-22, 1.0664e-19, 5.9700e-20, 4.7567e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "690000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3491e-40, 1.0445e-41, 2.5663e-21],\n",
      "        [3.0382e-22, 1.0689e-19, 5.9857e-20, 4.7251e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "691000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3355e-40, 1.0332e-41, 2.6190e-21],\n",
      "        [3.0265e-22, 1.0648e-19, 5.9598e-20, 4.7763e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "692000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3367e-40, 1.0343e-41, 2.6137e-21],\n",
      "        [3.0276e-22, 1.0652e-19, 5.9620e-20, 4.7712e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "693000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3415e-40, 1.0382e-41, 2.5952e-21],\n",
      "        [3.0319e-22, 1.0666e-19, 5.9715e-20, 4.7532e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "694000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3403e-40, 1.0372e-41, 2.5997e-21],\n",
      "        [3.0308e-22, 1.0663e-19, 5.9693e-20, 4.7575e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "695000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3392e-40, 1.0364e-41, 2.6041e-21],\n",
      "        [3.0298e-22, 1.0659e-19, 5.9667e-20, 4.7619e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "696000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3592e-40, 1.0528e-41, 2.5283e-21],\n",
      "        [3.0469e-22, 1.0719e-19, 6.0047e-20, 4.6884e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "697000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3524e-40, 1.0472e-41, 2.5535e-21],\n",
      "        [3.0411e-22, 1.0699e-19, 5.9921e-20, 4.7129e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "698000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3447e-40, 1.0409e-41, 2.5830e-21],\n",
      "        [3.0345e-22, 1.0676e-19, 5.9777e-20, 4.7416e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "699000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3409e-40, 1.0377e-41, 2.5975e-21],\n",
      "        [3.0313e-22, 1.0664e-19, 5.9700e-20, 4.7555e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "700000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3447e-40, 1.0407e-41, 2.5828e-21],\n",
      "        [3.0344e-22, 1.0676e-19, 5.9771e-20, 4.7415e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "701000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3453e-40, 1.0413e-41, 2.5800e-21],\n",
      "        [3.0352e-22, 1.0677e-19, 5.9788e-20, 4.7386e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "702000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3471e-40, 1.0427e-41, 2.5734e-21],\n",
      "        [3.0367e-22, 1.0683e-19, 5.9820e-20, 4.7322e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "703000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3446e-40, 1.0407e-41, 2.5826e-21],\n",
      "        [3.0346e-22, 1.0676e-19, 5.9771e-20, 4.7412e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "704000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3512e-40, 1.0461e-41, 2.5577e-21],\n",
      "        [3.0400e-22, 1.0695e-19, 5.9897e-20, 4.7171e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "705000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3433e-40, 1.0396e-41, 2.5879e-21],\n",
      "        [3.0334e-22, 1.0671e-19, 5.9744e-20, 4.7462e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "706000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3434e-40, 1.0396e-41, 2.5874e-21],\n",
      "        [3.0334e-22, 1.0672e-19, 5.9747e-20, 4.7459e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "707000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3497e-40, 1.0449e-41, 2.5630e-21],\n",
      "        [3.0389e-22, 1.0691e-19, 5.9868e-20, 4.7220e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "708000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3443e-40, 1.0405e-41, 2.5839e-21],\n",
      "        [3.0343e-22, 1.0675e-19, 5.9762e-20, 4.7423e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "709000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3403e-40, 1.0371e-41, 2.5995e-21],\n",
      "        [3.0306e-22, 1.0663e-19, 5.9686e-20, 4.7574e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "710000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3408e-40, 1.0375e-41, 2.5973e-21],\n",
      "        [3.0312e-22, 1.0664e-19, 5.9695e-20, 4.7554e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "711000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3514e-40, 1.0462e-41, 2.5567e-21],\n",
      "        [3.0403e-22, 1.0695e-19, 5.9899e-20, 4.7159e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "712000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3407e-40, 1.0374e-41, 2.5979e-21],\n",
      "        [3.0311e-22, 1.0663e-19, 5.9689e-20, 4.7558e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "713000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3476e-40, 1.0431e-41, 2.5713e-21],\n",
      "        [3.0371e-22, 1.0684e-19, 5.9820e-20, 4.7300e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "714000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3458e-40, 1.0416e-41, 2.5780e-21],\n",
      "        [3.0356e-22, 1.0679e-19, 5.9789e-20, 4.7367e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "715000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3441e-40, 1.0403e-41, 2.5845e-21],\n",
      "        [3.0340e-22, 1.0674e-19, 5.9758e-20, 4.7430e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "716000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3373e-40, 1.0346e-41, 2.6107e-21],\n",
      "        [3.0282e-22, 1.0653e-19, 5.9627e-20, 4.7682e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "717000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3513e-40, 1.0461e-41, 2.5573e-21],\n",
      "        [3.0401e-22, 1.0695e-19, 5.9892e-20, 4.7165e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "718000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3459e-40, 1.0417e-41, 2.5780e-21],\n",
      "        [3.0355e-22, 1.0679e-19, 5.9788e-20, 4.7368e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "719000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3467e-40, 1.0424e-41, 2.5747e-21],\n",
      "        [3.0363e-22, 1.0682e-19, 5.9806e-20, 4.7337e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "720000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3505e-40, 1.0455e-41, 2.5604e-21],\n",
      "        [3.0395e-22, 1.0692e-19, 5.9873e-20, 4.7197e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "721000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3542e-40, 1.0485e-41, 2.5464e-21],\n",
      "        [3.0427e-22, 1.0704e-19, 5.9948e-20, 4.7061e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "722000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.6115e-43, 1.3604e-40, 1.0536e-41, 2.5232e-21],\n",
      "        [3.0480e-22, 1.0722e-19, 6.0064e-20, 4.6834e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "723000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3570e-40, 1.0508e-41, 2.5357e-21],\n",
      "        [3.0451e-22, 1.0712e-19, 6.0000e-20, 4.6957e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "724000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3455e-40, 1.0412e-41, 2.5795e-21],\n",
      "        [3.0352e-22, 1.0677e-19, 5.9780e-20, 4.7383e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "725000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3421e-40, 1.0385e-41, 2.5922e-21],\n",
      "        [3.0324e-22, 1.0667e-19, 5.9716e-20, 4.7506e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "726000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3443e-40, 1.0405e-41, 2.5836e-21],\n",
      "        [3.0344e-22, 1.0675e-19, 5.9760e-20, 4.7422e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "727000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3430e-40, 1.0392e-41, 2.5887e-21],\n",
      "        [3.0331e-22, 1.0670e-19, 5.9731e-20, 4.7471e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "728000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3476e-40, 1.0431e-41, 2.5709e-21],\n",
      "        [3.0371e-22, 1.0684e-19, 5.9820e-20, 4.7300e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "729000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3498e-40, 1.0448e-41, 2.5626e-21],\n",
      "        [3.0389e-22, 1.0690e-19, 5.9859e-20, 4.7220e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "730000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3399e-40, 1.0367e-41, 2.6005e-21],\n",
      "        [3.0306e-22, 1.0661e-19, 5.9675e-20, 4.7583e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "731000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3386e-40, 1.0356e-41, 2.6056e-21],\n",
      "        [3.0293e-22, 1.0657e-19, 5.9647e-20, 4.7635e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "732000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3462e-40, 1.0419e-41, 2.5762e-21],\n",
      "        [3.0360e-22, 1.0680e-19, 5.9795e-20, 4.7350e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "733000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3403e-40, 1.0370e-41, 2.5988e-21],\n",
      "        [3.0307e-22, 1.0662e-19, 5.9682e-20, 4.7568e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "734000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3455e-40, 1.0412e-41, 2.5791e-21],\n",
      "        [3.0353e-22, 1.0677e-19, 5.9778e-20, 4.7377e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "735000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3437e-40, 1.0398e-41, 2.5858e-21],\n",
      "        [3.0338e-22, 1.0672e-19, 5.9746e-20, 4.7444e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "736000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3469e-40, 1.0424e-41, 2.5733e-21],\n",
      "        [3.0365e-22, 1.0681e-19, 5.9804e-20, 4.7324e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "737000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3407e-40, 1.0374e-41, 2.5971e-21],\n",
      "        [3.0314e-22, 1.0663e-19, 5.9689e-20, 4.7552e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "738000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3423e-40, 1.0385e-41, 2.5915e-21],\n",
      "        [3.0325e-22, 1.0668e-19, 5.9715e-20, 4.7497e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "739000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3538e-40, 1.0482e-41, 2.5472e-21],\n",
      "        [3.0423e-22, 1.0702e-19, 5.9937e-20, 4.7069e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "740000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3510e-40, 1.0458e-41, 2.5577e-21],\n",
      "        [3.0400e-22, 1.0694e-19, 5.9883e-20, 4.7172e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "741000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3432e-40, 1.0393e-41, 2.5873e-21],\n",
      "        [3.0332e-22, 1.0670e-19, 5.9737e-20, 4.7459e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "742000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3459e-40, 1.0416e-41, 2.5772e-21],\n",
      "        [3.0357e-22, 1.0678e-19, 5.9788e-20, 4.7360e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "743000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3413e-40, 1.0378e-41, 2.5948e-21],\n",
      "        [3.0318e-22, 1.0664e-19, 5.9698e-20, 4.7530e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "744000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3446e-40, 1.0405e-41, 2.5824e-21],\n",
      "        [3.0345e-22, 1.0675e-19, 5.9762e-20, 4.7412e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "745000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3393e-40, 1.0361e-41, 2.6024e-21],\n",
      "        [3.0301e-22, 1.0659e-19, 5.9660e-20, 4.7603e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "746000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3378e-40, 1.0349e-41, 2.6082e-21],\n",
      "        [3.0287e-22, 1.0654e-19, 5.9633e-20, 4.7660e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "747000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3398e-40, 1.0365e-41, 2.6002e-21],\n",
      "        [3.0305e-22, 1.0660e-19, 5.9675e-20, 4.7584e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "748000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3360e-40, 1.0333e-41, 2.6150e-21],\n",
      "        [3.0272e-22, 1.0649e-19, 5.9595e-20, 4.7725e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "749000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3503e-40, 1.0451e-41, 2.5600e-21],\n",
      "        [3.0394e-22, 1.0691e-19, 5.9866e-20, 4.7194e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "750000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3512e-40, 1.0458e-41, 2.5567e-21],\n",
      "        [3.0401e-22, 1.0693e-19, 5.9879e-20, 4.7162e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "751000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.6115e-43, 1.3544e-40, 1.0486e-41, 2.5444e-21],\n",
      "        [3.0430e-22, 1.0703e-19, 5.9945e-20, 4.7043e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "752000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3408e-40, 1.0374e-41, 2.5963e-21],\n",
      "        [3.0313e-22, 1.0663e-19, 5.9686e-20, 4.7545e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "753000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3444e-40, 1.0403e-41, 2.5822e-21],\n",
      "        [3.0344e-22, 1.0674e-19, 5.9757e-20, 4.7407e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "754000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3391e-40, 1.0360e-41, 2.6024e-21],\n",
      "        [3.0299e-22, 1.0658e-19, 5.9656e-20, 4.7603e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "755000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3384e-40, 1.0353e-41, 2.6055e-21],\n",
      "        [3.0293e-22, 1.0656e-19, 5.9642e-20, 4.7634e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "756000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3408e-40, 1.0374e-41, 2.5960e-21],\n",
      "        [3.0314e-22, 1.0663e-19, 5.9686e-20, 4.7541e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "757000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3383e-40, 1.0353e-41, 2.6057e-21],\n",
      "        [3.0292e-22, 1.0655e-19, 5.9640e-20, 4.7635e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "758000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3534e-40, 1.0476e-41, 2.5479e-21],\n",
      "        [3.0420e-22, 1.0700e-19, 5.9921e-20, 4.7076e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "759000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3384e-40, 1.0354e-41, 2.6052e-21],\n",
      "        [3.0293e-22, 1.0655e-19, 5.9638e-20, 4.7631e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "760000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3428e-40, 1.0389e-41, 2.5884e-21],\n",
      "        [3.0330e-22, 1.0668e-19, 5.9724e-20, 4.7470e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "761000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3452e-40, 1.0410e-41, 2.5791e-21],\n",
      "        [3.0351e-22, 1.0676e-19, 5.9769e-20, 4.7378e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "762000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3427e-40, 1.0388e-41, 2.5889e-21],\n",
      "        [3.0328e-22, 1.0668e-19, 5.9722e-20, 4.7472e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "763000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3430e-40, 1.0391e-41, 2.5876e-21],\n",
      "        [3.0331e-22, 1.0669e-19, 5.9722e-20, 4.7461e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "764000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3469e-40, 1.0423e-41, 2.5725e-21],\n",
      "        [3.0367e-22, 1.0681e-19, 5.9795e-20, 4.7315e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "765000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3486e-40, 1.0437e-41, 2.5659e-21],\n",
      "        [3.0380e-22, 1.0686e-19, 5.9833e-20, 4.7251e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "766000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3391e-40, 1.0358e-41, 2.6025e-21],\n",
      "        [3.0299e-22, 1.0657e-19, 5.9653e-20, 4.7605e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "767000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3420e-40, 1.0382e-41, 2.5912e-21],\n",
      "        [3.0325e-22, 1.0666e-19, 5.9706e-20, 4.7494e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "768000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3414e-40, 1.0378e-41, 2.5936e-21],\n",
      "        [3.0319e-22, 1.0664e-19, 5.9693e-20, 4.7519e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "769000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3441e-40, 1.0399e-41, 2.5832e-21],\n",
      "        [3.0342e-22, 1.0673e-19, 5.9744e-20, 4.7419e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "770000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3419e-40, 1.0382e-41, 2.5914e-21],\n",
      "        [3.0323e-22, 1.0666e-19, 5.9706e-20, 4.7499e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "771000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3434e-40, 1.0393e-41, 2.5862e-21],\n",
      "        [3.0336e-22, 1.0670e-19, 5.9731e-20, 4.7448e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "772000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3382e-40, 1.0351e-41, 2.6060e-21],\n",
      "        [3.0290e-22, 1.0655e-19, 5.9638e-20, 4.7638e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "773000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3487e-40, 1.0437e-41, 2.5656e-21],\n",
      "        [3.0381e-22, 1.0686e-19, 5.9831e-20, 4.7248e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "774000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3499e-40, 1.0447e-41, 2.5609e-21],\n",
      "        [3.0392e-22, 1.0690e-19, 5.9857e-20, 4.7202e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "775000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3439e-40, 1.0398e-41, 2.5835e-21],\n",
      "        [3.0340e-22, 1.0671e-19, 5.9742e-20, 4.7422e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "776000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3489e-40, 1.0438e-41, 2.5642e-21],\n",
      "        [3.0384e-22, 1.0687e-19, 5.9833e-20, 4.7235e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "777000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3408e-40, 1.0372e-41, 2.5951e-21],\n",
      "        [3.0314e-22, 1.0662e-19, 5.9682e-20, 4.7533e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "778000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3408e-40, 1.0372e-41, 2.5951e-21],\n",
      "        [3.0314e-22, 1.0663e-19, 5.9682e-20, 4.7532e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "779000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3421e-40, 1.0384e-41, 2.5904e-21],\n",
      "        [3.0325e-22, 1.0667e-19, 5.9707e-20, 4.7488e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "780000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3425e-40, 1.0386e-41, 2.5886e-21],\n",
      "        [3.0331e-22, 1.0668e-19, 5.9713e-20, 4.7470e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "781000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3445e-40, 1.0402e-41, 2.5811e-21],\n",
      "        [3.0346e-22, 1.0673e-19, 5.9751e-20, 4.7399e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "782000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3439e-40, 1.0396e-41, 2.5835e-21],\n",
      "        [3.0341e-22, 1.0671e-19, 5.9737e-20, 4.7419e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "783000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3502e-40, 1.0448e-41, 2.5589e-21],\n",
      "        [3.0394e-22, 1.0690e-19, 5.9855e-20, 4.7181e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "784000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3373e-40, 1.0343e-41, 2.6084e-21],\n",
      "        [3.0283e-22, 1.0651e-19, 5.9613e-20, 4.7661e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "785000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3403e-40, 1.0368e-41, 2.5968e-21],\n",
      "        [3.0309e-22, 1.0660e-19, 5.9667e-20, 4.7549e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "786000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3373e-40, 1.0343e-41, 2.6088e-21],\n",
      "        [3.0284e-22, 1.0651e-19, 5.9609e-20, 4.7666e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "787000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3414e-40, 1.0377e-41, 2.5925e-21],\n",
      "        [3.0319e-22, 1.0664e-19, 5.9689e-20, 4.7509e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "788000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3476e-40, 1.0427e-41, 2.5688e-21],\n",
      "        [3.0373e-22, 1.0683e-19, 5.9806e-20, 4.7280e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "789000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3467e-40, 1.0420e-41, 2.5721e-21],\n",
      "        [3.0364e-22, 1.0680e-19, 5.9788e-20, 4.7312e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "790000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3449e-40, 1.0405e-41, 2.5789e-21],\n",
      "        [3.0348e-22, 1.0674e-19, 5.9755e-20, 4.7376e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "791000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3478e-40, 1.0430e-41, 2.5676e-21],\n",
      "        [3.0374e-22, 1.0683e-19, 5.9811e-20, 4.7267e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "792000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3367e-40, 1.0339e-41, 2.6104e-21],\n",
      "        [3.0281e-22, 1.0650e-19, 5.9604e-20, 4.7680e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "793000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3513e-40, 1.0458e-41, 2.5545e-21],\n",
      "        [3.0405e-22, 1.0693e-19, 5.9875e-20, 4.7139e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "794000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3375e-40, 1.0343e-41, 2.6074e-21],\n",
      "        [3.0284e-22, 1.0652e-19, 5.9613e-20, 4.7652e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "795000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3428e-40, 1.0388e-41, 2.5870e-21],\n",
      "        [3.0331e-22, 1.0668e-19, 5.9716e-20, 4.7455e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "796000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3409e-40, 1.0372e-41, 2.5942e-21],\n",
      "        [3.0315e-22, 1.0663e-19, 5.9676e-20, 4.7525e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "797000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3426e-40, 1.0386e-41, 2.5874e-21],\n",
      "        [3.0331e-22, 1.0667e-19, 5.9713e-20, 4.7458e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "798000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3507e-40, 1.0454e-41, 2.5563e-21],\n",
      "        [3.0399e-22, 1.0692e-19, 5.9868e-20, 4.7159e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "799000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3509e-40, 1.0455e-41, 2.5559e-21],\n",
      "        [3.0401e-22, 1.0692e-19, 5.9870e-20, 4.7155e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "800000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3393e-40, 1.0358e-41, 2.6002e-21],\n",
      "        [3.0303e-22, 1.0658e-19, 5.9649e-20, 4.7583e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "801000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3425e-40, 1.0385e-41, 2.5880e-21],\n",
      "        [3.0329e-22, 1.0667e-19, 5.9711e-20, 4.7465e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "802000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3400e-40, 1.0364e-41, 2.5976e-21],\n",
      "        [3.0308e-22, 1.0660e-19, 5.9660e-20, 4.7557e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "803000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3364e-40, 1.0335e-41, 2.6117e-21],\n",
      "        [3.0277e-22, 1.0649e-19, 5.9591e-20, 4.7693e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "804000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3357e-40, 1.0329e-41, 2.6144e-21],\n",
      "        [3.0271e-22, 1.0646e-19, 5.9578e-20, 4.7719e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "805000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3452e-40, 1.0407e-41, 2.5774e-21],\n",
      "        [3.0353e-22, 1.0675e-19, 5.9758e-20, 4.7363e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "806000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3410e-40, 1.0374e-41, 2.5935e-21],\n",
      "        [3.0317e-22, 1.0663e-19, 5.9678e-20, 4.7517e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "807000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3446e-40, 1.0402e-41, 2.5796e-21],\n",
      "        [3.0347e-22, 1.0673e-19, 5.9749e-20, 4.7384e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "808000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3508e-40, 1.0452e-41, 2.5565e-21],\n",
      "        [3.0400e-22, 1.0692e-19, 5.9864e-20, 4.7159e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "809000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3458e-40, 1.0412e-41, 2.5749e-21],\n",
      "        [3.0357e-22, 1.0677e-19, 5.9769e-20, 4.7338e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "810000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.6115e-43, 1.3548e-40, 1.0486e-41, 2.5409e-21],\n",
      "        [3.0433e-22, 1.0704e-19, 5.9939e-20, 4.7007e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "811000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3409e-40, 1.0372e-41, 2.5937e-21],\n",
      "        [3.0316e-22, 1.0662e-19, 5.9676e-20, 4.7520e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "812000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3440e-40, 1.0398e-41, 2.5816e-21],\n",
      "        [3.0343e-22, 1.0671e-19, 5.9737e-20, 4.7402e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "813000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3456e-40, 1.0410e-41, 2.5754e-21],\n",
      "        [3.0356e-22, 1.0676e-19, 5.9766e-20, 4.7344e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "814000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3482e-40, 1.0431e-41, 2.5652e-21],\n",
      "        [3.0378e-22, 1.0683e-19, 5.9809e-20, 4.7243e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "815000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3460e-40, 1.0413e-41, 2.5743e-21],\n",
      "        [3.0358e-22, 1.0677e-19, 5.9769e-20, 4.7332e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "816000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3461e-40, 1.0413e-41, 2.5736e-21],\n",
      "        [3.0360e-22, 1.0677e-19, 5.9771e-20, 4.7326e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "817000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3394e-40, 1.0357e-41, 2.5995e-21],\n",
      "        [3.0302e-22, 1.0657e-19, 5.9644e-20, 4.7574e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "818000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3462e-40, 1.0414e-41, 2.5735e-21],\n",
      "        [3.0361e-22, 1.0678e-19, 5.9777e-20, 4.7325e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "819000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3412e-40, 1.0374e-41, 2.5922e-21],\n",
      "        [3.0319e-22, 1.0663e-19, 5.9678e-20, 4.7504e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "820000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3512e-40, 1.0456e-41, 2.5541e-21],\n",
      "        [3.0404e-22, 1.0692e-19, 5.9868e-20, 4.7135e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "821000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3508e-40, 1.0452e-41, 2.5556e-21],\n",
      "        [3.0401e-22, 1.0692e-19, 5.9861e-20, 4.7151e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "822000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3454e-40, 1.0409e-41, 2.5760e-21],\n",
      "        [3.0356e-22, 1.0676e-19, 5.9758e-20, 4.7350e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "823000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3481e-40, 1.0430e-41, 2.5655e-21],\n",
      "        [3.0378e-22, 1.0683e-19, 5.9806e-20, 4.7247e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "824000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3354e-40, 1.0326e-41, 2.6144e-21],\n",
      "        [3.0268e-22, 1.0645e-19, 5.9569e-20, 4.7718e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "825000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3451e-40, 1.0406e-41, 2.5767e-21],\n",
      "        [3.0353e-22, 1.0674e-19, 5.9755e-20, 4.7355e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "826000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3493e-40, 1.0440e-41, 2.5609e-21],\n",
      "        [3.0387e-22, 1.0687e-19, 5.9831e-20, 4.7202e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "827000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3430e-40, 1.0389e-41, 2.5847e-21],\n",
      "        [3.0335e-22, 1.0668e-19, 5.9715e-20, 4.7432e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "828000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3499e-40, 1.0445e-41, 2.5582e-21],\n",
      "        [3.0393e-22, 1.0688e-19, 5.9846e-20, 4.7175e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "829000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3493e-40, 1.0440e-41, 2.5607e-21],\n",
      "        [3.0387e-22, 1.0687e-19, 5.9831e-20, 4.7199e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "830000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3434e-40, 1.0392e-41, 2.5828e-21],\n",
      "        [3.0338e-22, 1.0669e-19, 5.9722e-20, 4.7413e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "831000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3380e-40, 1.0347e-41, 2.6037e-21],\n",
      "        [3.0292e-22, 1.0653e-19, 5.9616e-20, 4.7616e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "832000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3393e-40, 1.0358e-41, 2.5987e-21],\n",
      "        [3.0304e-22, 1.0657e-19, 5.9645e-20, 4.7570e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "833000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3425e-40, 1.0384e-41, 2.5868e-21],\n",
      "        [3.0329e-22, 1.0666e-19, 5.9700e-20, 4.7452e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "834000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3364e-40, 1.0333e-41, 2.6106e-21],\n",
      "        [3.0277e-22, 1.0648e-19, 5.9584e-20, 4.7682e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "835000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3403e-40, 1.0365e-41, 2.5952e-21],\n",
      "        [3.0311e-22, 1.0660e-19, 5.9660e-20, 4.7535e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "836000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3429e-40, 1.0388e-41, 2.5850e-21],\n",
      "        [3.0332e-22, 1.0667e-19, 5.9711e-20, 4.7435e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "837000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.6115e-43, 1.3543e-40, 1.0480e-41, 2.5423e-21],\n",
      "        [3.0430e-22, 1.0702e-19, 5.9923e-20, 4.7023e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "838000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3368e-40, 1.0336e-41, 2.6091e-21],\n",
      "        [3.0279e-22, 1.0649e-19, 5.9591e-20, 4.7668e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "839000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3414e-40, 1.0374e-41, 2.5913e-21],\n",
      "        [3.0321e-22, 1.0663e-19, 5.9680e-20, 4.7496e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "840000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3411e-40, 1.0372e-41, 2.5921e-21],\n",
      "        [3.0318e-22, 1.0662e-19, 5.9673e-20, 4.7504e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "841000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3482e-40, 1.0431e-41, 2.5646e-21],\n",
      "        [3.0379e-22, 1.0683e-19, 5.9809e-20, 4.7237e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "842000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3481e-40, 1.0430e-41, 2.5650e-21],\n",
      "        [3.0377e-22, 1.0683e-19, 5.9808e-20, 4.7241e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "843000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3478e-40, 1.0427e-41, 2.5657e-21],\n",
      "        [3.0375e-22, 1.0682e-19, 5.9800e-20, 4.7248e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "844000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3437e-40, 1.0395e-41, 2.5816e-21],\n",
      "        [3.0340e-22, 1.0670e-19, 5.9726e-20, 4.7402e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "845000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3382e-40, 1.0349e-41, 2.6033e-21],\n",
      "        [3.0293e-22, 1.0653e-19, 5.9620e-20, 4.7612e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "846000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3500e-40, 1.0445e-41, 2.5582e-21],\n",
      "        [3.0395e-22, 1.0688e-19, 5.9841e-20, 4.7176e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "847000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3452e-40, 1.0406e-41, 2.5762e-21],\n",
      "        [3.0353e-22, 1.0674e-19, 5.9751e-20, 4.7350e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "848000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3383e-40, 1.0349e-41, 2.6026e-21],\n",
      "        [3.0294e-22, 1.0653e-19, 5.9620e-20, 4.7606e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "849000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3497e-40, 1.0444e-41, 2.5587e-21],\n",
      "        [3.0392e-22, 1.0688e-19, 5.9837e-20, 4.7182e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "850000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3457e-40, 1.0409e-41, 2.5744e-21],\n",
      "        [3.0357e-22, 1.0676e-19, 5.9755e-20, 4.7334e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "851000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3486e-40, 1.0434e-41, 2.5631e-21],\n",
      "        [3.0382e-22, 1.0684e-19, 5.9815e-20, 4.7224e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "852000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3398e-40, 1.0361e-41, 2.5971e-21],\n",
      "        [3.0306e-22, 1.0658e-19, 5.9645e-20, 4.7552e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "853000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3472e-40, 1.0423e-41, 2.5682e-21],\n",
      "        [3.0370e-22, 1.0680e-19, 5.9788e-20, 4.7273e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "854000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3423e-40, 1.0382e-41, 2.5869e-21],\n",
      "        [3.0328e-22, 1.0665e-19, 5.9693e-20, 4.7454e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "855000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3405e-40, 1.0367e-41, 2.5939e-21],\n",
      "        [3.0312e-22, 1.0660e-19, 5.9660e-20, 4.7520e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "856000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3401e-40, 1.0364e-41, 2.5952e-21],\n",
      "        [3.0309e-22, 1.0658e-19, 5.9653e-20, 4.7532e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "857000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3391e-40, 1.0356e-41, 2.5991e-21],\n",
      "        [3.0301e-22, 1.0655e-19, 5.9635e-20, 4.7571e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "858000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3448e-40, 1.0403e-41, 2.5769e-21],\n",
      "        [3.0349e-22, 1.0673e-19, 5.9742e-20, 4.7357e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "859000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3428e-40, 1.0385e-41, 2.5846e-21],\n",
      "        [3.0332e-22, 1.0667e-19, 5.9704e-20, 4.7433e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "860000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3434e-40, 1.0389e-41, 2.5825e-21],\n",
      "        [3.0335e-22, 1.0668e-19, 5.9707e-20, 4.7412e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "861000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3371e-40, 1.0339e-41, 2.6068e-21],\n",
      "        [3.0284e-22, 1.0650e-19, 5.9595e-20, 4.7645e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "862000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3396e-40, 1.0358e-41, 2.5975e-21],\n",
      "        [3.0305e-22, 1.0657e-19, 5.9640e-20, 4.7557e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "863000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3371e-40, 1.0339e-41, 2.6068e-21],\n",
      "        [3.0284e-22, 1.0649e-19, 5.9593e-20, 4.7645e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "864000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3462e-40, 1.0413e-41, 2.5720e-21],\n",
      "        [3.0360e-22, 1.0677e-19, 5.9762e-20, 4.7309e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "865000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3382e-40, 1.0347e-41, 2.6026e-21],\n",
      "        [3.0294e-22, 1.0653e-19, 5.9609e-20, 4.7605e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "866000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3427e-40, 1.0384e-41, 2.5851e-21],\n",
      "        [3.0333e-22, 1.0666e-19, 5.9700e-20, 4.7433e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "867000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3426e-40, 1.0384e-41, 2.5853e-21],\n",
      "        [3.0331e-22, 1.0666e-19, 5.9700e-20, 4.7438e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "868000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3436e-40, 1.0391e-41, 2.5813e-21],\n",
      "        [3.0341e-22, 1.0668e-19, 5.9716e-20, 4.7400e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "869000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3477e-40, 1.0426e-41, 2.5660e-21],\n",
      "        [3.0375e-22, 1.0681e-19, 5.9797e-20, 4.7251e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "870000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3487e-40, 1.0434e-41, 2.5617e-21],\n",
      "        [3.0384e-22, 1.0685e-19, 5.9811e-20, 4.7211e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "871000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3471e-40, 1.0421e-41, 2.5681e-21],\n",
      "        [3.0371e-22, 1.0680e-19, 5.9786e-20, 4.7272e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "872000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3496e-40, 1.0441e-41, 2.5587e-21],\n",
      "        [3.0392e-22, 1.0686e-19, 5.9828e-20, 4.7181e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "873000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3407e-40, 1.0368e-41, 2.5926e-21],\n",
      "        [3.0315e-22, 1.0660e-19, 5.9658e-20, 4.7507e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "874000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3504e-40, 1.0448e-41, 2.5549e-21],\n",
      "        [3.0400e-22, 1.0689e-19, 5.9846e-20, 4.7145e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "875000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3419e-40, 1.0377e-41, 2.5878e-21],\n",
      "        [3.0325e-22, 1.0663e-19, 5.9682e-20, 4.7461e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "876000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3340e-40, 1.0311e-41, 2.6185e-21],\n",
      "        [3.0257e-22, 1.0639e-19, 5.9529e-20, 4.7757e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "877000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3489e-40, 1.0435e-41, 2.5608e-21],\n",
      "        [3.0385e-22, 1.0685e-19, 5.9811e-20, 4.7202e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "878000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3513e-40, 1.0455e-41, 2.5516e-21],\n",
      "        [3.0405e-22, 1.0691e-19, 5.9861e-20, 4.7112e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "879000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3520e-40, 1.0461e-41, 2.5490e-21],\n",
      "        [3.0411e-22, 1.0694e-19, 5.9872e-20, 4.7089e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "880000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3475e-40, 1.0423e-41, 2.5658e-21],\n",
      "        [3.0374e-22, 1.0681e-19, 5.9786e-20, 4.7250e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "881000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3465e-40, 1.0414e-41, 2.5696e-21],\n",
      "        [3.0365e-22, 1.0677e-19, 5.9764e-20, 4.7287e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "882000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3480e-40, 1.0427e-41, 2.5638e-21],\n",
      "        [3.0378e-22, 1.0681e-19, 5.9795e-20, 4.7230e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "883000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3518e-40, 1.0459e-41, 2.5491e-21],\n",
      "        [3.0410e-22, 1.0693e-19, 5.9870e-20, 4.7087e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "884000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3433e-40, 1.0388e-41, 2.5817e-21],\n",
      "        [3.0336e-22, 1.0667e-19, 5.9700e-20, 4.7404e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "885000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3356e-40, 1.0325e-41, 2.6112e-21],\n",
      "        [3.0272e-22, 1.0644e-19, 5.9560e-20, 4.7690e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "886000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3374e-40, 1.0339e-41, 2.6040e-21],\n",
      "        [3.0288e-22, 1.0650e-19, 5.9595e-20, 4.7619e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "887000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3402e-40, 1.0363e-41, 2.5931e-21],\n",
      "        [3.0311e-22, 1.0658e-19, 5.9644e-20, 4.7516e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "888000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3453e-40, 1.0405e-41, 2.5737e-21],\n",
      "        [3.0354e-22, 1.0673e-19, 5.9742e-20, 4.7326e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "889000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3391e-40, 1.0353e-41, 2.5978e-21],\n",
      "        [3.0300e-22, 1.0655e-19, 5.9624e-20, 4.7558e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "890000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3460e-40, 1.0410e-41, 2.5711e-21],\n",
      "        [3.0360e-22, 1.0676e-19, 5.9753e-20, 4.7302e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "891000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3410e-40, 1.0368e-41, 2.5901e-21],\n",
      "        [3.0317e-22, 1.0660e-19, 5.9658e-20, 4.7486e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "892000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3376e-40, 1.0342e-41, 2.6027e-21],\n",
      "        [3.0289e-22, 1.0650e-19, 5.9600e-20, 4.7607e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "893000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3487e-40, 1.0433e-41, 2.5599e-21],\n",
      "        [3.0383e-22, 1.0683e-19, 5.9804e-20, 4.7192e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "894000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3389e-40, 1.0351e-41, 2.5980e-21],\n",
      "        [3.0299e-22, 1.0654e-19, 5.9620e-20, 4.7564e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "895000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3322e-40, 1.0297e-41, 2.6241e-21],\n",
      "        [3.0242e-22, 1.0634e-19, 5.9493e-20, 4.7813e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "896000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3436e-40, 1.0391e-41, 2.5795e-21],\n",
      "        [3.0341e-22, 1.0668e-19, 5.9709e-20, 4.7383e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "897000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3487e-40, 1.0433e-41, 2.5600e-21],\n",
      "        [3.0383e-22, 1.0683e-19, 5.9804e-20, 4.7194e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "898000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3412e-40, 1.0370e-41, 2.5892e-21],\n",
      "        [3.0319e-22, 1.0661e-19, 5.9660e-20, 4.7475e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "899000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3354e-40, 1.0322e-41, 2.6111e-21],\n",
      "        [3.0269e-22, 1.0644e-19, 5.9553e-20, 4.7686e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "900000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3349e-40, 1.0318e-41, 2.6128e-21],\n",
      "        [3.0265e-22, 1.0642e-19, 5.9538e-20, 4.7703e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "901000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3443e-40, 1.0396e-41, 2.5762e-21],\n",
      "        [3.0346e-22, 1.0670e-19, 5.9722e-20, 4.7352e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "902000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3445e-40, 1.0396e-41, 2.5758e-21],\n",
      "        [3.0346e-22, 1.0670e-19, 5.9718e-20, 4.7347e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "903000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3382e-40, 1.0344e-41, 2.6001e-21],\n",
      "        [3.0292e-22, 1.0651e-19, 5.9602e-20, 4.7580e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "904000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3418e-40, 1.0375e-41, 2.5862e-21],\n",
      "        [3.0325e-22, 1.0662e-19, 5.9671e-20, 4.7448e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "905000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3448e-40, 1.0399e-41, 2.5747e-21],\n",
      "        [3.0350e-22, 1.0672e-19, 5.9729e-20, 4.7337e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "906000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3391e-40, 1.0351e-41, 2.5968e-21],\n",
      "        [3.0300e-22, 1.0654e-19, 5.9618e-20, 4.7548e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "907000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3449e-40, 1.0400e-41, 2.5740e-21],\n",
      "        [3.0352e-22, 1.0672e-19, 5.9731e-20, 4.7331e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "908000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5695e-43, 1.3292e-40, 1.0270e-41, 2.6356e-21],\n",
      "        [3.0216e-22, 1.0624e-19, 5.9429e-20, 4.7922e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "909000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3363e-40, 1.0330e-41, 2.6071e-21],\n",
      "        [3.0279e-22, 1.0646e-19, 5.9567e-20, 4.7648e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "910000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3388e-40, 1.0350e-41, 2.5978e-21],\n",
      "        [3.0300e-22, 1.0653e-19, 5.9616e-20, 4.7559e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "911000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3331e-40, 1.0302e-41, 2.6203e-21],\n",
      "        [3.0251e-22, 1.0636e-19, 5.9507e-20, 4.7776e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "912000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5695e-43, 1.3291e-40, 1.0270e-41, 2.6353e-21],\n",
      "        [3.0215e-22, 1.0624e-19, 5.9426e-20, 4.7919e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "913000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3314e-40, 1.0288e-41, 2.6266e-21],\n",
      "        [3.0235e-22, 1.0631e-19, 5.9471e-20, 4.7836e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "914000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3473e-40, 1.0420e-41, 2.5650e-21],\n",
      "        [3.0371e-22, 1.0679e-19, 5.9777e-20, 4.7244e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "915000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3494e-40, 1.0435e-41, 2.5570e-21],\n",
      "        [3.0389e-22, 1.0685e-19, 5.9811e-20, 4.7162e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "916000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3434e-40, 1.0388e-41, 2.5792e-21],\n",
      "        [3.0337e-22, 1.0667e-19, 5.9702e-20, 4.7381e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "917000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3438e-40, 1.0391e-41, 2.5783e-21],\n",
      "        [3.0341e-22, 1.0668e-19, 5.9704e-20, 4.7370e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "918000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3505e-40, 1.0445e-41, 2.5526e-21],\n",
      "        [3.0398e-22, 1.0688e-19, 5.9830e-20, 4.7122e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "919000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3454e-40, 1.0403e-41, 2.5717e-21],\n",
      "        [3.0354e-22, 1.0673e-19, 5.9737e-20, 4.7306e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "920000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3524e-40, 1.0461e-41, 2.5447e-21],\n",
      "        [3.0414e-22, 1.0694e-19, 5.9868e-20, 4.7046e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "921000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3378e-40, 1.0342e-41, 2.6009e-21],\n",
      "        [3.0290e-22, 1.0650e-19, 5.9589e-20, 4.7590e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "922000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3454e-40, 1.0403e-41, 2.5712e-21],\n",
      "        [3.0355e-22, 1.0673e-19, 5.9737e-20, 4.7302e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "923000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3432e-40, 1.0386e-41, 2.5796e-21],\n",
      "        [3.0337e-22, 1.0666e-19, 5.9695e-20, 4.7383e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "924000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3470e-40, 1.0417e-41, 2.5656e-21],\n",
      "        [3.0367e-22, 1.0677e-19, 5.9762e-20, 4.7247e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "925000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3410e-40, 1.0367e-41, 2.5885e-21],\n",
      "        [3.0318e-22, 1.0660e-19, 5.9653e-20, 4.7470e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "926000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3496e-40, 1.0440e-41, 2.5552e-21],\n",
      "        [3.0390e-22, 1.0686e-19, 5.9817e-20, 4.7146e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "927000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3482e-40, 1.0427e-41, 2.5606e-21],\n",
      "        [3.0380e-22, 1.0681e-19, 5.9788e-20, 4.7199e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "928000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3464e-40, 1.0412e-41, 2.5672e-21],\n",
      "        [3.0364e-22, 1.0676e-19, 5.9753e-20, 4.7263e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "929000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3374e-40, 1.0339e-41, 2.6018e-21],\n",
      "        [3.0288e-22, 1.0649e-19, 5.9585e-20, 4.7597e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "930000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3410e-40, 1.0367e-41, 2.5883e-21],\n",
      "        [3.0318e-22, 1.0659e-19, 5.9649e-20, 4.7467e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "931000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3351e-40, 1.0319e-41, 2.6116e-21],\n",
      "        [3.0266e-22, 1.0641e-19, 5.9535e-20, 4.7690e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "932000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3410e-40, 1.0367e-41, 2.5884e-21],\n",
      "        [3.0317e-22, 1.0660e-19, 5.9651e-20, 4.7468e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "933000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3417e-40, 1.0374e-41, 2.5856e-21],\n",
      "        [3.0323e-22, 1.0661e-19, 5.9664e-20, 4.7441e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "934000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3415e-40, 1.0371e-41, 2.5864e-21],\n",
      "        [3.0321e-22, 1.0661e-19, 5.9660e-20, 4.7449e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "935000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3498e-40, 1.0440e-41, 2.5544e-21],\n",
      "        [3.0392e-22, 1.0686e-19, 5.9817e-20, 4.7138e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "936000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3496e-40, 1.0438e-41, 2.5557e-21],\n",
      "        [3.0391e-22, 1.0685e-19, 5.9809e-20, 4.7151e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "937000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3471e-40, 1.0417e-41, 2.5652e-21],\n",
      "        [3.0369e-22, 1.0677e-19, 5.9762e-20, 4.7244e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "938000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3495e-40, 1.0437e-41, 2.5558e-21],\n",
      "        [3.0390e-22, 1.0685e-19, 5.9809e-20, 4.7152e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "939000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3442e-40, 1.0393e-41, 2.5762e-21],\n",
      "        [3.0344e-22, 1.0669e-19, 5.9709e-20, 4.7351e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "940000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3421e-40, 1.0377e-41, 2.5843e-21],\n",
      "        [3.0329e-22, 1.0663e-19, 5.9673e-20, 4.7426e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "941000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5835e-43, 1.3400e-40, 1.0358e-41, 2.5926e-21],\n",
      "        [3.0307e-22, 1.0656e-19, 5.9627e-20, 4.7509e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "942000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3358e-40, 1.0322e-41, 2.6091e-21],\n",
      "        [3.0270e-22, 1.0643e-19, 5.9544e-20, 4.7670e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "943000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3441e-40, 1.0392e-41, 2.5762e-21],\n",
      "        [3.0344e-22, 1.0668e-19, 5.9706e-20, 4.7350e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "944000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3468e-40, 1.0414e-41, 2.5662e-21],\n",
      "        [3.0368e-22, 1.0677e-19, 5.9760e-20, 4.7254e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "945000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3427e-40, 1.0381e-41, 2.5817e-21],\n",
      "        [3.0331e-22, 1.0664e-19, 5.9680e-20, 4.7404e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "946000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.6115e-43, 1.3546e-40, 1.0479e-41, 2.5368e-21],\n",
      "        [3.0432e-22, 1.0700e-19, 5.9904e-20, 4.6967e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "947000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3471e-40, 1.0417e-41, 2.5649e-21],\n",
      "        [3.0370e-22, 1.0678e-19, 5.9764e-20, 4.7241e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "948000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3357e-40, 1.0323e-41, 2.6091e-21],\n",
      "        [3.0272e-22, 1.0643e-19, 5.9547e-20, 4.7668e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "949000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3422e-40, 1.0378e-41, 2.5834e-21],\n",
      "        [3.0329e-22, 1.0663e-19, 5.9675e-20, 4.7420e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "950000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3402e-40, 1.0361e-41, 2.5910e-21],\n",
      "        [3.0312e-22, 1.0657e-19, 5.9631e-20, 4.7493e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "951000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3432e-40, 1.0385e-41, 2.5800e-21],\n",
      "        [3.0335e-22, 1.0665e-19, 5.9689e-20, 4.7386e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "952000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3407e-40, 1.0365e-41, 2.5892e-21],\n",
      "        [3.0316e-22, 1.0658e-19, 5.9644e-20, 4.7475e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "953000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3426e-40, 1.0379e-41, 2.5818e-21],\n",
      "        [3.0331e-22, 1.0663e-19, 5.9678e-20, 4.7403e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "954000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3405e-40, 1.0363e-41, 2.5899e-21],\n",
      "        [3.0315e-22, 1.0658e-19, 5.9638e-20, 4.7481e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "955000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3455e-40, 1.0403e-41, 2.5709e-21],\n",
      "        [3.0355e-22, 1.0672e-19, 5.9729e-20, 4.7300e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "956000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3430e-40, 1.0384e-41, 2.5806e-21],\n",
      "        [3.0334e-22, 1.0665e-19, 5.9686e-20, 4.7393e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "957000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3469e-40, 1.0416e-41, 2.5656e-21],\n",
      "        [3.0368e-22, 1.0677e-19, 5.9758e-20, 4.7248e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "958000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3456e-40, 1.0405e-41, 2.5704e-21],\n",
      "        [3.0356e-22, 1.0673e-19, 5.9735e-20, 4.7295e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "959000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3465e-40, 1.0412e-41, 2.5671e-21],\n",
      "        [3.0365e-22, 1.0675e-19, 5.9751e-20, 4.7263e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "960000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3377e-40, 1.0339e-41, 2.6009e-21],\n",
      "        [3.0289e-22, 1.0649e-19, 5.9585e-20, 4.7587e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "961000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3360e-40, 1.0326e-41, 2.6072e-21],\n",
      "        [3.0275e-22, 1.0644e-19, 5.9553e-20, 4.7650e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "962000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3389e-40, 1.0349e-41, 2.5966e-21],\n",
      "        [3.0298e-22, 1.0652e-19, 5.9605e-20, 4.7546e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "963000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3430e-40, 1.0382e-41, 2.5806e-21],\n",
      "        [3.0333e-22, 1.0665e-19, 5.9684e-20, 4.7391e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "964000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3394e-40, 1.0353e-41, 2.5939e-21],\n",
      "        [3.0306e-22, 1.0654e-19, 5.9615e-20, 4.7520e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "965000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3412e-40, 1.0367e-41, 2.5877e-21],\n",
      "        [3.0317e-22, 1.0659e-19, 5.9647e-20, 4.7461e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "966000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3358e-40, 1.0323e-41, 2.6082e-21],\n",
      "        [3.0273e-22, 1.0643e-19, 5.9545e-20, 4.7660e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "967000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3437e-40, 1.0389e-41, 2.5772e-21],\n",
      "        [3.0342e-22, 1.0667e-19, 5.9696e-20, 4.7360e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "968000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3376e-40, 1.0337e-41, 2.6014e-21],\n",
      "        [3.0288e-22, 1.0649e-19, 5.9580e-20, 4.7593e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "969000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3422e-40, 1.0377e-41, 2.5832e-21],\n",
      "        [3.0329e-22, 1.0662e-19, 5.9667e-20, 4.7419e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "970000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3396e-40, 1.0356e-41, 2.5933e-21],\n",
      "        [3.0306e-22, 1.0654e-19, 5.9620e-20, 4.7516e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "971000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3440e-40, 1.0391e-41, 2.5764e-21],\n",
      "        [3.0343e-22, 1.0668e-19, 5.9702e-20, 4.7352e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "972000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3402e-40, 1.0360e-41, 2.5911e-21],\n",
      "        [3.0310e-22, 1.0657e-19, 5.9631e-20, 4.7494e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "973000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3434e-40, 1.0385e-41, 2.5786e-21],\n",
      "        [3.0339e-22, 1.0666e-19, 5.9689e-20, 4.7374e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "974000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3441e-40, 1.0392e-41, 2.5757e-21],\n",
      "        [3.0343e-22, 1.0668e-19, 5.9702e-20, 4.7344e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "975000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3452e-40, 1.0400e-41, 2.5724e-21],\n",
      "        [3.0353e-22, 1.0671e-19, 5.9720e-20, 4.7315e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "976000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3417e-40, 1.0371e-41, 2.5854e-21],\n",
      "        [3.0322e-22, 1.0661e-19, 5.9655e-20, 4.7439e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "977000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3357e-40, 1.0322e-41, 2.6085e-21],\n",
      "        [3.0273e-22, 1.0643e-19, 5.9547e-20, 4.7664e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "978000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3457e-40, 1.0405e-41, 2.5696e-21],\n",
      "        [3.0358e-22, 1.0673e-19, 5.9735e-20, 4.7287e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "979000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3421e-40, 1.0375e-41, 2.5834e-21],\n",
      "        [3.0329e-22, 1.0662e-19, 5.9665e-20, 4.7419e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "980000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3472e-40, 1.0417e-41, 2.5639e-21],\n",
      "        [3.0369e-22, 1.0677e-19, 5.9762e-20, 4.7233e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "981000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3523e-40, 1.0459e-41, 2.5447e-21],\n",
      "        [3.0413e-22, 1.0692e-19, 5.9853e-20, 4.7044e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "982000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3434e-40, 1.0386e-41, 2.5782e-21],\n",
      "        [3.0337e-22, 1.0665e-19, 5.9687e-20, 4.7370e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "983000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3464e-40, 1.0410e-41, 2.5668e-21],\n",
      "        [3.0363e-22, 1.0674e-19, 5.9742e-20, 4.7260e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "984000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3396e-40, 1.0354e-41, 2.5929e-21],\n",
      "        [3.0306e-22, 1.0654e-19, 5.9616e-20, 4.7512e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "985000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3433e-40, 1.0385e-41, 2.5784e-21],\n",
      "        [3.0336e-22, 1.0665e-19, 5.9687e-20, 4.7371e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "986000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3404e-40, 1.0361e-41, 2.5894e-21],\n",
      "        [3.0312e-22, 1.0657e-19, 5.9629e-20, 4.7478e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "987000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3442e-40, 1.0392e-41, 2.5750e-21],\n",
      "        [3.0344e-22, 1.0668e-19, 5.9698e-20, 4.7338e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "988000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3374e-40, 1.0336e-41, 2.6008e-21],\n",
      "        [3.0287e-22, 1.0647e-19, 5.9573e-20, 4.7589e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "989000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3430e-40, 1.0382e-41, 2.5795e-21],\n",
      "        [3.0334e-22, 1.0664e-19, 5.9678e-20, 4.7384e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "990000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3523e-40, 1.0459e-41, 2.5443e-21],\n",
      "        [3.0414e-22, 1.0692e-19, 5.9853e-20, 4.7040e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "991000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3480e-40, 1.0423e-41, 2.5602e-21],\n",
      "        [3.0376e-22, 1.0679e-19, 5.9773e-20, 4.7194e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "992000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3513e-40, 1.0449e-41, 2.5479e-21],\n",
      "        [3.0405e-22, 1.0689e-19, 5.9833e-20, 4.7076e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "993000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3441e-40, 1.0391e-41, 2.5751e-21],\n",
      "        [3.0343e-22, 1.0667e-19, 5.9698e-20, 4.7341e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "994000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3389e-40, 1.0347e-41, 2.5949e-21],\n",
      "        [3.0299e-22, 1.0651e-19, 5.9595e-20, 4.7532e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "995000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3503e-40, 1.0442e-41, 2.5510e-21],\n",
      "        [3.0396e-22, 1.0686e-19, 5.9811e-20, 4.7104e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "996000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5975e-43, 1.3499e-40, 1.0438e-41, 2.5528e-21],\n",
      "        [3.0394e-22, 1.0685e-19, 5.9806e-20, 4.7126e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "997000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3418e-40, 1.0371e-41, 2.5839e-21],\n",
      "        [3.0324e-22, 1.0660e-19, 5.9653e-20, 4.7426e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "998000   actions:  tensor([3, 3, 1])\n",
      "loss=  tensor(2.6162, grad_fn=<DivBackward0>)   , return=  16231.8125\n",
      "discReturns/1000= tensor([6.4231, 6.5993, 5.6616])\n",
      "actionProbs tensor([[1.5835e-43, 1.3351e-40, 1.0316e-41, 2.6104e-21],\n",
      "        [3.0267e-22, 1.0640e-19, 5.9525e-20, 4.7682e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "--------------------------------------------------\n",
      "999000   actions:  tensor([3, 3, 3])\n",
      "loss=  tensor(2.6125, grad_fn=<DivBackward0>)   , return=  16223.8125\n",
      "discReturns/1000= tensor([6.4228, 6.5977, 5.6536])\n",
      "actionProbs tensor([[1.5975e-43, 1.3424e-40, 1.0377e-41, 2.5817e-21],\n",
      "        [3.0331e-22, 1.0663e-19, 5.9667e-20, 4.7404e-12],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "algorithm.solver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "advModeNames=\"\"\n",
    "for i in range(len(adversaryProbs)):\n",
    "    if adversaryProbs[i]!=0:\n",
    "        tmp=\"{:.1f}\".format(adversaryProbs[i])\n",
    "        advModeNames+=f\"{(AdversaryModes(i)).name}-{tmp}-\"\n",
    "    \n",
    "name=f\"ep {algorithm.numberEpisodes}, {advModeNames}, {game.advHistoryNum} hist, {neuralNet.lr} lr\"\n",
    "neuralNet.save(name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "profits = pd.DataFrame(game.profit).T\n",
    "prices = pd.DataFrame(game.prices).T\n",
    "demandPotential = pd.DataFrame(game.demandPotential).T\n",
    "learning = pd.DataFrame(algorithm.returns.mean(axis = 0),columns=['entry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = pd.DataFrame(algorithm.loss.mean(axis = 0),columns=['entry'])\n",
    "loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>205.0</td>\n",
       "      <td>195.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>207.5</td>\n",
       "      <td>192.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1\n",
       "0  200.0  200.0\n",
       "1  205.0  195.0\n",
       "2  207.5  192.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "demandPotential.plot()\n",
    "demandPotential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    16223.8125\n",
       "1    11694.8125\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profits.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5103.2500</td>\n",
       "      <td>4160.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5467.0000</td>\n",
       "      <td>3844.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5653.5625</td>\n",
       "      <td>3690.5625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1\n",
       "0  5103.2500  4160.2500\n",
       "1  5467.0000  3844.0000\n",
       "2  5653.5625  3690.5625"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "profits.plot()\n",
    "profits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125.50</td>\n",
       "      <td>135.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128.00</td>\n",
       "      <td>133.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>129.25</td>\n",
       "      <td>131.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1\n",
       "0  125.50  135.50\n",
       "1  128.00  133.00\n",
       "2  129.25  131.75"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prices.plot()\n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pricelearning = pd.DataFrame(game.prices.mean(axis = 0))\n",
    "# pricelearning.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_1 = learning.to_numpy()\n",
    "learning_2 = [0]*len(learning)\n",
    "for i in range(len(learning_1)):\n",
    "    learning_2[i] = learning_1[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_learning = np.convolve(learning_2, np.ones(1000)/1000, mode = 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_learning)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x155368a1730>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(learning['entry'],loss['entry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
