{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2905d491-8924-44ef-9521-e76a35dce019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# added SPE to the base strategies at first, then deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dea34fbb-ece9-4c29-9b1b-ef758f09668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from stable_baselines3 import SAC, PPO\n",
    "# from stable_baselines3.common.env_util import make_vec_env\n",
    "# from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "import time\n",
    "import sys\n",
    "from src.environments import ConPricingGame\n",
    "import src.globals as gl\n",
    "import src.classes as cl\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd831764-040c-4580-a8a0-6fc9986b3b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inputs):\n",
    "    \"\"\" traines one agent against the adversary, if the expected payoff of new agent is greater than expected payoff of NE, returns acceptable=true and the new strategy and payoff to be added to the strategies and matrix.\"\"\"\n",
    "    id,seed, job_name,env, base_agent, alg, adv_mixed_strategy,lr,target_payoff,db=inputs\n",
    "    \n",
    "    gl.initialize()\n",
    "    model_name = f\"{job_name}-{str(seed)}\"\n",
    "    models_dir = f\"{gl.MODELS_DIR}/{model_name}\"\n",
    "    log_dir = f\"{gl.LOG_DIR}/{model_name}\"\n",
    "    \n",
    "    acceptable = False\n",
    "    if base_agent is None:\n",
    "        number_episodes = gl.N_EPISODES_BASE * (1 + gl.EPISODE_INCREASE_PORTION * (adv_mixed_strategy.support_size-1))\n",
    "        if alg is SAC:\n",
    "            model = alg('MlpPolicy', env, learning_rate=lr,\n",
    "                        verbose=0, tensorboard_log=log_dir, gamma=gl.GAMMA, target_entropy=0, seed=seed)\n",
    "        else:\n",
    "            model = alg('MlpPolicy', env, learning_rate=lr,\n",
    "                        verbose=0, tensorboard_log=log_dir, gamma=gl.GAMMA,seed=seed)\n",
    "    else:\n",
    "        number_episodes = gl.N_EPISODES_LOAD * (1 + gl.EPISODE_INCREASE_PORTION * (adv_mixed_strategy.support_size-1))\n",
    "        base_agent_dir = f\"{gl.MODELS_DIR}/{base_agent}\"\n",
    "        if alg is SAC:\n",
    "            model = alg.load(base_agent_dir, env, learning_rate=lr,\n",
    "                             verbose=0, tensorboard_log=log_dir, gamma=gl.GAMMA, target_entropy=0)\n",
    "        else:\n",
    "            model = alg.load(base_agent_dir, env, learning_rate=lr,\n",
    "                             verbose=0, tensorboard_log=log_dir, gamma=gl.GAMMA)\n",
    "    start = time.time()\n",
    "    # for i in range(gl.NUM_MODEL_SAVE):\n",
    "    # tmp = (number_episodes/gl.NUM_MODEL_SAVE)\n",
    "    # model.learn(total_timesteps=tmp, reset_num_timesteps=False,\n",
    "    #             tb_log_name=model_name)\n",
    "    # model.save(os.path.join(models_dir, str(tmp*(i+1))))\n",
    "    \n",
    "    # https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#id3\n",
    "    #check to save and load replay buffer as well\n",
    "    model.learn(total_timesteps=number_episodes, tb_log_name=model_name)\n",
    "    model.save(models_dir)\n",
    "    running_time = time.time() - start\n",
    "    \n",
    "    model_strategy = cl.Strategy(strategy_type=cl.StrategyType.sb3_model,\n",
    "                                 model_or_func=alg, name=model_name, action_step=env.action_step,memory=env.memory)\n",
    "    \n",
    "    iter_rows = []\n",
    "    agent_payoffs = np.zeros(len(adv_mixed_strategy.strategies))\n",
    "    adv_payoffs = np.zeros(len(adv_mixed_strategy.strategies))\n",
    "    expected_payoff = 0\n",
    "    for strategy_index in range(len(adv_mixed_strategy.strategies)):\n",
    "        if adv_mixed_strategy.strategy_probs[strategy_index] > 0:\n",
    "            payoffs = []\n",
    "            for _ in range(gl.NUM_STOCHASTIC_ITER):\n",
    "                # returns = algorithm.play_trained_agent(adversary=(\n",
    "                #     (adv_mixed_strategy._strategies[strategy_index]).to_mixed_strategy()), iterNum=gl.num_stochastic_iter)\n",
    "                payoffs.append(model_strategy.play_against(\n",
    "                    env=env, adversary=adv_mixed_strategy.strategies[strategy_index]))\n",
    "                \n",
    "                #adv, agent_return, adv_return, rewards, adv_rewards, actions, prices, adv_prices, demands, adv_demands\n",
    "                iter_row = cl.Iter_row(adv=env.adversary_strategy.name, agent_return=sum(env.profit[0]), adv_return=sum(env.profit[1]), rewards=str(\n",
    "                    env.profit[0]), adv_rewards=str(env.profit[1]), actions=str(env.actions),prices=str(env.prices[0]), adv_prices=str(env.prices[1]) ,demands=str(env.demand_potential[0]), adv_demands=str(env.demand_potential[1]))\n",
    "\n",
    "                iter_rows.append(iter_row)\n",
    "\n",
    "            mean_payoffs = np.array(payoffs).mean(axis=0)\n",
    "\n",
    "            agent_payoffs[strategy_index] = mean_payoffs[0]\n",
    "            adv_payoffs[strategy_index] = mean_payoffs[1]\n",
    "            expected_payoff += (agent_payoffs[strategy_index]) * \\\n",
    "                (adv_mixed_strategy.strategy_probs[strategy_index])\n",
    "\n",
    "    acceptable=(expected_payoff > target_payoff)\n",
    "    # agent_id=db.insert_new_agent(model_name,number_episodes,costs[0], str(adv_mixed_strategy), expected_payoff,target_payoff, lr,memory, acceptable, pricing_game.action_step, seed,num_procs,running_time)\n",
    "    agent_id = db.insert_new_agent(db.AgentRow(model_name, base_agent, number_episodes, env.costs[0], str(\n",
    "        adv_mixed_strategy), expected_payoff, target_payoff,  str(alg),lr, env.memory, acceptable, env.action_step, seed, 1, running_time))\n",
    "    #num_processes=1 because it just uses one process in training this agent\n",
    "\n",
    "    if acceptable:\n",
    "        for row in iter_rows:\n",
    "            db.insert_new_iteration(agent_id, row.adv, row.agent_return, row.adv_return, row.rewards,\n",
    "                                    row.adv_rewards, row.actions, row.prices, row.adv_prices, row.demands, row.adv_demands)\n",
    "    \n",
    "    return (id,acceptable,model_strategy.name,agent_payoffs, adv_payoffs, expected_payoff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "286f8903-6ec8-4c8e-8868-a7da4d67bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_processes(db, env_class, costs, adv_mixed_strategy, target_payoff, num_procs, alg, lr, memory):\n",
    "    \"\"\"\n",
    "    trains multiple agents with multiprocessing against mixed_adversary. \n",
    "    \"\"\"\n",
    "    inputs=[]\n",
    "    seed = int(time.time())\n",
    "    adv_strt= adv_mixed_strategy.copy_unload()\n",
    "    \n",
    "    base_agents= cl.find_base_agents(db=db,alg=alg,memory=memory,cost=costs[0],mix_strt=adv_strt,size=num_procs)\n",
    "    \n",
    "    for p in range(num_procs):\n",
    "        env = env_class(tuple_costs=costs, adversary_mixed_strategy=adv_strt, memory=memory)\n",
    "        input_proc=(p,seed+p, job_name,env, base_agents[p],alg, adv_strt,lr,target_payoff,db)\n",
    "        inputs.append(input_proc)\n",
    "    results=[]\n",
    "    # with cf.ProcessPoolExecutor() as executor:\n",
    "    # # Submit all the tasks to the executor and get the future objects\n",
    "    #     futures = [executor.submit(train, input_proc) for input_proc in inputs]\n",
    "    #     for future in cf.as_completed(futures):\n",
    "    #         res=future.result()\n",
    "    pool = mp.Pool(processes=num_procs)\n",
    "    \n",
    "    outputs=pool.imap_unordered(train,inputs)\n",
    "    for output in outputs:\n",
    "        id,acceptable,strategy_name,agent_payoffs, adv_payoffs, expected_payoff=output\n",
    "        # id,acceptable,model_strategy,agent_payoffs, adv_payoffs, expected_payoff= train(inputs[0])\n",
    "        pricing_game = env_class(tuple_costs=costs, adversary_mixed_strategy=adv_strt, memory=memory)\n",
    "        model_strategy = cl.Strategy(strategy_type=cl.StrategyType.sb3_model,\n",
    "                                 model_or_func=alg, name=strategy_name, action_step=pricing_game.action_step,memory=memory)\n",
    "    # compute the payoff against all adv strategies, to be added to the matrix\n",
    "        if acceptable:\n",
    "            for strategy_index in range(len(adv_mixed_strategy.strategies)):\n",
    "                if adv_mixed_strategy.strategy_probs[strategy_index] == 0:\n",
    "                    payoffs = []\n",
    "                    for _ in range(gl.NUM_STOCHASTIC_ITER):\n",
    "                        payoffs.append(model_strategy.play_against(\n",
    "                            env=pricing_game, adversary=adv_mixed_strategy.strategies[strategy_index]))\n",
    "                    mean_payoffs = np.array(payoffs).mean(axis=0)\n",
    "\n",
    "                    agent_payoffs[strategy_index] = mean_payoffs[0]\n",
    "                    adv_payoffs[strategy_index] = mean_payoffs[1]\n",
    "            results.append((acceptable, agent_payoffs, adv_payoffs, model_strategy, expected_payoff, base_agents[id]))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7392fe9-84d2-467d-b610-3936ca16878e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training low-cost agents with alg=<class 'stable_baselines3.sac.sac.SAC'>, lr=0.0003, memory=12\n",
      "training low-cost agents with alg=<class 'stable_baselines3.sac.sac.SAC'>, lr=0.0003, memory=18\n",
      "training low-cost agents with alg=<class 'stable_baselines3.sac.sac.SAC'>, lr=0.0002, memory=12\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Observation spaces do not match: Box(0.0, 400.0, (38,), float32) != Box(0.0, 400.0, (26,), float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 60>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mem_i,memory \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(memories):\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining low-cost agents with alg=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(alg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, memory=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmemory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 89\u001b[0m     results\u001b[38;5;241m=\u001b[39m \u001b[43mtrain_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcosts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mgl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLOW_COST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHIGH_COST\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m                             \u001b[49m\u001b[43madv_mixed_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhigh_mixed_strat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_payoff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlow_cost_payoff\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mnum_procs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_procs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m     93\u001b[0m         acceptable, agent_payoffs, adv_payoffs, agent_strategy, expected_payoff,base_agent_name \u001b[38;5;241m=\u001b[39m result\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mtrain_processes\u001b[1;34m(db, env_class, costs, adv_mixed_strategy, target_payoff, num_procs, alg, lr, memory)\u001b[0m\n\u001b[0;32m     15\u001b[0m     results\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#     with cf.ProcessPoolExecutor() as executor:\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Submit all the tasks to the executor and get the future objects\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#         futures = [executor.submit(train, input) for input_proc in inputs]\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#         for future in cf.as_completed(futures):\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#             id,acceptable,model_strategy,agent_payoffs, adv_payoffs, expected_payoff=future.result()\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28mid\u001b[39m,acceptable,model_strategy,agent_payoffs, adv_payoffs, expected_payoff\u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     pricing_game \u001b[38;5;241m=\u001b[39m env_class(tuple_costs\u001b[38;5;241m=\u001b[39mcosts, adversary_mixed_strategy\u001b[38;5;241m=\u001b[39madv_strt, memory\u001b[38;5;241m=\u001b[39mmemory)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# compute the payoff against all adv strategies, to be added to the matrix\u001b[39;00m\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m     21\u001b[0m base_agent_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgl\u001b[38;5;241m.\u001b[39mMODELS_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_agent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m alg \u001b[38;5;129;01mis\u001b[39;00m SAC:\n\u001b[1;32m---> 23\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43malg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_agent_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGAMMA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_entropy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     model \u001b[38;5;241m=\u001b[39m alg\u001b[38;5;241m.\u001b[39mload(base_agent_dir, env, learning_rate\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m     27\u001b[0m                      verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, tensorboard_log\u001b[38;5;241m=\u001b[39mlog_dir, gamma\u001b[38;5;241m=\u001b[39mgl\u001b[38;5;241m.\u001b[39mGAMMA)\n",
      "File \u001b[1;32m~\\Documents\\Anaconda\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:716\u001b[0m, in \u001b[0;36mBaseAlgorithm.load\u001b[1;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[0;32m    714\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_env(env, data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# Check if given env is valid\u001b[39;00m\n\u001b[1;32m--> 716\u001b[0m \u001b[43mcheck_for_correct_spaces\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobservation_space\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maction_space\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;66;03m# Discard `_last_obs`, this will force the env to reset before training\u001b[39;00m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;66;03m# See issue https://github.com/DLR-RM/stable-baselines3/issues/597\u001b[39;00m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_reset \u001b[38;5;129;01mand\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Documents\\Anaconda\\lib\\site-packages\\stable_baselines3\\common\\utils.py:229\u001b[0m, in \u001b[0;36mcheck_for_correct_spaces\u001b[1;34m(env, observation_space, action_space)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03mChecks that the environment has same spaces as provided ones. Used by BaseAlgorithm to check if\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03mspaces match after loading the model with given env.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;124;03m:param action_space: Action space to check against\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observation_space \u001b[38;5;241m!=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space:\n\u001b[1;32m--> 229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObservation spaces do not match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action_space \u001b[38;5;241m!=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space:\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction spaces do not match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Observation spaces do not match: Box(0.0, 400.0, (38,), float32) != Box(0.0, 400.0, (26,), float32)"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    env_class = ConPricingGame\n",
    "    gl.initialize()\n",
    "\n",
    "    num_rounds = 3\n",
    "\n",
    "    job_name = \"rnd_Mar11\"\n",
    "    db_name = job_name+\".db\"\n",
    "    db = cl.DataBase(db_name)\n",
    "    low_strts, high_strts=db.get_list_of_added_strategies()\n",
    "    cl.set_job_name(job_name)\n",
    "    # num_procs = gl.NUM_PROCESS if (len(sys.argv) < 2) else int(sys.argv[1])\n",
    "    num_procs = 6\n",
    "\n",
    "\n",
    "\n",
    "    # changing params\n",
    "    lrs = [0.0003, 0.00016]\n",
    "    memories = [12,18]\n",
    "    # memories_agents=[[None]*len(memories)]*2\n",
    "    algs = [SAC]\n",
    "\n",
    "    equilibria = []\n",
    "\n",
    "    cl.create_directories()\n",
    "\n",
    "    # strt1 = cl.Strategy(\n",
    "    #     cl.StrategyType.static, model_or_func=cl.myopic, name=\"myopic\")\n",
    "    # strt2 = cl.Strategy(\n",
    "    #     cl.StrategyType.static, model_or_func=cl.const, name=\"const\", first_price=132)\n",
    "    # strt3 = cl.Strategy(\n",
    "    #     cl.StrategyType.static, model_or_func=cl.guess, name=\"guess\", first_price=132)\n",
    "    # strt4 = cl.Strategy(\n",
    "    #     cl.StrategyType.static, model_or_func=cl.spe, name=\"spe\")\n",
    "\n",
    "    train_env = env_class(tuple_costs=None, adversary_mixed_strategy=None, memory=12)\n",
    "    model_name=\"rnd_start\"\n",
    "    log_dir = f\"{gl.LOG_DIR}/{model_name}\"\n",
    "    model = SAC('MlpPolicy', train_env,\n",
    "                            verbose=0, tensorboard_log=log_dir, gamma=gl.GAMMA, target_entropy=0)\n",
    "    # model.learn(total_timesteps=1, tb_log_name=model_name)\n",
    "    model.save(f\"{gl.MODELS_DIR}/{model_name}\")\n",
    "\n",
    "    strt_rnd= cl.Strategy(strategy_type=cl.StrategyType.sb3_model,\n",
    "                                    model_or_func=SAC, name=model_name, action_step=None,memory=12)\n",
    "\n",
    "    bimatrix_game = cl.BimatrixGame(\n",
    "        low_cost_strategies=[strt_rnd]+low_strts, high_cost_strategies=[strt_rnd]+high_strts, env_class=env_class)\n",
    "\n",
    "    bimatrix_game.reset_matrix()\n",
    "    bimatrix_game.fill_matrix()\n",
    "\n",
    "\n",
    "\n",
    "    cl.prt(\"\\n\" + time.ctime(time.time())+\"\\n\"+(\"-\"*50)+\"\\n\")\n",
    "\n",
    "    dictionaries = bimatrix_game.compute_equilibria()\n",
    "    game_size=bimatrix_game.size()\n",
    "\n",
    "    # low_cost_probabilities, high_cost_probabilities, low_cost_payoff, high_cost_payoff = bimatrix_game.compute_equilibria()\n",
    "    for round in range(num_rounds):\n",
    "        cl.prt(f\"Round {round} of {num_rounds}\")\n",
    "        \n",
    "        added_low=0\n",
    "        added_high=0\n",
    "        # for equilibrium in dictionaries:\n",
    "        for equi_i in range(len(dictionaries)):\n",
    "            new_equi_low = 0\n",
    "            new_equi_high = 0\n",
    "            equi = dictionaries[equi_i]\n",
    "            # low_prob_str = \", \".join(\n",
    "            #     map(\"{0:.2f}\".format, equi[\"low_cost_probs\"]))\n",
    "            # high_prob_str = \", \".join(\n",
    "            #     map(\"{0:.2f}\".format, equi[\"high_cost_probs\"]))\n",
    "            cl.prt(\n",
    "                f'equi: {str(equi[\"low_cost_support\"])}, {str(equi[\"high_cost_support\"])}\\n payoffs= {equi[\"low_cost_payoff\"]:.2f}, {equi[\"high_cost_payoff\"]:.2f}')\n",
    "        \n",
    "            # train a low-cost agent\n",
    "            high_mixed_strat = cl.MixedStrategy(\n",
    "                strategies_lst=bimatrix_game.high_strategies, probablities_lst=((equi[\"high_cost_probs\"]+([0]*added_high)) if \n",
    "                                                                                added_high> 0 else equi[\"high_cost_probs\"]))\n",
    "        \n",
    "            \n",
    "            for alg in algs:\n",
    "                for lr in lrs:\n",
    "                    for mem_i,memory in enumerate(memories):\n",
    "                        \n",
    "                        print(f'training low-cost agents with alg={str(alg)}, lr={lr:.4f}, memory={memory}')\n",
    "        \n",
    "                        results= train_processes(db=db, env_class=env_class, costs=[gl.LOW_COST, gl.HIGH_COST], \n",
    "                                                adv_mixed_strategy=high_mixed_strat, target_payoff=equi[\"low_cost_payoff\"], \n",
    "                                                num_procs=num_procs, alg=alg, lr=lr, memory=memory)\n",
    "                        for result in results:\n",
    "                            acceptable, agent_payoffs, adv_payoffs, agent_strategy, expected_payoff,base_agent_name = result\n",
    "                            if acceptable:\n",
    "                                new_equi_low += 1\n",
    "                                added_low+=1\n",
    "                                bimatrix_game.low_strategies.append(agent_strategy)\n",
    "                                bimatrix_game.add_low_cost_row(agent_payoffs, adv_payoffs)\n",
    "                                cl.prt(\n",
    "                                    f'low-cost player {agent_strategy.name} , payoff= {expected_payoff:.2f} added, base={base_agent_name} ,alg={str(alg)}, lr={lr:.4f}, memory={memory}')\n",
    "        \n",
    "            # train a high-cost agent\n",
    "            low_mixed_strat = cl.MixedStrategy(\n",
    "                strategies_lst=bimatrix_game.low_strategies, probablities_lst=\n",
    "                ((equi[\"low_cost_probs\"]+([0]*added_low)) if added_low > 0 else equi[\"low_cost_probs\"]))\n",
    "            \n",
    "            \n",
    "            for alg in algs:\n",
    "                for lr in lrs:\n",
    "                    for memory in memories:\n",
    "                        print(f'training high-cost player with alg={str(alg)}, lr={lr:.4f}, memory={memory}')\n",
    "                        results= train_processes(db=db, env_class=env_class, costs=[ gl.HIGH_COST,gl.LOW_COST],\n",
    "                                                adv_mixed_strategy=low_mixed_strat, target_payoff=equi[\"high_cost_payoff\"],\n",
    "                                                num_procs=num_procs, alg=alg, lr=lr, memory=memory)\n",
    "                        for result in results:\n",
    "                            acceptable, agent_payoffs, adv_payoffs, agent_strategy, expected_payoff,base_agent_name = result \n",
    "                            if acceptable:\n",
    "                                new_equi_high += 1\n",
    "                                added_high+=1\n",
    "                                bimatrix_game.high_strategies.append(agent_strategy)\n",
    "                                bimatrix_game.add_high_cost_col(adv_payoffs, agent_payoffs)\n",
    "            \n",
    "                                cl.prt(\n",
    "                                    f'high-cost player {agent_strategy.name} , payoff= {expected_payoff:.2f} added, base={base_agent_name}, alg={str(alg)}, lr={lr:.4f}, memory={memory}')\n",
    "            if new_equi_high>0:\n",
    "                high_mixed_strat.strategy_probs+=[0]*new_equi_high\n",
    "        \n",
    "            # if new_equi_low>0 or new_equi_high>0:\n",
    "                # equilibria.append(\n",
    "                #     [equi[\"low_cost_probs\"], equi[\"high_cost_probs\"], equi[\"low_cost_payoff\"], equi[\"high_cost_payoff\"]])\n",
    "                #to do: add the equilibria to the db\n",
    "            db.insert_new_equi(game_size=game_size, low_strategy_txt=str(low_mixed_strat),high_strategy_txt=str(high_mixed_strat), low_payoff=equi[\"low_cost_payoff\"], high_payoff=equi[\"high_cost_payoff\"], low_new_num=new_equi_low, high_new_num=new_equi_high)\n",
    "                \n",
    "                \n",
    "        if added_low==0 and added_high==0:\n",
    "            gl.N_EPISODES_BASE *= 1.1\n",
    "            gl.N_EPISODES_LOAD *= 1.1\n",
    "        else:\n",
    "            dictionaries = bimatrix_game.compute_equilibria()\n",
    "            game_size=bimatrix_game.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef372678-7225-48bc-8c31-2fad59bcc7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env=ConPricingGame(tuple_costs=[57,71], adversary_mixed_strategy= (cl.Strategy(\n",
    "#     cl.StrategyType.static, model_or_func=cl.myopic, name=\"myopic\")).to_mixed_strategy(), memory=3)\n",
    "\n",
    "# policy = (PPO.load(\"models/\"+\"NOV24-1700860722\", env=env)).predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98de76c-95c6-4052-b30b-4b63e49f1604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
