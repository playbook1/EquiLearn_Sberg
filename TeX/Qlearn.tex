% Q-learning in Evolutionary Pricing Games
\documentclass[a4paper,12pt]{article}  %% important: a4paper first
%
\usepackage[notcite,notref]{showkeys}
\pdfoutput=1
\usepackage{natbib} 
\usepackage{amsthm}
\usepackage{newpxtext,newpxmath} 
\usepackage{microtype}
\linespread{1.10}        % Palatino needs more leading (space between lines)
\usepackage{xcolor}
\usepackage{pict2e} 
\usepackage{bimatrixgame}
\usepackage{tikz} 
\usetikzlibrary{shapes}
\usetikzlibrary{arrows.meta}
\usepackage{amssymb}
%\usepackage{smallsec}
\usepackage{graphicx}
%\usepackage[pdflatex]{hyperref}
\usepackage[hyphens]{url} 
\usepackage[colorlinks,linkcolor=purple,citecolor=blue]{hyperref}
%\usepackage{hyperref}
\urlstyle{sf}
\usepackage[format=hang,justification=justified,labelfont=bf,labelsep=quad]{caption} 
% \input macros-drawtree
\oddsidemargin=.46cm    % A4
\textwidth=15cm
\textheight=23.3cm
\topmargin=-1.3cm
\clubpenalty=10000
\widowpenalty=10000
\predisplaypenalty=1350
\sfcode`E=1000  % normal spacing if E followed by period, as in "EFCE."
\sfcode`P=1000  % normal spacing if P followed by period, as in "NP." 
\newdimen\einr
\einr1.7em
\newdimen\eeinr 
\eeinr 1.7\einr
\def\aabs#1{\par\hangafter=1\hangindent=\eeinr
    \noindent\hbox to\eeinr{\strut\hskip\einr#1\hfill}\ignorespaces}
\def\rmitem#1{\par\hangafter=1\hangindent=\einr
  \noindent\hbox to\einr{\ignorespaces#1\hfill}\ignorespaces} 
\newcommand\bullitem{\rmitem{\raise.17ex\hbox{\kern7pt\scriptsize$\bullet$}}} 
\def\subbull{\vskip-.8\parskip\aabs{\raise.2ex\hbox{\footnotesize$\circ$}}}
\let\sfield\mathcal
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{definition}[theorem]{Definition}
\def\reals{{\mathbb R}} 
\def\eps{\varepsilon}
\def\prob{\hbox{prob}}
\def\sign{\hbox{sign}}
\def\proof{\noindent{\em Proof.\enspace}}
\def\proofof#1{\noindent{\em Proof of #1.\enspace}}
\def\endproof{\hfill\strut\nobreak\hfill\tombstone\par\medbreak}
\def\tombstone{\hbox{\lower.4pt\vbox{\hrule\hbox{\vrule
  \kern7.6pt\vrule height7.6pt}\hrule}\kern.5pt}}
\def\eqalign#1{\,\vcenter{\openup.7ex\mathsurround=0pt
 \ialign{\strut\hfil$\displaystyle{##}$&$\displaystyle{{}##}$\hfil
 \crcr#1\crcr}}\,}
\def\zw#1\par{\vskip2ex{\textbf{#1}}\par\nobreak} 
\newdimen\pix  % bounding box height for eps files
\pix0.08ex
\newsavebox{\figA} 
\parindent0pt
\parskip1.3ex

\title{%
Q-Learning in the Evolutionary Pricing Game
}

\author{
}

%\date{Febuary 6, 2012}
\date{\today
\\[1ex]
}

\begin{document}
\maketitle

\begin{abstract}
We describe a possible set-up of Q-learning of an agent in
the ``duopoly with demand inertia'' pricing game.
We discussion in Section~\ref{s-appl} various issues
such as the reward function or visiting states of the Q-table.

% \noindent 
% \textbf{ACM classification:} 
% CCS
% $\to$ Theory of computation
% $\to$ Theory and algorithms for application domains
% $\to$ Algorithmic game theory and mechanism design
% $\to$ Solution concepts in game theory,
% exact and approximate computation of equilibria,
% representations of games and their complexity
% 
% 
% \strut
% 
% \noindent 
% \textbf{AMS subject classification:} 
% 91A18  (Games in extensive form)
% 
% \strut
% 
% \noindent 
% \textbf{JEL classification:} 
% C72 (Noncooperative Games)
% 
% \strut
% 
% \noindent 
% \textbf{Keywords:}
% extensive game,
% correlated equilibrium,
% polynomial-time algorithm,
% computational complexity.
% 
\end{abstract}

\section{Q-learning}

We use the Q-learning model described in \cite{WD92}, which
also proves a convergence result under certain conditions.
We have finite sets $X$ of \textit{states} and $A$ of
\textit{actions}.
At each time step $t$, the agent knows that she is in state
$x_t$ and chooses an action~$a_t$\,, with a probabilistic next
state and a current reward $r_t$, which has a mean value
$R(x,a)$.
The probability for the next state $y$ is determined by the
environment (which in our setting may change, see below) and
depends on the state $x$ and chosen action~$a$ according to 
\begin{equation}
\label{prob}
\prob[y\mid x,a]\,.
\end{equation}
A \textit{policy} $\pi$ maps states to actions.
The rewards are discounted with a weight factor $\gamma<1$
for future rewards.
That is, if $r_t$ is the reward for period $t=0,1,2,\ldots$,
then the total reward is
\begin{equation}
\label{total}
\sum_{t=0}^\infty \gamma^t \,r_t\,.
\end{equation}
The value of an initial state $x$ under policy $\pi$ is
\begin{equation}
\label{value}
V^\pi(x) = R(x,\pi(x))+\gamma\sum_{y\in X} \prob[y\mid x, \pi(x)]\,V^\pi(y) \,.
\end{equation}
An optimal \textit{stationary} (that is, time-independent)
policy $\pi^*$ fulfills
\begin{equation}
\label{opt}
V^*(x):=V^{\pi^*}(x)=\max_{a\in A}\biggl(
R(x,a)+\gamma\sum_{y\in X} \prob[y\mid x,
a]\,V^{\pi^*}(y)\biggr)\,.
\end{equation}
For a policy $\pi$, the Q-value (or action-value)
is the expected discounted reward for action $a$ at state
$x$ and following policy $\pi$ thereafter:
\begin{equation}
\label{defq}
Q^\pi(x,a) = R(x,\pi(x))+\gamma\sum_{y\in X} \prob[y\mid x, \pi(x)]\,V^\pi(y) \,.
\end{equation}
The object in Q-learning is to estimate the
Q-values for an optimal policy $\pi^*$.
Define them as $Q^*(x, a):= Q^{\pi^*}(x, a)$.
Hence, by (\ref{value}), $V^*(x) =\max_{a\in A} Q^*(x, a)$,
and if $a^*$ is an action at which the maximum is attained
in state~$x$, then an optimal policy can be formed as
$\pi^*(x) = a^*$.

In Q-learning over time steps $t$ (assuming some
initialization of the Q-values $Q_0(x,a)$), the agent
assumes the best she can do from state $y$ is 
$\max_{b\in A}Q_{t-1}(y,b)$. 
The Q-values are only updated for the current state $x_t$
and action $a_t$ and with a learning rate $\alpha_t$\,, which
decreases with~$t$, according to 
\begin{equation}
\label{wd1}
Q_t(x_t,a_t)=(1-\alpha_t)\,Q_{t-1}(x_t,a_t)~+~\alpha_t\Bigl[
r_t+\gamma\,\max_{b\in A}Q_{t-1}(y_t,b)\Bigr], 
\end{equation}
where $y_t$ is the \textit{observed} subsequent state after
taking action $a_t$ in state~$x_t$\,.
Note that this is about \textit{learning} the Q-values,
assuming the agent will choose the best possible action $b$ in
the future, based on the evidence $Q_{t-1}(y_t,b)$ of the
past for the observed next state $y_t$\,.
The action $a_t$ is not chosen according to an optimization
method.
However, this can be combined with \textit{$\eps$-greedy}
choices of $a_t$ where a random action is taken with
probability $\eps$, and the currently considered optimal one
with probability $1-\eps$, where $\eps$ is tending to zero
over time.

\citet {WD92} proved that Q-learning converges to an optimal
strategy if the rewards $r_n$ are bounded, 
every state-action pair is visited infinitely often, and the
learning rate is both slow enough to allow for learning
($\sum_{t\ge0} \alpha_t=\infty$) and fast enough to
accumulate knowledge ($\sum_{t\ge0} \alpha_t^2<\infty$).
% Once the agent has ``learned enough'', actions will be
% chosen according to the Q-values. This ``greedy'' approach
% can also be combined with the learning process.

\section{Application to the duopoly pricing game} 
\label{s-appl}

We want to apply Q-learning to learn strategies, represented 
by agents, in the oligopoly game by \citet{Selten1965} for
the special duopoly case of the strategy experiments by
\citet{Keser1992}.
In this game, the learning environment is in several
respects non-stationary, which we will discuss and address.

As studied by \citet{Keser1992}, the game is played over
fixed number $T$ of rounds, here $T=25$.
Each firm $i$ has a \textit{demand potential} $D_i$
that determines its number of $D_i-p_i$ of sold units of a
good when setting a price $p_i$ (firm $i$'s decision in each
period), with a profit of $p_i-c_i$ per unit for the firm's
production cost $c_i$.
The firms have different costs, $c_1=57$ and $c_2=71$.
The myopic monopoly profit maximizes $(D_i-p_i)(p_i-c_i)$
when $p_i=(D_i+c_i)/2$.

At the start of the $T$ periods, both firms have the same
demand potential $D_1=D-2=200$.
After each period, the cheaper firm gains demand potential
from the more expensive firm in proportion to their price
difference, according to
\begin{equation}
\label{demand}
\begin{array}{rcl}
D_1^{t+1}&=&D_1^t+\frac12({p_{2}^t-p_1^t})\,,
\\[1ex]
D_2^{t+1}&=&D_2^t+\frac12({p_{1}^t-p_2^t})\,.
\end{array}
\end{equation}
The total profits are summed up (there is also a discount
factor of 1 percent per time period that favors early
profits, which we ignore).

An agent learns a \textit{strategy} in this game, which
chooses the price for each period, in principle based on the
full history of the player's own and the opponent's prices
for all past periods.

\subsection{States and actions}

To simplify the state set, we propose the demand potential $D_i$ of
firm $i$ as its \textit{state}, discretized for example to
multiples of~10.
As its action $a$, we take much its price is \textit{lower}
compared to its myopically optimal price $(D_i+c_i)/2$, that
is, $p_i=(D_i+c_i)/2-a$, because it is never optimal to ask
a higher price, and because this will be a less variable
choice of the firm's action than the price itself.
A first discretization of~$a$ could be in multiples of 5,
that is, $a\in\{0,5,10,\ldots\}$.
Recall that a firm is not allowed to charge a price below
its cost, with the constraint $p_i\ge c_i$, which also
restricts the possibilities for~$a$ depending on its demand
potential.

\subsection{Learning environment}
\label{s-env}

We want the agent to learn how to play over $T$ periods.
This is done by playing against another, fixed agent, who
does \textit{not} learn at the same time, over $T$ periods.
Rather, that opponent is drawn from a pool of existing
strategies, to which we will add a newly successfully
trained agent.
The probability of the opponent's choice is given by a mixed
equilibrium among the existing agents.

Because the agent is meant to play well over the $T$
periods, this should also be the scenario how the state
(the demand potential) evolves.
That is, the agent cannot freely or randomly choose the next
state.
Rather, the next state is determined by the game rules, and
depends on the current state, the action $a$ chosen by the
agent, and the price of the opponent.
The price of the opponent is not known (and presumably
random depending on which opponent has been chosen).
However, given the opponent's price, the game rules imply
that choosing $a+b$ rather than $a$ means the next state
(demand potential) is $y+b/2$ rather than~$y$.

There is a natural tension between short-term reward and
future reward.
If $x$ is the current state and $c$ the agent's cost,
then the myopic profit is $(x-c)^2/4$.
If the price is reduced by $a$ from the myopic price
$(x+c)/2$, then the current profit is $(x-c)^2/4-a^2$.
However, higher values of $a$ favor better states in the
future because they increase~$x$. 

The Q-values should reflect the higher future reward.

\subsection{Reward update}

As the game is defined, what counts in an interaction is the
player's total reward over $T$ periods.
The values of the $Q$-table should reflect that total
reward.
This is not easy to do consistently unless the current time
period is added to the state, that is, a state is a pair
$(d,t)$ for a (discretized) demand potential and time
period~$t$.
However, this multiplies the number of states by $T$.

Instead, consider now the \textit{discounted} version of
this game over infinitely many periods with discount factor
$\gamma$, which can be implemented by taking $\gamma$ to be
a continuation probability from one period to the next.
For example, if $\gamma=0.96$, then the expected number of
periods is $1/(1-\gamma)=25$.
The actual length of interaction is random and has
exponentially small probability for very long runs, so it
can be simulated without difficulty.
The discounted version has the advantage of a stationary
environment because the future looks the same at each time
period.

In this version, consider the discounted overall reward in
(\ref{total}). 
If the reward $r_t$ in each period is constant, say given
by~$r$, then the total reward is $r/(1-\gamma)$. 
It makes sense to normalize this by multiplying it with 
$(1-\gamma)$ to represent then the per-period reward,
which is independent of $\gamma$ if that reward is constant.
This normalization is useful for the comparison of different
discount factors.

The Q-values in (\ref{wd1}) can be scaled by multiplication
with any constant factor.
If that factor is $(1-\gamma)$, then this equation becomes
\begin{equation}
\label{Qnorm}
Q_t(x_t,a_t)=(1-\alpha_t)\,Q_{t-1}(x_t,a_t)~+~\alpha_t\Bigl[
(1-\gamma)\,r_t+\gamma\,\max_{b\in A}Q_{t-1}(y_t,b)\Bigr]. 
\end{equation}
That is, the per-period reward $r_t$ is multiplied by
$(1-\gamma)$, which for $\gamma$ near~1 is relatively small.
This is important if the Q-values are meant to represent
average payoffs per period.

One could adapt (\ref{Qnorm}) to the case of fixed run
lengths over $T$ periods, by letting $\gamma$ change with~$t$.
For example, the reward $r_0$ in the first period (if we
number the periods $t=0,\ldots,T-1$) has weight $1/T$
compared to the rest of the game.
The reward $r_1$ has weight $1/(T-1)$, and in general 
$r_t$ has weight $1/(T-t)$, which is the full weight $1$
for the last period $t=T-1$, in agreement with how the game
is played.
The overall reward is then the average reward
$\frac1T\sum_{t=0}^{T-1}r_t$ as intended.
By writing (\ref{Qnorm}) with $\gamma=1-1/(T-t)$, this could
be the update rule for the values of $Q$ even ``by
stealth'', that is, with the agent not informed about~$t$.
The assumption is that the learning takes place over the
fixed number $T$ of periods, and is re-started (but keeping
the computed values) when playing against a new opponent.

The discounted version (\ref{Qnorm}) of this game seems
safer in this regard.
The goal is that the Q-values approximate the expected
future average reward \textit{per period} for a given state
(demand potential), in order to be comparable across
different run lengths and possibly different discount
factors.
Varying the discount factor should be interesting with
respect to the evolving behavior.
This requires the current reward $r_t$ to be weighted with
$(1-\gamma)$ in (\ref{Qnorm}).

\subsection{Visiting all states}

In order for Q-learning to converge, all state-action pairs
need to be tried sufficiently often.
However, many demand potentials are unrealistic to be
encountered in typical play, even though the agent needs
to be prepared what to do when such a state is encountered.
The fact that actions $a$ are relative to the myopic optimal
price already leads to reasonable behavior.

As mentioned in~\ref{s-env}, it makes sense to let an agent
play over a number of periods against her opponent and
thereby visit the states (demand potentials) typically
encountered in actual play.
This restricts the states that will be visited.
For example, against a myopic opponent who on their side
always plays $a=0$, the agent will not lose own demand
potential but only gain it (at least this holds for the
low-cost firm).

However, we \textit{expect agents to learn} to play more
aggressively against such opponents by lowering their prices
(with own higher values of~$a$), which lowers their
opponent's demand potential.
Once such opponents are part of the population, they will
lead to lower demand potentials of the agent herself, and
thus give opportunities to learn.

Recall that if the agent plays over $T$ periods (with $T$
fixed or random) against her opponent, the next state is
part of the trajectory of states and cannot just be chosen
freely in order to learn.\footnote{This must be a common
problem in machine learning where you learn ``as you go''.}
The agent's free choice of actions does not imply that they
can freely reach all states with their next move.

Another possibility to reach different states during the
learning phase is a \textit{random} rather than equal split
of the initial demand potentials.
In the extreme, such a restart could be made even after one
or two time steps, in order to populate the Q-table more
quickly.
However, it is not clear whether this will then reflect
realistic future rewards. 

\subsection{Initializing Q-values}

Another question is how to populate the initial Q-values.
Rather than letting them to converge from a random or zero
initial value, one could try to use more realistic values.
For example, if the future demand potential is constant at~$y$
and the agent plays $a$ and has cost $c$, then their reward
per period is $(y-c)^2/4-a^2$.
This is a reasonable table entry, which could be modified
with some random alteration of~$a$.
The assumption that a future demand potential is constant is
justified if prices of the two players converge to each
other, which has been observed in play.
One could also compare this with the Q-values that
eventually result.

At any rate, it should speed up learning significantly, but
may risk leading to too constrained behavior.

\subsection{Discretization}

The actual state is a continuous real.
It could be discretized to integers, but the number of
states would still be very large.
With discretization at multiples of 10, the state for the
Q-update could be chosen randomly according to the closest
states: E.g. if the actual state is 207, chose 210 and 200
with probabilities 0.7 and 0.3, respectively.
Alternatively one could even do the update in (\ref{Qnorm}) 
in a weighted manner.

An alternative are \textit{tilings} of the state that use
overlapping grids (essentially separate Q-tables) that are
updated simultaneously with each action and later
interpolated.

\textit{Deploying} the agent, once she has ``learned
enough'' (which in itself is to be determined)
requires defining a chosen action by the agent as a function
of the state.
According to the learned Q-table, this should be the optimal
action.
This action can be interpolated from the optimal actions for
the states, which serve as breakpoints in a linear
interpolation. 
This will simplify the description of an agent
substantially, although it may be of some interest to keep
the learned Q-table.

Later extensions would also involve more complex states, like
pairs of triples of demand potentials for the last two or
three periods rather than just one period.

\subsection{The population game and its evolution}

Once an agent has been trained (as a low-cost or high-cost
firm), it will be added to the population game as a new row
or column.
For that purpose, the new agent plays, as it will be
deployed, against every existing agent and records a pair of
payoffs, one for each player, in the bimatrix game.

If the length of the interaction is random, it may be
necessary to play against the same opponent more than once
and take averages (it would be useful to record the
resulting variance in payoffs).

In particular, this determines the expected payoffs against
the mixed equilibrium strategies used as the learning
environment in~\ref{s-env}.
One would assume that the new agent does better than the
other (say, row) players in the equilibrium.
This can also be used as a criterion for declaring learning
as successfull, because the new agent is a potential
``entrant'' in an evolutionary setting.

With the new agent added, new equilibria are determined.
It could happen that new agent merely destroys or changes the
equilibrium that it has been trained against, but does not
participate in any equilibrium.
To be tested if this occurs.

% \section{Strategic substitutes}
% 
% Pricing games are usually games of \textit{strategic
% complements}, that is, the best response to more aggressive
% behavior to be more aggressive.
% That is, a low price of the opponent induces a low price as
% a best response, and high price induces a high rice as a
% best response.
% 
% However, the present game has a delayed interaction effect,
% which seems to correspond to \textit{strategic
% substitutes} where the best response \textit{decreases}
% rather increases with the opponent's price.
% Namely, if the opponent chooses a lower price, then the
% player's own demand potential decreases in the next period.
% If that demand potential is $y$ and own cost is $c$, then
% the myopic best response is $(y+c)/2$, which is increasing
% and decreasing with $y$.
% But a lower $y$ resulted from a lower opponent price ...
% which suggests complements again.
% 

% \section{Related work}
% 
% This is not a full study yet and related work is surely
% vast.
% 
% Of interest is \citet{calvano2020} (and its summary on a
% blog at 
% \url{https://www.law.ox.ac.uk/business-law-blog/blog/2019/02/artificial-intelligence-algorithmic-pricing-and-collusion}).
% They use a Bertrand model of competition (which is also
% price-setting, although its relationship to Selten's model
% of demand inertia will need to be examined).
% Their findings show that agents using Q-learning learn how
% to become semi-collusive based on their own learning,
% without communication.
% 
% Our proposed approach in Section~\ref{s-opt}
% based on Selten's model has the
% ``design'' aspect (actively designing a good strategy) 
% and its comparison with \citet{Keser1992}
% as an additional feature, but may confirm these findings.

% Claudia Keser (1992), Experimental Duopoly Markets with
% Demand Inertia: Game-Playing Experiments and the Strategy
% Method. PhD thesis, University of Bonn, and Vol. 391 of
% Lecture Notes in Economics and Mathematical Systems,
% Springer Verlag.
% 
% Claudia Keser (1993), Some results of experimental duopoly
% markets with demand inertia. Journal of Industrial Economics
% 41(2), 133-151.
% 
% Reinhard Selten (1965), Spieltheoretische Behandlung eines
% Oligopolmodells mit Nachfragetraegheit. Zeitschrift fur die
% Gesamte Staatswissenschaft 121, pp. 301-324 and pp. 667-689.
% \strut


%\bibliographystyle{ecta}
%\bibliographystyle{acm}
\small
\bibliographystyle{book}
\bibliography{bib-evol} 

\end{document}

